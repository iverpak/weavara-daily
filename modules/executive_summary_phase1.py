"""
Executive Summary Phase 1 - Article Theme Extraction

This module generates structured JSON executive summaries from news articles ONLY.
NO filing data is used in Phase 1 - that comes in Phase 2.

Key functions:
- generate_executive_summary_phase1(): Main entry point for Phase 1 generation
- validate_phase1_json(): Schema validator
- convert_phase1_to_sections_dict(): Simple bullets for Email #3 (user-facing)
- convert_phase3_to_email2_sections(): Full QA format for Email #2 with deduplication
  (NOTE: Email #2 requires Phase 3 JSON - always runs AFTER Phase 3 completes)
"""

import json
import logging
import re
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple
import requests

from modules.executive_summary_utils import should_include_bullet

LOG = logging.getLogger(__name__)

# Escape hatch prefix for context fields (matches strip_escape_hatch_context in phase2)
ESCAPE_HATCH_PREFIX = "No relevant filing context found"


def _strip_escape_hatch(context: str) -> str:
    """
    Return empty string if context is escape hatch phrase.

    Used for Phase 4 paragraph contexts (bottom_line, upside, downside).
    Matches the pattern used in strip_escape_hatch_context() for bullet contexts.

    Args:
        context: Context string from Phase 4 output

    Returns:
        Empty string if escape hatch, otherwise original context
    """
    if not context:
        return ""
    if context.startswith(ESCAPE_HATCH_PREFIX):
        return ""
    return context


# Phase 1 System Prompt (embedded from modules/_build_executive_summary_prompt_phase1)
def get_phase1_system_prompt(ticker: str) -> str:
    """
    Get Phase 1 system prompt (static, no ticker substitution for caching).

    The prompt is now ticker-agnostic for optimal prompt caching.
    Ticker context is provided in user_content instead.

    Args:
        ticker: Stock ticker (unused, kept for API compatibility)

    Returns:
        Static system prompt string
    """
    # Read the prompt file (NO ticker substitution - enables prompt caching)
    try:
        import os
        prompt_path = os.path.join(os.path.dirname(__file__), '_build_executive_summary_prompt_phase1')
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_template = f.read()
        return prompt_template  # Return as-is (no {TICKER} replacement)
    except Exception as e:
        LOG.error(f"Failed to load Phase 1 prompt: {e}")
        raise


def _build_phase1_user_content(
    ticker: str,
    categories: Dict[str, List[Dict]],
    config: Dict
) -> str:
    """
    Build user_content for Phase 1 (articles only, NO filings).

    Ports logic from _build_executive_summary_prompt() lines 14145-14250:
    - Collect flagged articles from company/industry/competitor/upstream/downstream
    - Add category tags [COMPANY], [INDUSTRY - keyword], [COMPETITOR], [UPSTREAM], [DOWNSTREAM]
    - Sort by published_at DESC (newest first)
    - Build unified timeline (up to 50 articles)

    Args:
        ticker: Stock ticker
        categories: Dict with keys: company, industry, competitor, upstream, downstream
        config: Ticker config dict

    Returns:
        Formatted article timeline string
    """
    company_name = config.get("name", ticker)

    # Collect ALL flagged articles across all categories
    all_flagged_articles = []

    # Company articles
    for article in categories.get("company", []):
        if article.get("ai_summary"):
            article['_category'] = 'COMPANY'
            article['_category_tag'] = '[COMPANY]'
            all_flagged_articles.append(article)

    # Industry articles
    for article in categories.get("industry", []):
        if article.get("ai_summary"):
            keyword = article.get("search_keyword", "Industry")
            article['_category'] = 'INDUSTRY'
            article['_category_tag'] = f'[INDUSTRY - {keyword}]'
            all_flagged_articles.append(article)

    # Competitor articles
    for article in categories.get("competitor", []):
        if article.get("ai_summary"):
            article['_category'] = 'COMPETITOR'
            article['_category_tag'] = '[COMPETITOR]'
            all_flagged_articles.append(article)

    # Upstream articles (value chain)
    for article in categories.get("upstream", []):
        if article.get("ai_summary"):
            article['_category'] = 'VALUE_CHAIN'
            article['_category_tag'] = '[UPSTREAM]'
            all_flagged_articles.append(article)

    # Downstream articles (value chain)
    for article in categories.get("downstream", []):
        if article.get("ai_summary"):
            article['_category'] = 'VALUE_CHAIN'
            article['_category_tag'] = '[DOWNSTREAM]'
            all_flagged_articles.append(article)

    # Build unified timeline with category tags (if articles exist)
    unified_timeline = []
    if all_flagged_articles:
        # Sort all articles globally by published_at DESC (newest first)
        all_flagged_articles.sort(
            key=lambda x: x.get("published_at") or datetime.min.replace(tzinfo=timezone.utc),
            reverse=True
        )

        # Build timeline with sequential indices - only count articles with ai_summary
        # No limit - process all flagged articles from triage (max ~79 based on triage caps)
        timeline_idx = 0
        for article in all_flagged_articles:
            title = article.get("title", "")
            ai_summary = article.get("ai_summary", "")
            domain = article.get("domain", "")
            published_at = article.get("published_at")

            # Skip articles without ai_summary (safety check)
            if not ai_summary:
                continue

            # Format date with year for clarity in staleness checks
            if published_at:
                # Use format like "Oct 22, 2025" (includes year for temporal comparisons)
                date_str = published_at.strftime("%b %d, %Y")
            else:
                date_str = "Unknown date"

            category_tag = article.get("_category_tag", "[UNKNOWN]")

            # Get domain formal name (simplified - just use domain if lookup not available)
            source_name = domain if domain else "Unknown Source"
            # Use 0-indexed article numbers [0], [1], [2] for source tracking
            unified_timeline.append(f"[{timeline_idx}] {category_tag} {title} [{source_name}] {date_str}: {ai_summary}")
            timeline_idx += 1

    # Calculate report context (current date, day of week)
    current_date = datetime.now().strftime("%B %d, %Y")
    day_of_week = datetime.now().strftime("%A")

    # Build user_content
    # CRITICAL: Add ticker context here (not in system prompt) for prompt caching optimization
    ticker_header = f"TARGET COMPANY: {ticker} ({company_name})\n\n"

    # Add explicit current date for temporal staleness checks (matches Phase 2)
    current_date_header = f"CURRENT DATE: {current_date}\n\n"

    if not all_flagged_articles:
        user_content = (
            ticker_header +
            current_date_header +
            f"REPORT CONTEXT:\n"
            f"Report type: {day_of_week}\n\n"
            f"---\n\n"
            f"FLAGGED ARTICLE COUNT: 0\n\n"
            f"NO FLAGGED ARTICLES - Generate quiet day summary per template."
        )
    else:
        article_count = len(all_flagged_articles)
        user_content = (
            ticker_header +
            current_date_header +
            f"REPORT CONTEXT:\n"
            f"Report type: {day_of_week}\n\n"
            f"---\n\n"
            f"FLAGGED ARTICLE COUNT: {article_count}\n\n"
            f"UNIFIED ARTICLE TIMELINE (newest to oldest):\n"
            + "\n".join(unified_timeline)
        )

    return user_content


def _generate_phase1_gemini(
    ticker: str,
    categories: Dict[str, List[Dict]],
    config: Dict,
    gemini_api_key: str
) -> Optional[Dict]:
    """
    Generate Phase 1 executive summary using Gemini 3.0 Flash Preview (primary).

    Migration Notes (Dec 2025):
    - Upgraded from Gemini 2.5 Pro to Gemini 3.0 Flash Preview
    - New SDK: google-genai (not google-generativeai)
    - Temperature: 1.0 (required for reasoning) with seed=42 for determinism
    - Thinking Level: HIGH for best accuracy

    Args:
        ticker: Stock ticker
        categories: Dict with keys: company, industry, competitor
        config: Ticker configuration dict
        gemini_api_key: Google Gemini API key

    Returns:
        dict with:
            json_output: Full Phase 1 JSON structure
            model_used: "gemini-3-flash-preview"
            prompt_tokens: int
            completion_tokens: int
            thought_tokens: int (reasoning tokens, billed as output)
            cached_tokens: int (tokens served from cache)
            generation_time_ms: int
        Or None if failed
    """
    from google.genai import types
    from modules.gemini_3_utils import (
        create_gemini_3_client,
        call_with_retry,
        extract_usage_metadata,
        extract_response_text,
        build_thinking_config,
        calculate_flash_3_cost
    )

    try:
        # Build system prompt (static, cacheable)
        system_prompt = get_phase1_system_prompt(ticker)

        # Build user content from articles
        user_content = _build_phase1_user_content(ticker, categories, config)

        # Estimate token counts for logging
        system_tokens_est = len(system_prompt) // 4
        user_tokens_est = len(user_content) // 4
        total_tokens_est = system_tokens_est + user_tokens_est
        LOG.info(f"[{ticker}] Phase 1 Gemini prompt size: system={len(system_prompt)} chars (~{system_tokens_est} tokens), user={len(user_content)} chars (~{user_tokens_est} tokens), total=~{total_tokens_est} tokens")

        # Create client with 120s timeout for HIGH thinking
        client = create_gemini_3_client(gemini_api_key, timeout=120.0)

        # Build contents with system prompt first (enables implicit caching)
        contents = [
            types.Part.from_text(text=system_prompt),
            types.Part.from_text(text=user_content)
        ]

        # Configure for HIGH thinking with deterministic output
        config_obj = build_thinking_config(
            thinking_level="HIGH",
            include_thoughts=False,
            temperature=1.0,
            max_output_tokens=20000,
            seed=42,
            response_mime_type="application/json"
        )

        LOG.info(f"[{ticker}] Phase 1: Calling Gemini 3.0 Flash Preview (thinking=HIGH)")

        start_time = time.time()

        # Call with smart retry (handles 429 vs 503 vs timeout differently)
        response = call_with_retry(
            client=client,
            model="gemini-3-flash-preview",
            contents=contents,
            config=config_obj,
            max_retries=2,
            ticker=ticker
        )

        generation_time_ms = int((time.time() - start_time) * 1000)

        if response is None:
            LOG.error(f"[{ticker}] âŒ Phase 1: No response from Gemini after retries")
            return None

        # Extract text (filters out thought parts)
        response_text = extract_response_text(response)

        if not response_text or len(response_text.strip()) < 10:
            LOG.error(f"[{ticker}] âŒ Phase 1: Gemini returned empty response")
            return None

        # Parse JSON from response using unified parser
        from modules.json_utils import extract_json_from_claude_response
        json_output = extract_json_from_claude_response(response_text, ticker)

        if not json_output:
            LOG.error(f"[{ticker}] âŒ Phase 1: Failed to extract JSON from Gemini response")
            return None

        # Extract token usage including thinking and cache tokens
        usage = extract_usage_metadata(response)

        # Calculate cost
        cost = calculate_flash_3_cost(usage)

        LOG.info(
            f"âœ… [{ticker}] Phase 1 Gemini 3.0 success: "
            f"{usage['prompt_tokens']} prompt ({usage['cached_tokens']} cached), "
            f"{usage['thought_tokens']} thought, {usage['output_tokens']} output, "
            f"{generation_time_ms}ms, ${cost:.4f}"
        )

        return {
            "json_output": json_output,
            "model_used": "gemini-3-flash-preview",
            "prompt_tokens": usage['prompt_tokens'],
            "completion_tokens": usage['output_tokens'],
            "thought_tokens": usage['thought_tokens'],
            "cached_tokens": usage['cached_tokens'],
            "generation_time_ms": generation_time_ms
        }

    except Exception as e:
        LOG.error(f"âŒ [{ticker}] Exception in Phase 1 Gemini generation: {e}", exc_info=True)
        return None


def _generate_phase1_claude(
    ticker: str,
    categories: Dict[str, List[Dict]],
    config: Dict,
    anthropic_api_key: str
) -> Optional[Dict]:
    """
    Generate Phase 1 executive summary using Claude Sonnet 4.5 (fallback).

    Args:
        ticker: Stock ticker
        categories: Dict with keys: company, industry, competitor
        config: Ticker configuration dict
        anthropic_api_key: Anthropic API key

    Returns:
        dict with:
            json_output: Full Phase 1 JSON structure
            model_used: "claude-sonnet-4-5-20250929"
            prompt_tokens: int
            completion_tokens: int
            generation_time_ms: int
        Or None if failed
    """
    try:
        # Build system prompt
        system_prompt = get_phase1_system_prompt(ticker)

        # Build user content from articles
        user_content = _build_phase1_user_content(ticker, categories, config)

        # Log prompt sizes
        system_tokens_est = len(system_prompt) // 4
        user_tokens_est = len(user_content) // 4
        total_tokens_est = system_tokens_est + user_tokens_est
        LOG.info(f"[{ticker}] Phase 1 prompt size: system={len(system_prompt)} chars (~{system_tokens_est} tokens), user={len(user_content)} chars (~{user_tokens_est} tokens), total=~{total_tokens_est} tokens")

        # Call Claude API
        headers = {
            "x-api-key": anthropic_api_key,
            "anthropic-version": "2023-06-01",  # Prompt caching support
            "content-type": "application/json"
        }

        data = {
            "model": "claude-sonnet-4-5-20250929",  # Sonnet 4.5
            "max_tokens": 20000,  # Generous limit for comprehensive output
            "temperature": 0.0,   # Deterministic
            "system": [
                {
                    "type": "text",
                    "text": system_prompt,
                    "cache_control": {"type": "ephemeral"}  # Enable prompt caching
                }
            ],
            "messages": [
                {
                    "role": "user",
                    "content": user_content
                }
            ]
        }

        LOG.info(f"[{ticker}] Calling Claude API for Phase 1 executive summary")

        # Retry logic for transient errors (503, 429, 500)
        max_retries = 2
        response = None
        generation_time_ms = 0

        for attempt in range(max_retries + 1):
            try:
                api_start_time = time.time()
                response = requests.post(
                    "https://api.anthropic.com/v1/messages",
                    headers=headers,
                    json=data,
                    timeout=180  # 3 minutes
                )
                generation_time_ms = int((time.time() - api_start_time) * 1000)

                # Success - break retry loop
                if response.status_code == 200:
                    break

                # Transient errors - retry with exponential backoff
                if response.status_code in [429, 500, 503] and attempt < max_retries:
                    wait_time = 2 ** attempt  # 1s, 2s, 4s
                    error_preview = response.text[:200] if response.text else "No details"
                    LOG.warning(f"[{ticker}] âš ï¸ API error {response.status_code} (attempt {attempt + 1}/{max_retries + 1}): {error_preview}")
                    LOG.warning(f"[{ticker}] ğŸ”„ Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue

                # Non-retryable error or max retries reached - break
                break

            except requests.exceptions.Timeout as e:
                if attempt < max_retries:
                    wait_time = 2 ** attempt
                    LOG.warning(f"[{ticker}] â±ï¸ Request timeout (attempt {attempt + 1}/{max_retries + 1}), retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    LOG.error(f"[{ticker}] âŒ Request timeout after {max_retries + 1} attempts")
                    return None

            except requests.exceptions.RequestException as e:
                if attempt < max_retries:
                    wait_time = 2 ** attempt
                    LOG.warning(f"[{ticker}] ğŸ”Œ Network error (attempt {attempt + 1}/{max_retries + 1}): {e}, retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    LOG.error(f"[{ticker}] âŒ Network error after {max_retries + 1} attempts: {e}")
                    return None

        # Check if we got a response
        if response is None:
            LOG.error(f"[{ticker}] âŒ No response received after {max_retries + 1} attempts")
            return None

        if response.status_code == 200:
            result = response.json()

            # Extract JSON from response using unified parser (4-tier fallback strategy)
            content = result.get("content", [{}])[0].get("text", "")

            # Use shared JSON extraction utility (handles all response formats)
            from modules.json_utils import extract_json_from_claude_response
            json_output = extract_json_from_claude_response(content, ticker)

            if not json_output:
                LOG.error(f"[{ticker}] Failed to extract Phase 1 JSON from response")
                return None

            # Track usage
            usage = result.get("usage", {})
            prompt_tokens = usage.get("input_tokens", 0)
            completion_tokens = usage.get("output_tokens", 0)

            # Log cache performance
            cache_creation = usage.get("cache_creation_input_tokens", 0)
            cache_read = usage.get("cache_read_input_tokens", 0)
            if cache_creation > 0:
                LOG.info(f"[{ticker}] ğŸ’¾ CACHE CREATED: {cache_creation} tokens (Phase 1)")
            elif cache_read > 0:
                LOG.info(f"[{ticker}] âš¡ CACHE HIT: {cache_read} tokens (Phase 1) - 90% savings!")

            LOG.info(f"âœ… [{ticker}] Phase 1 generated JSON ({len(content)} chars, {prompt_tokens} prompt tokens, {completion_tokens} completion tokens, {generation_time_ms}ms)")

            return {
                "json_output": json_output,
                "model_used": "claude-sonnet-4-5-20250929",
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "generation_time_ms": generation_time_ms
            }
        else:
            error_text = response.text[:500] if response.text else "No error details"
            LOG.error(f"âŒ [{ticker}] Claude API error {response.status_code}: {error_text}")
            return None

    except Exception as e:
        LOG.error(f"âŒ [{ticker}] Exception calling Claude for Phase 1: {e}", exc_info=True)
        return None


def generate_executive_summary_phase1(
    ticker: str,
    categories: Dict[str, List[Dict]],
    config: Dict,
    anthropic_api_key: str,
    gemini_api_key: str = None
) -> Optional[Dict]:
    """
    Generate Phase 1 executive summary with Gemini 3.0 Flash Preview (primary) and Claude fallback.

    This is the main entry point for Phase 1 generation. It attempts Gemini first
    for cost savings, then falls back to Claude if Gemini fails.

    Args:
        ticker: Stock ticker (e.g., "AAPL", "RY.TO")
        categories: Dict with keys: company, industry, competitor
                   Each contains list of article dicts
        config: Ticker config dict (contains company_name, etc.)
        anthropic_api_key: Anthropic API key for Claude fallback
        gemini_api_key: Google Gemini API key (optional)

    Returns:
        {
            "json_output": {...},  # Full Phase 1 JSON structure
            "model_used": "gemini-3-flash-preview" or "claude-sonnet-4-5-20250929",
            "prompt_tokens": 28500,
            "completion_tokens": 3500,
            "thought_tokens": 5000,  # Gemini 3.0 only
            "cached_tokens": 2000,   # Gemini 3.0 only
            "generation_time_ms": 45000
        }
        Or None if both providers failed
    """
    # Try Gemini 3.0 Flash Preview first (primary)
    if gemini_api_key:
        LOG.info(f"[{ticker}] Phase 1: Attempting Gemini 3.0 Flash Preview (primary)")
        gemini_result = _generate_phase1_gemini(
            ticker=ticker,
            categories=categories,
            config=config,
            gemini_api_key=gemini_api_key
        )

        if gemini_result and gemini_result.get("json_output"):
            LOG.info(f"[{ticker}] âœ… Phase 1: Gemini 3.0 Flash Preview succeeded")
            return gemini_result
        else:
            LOG.warning(f"[{ticker}] âš ï¸ Phase 1: Gemini 3.0 Flash Preview failed, falling back to Claude Sonnet")
    else:
        LOG.warning(f"[{ticker}] âš ï¸ No Gemini API key provided, using Claude Sonnet only")

    # Fall back to Claude Sonnet 4.5
    if anthropic_api_key:
        LOG.info(f"[{ticker}] Phase 1: Using Claude Sonnet 4.5 (fallback)")
        claude_result = _generate_phase1_claude(
            ticker=ticker,
            categories=categories,
            config=config,
            anthropic_api_key=anthropic_api_key
        )

        if claude_result and claude_result.get("json_output"):
            LOG.info(f"[{ticker}] âœ… Phase 1: Claude Sonnet succeeded (fallback)")
            return claude_result
        else:
            LOG.error(f"[{ticker}] âŒ Phase 1: Claude Sonnet also failed")
    else:
        LOG.error(f"[{ticker}] âŒ No Anthropic API key provided for fallback")

    # Both failed
    LOG.error(f"[{ticker}] âŒ Phase 1: Both Gemini and Claude failed - cannot generate executive summary")
    return None


def _validate_source_articles(source_articles, context: str) -> Tuple[bool, str]:
    """
    Validate source_articles field is array of non-negative integers.

    Args:
        source_articles: The source_articles value to validate
        context: Description for error messages (e.g., "major_developments[0]")

    Returns:
        (is_valid, error_message)
    """
    if not isinstance(source_articles, list):
        return False, f"{context} source_articles must be array"

    for idx, val in enumerate(source_articles):
        if not isinstance(val, int) or val < 0:
            return False, f"{context} source_articles[{idx}] must be non-negative integer"

    return True, ""


def validate_phase1_json(json_output: Dict) -> Tuple[bool, str]:
    """
    Validate Phase 1 JSON matches expected schema.

    Checks:
    - "sections" key exists
    - All 7 required bullet sections present (no paragraphs - Phase 4 generates those)
    - Bullet sections have correct structure (bullet_id, topic_label, content required; filing_hints auto-fixed if missing/malformed)
    - source_articles arrays contain valid non-negative integers

    Auto-fixes:
    - filing_hints: If missing or malformed, auto-fixed to {"10-K": [], "10-Q": [], "Transcript": []}
      Phase 2 handles empty hints gracefully (uses escape hatch for context)

    Returns:
        (is_valid, error_message)
    """
    try:
        if "sections" not in json_output:
            return False, "Missing 'sections' key"

        sections = json_output["sections"]

        # Check all 7 required sections present (bullets only, no paragraphs)
        required_sections = [
            "major_developments", "financial_performance",
            "risk_factors", "wall_street_sentiment", "competitive_industry_dynamics",
            "upcoming_catalysts", "key_variables"
        ]

        for section_name in required_sections:
            if section_name not in sections:
                return False, f"Missing required section: {section_name}"

        # Validate bullet sections
        bullet_sections = [
            "major_developments", "financial_performance", "risk_factors",
            "wall_street_sentiment", "competitive_industry_dynamics", "upcoming_catalysts"
        ]

        for section_name in bullet_sections:
            section_content = sections[section_name]
            if not isinstance(section_content, list):
                return False, f"{section_name} must be array"

            for i, bullet in enumerate(section_content):
                if not isinstance(bullet, dict):
                    return False, f"{section_name}[{i}] must be object"

                required_fields = ["bullet_id", "topic_label", "content"]
                for field in required_fields:
                    if field not in bullet:
                        return False, f"{section_name}[{i}] missing '{field}'"

                # Auto-fix filing_hints if missing or malformed (optional field - Phase 2 handles empty hints)
                default_hints = {"10-K": [], "10-Q": [], "Transcript": []}

                if "filing_hints" not in bullet or not isinstance(bullet.get("filing_hints"), dict):
                    # Missing or not a dict - replace with default
                    if "filing_hints" in bullet:
                        LOG.warning(f"Auto-fixed malformed filing_hints in {section_name}[{i}] (was {type(bullet.get('filing_hints')).__name__}, now empty dict)")
                    else:
                        LOG.warning(f"Auto-fixed missing filing_hints in {section_name}[{i}] (added empty structure)")
                    bullet["filing_hints"] = default_hints
                else:
                    # Exists and is a dict - ensure all 3 keys exist with valid arrays
                    filing_hints = bullet["filing_hints"]
                    for filing_type in ["10-K", "10-Q", "Transcript"]:
                        if filing_type not in filing_hints:
                            LOG.warning(f"Auto-fixed filing_hints in {section_name}[{i}]: added missing '{filing_type}' key")
                            filing_hints[filing_type] = []
                        elif not isinstance(filing_hints[filing_type], list):
                            LOG.warning(f"Auto-fixed filing_hints in {section_name}[{i}]: '{filing_type}' was {type(filing_hints[filing_type]).__name__}, now empty array")
                            filing_hints[filing_type] = []

                # Empty arrays are valid (means no filing context needed - Phase 2 uses escape hatch)

                # Validate source_articles if present (optional for backward compatibility)
                if "source_articles" in bullet:
                    is_valid, err = _validate_source_articles(bullet["source_articles"], f"{section_name}[{i}]")
                    if not is_valid:
                        return False, err

        # Validate key_variables (no filing_hints required)
        key_variables = sections["key_variables"]
        if not isinstance(key_variables, list):
            return False, "key_variables must be array"

        for i, var in enumerate(key_variables):
            if not isinstance(var, dict):
                return False, f"key_variables[{i}] must be object"
            required_fields = ["bullet_id", "topic_label", "content"]
            for field in required_fields:
                if field not in var:
                    return False, f"key_variables[{i}] missing '{field}'"
            # Validate source_articles if present (optional for backward compatibility)
            if "source_articles" in var:
                is_valid, err = _validate_source_articles(var["source_articles"], f"key_variables[{i}]")
                if not is_valid:
                    return False, err

        # Note: Paragraph sections (bottom_line, upside_scenario, downside_scenario) are generated by Phase 4
        # from surviving bullets, not by Phase 1, so no validation needed here.

        return True, ""

    except Exception as e:
        return False, f"Validation exception: {str(e)}"


def get_used_article_indices(phase_json: Dict, report_type: str = 'weekly') -> set:
    """
    Collect article indices from bullets/paragraphs that pass Email #3 filtering.

    This function applies the same filtering logic as convert_phase1_to_sections_dict()
    to determine which bullets survive filtering, then collects all source_articles
    from those surviving bullets.

    Use this to filter the Source Articles section in Email #3 to only show
    articles that actually contributed to the final report.

    Args:
        phase_json: Phase 1+2 (or Phase 2+3) merged JSON with source_articles fields
        report_type: 'daily' or 'weekly' - determines which sections to include.
                     Daily reports hide upcoming_catalysts and key_variables.
                     Default 'weekly' includes all sections (backward compatible).

    Returns:
        Set of 0-indexed article numbers that contributed to the final report.
        Empty set if no source_articles tracking is present.
    """
    used_indices = set()
    sections = phase_json.get("sections", {})

    # Bullet sections that get filtered
    bullet_sections = [
        "major_developments", "financial_performance", "risk_factors",
        "wall_street_sentiment", "competitive_industry_dynamics",
        "upcoming_catalysts", "key_variables"
    ]

    # Daily reports hide certain sections - don't collect article indices from them
    # This ensures Source Articles only shows articles used in VISIBLE sections
    if report_type == 'daily':
        bullet_sections = [s for s in bullet_sections if s not in ('upcoming_catalysts', 'key_variables')]

    for section_name in bullet_sections:
        section_content = sections.get(section_name, [])
        if not isinstance(section_content, list):
            continue

        for bullet in section_content:
            if not isinstance(bullet, dict):
                continue

            # Apply same filter as Email #3 (uses unified filter from utils)
            if should_include_bullet(bullet):
                # Collect source articles from this bullet
                source_articles = bullet.get('source_articles', [])
                if isinstance(source_articles, list):
                    used_indices.update(source_articles)

    # Note: Phase 4 paragraphs (bottom_line, upside_scenario, downside_scenario) are
    # synthesized FROM surviving bullets, so their source_articles are a subset of
    # the bullet source_articles we already collected above. No need to collect separately.

    return used_indices


def convert_phase1_to_sections_dict(phase1_json: Dict) -> Dict[str, List[Dict]]:
    """
    Convert Phase 1+2+3 JSON to Email #3 user-facing format with bullet_id matching.

    FORMAT (Dec 2025):
    **[Entity] Topic â€¢ Sentiment**
    Content paragraph (Dec 04) â€” <em>Context paragraph in italics</em>

    Context is stored separately as 'context_suffix' so add_dates_to_email_sections()
    can insert the date between content and context.

    NOTE: Phase 3 is now dedup-only - it no longer generates content_integrated/context_integrated.
    Uses Phase 1 content and Phase 2 context directly.

    For paragraphs (bottom_line, upside, downside), uses Phase 4 output if available.

    Args:
        phase1_json: Phase 3 merged JSON (Phase 1+2 metadata + Phase 3 deduplication tags)

    Returns:
        sections dict: {section_name: [{'bullet_id': '...', 'formatted': '...', 'context_suffix': '...'}, ...]}
    """
    from modules.executive_summary_utils import format_bullet_header, add_dates_to_email_sections

    sections = {
        "bottom_line": [],
        "major_developments": [],
        "financial_operational": [],
        "risk_factors": [],
        "wall_street": [],
        "competitive_industry": [],
        "upcoming_catalysts": [],
        "upside_scenario": [],
        "downside_scenario": [],
        "key_variables": []
    }

    json_sections = phase1_json.get("sections", {})
    phase4 = phase1_json.get('phase4', {})

    # Bottom Line - Use Phase 4 output (Phase 1 no longer generates this)
    phase4_bl = phase4.get('phase4_bottom_line', {})
    if phase4_bl and phase4_bl.get("content"):
        sections["bottom_line"] = [{
            'formatted': phase4_bl["content"],
            'context_suffix': _strip_escape_hatch(phase4_bl.get("context", ""))
        }]

    # Helper function to format bullets (simple, no metadata)
    def format_bullet_simple(bullet: Dict, section_name: str) -> Dict:
        """Format bullet with header, content, and context_suffix.

        Returns:
            {'bullet_id': '...', 'formatted': '...', 'context_suffix': '...'}

        Context is stored separately so add_dates_to_email_sections() can insert
        the date between content and context: <content> (date) â€” <em><context></em>

        NOTE: Phase 3 no longer edits content. Uses Phase 1 content + Phase 2 context directly.
        """
        # Use shared utility for header (hide reason in Email #3 user-facing emails)
        # Pass section_name to control sentiment display (hidden for some sections)
        header = format_bullet_header(bullet, show_reason=False, section_name=section_name)

        # Use Phase 1 content and Phase 2 context directly
        # (Phase 3 no longer generates content_integrated/context_integrated)
        content = bullet.get('content', '')
        context_suffix = bullet.get('context', '')

        return {
            'bullet_id': bullet['bullet_id'],
            'formatted': f"{header}\n{content}",
            'context_suffix': context_suffix
        }

    # All bullet sections
    section_mapping = {
        "major_developments": "major_developments",
        "financial_performance": "financial_operational",
        "risk_factors": "risk_factors",
        "wall_street_sentiment": "wall_street",
        "competitive_industry_dynamics": "competitive_industry",
        "upcoming_catalysts": "upcoming_catalysts",
        "key_variables": "key_variables"
    }

    for json_key, sections_key in section_mapping.items():
        if json_key in json_sections:
            # Apply filter to ALL bullet sections (uses unified filter from utils)
            filtered_bullets = [
                b for b in json_sections[json_key]
                if should_include_bullet(b)
            ]

            sections[sections_key] = [
                format_bullet_simple(b, json_key)
                for b in filtered_bullets
            ]

    # Scenarios - Use Phase 4 output (Phase 1 no longer generates these)
    for sections_key, phase4_key in [
        ("upside_scenario", "phase4_upside_scenario"),
        ("downside_scenario", "phase4_downside_scenario")
    ]:
        phase4_scenario = phase4.get(phase4_key, {})
        if phase4_scenario and phase4_scenario.get("content"):
            sections[sections_key] = [{
                'formatted': phase4_scenario["content"],
                'context_suffix': _strip_escape_hatch(phase4_scenario.get("context", ""))
            }]

    # Add dates to all sections using bullet_id matching
    sections = add_dates_to_email_sections(sections, phase1_json)

    return sections


def convert_phase3_to_email2_sections(phase3_json: Dict) -> Dict[str, List[Dict]]:
    """
    Convert Phase 3 merged JSON to Email #2 QA format with unified filter display.

    FORMAT (Dec 2025 - Simplified):
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [bullet_id] Topic Label â€¢ Sentiment (reason)

    <content>
    Context: <context>

    Metadata: Impact: high | Sentiment: bullish | Relevance: direct | Reason: xxx
    Filing hints: 10-K (Section A, B); 10-Q (Section C)
    Source Articles: [0, 3, 5]

    Filter Status: âœ… INCLUDED
    -- OR --
    Filter Status: âŒ FILTERED (duplicate â†’ absorbed by other_bullet_id)

    ID: bullet_id
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Filter reasons (unified):
    - stale: Old news already priced in (from Phase 1.5)
    - relevance=none: Not relevant to target company (from Phase 2)
    - indirect + low impact: Weak indirect relevance (from Phase 2)
    - direct + low impact: Low impact direct relevance (from Phase 2)
    - duplicate: Absorbed by another bullet (from Phase 3)

    Paragraphs (bottom_line, upside, downside) show Phase 4 output as main content.

    Args:
        phase3_json: Phase 3 merged JSON (Phase 1+2 metadata + Phase 3 deduplication)

    Returns:
        sections dict: {section_name: [{'bullet_id': '...', 'formatted': '...'}, ...]}
    """
    from modules.executive_summary_utils import format_bullet_header, add_dates_to_email_sections

    sections = {
        "bottom_line": [],
        "major_developments": [],
        "financial_operational": [],
        "risk_factors": [],
        "wall_street": [],
        "competitive_industry": [],
        "upcoming_catalysts": [],
        "upside_scenario": [],
        "downside_scenario": [],
        "key_variables": []
    }

    json_sections = phase3_json.get("sections", {})
    phase4 = phase3_json.get('phase4', {})

    # Bottom Line - Use Phase 4 output (Phase 1 no longer generates this)
    phase4_bl = phase4.get('phase4_bottom_line', {})
    if phase4_bl and phase4_bl.get("content"):
        content = phase4_bl.get("content", "")
        context = _strip_escape_hatch(phase4_bl.get("context", ""))
        source_articles = phase4_bl.get("source_articles", [])
        result = content
        if context:
            result += f"<br><strong>Context:</strong> {context}"
        result += f"<br><br>Source Articles: {source_articles}"
        sections["bottom_line"] = [result]

    # Helper function to get unified filter status
    def get_unified_filter_status(bullet: Dict) -> Tuple[str, str]:
        """
        Get unified filter status combining all filter reasons.

        Returns:
            Tuple of (status: "included"|"filtered", reason: str or None)
            - ("included", None) - bullet passes all filters
            - ("included", "primary â†’ absorbs [...]") - primary bullet info
            - ("filtered", "stale") - filtered due to staleness
            - ("filtered", "relevance=none") - no relevance
            - ("filtered", "indirect + low impact") - weak indirect
            - ("filtered", "direct + low impact") - low impact direct
            - ("filtered", "duplicate â†’ absorbed by xxx") - absorbed by another
        """
        # Check deduplication first (Phase 3)
        dedup = bullet.get('deduplication', {})
        dedup_status = dedup.get('status', 'unique')

        if dedup_status == 'duplicate':
            absorbed_by = dedup.get('absorbed_by', 'unknown')
            shared_theme = dedup.get('shared_theme', '')
            reason = f"duplicate â†’ absorbed by {absorbed_by}"
            if shared_theme:
                reason += f" (theme: {shared_theme})"
            return ("filtered", reason)

        # Check existing filter status (staleness from Phase 1.5, relevance from Phase 2)
        existing_status = bullet.get('filter_status', '').lower()
        existing_reason = bullet.get('filter_reason', '')

        if existing_status == 'filtered_out' and existing_reason:
            return ("filtered", existing_reason)

        # Not filtered - check if it's a primary bullet
        if dedup_status == 'primary':
            absorbs = dedup.get('absorbs', [])
            shared_theme = dedup.get('shared_theme', '')
            info = f"primary â†’ absorbs {absorbs}"
            if shared_theme:
                info += f" (theme: {shared_theme})"
            return ("included", info)

        return ("included", None)

    # Helper function to format bullets with unified filter display
    def format_bullet_unified(bullet: Dict) -> Dict:
        """Format bullet with unified filter status display.

        Simplified QA format (Dec 2025):
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        [bullet_id] Topic Label â€¢ Sentiment (reason)

        <content>
        Context: <context>

        Metadata: Impact: high | Sentiment: bullish | Relevance: direct | Reason: xxx
        Filing hints: 10-K (Section A); 10-Q (Section B)
        Source Articles: [0, 3, 5]

        Filter Status: âœ… INCLUDED
        -- OR --
        Filter Status: âŒ FILTERED (duplicate â†’ absorbed by other_bullet_id)

        ID: bullet_id
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """
        import json as json_module

        bullet_id = bullet.get('bullet_id', 'N/A')
        filter_status, filter_info = get_unified_filter_status(bullet)
        is_filtered = (filter_status == 'filtered')

        # Header line with [bullet_id] prefix (includes topic_label, sentiment, reason)
        header = format_bullet_header(bullet)
        # Insert bullet_id prefix after the opening **
        if header.startswith('**'):
            header = f"**[{bullet_id}] {header[2:]}"
        else:
            header = f"[{bullet_id}] {header}"

        # For filtered bullets, add strikethrough and grey styling to header
        if is_filtered:
            header = f"<span style='text-decoration: line-through; color: #888;'>{header}</span>"

        # Start with header
        result = f"{header}\n"

        # Content (Phase 1) and Context (Phase 2) - single display, no phase labels
        content = bullet.get('content', '')
        context = bullet.get('context', '')

        result += f"<br>{content}"
        if context:
            result += f"<br><strong>Context:</strong> {context}"

        # Metadata line - order: Entity | Impact | Sentiment | Relevance | Reason
        metadata_parts = []
        entity_val = bullet.get('entity')
        if entity_val and entity_val != 'N/A':
            metadata_parts.append(f"Entity: {entity_val}")
        impact_val = bullet.get('impact', 'N/A')
        metadata_parts.append(f"Impact: {impact_val}")
        sentiment_val = bullet.get('sentiment', 'N/A')
        metadata_parts.append(f"Sentiment: {sentiment_val}")
        relevance_val = bullet.get('relevance', 'N/A')
        metadata_parts.append(f"Relevance: {relevance_val}")
        reason_val = bullet.get('reason', 'N/A')
        metadata_parts.append(f"Reason: {reason_val}")

        if metadata_parts:
            result += f"<br><br>Metadata: {' | '.join(metadata_parts)}"

        # Filing hints (compact)
        hints = bullet.get("filing_hints", {})
        hint_parts = []
        for filing_type, sections_list in hints.items():
            if sections_list:
                hint_parts.append(f"{filing_type} ({', '.join(sections_list)})")
        if hint_parts:
            result += f"<br>Filing hints: {'; '.join(hint_parts)}"

        # Filing keywords
        keywords = bullet.get("filing_keywords", [])
        if keywords:
            result += f"<br>Filing keywords: {json_module.dumps(keywords)}"

        # Source articles
        source_articles = bullet.get('source_articles', [])
        if source_articles:
            result += f"<br>Source Articles: {source_articles}"

        # Unified Filter Status - single block with all filter info
        result += "<br><br>"
        if is_filtered:
            result += f"Filter Status: <span style='color: #dc3545; font-weight: bold;'>âŒ FILTERED ({filter_info})</span>"
        elif filter_info:
            # Included with extra info (e.g., primary bullet)
            result += f"Filter Status: <span style='color: #28a745;'>âœ… INCLUDED</span>"
            result += f"<br><span style='color: #007bff;'>ğŸ”— {filter_info}</span>"
        else:
            result += f"Filter Status: <span style='color: #28a745;'>âœ… INCLUDED</span>"

        # Bullet ID at the very end
        result += f"<br><br>ID: {bullet_id}"

        return {
            'bullet_id': bullet_id,
            'formatted': result
        }

    # All bullet sections
    section_mapping = {
        "major_developments": "major_developments",
        "financial_performance": "financial_operational",
        "risk_factors": "risk_factors",
        "wall_street_sentiment": "wall_street",
        "competitive_industry_dynamics": "competitive_industry",
        "upcoming_catalysts": "upcoming_catalysts",
        "key_variables": "key_variables"
    }

    for json_key, sections_key in section_mapping.items():
        if json_key in json_sections:
            sections[sections_key] = [
                format_bullet_unified(b)
                for b in json_sections[json_key]
            ]

    # Scenarios (paragraph sections) - Use Phase 4 output (Phase 1 no longer generates these)
    for sections_key, phase4_key in [
        ("upside_scenario", "phase4_upside_scenario"),
        ("downside_scenario", "phase4_downside_scenario")
    ]:
        phase4_scenario = phase4.get(phase4_key, {})
        if phase4_scenario and phase4_scenario.get("content"):
            content = phase4_scenario.get("content", "")
            context = _strip_escape_hatch(phase4_scenario.get("context", ""))
            source_articles = phase4_scenario.get("source_articles", [])
            result = content
            if context:
                result += f"<br><strong>Context:</strong> {context}"
            result += f"<br><br>Source Articles: {source_articles}"
            sections[sections_key] = [result]

    # Add dates to all sections using bullet_id matching
    sections = add_dates_to_email_sections(sections, phase3_json)

    return sections
