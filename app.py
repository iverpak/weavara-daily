import os
import sys
import time
import logging
import traceback
import hashlib
import re
import pytz
import json
import secrets
import uuid
import openai
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from datetime import datetime, timedelta, timezone, date
from typing import List, Optional, Dict, Any, Tuple, Set, Union
from contextlib import contextmanager
from urllib.parse import urlparse, parse_qs, unquote, quote
import csv
import io
import newspaper
from newspaper import Article
import random
from urllib.robotparser import RobotFileParser

import psycopg
from psycopg.rows import dict_row
from fastapi import FastAPI, Request, HTTPException, Query, Body, UploadFile, File
from fastapi.responses import JSONResponse, PlainTextResponse, HTMLResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, EmailStr

import feedparser
import requests
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

from bs4 import BeautifulSoup

import base64
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from collections import defaultdict

# REMOVED: Playwright no longer used (Scrapfly-only as of Oct 2025)
# from playwright.async_api import async_playwright
import asyncio
import signal
from contextlib import contextmanager

import os
import tracemalloc
from functools import wraps
import threading
from threading import BoundedSemaphore
from concurrent.futures import ThreadPoolExecutor, as_completed

import aiohttp

import gc
import psutil
import tracemalloc
from memory_monitor import (
    memory_monitor,
    monitor_phase,
    resource_cleanup_context,
    full_resource_cleanup
)

# Module imports (Oct 2025 - modularization)
from modules.transcript_summaries import (
    fetch_fmp_transcript_list,
    fetch_fmp_transcript,
    fetch_fmp_press_releases,
    fetch_fmp_press_release_by_date,
    save_transcript_summary_to_database
)
from modules.company_profiles import (
    extract_pdf_text,
    extract_text_file,
    fetch_sec_html_text,
    generate_company_profile_with_gemini,
    generate_sec_filing_profile_with_gemini,
    generate_company_profile_email,
    save_company_profile_to_database,
    GEMINI_INVESTOR_DECK_PROMPT,
    # 8-K SEC filing functions
    ITEM_CODE_MAP,
    get_cik_for_ticker,
    parse_sec_8k_filing_list,
    get_8k_html_url,
    extract_8k_item_codes,    # NEW: Lightweight item code extraction
    get_all_8k_exhibits,      # NEW: Parse all EX-99.* files from documents page
    get_main_8k_url,          # NEW: Fallback to main 8-K body when no exhibits
    extract_8k_html_content,  # Simplified: Single exhibit HTML extraction
    classify_exhibit_type,    # NEW: Auto-classify exhibits (earnings_release, investor_presentation, etc.)
    should_process_exhibit,   # NEW: Filter zero-value exhibits (auditor letters, consents, XBRL, etc.)
    # Parsed press releases (Nov 2025 - unified Gemini summaries for FMP PRs and 8-K exhibits)
    generate_earnings_release_with_gemini,  # NEW: Comprehensive earnings release analysis
    get_all_parsed_press_releases,  # Query company_releases table
    # Company releases (Nov 2025 - JSON-based email system for 8-K and FMP releases)
    parse_company_release_sections,
    build_company_release_html,
    generate_company_release_email
)
# Article summaries module (Nov 2025 - Gemini primary, Claude fallback)
import modules.article_summaries as article_summaries

# Triage module (Nov 2025 - Claude + Gemini dual scoring with smart fallback)
from modules.triage import (
    triage_company_articles_dual,
    triage_industry_articles_dual,
    triage_competitor_articles_dual,
    triage_upstream_articles_dual,
    triage_downstream_articles_dual
)

# ============================================================================
# AIOHTTP CONNECTION MANAGEMENT - Prevents Connection Exhaustion & Deadlock
# ============================================================================
# Problem: Creating new session per API call leads to:
# - 180+ sessions with 3 concurrent tickers
# - File descriptor exhaustion â†’ server freeze
# - TCP handshake overhead (slow)
#
# Solution: Thread-local connectors + thread-local sessions
# - One connector per thread (isolated event loop per thread)
# - One session per thread (reused for all calls in that thread)
# - Cleanup on thread exit (prevents resource leaks)
#
# FIX (Oct 2025): Made connector thread-local to prevent "Event loop is closed"
# errors when multiple ThreadPoolExecutor workers use asyncio.run() concurrently.
# Each thread gets its own connector bound to its own event loop.
# ============================================================================

# Thread-local storage for connectors and sessions (one per thread)
_thread_local = threading.local()

def _get_or_create_connector():
    """
    Get or create HTTP connector for current thread (thread-safe lazy initialization).

    Each ThreadPoolExecutor worker thread gets its own connector, which prevents
    "Event loop is closed" errors when multiple threads run asyncio.run() concurrently.

    The connector is bound to the thread's event loop and is automatically cleaned up
    when the thread exits.
    """
    if not hasattr(_thread_local, 'connector') or _thread_local.connector is None:
        import threading
        thread_name = threading.current_thread().name
        _thread_local.connector = aiohttp.TCPConnector(
            limit=100,              # Total connections across all hosts (per thread)
            limit_per_host=30,      # Max 30 concurrent per host (OpenAI/Anthropic/ScrapFly)
            ttl_dns_cache=300,      # Cache DNS for 5 minutes
            enable_cleanup_closed=True  # Clean up closed connections
        )
        LOG.debug(f"ðŸ”Œ Created new HTTP connector for thread: {thread_name}")
    return _thread_local.connector

def get_http_session() -> aiohttp.ClientSession:
    """
    Get or create aiohttp session for current thread.

    Returns cached session if exists, creates new one if not.
    Session is reused for all HTTP calls in the thread (connection pooling).

    Benefits:
    - Prevents connection exhaustion (max 100 connections per thread)
    - Faster (reuses TCP connections, no handshake overhead)
    - Lower memory usage (one session per thread vs hundreds)
    - Thread-safe (each thread has its own session + connector)
    """
    if not hasattr(_thread_local, 'session') or _thread_local.session is None or _thread_local.session.closed:
        _thread_local.session = aiohttp.ClientSession(
            connector=_get_or_create_connector(),
            connector_owner=False  # Don't close connector when session closes
        )
    return _thread_local.session

async def cleanup_http_session():
    """
    Close thread-local session and connector (call before thread exits).

    This ensures proper cleanup of HTTP resources when a ThreadPoolExecutor
    worker thread completes processing a job.
    """
    # Close session first
    if hasattr(_thread_local, 'session') and _thread_local.session is not None:
        if not _thread_local.session.closed:
            await _thread_local.session.close()
        _thread_local.session = None

    # Close connector second (session must be closed first)
    if hasattr(_thread_local, 'connector') and _thread_local.connector is not None:
        if not _thread_local.connector.closed:
            await _thread_local.connector.close()
        _thread_local.connector = None

# ============================================================================
import yfinance as yf
from collections import deque

# Global session for OpenAI API calls with retries
_openai_session = None

# Polygon.io rate limiter (5 calls/minute for free tier)
POLYGON_API_KEY = os.getenv("POLYGON_API_KEY")
POLYGON_RATE_LIMIT = 5  # calls per minute
POLYGON_CALL_TIMES = deque(maxlen=POLYGON_RATE_LIMIT)  # Track last N call times

import asyncio

# Global ticker processing lock
TICKER_PROCESSING_LOCK = asyncio.Lock()

# Concurrency Configuration and Semaphores
SCRAPE_BATCH_SIZE = int(os.getenv("SCRAPE_BATCH_SIZE", "5"))
OPENAI_MAX_CONCURRENCY = int(os.getenv("OPENAI_MAX_CONCURRENCY", "5"))
CLAUDE_MAX_CONCURRENCY = int(os.getenv("CLAUDE_MAX_CONCURRENCY", "5"))  # Claude concurrency
# REMOVED: Playwright no longer used (Scrapfly-only as of Oct 2025)
# PLAYWRIGHT_MAX_CONCURRENCY = int(os.getenv("PLAYWRIGHT_MAX_CONCURRENCY", "5"))
SCRAPFLY_MAX_CONCURRENCY = int(os.getenv("SCRAPFLY_MAX_CONCURRENCY", "5"))
TRIAGE_MAX_CONCURRENCY = int(os.getenv("TRIAGE_MAX_CONCURRENCY", "5"))
MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "2"))  # Parallel ticker processing

# Global semaphores for concurrent processing (thread-safe, work across event loops)
# These work in both sync and async contexts (with minor blocking during acquisition)
OPENAI_SEM = BoundedSemaphore(OPENAI_MAX_CONCURRENCY)
CLAUDE_SEM = BoundedSemaphore(CLAUDE_MAX_CONCURRENCY)
# REMOVED: Playwright no longer used (Scrapfly-only as of Oct 2025)
# PLAYWRIGHT_SEM = BoundedSemaphore(PLAYWRIGHT_MAX_CONCURRENCY)
SCRAPFLY_SEM = BoundedSemaphore(SCRAPFLY_MAX_CONCURRENCY)
TRIAGE_SEM = BoundedSemaphore(TRIAGE_MAX_CONCURRENCY)

# NOTE: Removed asyncio semaphores (2025-10-07)
# Reason: asyncio.Semaphore binds to specific event loop, causing errors when
# using asyncio.run() in ThreadPoolExecutor (job queue system).
# Solution: Use threading.BoundedSemaphore which works across threads and event loops.
# Trade-off: Tiny blocking overhead (~microseconds) vs 100% async, but negligible impact.

def get_openai_session():
    """Get a requests session with retry logic for OpenAI API calls"""
    global _openai_session
    if _openai_session is None:
        _openai_session = requests.Session()
        retry_strategy = Retry(
            total=3,                    # up to 3 retries
            backoff_factor=0.8,         # 0.8s, 1.6s, 3.2s delays
            status_forcelist=(429, 500, 502, 503, 504),  # retry on these HTTP codes
            allowed_methods=frozenset(["POST"])
        )
        _openai_session.mount("https://", HTTPAdapter(max_retries=retry_strategy))
    return _openai_session

def extract_text_from_responses(result: dict) -> str:
    """
    Robustly extract assistant text from Responses API.
    Covers:
      - top-level `output_text`
      - `output[*].content[*].text` when type in {"output_text", "text"}
    Returns "" if none.
    """
    try:
        # 1) Convenience field (some models return this directly)
        if isinstance(result.get("output_text"), str) and result["output_text"].strip():
            return result["output_text"].strip()

        # 2) Structured content (standard Responses format)
        for block in result.get("output", []) or []:
            for item in block.get("content", []) or []:
                t = item.get("type")
                if t in ("output_text", "text"):
                    txt = item.get("text")
                    if isinstance(txt, str) and txt.strip():
                        return txt.strip()
        return ""
    except Exception:
        return ""

def parse_json_with_fallback(text: str, ticker: str = "") -> dict:
    """Parse JSON with fallback extraction for malformed responses

    Uses unified JSON extraction with 4-tier fallback strategy to handle
    all Claude/Gemini response formats including markdown wrappers.
    """
    if not text:
        return {}

    # Use unified JSON extraction utility (handles all response formats)
    from modules.json_utils import extract_json_from_claude_response
    result = extract_json_from_claude_response(text, ticker)

    if result:
        return result

    # All extraction strategies failed - return minimum defaults
    LOG.warning(f"Failed to parse JSON for {ticker}, using defaults")
    return {
        "ticker": ticker,
        "company_name": ticker,
        "industry_keywords": [],
        "competitors": [],
        "sector": "",
        "industry": "",
        "sub_industry": "",
        "sector_profile": {"core_inputs": [], "core_channels": [], "core_geos": [], "benchmarks": []},
        "aliases_brands_assets": {"aliases": [], "brands": [], "assets": []}
    }

@contextmanager
def timeout_handler(seconds: int):
    """Context manager for timeout handling on Unix systems"""
    def timeout_signal_handler(signum, frame):
        raise TimeoutError(f"Operation timed out after {seconds} seconds")

    # Set the signal handler and alarm
    old_handler = signal.signal(signal.SIGALRM, timeout_signal_handler)
    signal.alarm(seconds)

    try:
        yield
    finally:
        # Restore the old signal handler and cancel alarm
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)


# =============================================================================
# CLAUDE API COST TRACKING
# =============================================================================

# Claude API Pricing (per 1M tokens as of January 2025)
CLAUDE_PRICING = {
    "input": 3.00,           # $3.00 per 1M input tokens
    "output": 15.00,         # $15.00 per 1M output tokens
    "cache_write": 3.75,     # $3.75 per 1M cache write tokens
    "cache_read": 0.30       # $0.30 per 1M cache read tokens (90% savings)
}

# Gemini Flash API Pricing (per 1M tokens as of December 2025)
GEMINI_FLASH_PRICING = {
    "input": 0.30,           # $0.30 per 1M input tokens
    "output": 2.50,          # $2.50 per 1M output tokens
}

# Gemini Pro API Pricing (per 1M tokens as of December 2025, under 200k context)
GEMINI_PRO_PRICING = {
    "input": 1.25,           # $1.25 per 1M input tokens
    "output": 10.00,         # $10.00 per 1M output tokens
}

# Gemini 3.0 Flash Preview Pricing (per 1M tokens as of December 2025)
# Supports implicit caching and thinking tokens
GEMINI_FLASH_3_PRICING = {
    "input": 0.50,           # $0.50 per 1M input tokens
    "cache_read": 0.05,      # $0.05 per 1M cached tokens (90% discount)
    "output": 3.00,          # $3.00 per 1M output tokens (includes thinking)
}
# NOTE: Gemini 3.0 supports implicit caching after 2,048 token prefix match

# Thread-local storage for cost tracking (one tracker per ticker thread)
_cost_tracker = threading.local()


def get_cost_tracker():
    """Get or create cost tracker for current thread"""
    if not hasattr(_cost_tracker, 'data'):
        _cost_tracker.data = {
            "total_cost": 0.0,
            "by_function": {},
            "cache_stats": {
                "cache_creations": 0,
                "cache_creation_tokens": 0,
                "cache_hits": 0,
                "cache_hit_tokens": 0,
                "total_cache_write_cost": 0.0,
                "total_cache_read_cost": 0.0,
                "total_savings": 0.0
            }
        }
    return _cost_tracker.data


def reset_cost_tracker():
    """Reset cost tracker for new ticker"""
    if hasattr(_cost_tracker, 'data'):
        del _cost_tracker.data


def calculate_claude_api_cost(usage: dict, function_name: str, model_name: str = None) -> dict:
    """
    Calculate cost from Claude API usage response and track it.

    Args:
        usage: Usage dict from Claude API response
        function_name: Name of the function (for categorization)
        model_name: Full model name for tracking (e.g., "claude-sonnet-4-5-20250929", optional)

    Returns:
        Cost breakdown dict
    """
    input_tokens = usage.get("input_tokens", 0)
    output_tokens = usage.get("output_tokens", 0)
    cache_creation = usage.get("cache_creation_input_tokens", 0)
    cache_read = usage.get("cache_read_input_tokens", 0)

    # Calculate costs (divide by 1M to get actual cost)
    input_cost = (input_tokens / 1_000_000) * CLAUDE_PRICING["input"]
    output_cost = (output_tokens / 1_000_000) * CLAUDE_PRICING["output"]
    cache_write_cost = (cache_creation / 1_000_000) * CLAUDE_PRICING["cache_write"]
    cache_read_cost = (cache_read / 1_000_000) * CLAUDE_PRICING["cache_read"]

    call_cost = input_cost + output_cost + cache_write_cost + cache_read_cost

    # Calculate savings from cache hit (what we would have paid without caching)
    cache_savings = 0
    if cache_read > 0:
        # Cache read is 90% cheaper than regular input
        regular_input_cost = (cache_read / 1_000_000) * CLAUDE_PRICING["input"]
        cache_savings = regular_input_cost - cache_read_cost

    # Track in thread-local tracker
    tracker = get_cost_tracker()
    tracker["total_cost"] += call_cost

    if function_name not in tracker["by_function"]:
        tracker["by_function"][function_name] = {
            "calls": 0,
            "cost": 0.0
        }

    tracker["by_function"][function_name]["calls"] += 1
    tracker["by_function"][function_name]["cost"] += call_cost

    # Store tokens and model name for executive summary phases only
    if function_name.startswith("executive_summary_phase"):
        if "tokens" not in tracker["by_function"][function_name]:
            tracker["by_function"][function_name]["tokens"] = {
                "input": 0,
                "output": 0
            }
        tracker["by_function"][function_name]["tokens"]["input"] += input_tokens
        tracker["by_function"][function_name]["tokens"]["output"] += output_tokens

        # Store model name for Phase 1/2/3 (only if provided)
        if model_name:
            tracker["by_function"][function_name]["model"] = model_name

    # Track cache stats
    if cache_creation > 0:
        tracker["cache_stats"]["cache_creations"] += 1
        tracker["cache_stats"]["cache_creation_tokens"] += cache_creation
        tracker["cache_stats"]["total_cache_write_cost"] += cache_write_cost

    if cache_read > 0:
        tracker["cache_stats"]["cache_hits"] += 1
        tracker["cache_stats"]["cache_hit_tokens"] += cache_read
        tracker["cache_stats"]["total_cache_read_cost"] += cache_read_cost
        tracker["cache_stats"]["total_savings"] += cache_savings

    return {
        "call_cost": round(call_cost, 6),
        "input_cost": round(input_cost, 6),
        "output_cost": round(output_cost, 6),
        "cache_write_cost": round(cache_write_cost, 6),
        "cache_read_cost": round(cache_read_cost, 6),
        "cache_savings": round(cache_savings, 6),
        "tokens": {
            "input": input_tokens,
            "output": output_tokens,
            "cache_creation": cache_creation,
            "cache_read": cache_read
        }
    }


def calculate_gemini_api_cost(usage: dict, function_name: str, model: str = "flash", model_name: str = None) -> dict:
    """
    Calculate cost from Gemini API usage response and track it.

    Args:
        usage: Usage dict from Gemini API response with keys:
               - input_tokens: prompt token count
               - output_tokens: candidates token count
               For Flash 3.0, also supports:
               - thought_tokens: reasoning tokens (billed as output)
               - cached_tokens: tokens served from cache (90% discount)
        function_name: Name of the function (for categorization)
        model: "flash", "pro", or "flash3" (default: "flash")
        model_name: Full model name for tracking (e.g., "gemini-2.5-pro", optional)

    Returns:
        Cost breakdown dict
    """
    input_tokens = usage.get("input_tokens", 0)
    output_tokens = usage.get("output_tokens", 0)
    thought_tokens = usage.get("thought_tokens", 0)  # Flash 3.0 only
    cached_tokens = usage.get("cached_tokens", 0)    # Flash 3.0 only

    # Determine pricing based on model
    if "flash3" in model.lower() or "3-flash" in model.lower() or "gemini-3" in (model_name or "").lower():
        # Gemini 3.0 Flash Preview pricing with caching and thinking
        pricing = GEMINI_FLASH_3_PRICING
        uncached_input = input_tokens - cached_tokens
        input_cost = (uncached_input / 1_000_000) * pricing["input"]
        cache_cost = (cached_tokens / 1_000_000) * pricing["cache_read"]
        # Output includes thinking tokens
        output_cost = ((output_tokens + thought_tokens) / 1_000_000) * pricing["output"]
        call_cost = input_cost + cache_cost + output_cost
    elif "pro" in model.lower():
        pricing = GEMINI_PRO_PRICING
        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]
        cache_cost = 0
        call_cost = input_cost + output_cost
    else:
        pricing = GEMINI_FLASH_PRICING
        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]
        cache_cost = 0
        call_cost = input_cost + output_cost

    # Track in thread-local tracker
    tracker = get_cost_tracker()
    tracker["total_cost"] += call_cost

    if function_name not in tracker["by_function"]:
        tracker["by_function"][function_name] = {
            "calls": 0,
            "cost": 0.0
        }

    tracker["by_function"][function_name]["calls"] += 1
    tracker["by_function"][function_name]["cost"] += call_cost

    # Store tokens and model name for executive summary phases only
    if function_name.startswith("executive_summary_phase"):
        if "tokens" not in tracker["by_function"][function_name]:
            tracker["by_function"][function_name]["tokens"] = {
                "input": 0,
                "output": 0,
                "thought": 0,
                "cached": 0
            }
        tracker["by_function"][function_name]["tokens"]["input"] += input_tokens
        tracker["by_function"][function_name]["tokens"]["output"] += output_tokens
        tracker["by_function"][function_name]["tokens"]["thought"] += thought_tokens
        tracker["by_function"][function_name]["tokens"]["cached"] += cached_tokens

        # Store model name for Phase 1/2/3 (only if provided)
        if model_name:
            tracker["by_function"][function_name]["model"] = model_name

    return {
        "call_cost": round(call_cost, 6),
        "input_cost": round(input_cost, 6),
        "output_cost": round(output_cost, 6),
        "cache_cost": round(cache_cost, 6) if cache_cost else 0,
        "tokens": {
            "input": input_tokens,
            "output": output_tokens,
            "thought": thought_tokens,
            "cached": cached_tokens
        }
    }


def log_cost_summary(ticker: str, company_name: str = ""):
    """Log emoji-rich cost summary for ticker"""
    tracker = get_cost_tracker()

    if tracker["total_cost"] == 0:
        LOG.info(f"[{ticker}] ðŸ’° No Claude API costs (no calls made)")
        return

    # Calculate what cost would have been without caching
    cost_without_caching = tracker["total_cost"] + tracker["cache_stats"]["total_savings"]
    savings_percentage = 0
    if cost_without_caching > 0:
        savings_percentage = (tracker["cache_stats"]["total_savings"] / cost_without_caching) * 100

    # Header
    LOG.info(f"[{ticker}] ðŸ’° â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    if company_name:
        LOG.info(f"[{ticker}] ðŸ’° API COST SUMMARY - {ticker} ({company_name})")
    else:
        LOG.info(f"[{ticker}] ðŸ’° API COST SUMMARY - {ticker}")
    LOG.info(f"[{ticker}] ðŸ’° â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    LOG.info(f"[{ticker}] ðŸ’µ Total Cost: ${tracker['total_cost']:.4f}")
    LOG.info(f"[{ticker}] ðŸ’° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # By function breakdown
    LOG.info(f"[{ticker}] ðŸ“ž API Calls by Function:")

    # Define friendly names and emojis
    function_display = {
        # Triage - 10 entries (5 categories Ã— 2 providers)
        "triage_company_claude": ("ðŸŽ¯ Triage Company (Claude)", 1),
        "triage_company_gemini": ("ðŸŽ¯ Triage Company (Gemini)", 2),
        "triage_industry_claude": ("ðŸ­ Triage Industry (Claude)", 3),
        "triage_industry_gemini": ("ðŸ­ Triage Industry (Gemini)", 4),
        "triage_competitor_claude": ("ðŸ¢ Triage Competitor (Claude)", 5),
        "triage_competitor_gemini": ("ðŸ¢ Triage Competitor (Gemini)", 6),
        "triage_upstream_claude": ("â¬†ï¸ Triage Upstream (Claude)", 7),
        "triage_upstream_gemini": ("â¬†ï¸ Triage Upstream (Gemini)", 8),
        "triage_downstream_claude": ("â¬‡ï¸ Triage Downstream (Claude)", 9),
        "triage_downstream_gemini": ("â¬‡ï¸ Triage Downstream (Gemini)", 10),
        # Article summaries
        "article_summary_company": ("ðŸ“ Company Summaries", 11),
        "article_summary_competitor": ("ðŸ† Competitor Summaries", 12),
        "article_summary_upstream": ("â¬†ï¸ Upstream Summaries", 13),
        "article_summary_downstream": ("â¬‡ï¸ Downstream Summaries", 14),
        "article_summary_industry": ("ðŸŒ Industry Summaries", 15),
        "fundamental_driver_scoring": ("âš–ï¸ Industry Scoring", 16),
        # Executive summary phases
        "executive_summary": ("ðŸ“Š Executive Summary (OLD)", 17),
        "executive_summary_phase1": ("ðŸ“Š Executive Summary - Phase 1", 18),
        "executive_summary_phase1_5": ("ðŸ” Executive Summary - Phase 1.5", 19),
        "executive_summary_phase2": ("ðŸ“„ Executive Summary - Phase 2", 20),
        "executive_summary_phase3": ("ðŸ“ Executive Summary - Phase 3", 21),
        "executive_summary_phase4": ("âœï¸ Executive Summary - Phase 4", 22),
        # Other
        "research_summary": ("ðŸ“‹ Research Summary (Transcript/PR)", 23),
        "ticker_metadata_generation": ("ðŸ·ï¸ Ticker Metadata Generation", 24)
    }

    # Sort by display order
    sorted_functions = sorted(
        tracker["by_function"].items(),
        key=lambda x: function_display.get(x[0], (x[0], 99))[1]
    )

    for func_name, data in sorted_functions:
        display_name, _ = function_display.get(func_name, (func_name, 99))
        calls = data["calls"]
        cost = data["cost"]
        plural = "calls" if calls != 1 else "call"

        # Add model name to display for Phase 1/2/3 if available
        model_suffix = ""
        if func_name.startswith("executive_summary_phase") and "model" in data:
            model_suffix = f" ({data['model']})"

        LOG.info(f"[{ticker}]   {display_name}: {calls:3d} {plural:5s} â†’  ${cost:.4f}{model_suffix}")

        # Show token breakdown for executive summary phases only
        if func_name.startswith("executive_summary_phase") and "tokens" in data:
            input_tok = data["tokens"]["input"]
            output_tok = data["tokens"]["output"]
            LOG.info(f"[{ticker}]      Input: {input_tok:,} tokens | Output: {output_tok:,} tokens")

    LOG.info(f"[{ticker}] ðŸ’° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # Cache performance
    cache_stats = tracker["cache_stats"]
    LOG.info(f"[{ticker}] ðŸ’¾ Prompt Caching Performance:")

    if cache_stats["cache_creations"] > 0:
        LOG.info(f"[{ticker}]   âœ¨ Cache Created:  {cache_stats['cache_creations']:2d} times "
                f"({cache_stats['cache_creation_tokens']:,} tokens)  â†’  Cost: ${cache_stats['total_cache_write_cost']:.4f}")

    if cache_stats["cache_hits"] > 0:
        LOG.info(f"[{ticker}]   âš¡ Cache Hits:    {cache_stats['cache_hits']:2d} times "
                f"({cache_stats['cache_hit_tokens']:,} tokens)  â†’  Cost: ${cache_stats['total_cache_read_cost']:.4f}")
        LOG.info(f"[{ticker}]   ðŸ’° Cache Savings: ${cache_stats['total_savings']:.4f} ({savings_percentage:.1f}% cost reduction)")
        LOG.info(f"[{ticker}]   ðŸ“ˆ Without caching, cost would have been: ${cost_without_caching:.4f}")
    else:
        LOG.info(f"[{ticker}]   â„¹ï¸ No cache hits yet (first run for these prompts)")

    LOG.info(f"[{ticker}] ðŸ’° â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

def clean_null_bytes(text: str) -> str:
    """Remove NULL bytes and other problematic characters that cause PostgreSQL errors"""
    if not text:
        return text
    # Remove NULL bytes, control characters, and other problematic Unicode
    cleaned = text.replace('\x00', '').replace('\0', '')
    # Remove other control characters that can cause XML/database issues
    cleaned = ''.join(char for char in cleaned if ord(char) >= 32 or char in '\n\r\t')
    return cleaned

def parse_quality_score(ai_summary: str) -> tuple[str, float]:
    """
    Parse quality score from AI summary response using JSON format.

    Expected format: Summary ends with JSON on last line: {"quality": 7.5}

    Returns:
        (cleaned_summary, quality_score) where quality_score is None if not found
    """
    if not ai_summary:
        return ai_summary, None

    # Try to extract JSON from last line
    lines = ai_summary.strip().split('\n')
    if not lines:
        return ai_summary, None

    last_line = lines[-1].strip()
    quality_score = None

    # Try JSON format first (new format)
    try:
        # Handle markdown code blocks if present
        if last_line.startswith('```'):
            # JSON might be wrapped in code block
            if len(lines) >= 3 and lines[-1] == '```':
                last_line = lines[-2].strip()
                lines = lines[:-2]  # Remove closing ``` and JSON line

        quality_data = json.loads(last_line)
        if "quality" in quality_data:
            quality_score = float(quality_data["quality"])
            # Clamp to 0-10 range
            quality_score = max(0.0, min(10.0, quality_score))
            # Remove JSON line from summary
            cleaned_summary = '\n'.join(lines[:-1]).strip()
            return cleaned_summary, quality_score
    except (json.JSONDecodeError, ValueError, KeyError, TypeError):
        pass  # Not JSON format, try legacy format

    # Fallback: Try legacy plain text format "QUALITY: X.X"
    summary_lines = []
    for line in lines:
        if line.strip().startswith('QUALITY:'):
            try:
                score_str = line.replace('QUALITY:', '').strip()
                quality_score = float(score_str)
                # Clamp to 0-10 range
                quality_score = max(0.0, min(10.0, quality_score))
            except (ValueError, AttributeError):
                pass  # Invalid format, skip
        else:
            summary_lines.append(line)

    cleaned_summary = '\n'.join(summary_lines).strip()
    return cleaned_summary, quality_score

def normalize_priority_to_int(priority):
    """Normalize priority to consistent integer format (1=High, 2=Medium, 3=Low)"""
    if isinstance(priority, int):
        return max(1, min(3, priority))
    elif isinstance(priority, str):
        priority_upper = priority.upper()
        if priority_upper in ["HIGH", "H", "1"]:
            return 1
        elif priority_upper in ["MEDIUM", "MED", "M", "2"]:
            return 2
        elif priority_upper in ["LOW", "L", "3"]:
            return 3
    return 2  # Default to Medium

def normalize_priority_to_display(priority):
    """Convert integer priority to display format"""
    if isinstance(priority, int):
        if priority == 1:
            return "High"
        elif priority == 2:
            return "Medium"
        elif priority == 3:
            return "Low"
    elif isinstance(priority, str):
        priority_upper = priority.upper()
        if priority_upper in ["HIGH", "H"]:
            return "High"
        elif priority_upper in ["MEDIUM", "MED", "M"]:
            return "Medium"
        elif priority_upper in ["LOW", "L"]:
            return "Low"
    return "Medium"

def _normalize_host(url_or_domain: str) -> str:
    """
    Returns a lowercase hostname (no scheme/port/path). 
    Accepts either a full URL or a bare domain.
    """
    s = (url_or_domain or "").strip().lower()
    if not s:
        return ""
    if "://" not in s:
        # treat as bare domain
        host = s.split("/")[0]
    else:
        host = urlparse(s).hostname or ""
    # strip common prefixes so matching is stable
    for prefix in ("www.", "m."):
        if host.startswith(prefix):
            host = host[len(prefix):]
    return host

def _domain_matches(host: str, needle: str) -> bool:
    """
    True if host == needle OR host ends with '.' + needle
    This safely matches subdomains (e.g., foo.bar.com vs bar.com),
    and also supports specific subdomains like 'video.media.yql.yahoo.com'.
    """
    if not host or not needle:
        return False
    return host == needle or host.endswith("." + needle)

def needs_antibot(url_or_domain: str) -> bool:
    """
    Should we enable Scrapfly anti-bot + JS rendering for this target?
    """
    host = _normalize_host(url_or_domain)
    if not host:
        return False
    for dom in PROBLEMATIC_SCRAPE_DOMAINS:
        if _domain_matches(host, dom):
            return True
    return False

# ------------------------------------------------------------------------------
# Logging
# ------------------------------------------------------------------------------
LOG = logging.getLogger("quantbrief")
LOG.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
if not LOG.handlers:
    LOG.addHandler(handler)

# ------------------------------------------------------------------------------
# Config / Environment
# ------------------------------------------------------------------------------
APP = FastAPI(title="Weavara Stock Intelligence Platform")

# Templates setup for landing page
templates = Jinja2Templates(directory="templates")
app = APP  # for uvicorn

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    LOG.warning("DATABASE_URL not set - database features disabled")

ADMIN_TOKEN = os.getenv("ADMIN_TOKEN", "changeme-admin-token")

# OpenAI Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1/responses")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5")

# Anthropic Claude Configuration
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
ANTHROPIC_API_URL = os.getenv("ANTHROPIC_API_URL", "https://api.anthropic.com/v1/messages")
ANTHROPIC_MODEL = os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-5-20250929")
USE_CLAUDE_FOR_SUMMARIES = os.getenv("USE_CLAUDE_FOR_SUMMARIES", "true").lower() == "true"
USE_CLAUDE_FOR_METADATA = os.getenv("USE_CLAUDE_FOR_METADATA", "true").lower() == "true"

SCRAPFLY_API_KEY = os.getenv("SCRAPFLY_API_KEY")
FMP_API_KEY = os.getenv("FMP_API_KEY")  # Financial Modeling Prep API key
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")  # Google Gemini API key for company profiles

GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")  # Personal Access Token
GITHUB_REPO = os.getenv("GITHUB_REPO")    # e.g., "username/repo-name"
GITHUB_CSV_PATH = os.getenv("GITHUB_CSV_PATH", "data/ticker_reference.csv")  # Path in repo

# Email configuration
def _first(*vals) -> Optional[str]:
    for v in vals:
        if v:
            return v
    return None

SMTP_HOST = _first(os.getenv("MAILGUN_SMTP_SERVER"), os.getenv("SMTP_HOST"))
SMTP_PORT = int(_first(os.getenv("MAILGUN_SMTP_PORT"), os.getenv("SMTP_PORT"), "587"))
SMTP_USERNAME = _first(os.getenv("MAILGUN_SMTP_LOGIN"), os.getenv("SMTP_USERNAME"))
SMTP_PASSWORD = _first(os.getenv("MAILGUN_SMTP_PASSWORD"), os.getenv("SMTP_PASSWORD"))
SMTP_STARTTLS = os.getenv("SMTP_STARTTLS", "1") not in ("0", "false", "False", "")

EMAIL_FROM = _first(os.getenv("MAILGUN_FROM"), os.getenv("EMAIL_FROM"), SMTP_USERNAME)
ADMIN_EMAIL = os.getenv("ADMIN_EMAIL")

# Staging mode - prevents accidental emails to real users
STAGING_MODE = os.getenv("STAGING_MODE", "false").lower() == "true"
STAGING_ALLOWED_EMAILS = [
    "weavara.research@gmail.com",
    "ilia.verpakhovski@gmail.com",
    "timelesstalesvisualized@gmail.com",
    "stockdigest.research@gmail.com",
]

# Legal document versions
TERMS_VERSION = "1.0"
PRIVACY_VERSION = "1.0"
TERMS_LAST_UPDATED = "October 7, 2025"
PRIVACY_LAST_UPDATED = "October 7, 2025"

# Background job tracking for financials checking (in-memory, single-job at a time)
_financials_check_job = {
    "job_id": None,
    "status": "idle",  # idle | running | complete | error
    "progress": 0,  # 0-100
    "message": "",
    "started_at": None,
    "completed_at": None,
    "results": None,  # Will store ticker list when complete
    "error": None
}
_financials_check_lock = threading.Lock()

# FIXED: Enhanced spam filtering with more comprehensive domain list
# These domains are blocked at ingestion - articles never stored
SPAM_DOMAINS = {
    # Original spam domains
    "marketbeat.com", "www.marketbeat.com", "marketbeat",
    "newser.com", "www.newser.com", "newser",
    "khodrobank.com", "www.khodrobank.com", "khodrobank",
    "defenseworld.net", "www.defenseworld.net", "defenseworld",
    "defenseworld.com", "www.defenseworld.com", "defenseworld",
    "defense-world.net", "www.defense-world.net", "defense-world",
    "defensenews.com", "www.defensenews.com", "defensenews",
    "facebook.com", "www.facebook.com", "facebook",
    # Low-quality financial sites - block at ingestion
    "msn.com", "www.msn.com",
    "tipranks.com", "www.tipranks.com",
    "simplywall.st", "www.simplywall.st",
    "sharewise.com", "www.sharewise.com",
    "stockstory.org", "www.stockstory.org",
    "news.stocktradersdaily.com", "stocktradersdaily.com", "www.stocktradersdaily.com",
    "earlytimes.in", "www.earlytimes.in",
    "investing.com", "www.investing.com", "in.investing.com",  # Including India subdomain
    "fool.com", "www.fool.com", "fool.ca", "www.fool.ca",
    "marketsmojo.com", "www.marketsmojo.com",
    "stocktitan.net", "www.stocktitan.net",
    "insidermonkey.com", "www.insidermonkey.com",
    "247wallst.com", "www.247wallst.com",  # 24/7 Wall St (retail stock picks)
    "zacks.com", "www.zacks.com",
    "markets.financialcontent.com", "www.markets.financialcontent.com",  # Added Oct 2025
    # Retail analysis sites - automated ratings, stock screeners, AI predictions (Oct 2025)
    "gurufocus.com", "www.gurufocus.com",  # CRITICAL: Moved from PROBLEMATIC_SCRAPE_DOMAINS
    "stockanalysis.com", "www.stockanalysis.com",
    "stockrover.com", "www.stockrover.com",
    "stockcharts.com", "www.stockcharts.com",
    "stockinvest.us", "www.stockinvest.us",
    "wallstreetzen.com", "www.wallstreetzen.com",
    "stockopedia.com", "www.stockopedia.com",
    "stockrow.com", "www.stockrow.com",
    # Vietnamese news sites - not material for US/Canadian market analysis (Oct 2025)
    "nchmf.gov.vn", "www.nchmf.gov.vn",  # Vietnamese weather/climate center
    "baovietnamnet.vn", "www.baovietnamnet.vn", "baovietnamnet",  # Vietnamese news portal
    # Brazilian government/cultural sites - auto-generated stock content (Nov 2025)
    "fcp.pa.gov.br", "www.fcp.pa.gov.br",  # FundaÃ§Ã£o Cultural do ParÃ¡ - syndicated retail stock analysis
    # Stock guru / trading course sites (Oct 2025)
    "timothysykes.com", "www.timothysykes.com",  # Penny stock promoter, trading courses, pure retail picks
    # Low-quality international aggregators (Oct 2025)
    "beritasriwijaya.co.id", "www.beritasriwijaya.co.id",  # Indonesian news aggregator, syndicated content
    # Retail investment screeners - content filtering update (Oct 2025)
    "finviz.com", "www.finviz.com",  # Stock screener, technical analysis charts
    "investors.com", "www.investors.com",  # Investor's Business Daily - technical analysis, chart patterns
    "directorstalk.com", "www.directorstalk.com",  # Retail stock analysis aggregator
    "directorstalkinterviews.com", "www.directorstalkinterviews.com",  # Retail interviews aggregator
    # General spam/low-quality aggregators (Oct 2025)
    "barchart.com", "www.barchart.com",  # Anti-bot issues, promoted from ANTIBOT_DOMAINS
    "aol.com", "www.aol.com",  # Low-quality news aggregator
    "tradingnews.com", "www.tradingnews.com",  # Low-quality trading news aggregator
    # Retail stock picking sites - automated content and listicles (Nov 2025)
    "tradingview.com", "www.tradingview.com",  # "Why [Stock] Is Up Today" automated daily content, multi-ticker noise
    "theglobeandmail.com", "www.theglobeandmail.com",  # "X Stocks to Buy/Avoid" retail listicles, multi-stock roundups
    # AI slop / low-quality aggregators (Nov 2025)
    "meyka.com", "www.meyka.com",  # Unknown/suspicious finance domain
    "ad-hoc-news.de", "www.ad-hoc-news.de",  # German aggregator, republishes content
    "setenews.com", "www.setenews.com",  # Low-quality aggregator
    # Mining/resource stock promoters (Dec 2025)
    "nai500.com", "www.nai500.com",  # Mining stock promoter site
    # User-generated content platforms (Dec 2025)
    "vocal.media", "www.vocal.media",  # UGC platform, unvetted articles
    "seekingalpha.com", "www.seekingalpha.com",  # Contributor-driven, unvetted retail analysis
    "gobankingrates.com", "www.gobankingrates.com",  # Personal finance listicles, retail-oriented
    # Tech news aggregator (Dec 2025)
    "ts2.tech", "www.ts2.tech",  # Low-quality tech aggregator
}

QUALITY_DOMAINS = {
    "reuters.com", "bloomberg.com", "wsj.com", "ft.com",
    "barrons.com", "cnbc.com", "marketwatch.com",
    "apnews.com",
}

# Domains that can be ingested but NOT scraped (heavy JS/bot protection)
PROBLEMATIC_SCRAPE_DOMAINS = {
    "defenseworld.net", "defense-world.net", "defensenews.com",
    # Note: gurufocus.com moved to SPAM_DOMAINS (Oct 2025) - blocked at ingestion now
}

# Known paywall domains to skip during content scraping
PAYWALL_DOMAINS = {
    # Hard paywalls only
    "wsj.com", "www.wsj.com",
    "ft.com", "www.ft.com",
    "economist.com", "www.economist.com",
    "nytimes.com", "www.nytimes.com",
    "seekingalpha.com", "www.seekingalpha.com",
    "washingtonpost.com", "www.washingtonpost.com",
    "barrons.com", "www.barrons.com",
    "marketwatch.com", "www.marketwatch.com",
    "bloomberg.com", "www.bloomberg.com",

    # Academic/research paywalls
    "sciencedirect.com", "www.sciencedirect.com",

    # Specific financial paywalls that consistently block
    "thefly.com", "www.thefly.com",
    "accessnewswire.com", "www.accessnewswire.com",
}

DOMAIN_TIERS = {
    "reuters.com": 1.0, 
    "wsj.com": 1.0, 
    "ft.com": 1.0,
    "bloomberg.com": 1.0,
    "sec.gov": 1.0,
    "apnews.com": 0.9,
    "nasdaq.com": 0.8,
    "techcrunch.com": 0.7, 
    "cnbc.com": 0.7, 
    "marketwatch.com": 0.7,
    "barrons.com": 0.7,
    "investors.com": 0.6,
    "finance.yahoo.com": 0.4, 
    "yahoo.com": 0.4, 
    "news.yahoo.com": 0.4,
    "msn.com": 0.4,
    "seekingalpha.com": 0.4, 
    "fool.com": 0.4, 
    "zacks.com": 0.4,
    "benzinga.com": 0.4,
    "tipranks.com": 0.4, 
    "simplywall.st": 0.4, 
    "investing.com": 0.4,
    "insidermonkey.com": 0.4,
    "businesswire.com": 0.3,
    "streetinsider.com": 0.3,
    "thefly.com": 0.3,
    "globenewswire.com": 0.2, 
    "prnewswire.com": 0.2, 
    "openpr.com": 0.2, 
    "financialcontent.com": 0.2,
    "accesswire.com": 0.2,
    "defensenews.com": 0.2,
    "defense-world.net": 0.1,
    "defenseworld.net": 0.1,
}

# Source hints for upgrading aggregator content
SOURCE_TIER_HINTS = [
    (r"\b(reuters)\b", 1.0),
    (r"\b(bloomberg)\b", 1.0),
    (r"\b(wall street journal|wsj)\b", 1.0),
    (r"\b(financial times|ft)\b", 1.0),
    (r"\b(associated press|ap)\b", 0.9),
    (r"\b(cnbc)\b", 0.7),
    (r"\b(marketwatch)\b", 0.7),
    (r"\b(barron's|barrons)\b", 0.7),
    (r"\b(motley fool|the motley fool)\b", 0.4),
    (r"\b(seeking alpha)\b", 0.4),
    (r"\b(zacks)\b", 0.4),
    (r"\b(benzinga)\b", 0.4),
    (r"\b(globenewswire|pr newswire|business wire)\b", 0.2),
]

# Enhanced scraping configuration
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

DOMAIN_STRATEGIES = {
    'simplywall.st': {
        'delay_range': (5, 8),
        'headers': {'Accept': 'text/html,application/xhtml+xml'},
    },
    'seekingalpha.com': {
        'delay_range': (3, 6),
        'referrer': 'https://www.google.com/search?q=stock+analysis',
    },
    'zacks.com': {
        'delay_range': (4, 7),
        'headers': {'Cache-Control': 'no-cache'},
    },
    'benzinga.com': {
        'delay_range': (4, 7),
        'headers': {'Accept': 'text/html,application/xhtml+xml'},
    },
    'tipranks.com': {
        'delay_range': (5, 8),
        'referrer': 'https://www.google.com/',
    }
}

# Domains that should skip TIER 1 (requests) and go directly to TIER 2 (Scrapfly)
# These are typically JavaScript-heavy sites that timeout on standard requests
# UPDATED 2025-10-06: Now skips to Scrapfly (Playwright commented out)
SKIP_TIER1_DOMAINS = {
    'businesswire.com',  # Slow to load, JavaScript-heavy
}

# FIXED: Ticker-specific ingestion stats to prevent race conditions
ticker_ingestion_stats = {}

def get_ticker_ingestion_stats(ticker: str) -> dict:
    """Get or create ticker-specific ingestion stats"""
    if ticker not in ticker_ingestion_stats:
        ticker_ingestion_stats[ticker] = {
            "company_ingested": 0,
            "industry_ingested_by_keyword": {},
            "competitor_ingested_by_keyword": {},
            "limits": {
                "company": 100,
                "industry_per_keyword": 50,
                "competitor_per_keyword": 50
            }
        }
    return ticker_ingestion_stats[ticker]

# Legacy global stats for backward compatibility (deprecated)
ingestion_stats = {
    "company_ingested": 0,
    "industry_ingested_by_keyword": {},
    "competitor_ingested_by_keyword": {},
    "value_chain_ingested_by_keyword": {},
    "limits": {
        "company": 100,
        "industry_per_keyword": 50,
        "competitor_per_keyword": 50,
        "value_chain_per_keyword": 50
    }
}

# FIXED: Ticker-specific scraping stats to prevent race conditions
ticker_scraping_stats = {}

def get_ticker_scraping_stats(ticker: str) -> dict:
    """Get or create ticker-specific scraping stats"""
    if ticker not in ticker_scraping_stats:
        ticker_scraping_stats[ticker] = {
            "company_scraped": 0,
            "industry_scraped_by_keyword": {},
            "competitor_scraped_by_keyword": {},
            "successful_scrapes": 0,
            "failed_scrapes": 0,
            "limits": {
                "company": 20,
                "industry_per_keyword": 8,
                "competitor_per_keyword": 5
            }
        }
    return ticker_scraping_stats[ticker]

# Legacy global stats for backward compatibility (deprecated)
scraping_stats = {
    "company_scraped": 0,
    "industry_scraped_by_keyword": {},
    "competitor_scraped_by_keyword": {},
    "successful_scrapes": 0,
    "failed_scrapes": 0,
    "limits": {
        "company": 20,
        "industry_per_keyword": 8,
        "competitor_per_keyword": 5
    }
}


# Global Scrapfly statistics tracking  
scrapfly_stats = {
    "requests_made": 0,
    "successful": 0,
    "failed": 0,
    "cost_estimate": 0.0,
    "by_domain": defaultdict(lambda: {"attempts": 0, "successes": 0})
}


# Enhanced scraping statistics to track all methods
enhanced_scraping_stats = {
    "total_attempts": 0,
    "requests_success": 0,
    "playwright_success": 0,
    "scrapfly_success": 0,  # ADD THIS
    "total_failures": 0,
    "by_method": {
        "requests": {"attempts": 0, "successes": 0},
        "playwright": {"attempts": 0, "successes": 0},
        "scrapfly": {"attempts": 0, "successes": 0},  # ADD THIS
    }
}

def reset_ingestion_stats():
    """Reset ingestion stats for new run"""
    global ingestion_stats
    ingestion_stats = {
        "company_ingested": 0,
        "industry_ingested_by_keyword": {},
        "competitor_ingested_by_keyword": {},
        "value_chain_ingested_by_keyword": {},
        "limits": {
            "company": 50,
            "industry_per_keyword": 25,
            "competitor_per_keyword": 25,
            "value_chain_per_keyword": 25
        }
    }

def reset_scraping_stats():
    """Reset scraping stats for new run"""
    global scraping_stats
    scraping_stats = {
        "company_scraped": 0,
        "industry_scraped_by_keyword": {},
        "competitor_scraped_by_keyword": {},
        "successful_scrapes": 0,
        "failed_scrapes": 0,
        "limits": {
            "company": 20,
            "industry_per_keyword": 8,
            "competitor_per_keyword": 5
        }
    }

def _update_ingestion_stats(category: str, keyword: str):
    """Helper to update ingestion statistics - FIXED for competitor consolidation"""
    global ingestion_stats
    
    if category == "company":
        ingestion_stats["company_ingested"] += 1
        LOG.info(f"INGESTION: Company {ingestion_stats['company_ingested']}/{ingestion_stats['limits']['company']}")
    
    elif category == "industry":
        if keyword not in ingestion_stats["industry_ingested_by_keyword"]:
            ingestion_stats["industry_ingested_by_keyword"][keyword] = 0
        ingestion_stats["industry_ingested_by_keyword"][keyword] += 1
        keyword_count = ingestion_stats["industry_ingested_by_keyword"][keyword]
        LOG.info(f"INGESTION: Industry '{keyword}' {keyword_count}/{ingestion_stats['limits']['industry_per_keyword']}")
    
    elif category == "competitor":
        # FIXED: Use competitor_ticker as the consolidation key, not search_keyword
        if keyword not in ingestion_stats["competitor_ingested_by_keyword"]:
            ingestion_stats["competitor_ingested_by_keyword"][keyword] = 0
        ingestion_stats["competitor_ingested_by_keyword"][keyword] += 1
        keyword_count = ingestion_stats["competitor_ingested_by_keyword"][keyword]
        LOG.info(f"INGESTION: Competitor '{keyword}' {keyword_count}/{ingestion_stats['limits']['competitor_per_keyword']}")

def _check_ingestion_limit(category: str, keyword: str) -> bool:
    """Check if we can ingest more articles for this category/keyword - FIXED for competitor consolidation"""
    global ingestion_stats
    
    if category == "company":
        return ingestion_stats["company_ingested"] < ingestion_stats["limits"]["company"]
    
    elif category == "industry":
        keyword_count = ingestion_stats["industry_ingested_by_keyword"].get(keyword, 0)
        return keyword_count < ingestion_stats["limits"]["industry_per_keyword"]
    
    elif category == "competitor":
        # FIXED: Use competitor_ticker as the consolidation key, not search_keyword
        keyword_count = ingestion_stats["competitor_ingested_by_keyword"].get(keyword, 0)
        return keyword_count < ingestion_stats["limits"]["competitor_per_keyword"]
    
    return False

# Global tracking for domain access timing
domain_last_accessed = {}
last_scraped_domain = None

# Global Playwright statistics tracking
playwright_stats = {
    "attempted": 0,
    "successful": 0,
    "failed": 0,
    "by_domain": defaultdict(lambda: {"attempts": 0, "successes": 0})
}

def log_playwright_stats():
    """Log current Playwright performance statistics"""
    if playwright_stats["attempted"] == 0:
        return
    
    success_rate = (playwright_stats["successful"] / playwright_stats["attempted"]) * 100
    LOG.info(f"PLAYWRIGHT STATS: {success_rate:.1f}% success rate ({playwright_stats['successful']}/{playwright_stats['attempted']})")
    
    # Log top performing and failing domains
    domain_stats = playwright_stats["by_domain"]
    if domain_stats:
        successful_domains = [(domain, stats) for domain, stats in domain_stats.items() if stats["successes"] > 0]
        failed_domains = [(domain, stats) for domain, stats in domain_stats.items() if stats["successes"] == 0 and stats["attempts"] > 0]
        
        if successful_domains:
            LOG.info(f"PLAYWRIGHT SUCCESS DOMAINS: {len(successful_domains)} domains working")
        if failed_domains:
            LOG.info(f"PLAYWRIGHT FAILED DOMAINS: {len(failed_domains)} domains still blocked")

# ------------------------------------------------------------------------------
# Database helpers with Connection Pooling
# ------------------------------------------------------------------------------

# Global connection pool (initialized at startup)
DB_POOL = None

def init_connection_pool():
    """Initialize connection pool at application startup with retry logic"""
    global DB_POOL
    if DB_POOL is not None:
        LOG.warning("Connection pool already initialized")
        return

    if not DATABASE_URL:
        raise RuntimeError("DATABASE_URL not configured")

    try:
        from psycopg_pool import ConnectionPool
    except ImportError:
        LOG.error("âŒ psycopg_pool not installed! Install with: pip install psycopg[pool]")
        raise

    # Retry logic: 3 attempts with exponential backoff
    max_attempts = 3
    backoff_seconds = [2, 5, 10]  # Wait 2s, 5s, 10s between attempts

    for attempt in range(1, max_attempts + 1):
        try:
            LOG.info(f"ðŸ”„ Connection pool initialization attempt {attempt}/{max_attempts}...")

            DB_POOL = ConnectionPool(
                DATABASE_URL,
                min_size=5,      # Minimum connections to keep open
                max_size=80,     # Maximum connections (under 100 DB limit for Basic-1GB)
                timeout=30,      # Wait up to 30s for a connection
                max_idle=300,    # Close idle connections after 5 minutes
                kwargs={"row_factory": dict_row}
            )

            # Test the pool with timeout
            LOG.info("   Testing pool connection...")
            with DB_POOL.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT 1")

            LOG.info(f"âœ… Database connection pool initialized (min: 5, max: 80) on attempt {attempt}")
            return  # Success!

        except Exception as e:
            error_msg = str(e)
            LOG.error(f"âŒ Pool initialization attempt {attempt}/{max_attempts} failed: {error_msg}")

            # Clean up failed pool
            if DB_POOL is not None:
                try:
                    DB_POOL.close()
                except:
                    pass
                DB_POOL = None

            # If not last attempt, wait and retry
            if attempt < max_attempts:
                wait_time = backoff_seconds[attempt - 1]
                LOG.warning(f"â³ Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                # Last attempt failed - raise exception to fail startup
                LOG.error(f"ðŸ’¥ CRITICAL: Failed to initialize connection pool after {max_attempts} attempts")
                LOG.error(f"   Database may be down or unreachable")
                LOG.error(f"   Last error: {error_msg}")
                raise RuntimeError(f"Cannot start application without database connection pool (tried {max_attempts} times)")

def close_connection_pool():
    """Close connection pool at application shutdown"""
    global DB_POOL
    if DB_POOL is not None:
        try:
            DB_POOL.close()
            LOG.info("âœ… Database connection pool closed")
            DB_POOL = None
        except Exception as e:
            LOG.error(f"âŒ Error closing connection pool: {e}")

@contextmanager
def db():
    """
    Get database connection from pool with automatic timeouts.

    Sets per-connection timeouts to prevent zombie transactions:
    - idle_in_transaction_session_timeout: 300s (5 min) - kills idle transactions
    - lock_timeout: 30s - fails if can't acquire lock
    - statement_timeout: 120s (2 min) - kills long-running queries
    """
    global DB_POOL

    # Use pool if available (production)
    if DB_POOL is not None:
        with DB_POOL.connection() as conn:
            try:
                # Set timeouts for this connection
                with conn.cursor() as cur:
                    cur.execute("SET idle_in_transaction_session_timeout = '300000';")  # 5 min
                    cur.execute("SET lock_timeout = '30000';")  # 30 sec
                    cur.execute("SET statement_timeout = '120000';")  # 2 min

                yield conn
                conn.commit()
            except Exception:
                conn.rollback()
                raise
    else:
        # Fallback to direct connection (development/testing)
        if not DATABASE_URL:
            raise RuntimeError("DATABASE_URL not configured")
        conn = psycopg.connect(DATABASE_URL, row_factory=dict_row)
        try:
            # Set timeouts for this connection
            with conn.cursor() as cur:
                cur.execute("SET idle_in_transaction_session_timeout = '300000';")  # 5 min
                cur.execute("SET lock_timeout = '30000';")  # 30 sec
                cur.execute("SET statement_timeout = '120000';")  # 2 min

            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()

def with_deadlock_retry(max_retries=100):
    """
    Decorator to automatically retry database operations on deadlock detection.

    PostgreSQL deadlocks are transient - one transaction is automatically killed
    to break the cycle, and retrying immediately almost always succeeds.

    Args:
        max_retries: Maximum retry attempts (default 100, effectively unlimited for deadlocks)

    Usage:
        @with_deadlock_retry()
        def my_db_operation(...):
            with db() as conn:
                # database operations
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except psycopg.errors.DeadlockDetected as e:
                    if attempt < max_retries - 1:
                        # Exponential backoff capped at 1 second
                        delay = min(0.1 * (2 ** attempt), 1.0)
                        LOG.warning(
                            f"âš ï¸ Deadlock detected in {func.__name__} (attempt {attempt + 1}/{max_retries}), "
                            f"retrying in {delay:.1f}s: {str(e)[:100]}"
                        )
                        time.sleep(delay)
                        continue
                    else:
                        LOG.error(f"ðŸ’€ Deadlock in {func.__name__} failed after {max_retries} retries")
                        raise
            return None  # Should never reach here
        return wrapper
    return decorator

def ensure_schema():
    """
    FRESH DATABASE - Complete schema creation for new architecture.

    Uses PostgreSQL advisory lock to prevent concurrent initialization.
    Only ONE process can run schema init at a time.

    Scenarios handled:
    1. Fresh database: Creates full schema
    2. Existing database: Runs idempotent DDL (IF NOT EXISTS)
    3. Concurrent startups (rolling deploy): Second instance skips
    4. Schema evolution: ALTER TABLE IF NOT EXISTS handles new columns
    """
    SCHEMA_LOCK_ID = 123456  # Arbitrary unique ID for advisory lock

    LOG.info("ðŸ”„ Creating complete database schema for NEW ARCHITECTURE")

    with db() as conn:
        with conn.cursor() as cur:
            # STEP 1: Try to acquire advisory lock (non-blocking)
            LOG.info("ðŸ”’ Attempting to acquire schema initialization lock...")
            cur.execute("SELECT pg_try_advisory_lock(%s)", (SCHEMA_LOCK_ID,))
            result = cur.fetchone()
            # Handle both tuple and dict-like cursor results
            lock_acquired = result[0] if isinstance(result, tuple) else result['pg_try_advisory_lock']

            if not lock_acquired:
                # Another process is currently running schema init
                LOG.info("â³ Schema initialization already in progress by another process")
                LOG.info("â© Skipping schema init - assuming other process will complete it")
                return  # Fast exit - let the other process finish

            try:
                LOG.info("âœ… Schema lock acquired - proceeding with initialization")

                # STEP 1.5: Enable pg_trgm extension for fuzzy search (Nov 2025)
                # Used for ticker/company name suggestions on landing page
                cur.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm")
                LOG.info("âœ… pg_trgm extension enabled for fuzzy search")

                # STEP 2: Check if core schema already exists (optimization)
                cur.execute("""
                    SELECT COUNT(*) FROM information_schema.tables
                    WHERE table_name IN ('articles', 'ticker_reference', 'feeds')
                """)
                result = cur.fetchone()
                # Handle both tuple and dict-like cursor results
                existing_core_tables = result[0] if isinstance(result, tuple) else result['count']

                if existing_core_tables == 3:
                    LOG.info("âœ… Core schema exists - running idempotent DDL for completeness")
                else:
                    LOG.info("ðŸ”§ Core tables missing - creating full schema...")

                # STEP 3: Execute DDL (idempotent - safe to run multiple times)
                cur.execute("""
                    -- Articles table: ticker-agnostic content storage
                CREATE TABLE IF NOT EXISTS articles (
                    id SERIAL PRIMARY KEY,
                    url_hash VARCHAR(32) UNIQUE NOT NULL,
                    url TEXT NOT NULL,
                    resolved_url TEXT,
                    title TEXT NOT NULL,
                    description TEXT,
                    domain VARCHAR(255),
                    published_at TIMESTAMP,
                    scraped_content TEXT,
                    content_scraped_at TIMESTAMP,
                    scraping_failed BOOLEAN DEFAULT FALSE,
                    scraping_error TEXT,
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );

                -- Add quality score column to articles (Oct 2025)
                ALTER TABLE articles ADD COLUMN IF NOT EXISTS quality_score NUMERIC(3,1);

                -- Add ingestion source tracking (Jan 2025)
                ALTER TABLE articles ADD COLUMN IF NOT EXISTS ingestion_source VARCHAR(50);

                -- NOTE: ticker_reference columns (financial_*, geographic_markets, subsidiaries, upstream_*, downstream_*)
                -- are defined in the CREATE TABLE statement below. No ALTER needed.

                -- NEW ARCHITECTURE: Feeds table (category-neutral, shareable feeds)
                CREATE TABLE IF NOT EXISTS feeds (
                    id SERIAL PRIMARY KEY,
                    url VARCHAR(2048) UNIQUE NOT NULL,
                    name VARCHAR(255) NOT NULL,
                    search_keyword VARCHAR(255),
                    feed_ticker VARCHAR(10),  -- Renamed from competitor_ticker (Nov 24, 2025)
                    company_name VARCHAR(255),
                    retain_days INTEGER DEFAULT 90,
                    active BOOLEAN DEFAULT TRUE,
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );

                -- Add company_name column if it doesn't exist (migration)
                ALTER TABLE feeds ADD COLUMN IF NOT EXISTS company_name VARCHAR(255);

                -- Rename competitor_ticker to feed_ticker (Nov 24, 2025 - Issue #3)
                DO $$
                BEGIN
                    IF EXISTS (SELECT 1 FROM information_schema.columns
                              WHERE table_name = 'feeds' AND column_name = 'competitor_ticker') THEN
                        ALTER TABLE feeds RENAME COLUMN competitor_ticker TO feed_ticker;
                    END IF;
                END $$;

                -- NEW ARCHITECTURE: Ticker-Feed relationships with per-relationship categories
                CREATE TABLE IF NOT EXISTS ticker_feeds (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(10) NOT NULL,
                    feed_id INTEGER NOT NULL REFERENCES feeds(id) ON DELETE CASCADE,
                    category VARCHAR(20) NOT NULL DEFAULT 'company',
                    active BOOLEAN DEFAULT TRUE,
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW(),
                    UNIQUE(ticker, feed_id)
                );

                -- Add value_chain_type column to ticker_feeds for upstream/downstream tracking (Nov 24, 2025)
                -- MOVED FROM feeds table to ticker_feeds table to fix feed contamination
                ALTER TABLE ticker_feeds ADD COLUMN IF NOT EXISTS value_chain_type VARCHAR(10) CHECK (value_chain_type IN ('upstream', 'downstream') OR value_chain_type IS NULL);

                -- Add CHECK constraint for category (Nov 24, 2025 - Data integrity)
                DO $$
                BEGIN
                    IF NOT EXISTS (SELECT 1 FROM pg_constraint WHERE conname = 'check_category') THEN
                        ALTER TABLE ticker_feeds ADD CONSTRAINT check_category
                            CHECK (category IN ('company', 'industry', 'competitor', 'value_chain'));
                    END IF;
                END $$;

                -- Ticker-Articles relationship table (NORMALIZED - Nov 24, 2025)
                -- Removed: category, search_keyword, competitor_ticker, value_chain_type
                -- These are now derived via JOIN to ticker_feeds and feeds tables
                CREATE TABLE IF NOT EXISTS ticker_articles (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(10) NOT NULL,
                    article_id INTEGER NOT NULL REFERENCES articles(id) ON DELETE CASCADE,
                    feed_id INTEGER REFERENCES feeds(id) ON DELETE CASCADE,  -- Changed from SET NULL to CASCADE
                    sent_in_digest BOOLEAN DEFAULT FALSE,
                    ai_summary TEXT,
                    ai_model VARCHAR(50),
                    found_at TIMESTAMP DEFAULT NOW(),
                    UNIQUE(ticker, article_id)
                );

                -- Add ai_summary and ai_model columns to ticker_articles if they don't exist (for existing databases)
                ALTER TABLE ticker_articles ADD COLUMN IF NOT EXISTS ai_summary TEXT;
                ALTER TABLE ticker_articles ADD COLUMN IF NOT EXISTS ai_model VARCHAR(50);

                -- Add relevance scoring columns for industry article gate (Oct 2025)
                ALTER TABLE ticker_articles ADD COLUMN IF NOT EXISTS relevance_score NUMERIC(3,1);
                ALTER TABLE ticker_articles ADD COLUMN IF NOT EXISTS relevance_reason TEXT;
                ALTER TABLE ticker_articles ADD COLUMN IF NOT EXISTS is_rejected BOOLEAN DEFAULT FALSE;

                -- NORMALIZATION MIGRATION (Nov 24, 2025 - Issue #2 + #3)
                -- Drop denormalized columns (derived via JOIN from ticker_feeds/feeds)
                DO $$
                BEGIN
                    -- Drop category column
                    IF EXISTS (SELECT 1 FROM information_schema.columns
                              WHERE table_name = 'ticker_articles' AND column_name = 'category') THEN
                        ALTER TABLE ticker_articles DROP COLUMN category;
                    END IF;

                    -- Drop search_keyword column
                    IF EXISTS (SELECT 1 FROM information_schema.columns
                              WHERE table_name = 'ticker_articles' AND column_name = 'search_keyword') THEN
                        ALTER TABLE ticker_articles DROP COLUMN search_keyword;
                    END IF;

                    -- Drop competitor_ticker column
                    IF EXISTS (SELECT 1 FROM information_schema.columns
                              WHERE table_name = 'ticker_articles' AND column_name = 'competitor_ticker') THEN
                        ALTER TABLE ticker_articles DROP COLUMN competitor_ticker;
                    END IF;

                    -- Drop value_chain_type column
                    IF EXISTS (SELECT 1 FROM information_schema.columns
                              WHERE table_name = 'ticker_articles' AND column_name = 'value_chain_type') THEN
                        ALTER TABLE ticker_articles DROP COLUMN value_chain_type;
                    END IF;

                    -- Change feed_id foreign key from SET NULL to CASCADE
                    IF EXISTS (SELECT 1 FROM information_schema.table_constraints
                              WHERE table_name = 'ticker_articles'
                              AND constraint_name = 'ticker_articles_feed_id_fkey') THEN
                        ALTER TABLE ticker_articles DROP CONSTRAINT ticker_articles_feed_id_fkey;
                        ALTER TABLE ticker_articles ADD CONSTRAINT ticker_articles_feed_id_fkey
                            FOREIGN KEY (feed_id) REFERENCES feeds(id) ON DELETE CASCADE;
                    END IF;
                END $$;

                -- ticker_reference table - EXACT match to CSV structure
                CREATE TABLE IF NOT EXISTS ticker_reference (
                    ticker VARCHAR(20) PRIMARY KEY,
                    country VARCHAR(5),
                    company_name VARCHAR(255),
                    industry VARCHAR(255),
                    sector VARCHAR(255),
                    sub_industry VARCHAR(255),
                    exchange VARCHAR(20),
                    currency VARCHAR(3),
                    market_cap_category VARCHAR(20),
                    active BOOLEAN DEFAULT TRUE,
                    is_etf BOOLEAN DEFAULT FALSE,
                    yahoo_ticker VARCHAR(20),
                    industry_keyword_1 VARCHAR(255),
                    industry_keyword_2 VARCHAR(255),
                    industry_keyword_3 VARCHAR(255),
                    ai_generated BOOLEAN DEFAULT FALSE,
                    ai_enhanced_at TIMESTAMP,
                    competitor_1_name VARCHAR(255),
                    competitor_1_ticker VARCHAR(20),
                    competitor_2_name VARCHAR(255),
                    competitor_2_ticker VARCHAR(20),
                    competitor_3_name VARCHAR(255),
                    competitor_3_ticker VARCHAR(20),
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW(),
                    data_source VARCHAR(50) DEFAULT 'csv_import',
                    last_github_sync TIMESTAMP,
                    financial_last_price NUMERIC(15, 2),
                    financial_price_change_pct NUMERIC(10, 4),
                    financial_yesterday_return_pct NUMERIC(10, 4),
                    financial_ytd_return_pct NUMERIC(10, 4),
                    financial_market_cap NUMERIC(20, 2),
                    financial_enterprise_value NUMERIC(20, 2),
                    financial_volume NUMERIC(15, 0),
                    financial_avg_volume NUMERIC(15, 0),
                    financial_analyst_target NUMERIC(15, 2),
                    financial_analyst_range_low NUMERIC(15, 2),
                    financial_analyst_range_high NUMERIC(15, 2),
                    financial_analyst_count INTEGER,
                    financial_analyst_recommendation VARCHAR(50),
                    financial_snapshot_date DATE,
                    geographic_markets TEXT,
                    subsidiaries TEXT,
                    upstream_1_name VARCHAR(255),
                    upstream_1_ticker VARCHAR(20),
                    upstream_2_name VARCHAR(255),
                    upstream_2_ticker VARCHAR(20),
                    downstream_1_name VARCHAR(255),
                    downstream_1_ticker VARCHAR(20),
                    downstream_2_name VARCHAR(255),
                    downstream_2_ticker VARCHAR(20)
                );

                CREATE TABLE IF NOT EXISTS domain_names (
                    domain VARCHAR(255) PRIMARY KEY,
                    formal_name VARCHAR(255) NOT NULL,
                    ai_generated BOOLEAN DEFAULT FALSE,
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );

                -- Add quality and scrape tracking columns to domain_names (Oct 2025)
                ALTER TABLE domain_names ADD COLUMN IF NOT EXISTS quality_score_avg NUMERIC(3,1);
                ALTER TABLE domain_names ADD COLUMN IF NOT EXISTS quality_count INTEGER DEFAULT 0;
                ALTER TABLE domain_names ADD COLUMN IF NOT EXISTS scrape_attempts INTEGER DEFAULT 0;
                ALTER TABLE domain_names ADD COLUMN IF NOT EXISTS scrape_failures INTEGER DEFAULT 0;
                ALTER TABLE domain_names ADD COLUMN IF NOT EXISTS scrape_success_rate NUMERIC(5,2);

                -- All indexes for performance
                CREATE INDEX IF NOT EXISTS idx_articles_url_hash ON articles(url_hash);
                CREATE INDEX IF NOT EXISTS idx_articles_domain ON articles(domain);
                CREATE INDEX IF NOT EXISTS idx_articles_published_at ON articles(published_at DESC);
                CREATE INDEX IF NOT EXISTS idx_articles_ingestion_source ON articles(ingestion_source);

                CREATE INDEX IF NOT EXISTS idx_feeds_url ON feeds(url);
                CREATE INDEX IF NOT EXISTS idx_feeds_active ON feeds(active);

                CREATE INDEX IF NOT EXISTS idx_ticker_feeds_ticker ON ticker_feeds(ticker);
                CREATE INDEX IF NOT EXISTS idx_ticker_feeds_feed_id ON ticker_feeds(feed_id);
                CREATE INDEX IF NOT EXISTS idx_ticker_feeds_category ON ticker_feeds(category);
                CREATE INDEX IF NOT EXISTS idx_ticker_feeds_active ON ticker_feeds(active);

                CREATE INDEX IF NOT EXISTS idx_ticker_articles_ticker ON ticker_articles(ticker);
                CREATE INDEX IF NOT EXISTS idx_ticker_articles_article_id ON ticker_articles(article_id);
                CREATE INDEX IF NOT EXISTS idx_ticker_articles_feed_id ON ticker_articles(feed_id);
                CREATE INDEX IF NOT EXISTS idx_ticker_articles_sent_in_digest ON ticker_articles(sent_in_digest);
                CREATE INDEX IF NOT EXISTS idx_ticker_articles_found_at ON ticker_articles(found_at DESC);

                CREATE INDEX IF NOT EXISTS idx_ticker_reference_ticker ON ticker_reference(ticker);
                CREATE INDEX IF NOT EXISTS idx_ticker_reference_active ON ticker_reference(active);
                CREATE INDEX IF NOT EXISTS idx_ticker_reference_company_name ON ticker_reference(company_name);

                -- GIN indexes for fuzzy search using pg_trgm (Nov 2025)
                -- Used for ticker/company name suggestions on landing page
                CREATE INDEX IF NOT EXISTS idx_ticker_reference_ticker_trgm ON ticker_reference USING GIN (ticker gin_trgm_ops);
                CREATE INDEX IF NOT EXISTS idx_ticker_reference_company_name_trgm ON ticker_reference USING GIN (company_name gin_trgm_ops);

                -- JOB QUEUE: Batch tracking table
                CREATE TABLE IF NOT EXISTS ticker_processing_batches (
                    batch_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    status VARCHAR(20) DEFAULT 'queued' CHECK (status IN
                        ('queued', 'processing', 'completed', 'failed', 'cancelled')),
                    total_jobs INT NOT NULL,
                    completed_jobs INT DEFAULT 0,
                    failed_jobs INT DEFAULT 0,
                    created_at TIMESTAMP DEFAULT NOW(),
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    created_by VARCHAR(100) DEFAULT 'api',
                    config JSONB,
                    error_summary TEXT
                );

                -- JOB QUEUE: Individual ticker job tracking
                CREATE TABLE IF NOT EXISTS ticker_processing_jobs (
                    job_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    batch_id UUID NOT NULL REFERENCES ticker_processing_batches(batch_id) ON DELETE CASCADE,
                    ticker VARCHAR(20) NOT NULL,

                    -- Execution state
                    status VARCHAR(20) DEFAULT 'queued' CHECK (status IN
                        ('queued', 'processing', 'completed', 'failed', 'cancelled', 'timeout')),
                    phase VARCHAR(50),
                    progress INT DEFAULT 0 CHECK (progress >= 0 AND progress <= 100),

                    -- Results
                    result JSONB,
                    error_message TEXT,
                    error_stacktrace TEXT,

                    -- Retry logic
                    retry_count INT DEFAULT 0,
                    max_retries INT DEFAULT 2,
                    last_retry_at TIMESTAMP,

                    -- Resource tracking
                    worker_id VARCHAR(100),
                    memory_mb FLOAT,
                    duration_seconds FLOAT,

                    -- Audit trail
                    created_at TIMESTAMP DEFAULT NOW(),
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    last_updated TIMESTAMP DEFAULT NOW(),

                    -- Configuration
                    config JSONB,

                    -- Timeout protection
                    timeout_at TIMESTAMP
                );

                -- JOB QUEUE: Indexes for performance
                CREATE INDEX IF NOT EXISTS idx_jobs_status_queued ON ticker_processing_jobs(status, created_at)
                    WHERE status = 'queued';
                CREATE INDEX IF NOT EXISTS idx_jobs_status_processing ON ticker_processing_jobs(status, timeout_at)
                    WHERE status = 'processing';
                CREATE INDEX IF NOT EXISTS idx_jobs_batch ON ticker_processing_jobs(batch_id);
                CREATE INDEX IF NOT EXISTS idx_jobs_ticker ON ticker_processing_jobs(ticker);
                CREATE INDEX IF NOT EXISTS idx_batches_status ON ticker_processing_batches(status, created_at);

                -- EXECUTIVE SUMMARIES: Store final AI-generated summaries per ticker per date
                CREATE TABLE IF NOT EXISTS executive_summaries (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(10) NOT NULL,
                    summary_date DATE NOT NULL,
                    summary_text TEXT NOT NULL,
                    ai_provider VARCHAR(50) NOT NULL,
                    article_ids TEXT,
                    company_articles_count INTEGER,
                    industry_articles_count INTEGER,
                    competitor_articles_count INTEGER,
                    generated_at TIMESTAMP DEFAULT NOW(),

                    UNIQUE(ticker, summary_date)
                );

                CREATE INDEX IF NOT EXISTS idx_exec_summ_ticker_date ON executive_summaries(ticker, summary_date DESC);

                -- PHASE 1 EXECUTIVE SUMMARIES: Add JSONB column for structured output
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS summary_json JSONB;
                CREATE INDEX IF NOT EXISTS idx_executive_summaries_json ON executive_summaries USING GIN (summary_json);

                -- Add metadata columns for Phase 1 monitoring
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS generation_phase VARCHAR(20) DEFAULT 'phase1';
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS prompt_tokens INTEGER;
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS completion_tokens INTEGER;
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS generation_time_ms INTEGER;

                -- Add value_chain_articles_count for upstream/downstream tracking
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS value_chain_articles_count INTEGER DEFAULT 0;

                -- Add ai_models column for tracking AI models used per phase (Dec 2025)
                ALTER TABLE executive_summaries ADD COLUMN IF NOT EXISTS ai_models JSONB;

                -- Expand ai_provider to fit full model names like 'gemini-3-flash-preview' (Dec 2025)
                ALTER TABLE executive_summaries ALTER COLUMN ai_provider TYPE VARCHAR(50);

                -- TRANSCRIPT SUMMARIES: Store earnings transcript and press release summaries
                CREATE TABLE IF NOT EXISTS transcript_summaries (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(20) NOT NULL,
                    company_name VARCHAR(255),
                    report_type VARCHAR(20) NOT NULL,  -- 'transcript' or 'press_release'
                    fiscal_quarter VARCHAR(10),  -- 'Q3' or NULL for press releases (renamed from quarter Nov 24, 2025)
                    fiscal_year INTEGER,  -- 2024 or NULL for press releases (renamed from year Nov 24, 2025)
                    report_date DATE,  -- Date of transcript/PR
                    pr_title VARCHAR(500),  -- Press release title (NULL for transcripts)
                    summary_text TEXT NOT NULL,  -- Full AI-generated summary (markdown for Phase 2)
                    summary_json JSONB,  -- Structured JSON (for email generation v2)
                    prompt_version VARCHAR(10) DEFAULT 'v2',  -- Prompt version used
                    source_url TEXT,  -- FMP API URL for reference
                    ai_provider VARCHAR(20) NOT NULL,  -- 'claude' or 'openai'
                    ai_model VARCHAR(50),  -- Model name (e.g., 'claude-sonnet-4.5', 'gemini-2.5-flash')
                    generated_at TIMESTAMPTZ DEFAULT NOW(),

                    -- Job tracking
                    job_id VARCHAR(50),
                    processing_duration_seconds INTEGER,

                    -- UPSERT constraint: One summary per ticker+type+fiscal_quarter+fiscal_year
                    UNIQUE(ticker, report_type, fiscal_quarter, fiscal_year)
                );

                -- Migration: Rename quarter/year to fiscal_quarter/fiscal_year (Nov 24, 2025)
                DO $$
                BEGIN
                    -- Rename quarter to fiscal_quarter
                    IF EXISTS (SELECT 1 FROM information_schema.columns
                              WHERE table_name = 'transcript_summaries' AND column_name = 'quarter') THEN
                        -- Drop old constraint first
                        IF EXISTS (SELECT 1 FROM pg_constraint WHERE conname = 'transcript_summaries_unique_key') THEN
                            ALTER TABLE transcript_summaries DROP CONSTRAINT transcript_summaries_unique_key;
                        END IF;
                        -- Rename columns
                        ALTER TABLE transcript_summaries RENAME COLUMN quarter TO fiscal_quarter;
                        ALTER TABLE transcript_summaries RENAME COLUMN year TO fiscal_year;
                        -- Re-add constraint with new column names
                        ALTER TABLE transcript_summaries ADD CONSTRAINT transcript_summaries_unique_key
                            UNIQUE(ticker, report_type, fiscal_quarter, fiscal_year);
                    ELSIF NOT EXISTS (SELECT 1 FROM pg_constraint WHERE conname = 'transcript_summaries_unique_key') THEN
                        -- Fresh install: add constraint
                        ALTER TABLE transcript_summaries ADD CONSTRAINT transcript_summaries_unique_key
                            UNIQUE(ticker, report_type, fiscal_quarter, fiscal_year);
                    END IF;
                END $$;

                -- Migration: Add ai_model column if it doesn't exist
                DO $$ BEGIN
                    ALTER TABLE transcript_summaries ADD COLUMN ai_model VARCHAR(50);
                EXCEPTION
                    WHEN duplicate_column THEN NULL;  -- Column already exists, ignore
                END $$;

                -- Migration: Add summary_json column if it doesn't exist (Nov 2025)
                DO $$ BEGIN
                    ALTER TABLE transcript_summaries ADD COLUMN summary_json JSONB;
                EXCEPTION
                    WHEN duplicate_column THEN NULL;  -- Column already exists, ignore
                END $$;

                -- Migration: Add prompt_version column if it doesn't exist (Nov 2025)
                DO $$ BEGIN
                    ALTER TABLE transcript_summaries ADD COLUMN prompt_version TEXT;
                EXCEPTION
                    WHEN duplicate_column THEN NULL;  -- Column already exists, ignore
                END $$;

                CREATE INDEX IF NOT EXISTS idx_transcript_summaries_ticker ON transcript_summaries(ticker);
                CREATE INDEX IF NOT EXISTS idx_transcript_summaries_quarter ON transcript_summaries(fiscal_quarter, fiscal_year);
                CREATE INDEX IF NOT EXISTS idx_transcript_summaries_type ON transcript_summaries(report_type);
                CREATE INDEX IF NOT EXISTS idx_transcript_summaries_date ON transcript_summaries(report_date DESC);

                -- Migration: Add constraint to ensure transcript_summaries only stores transcripts
                DO $$ BEGIN
                    ALTER TABLE transcript_summaries ADD CONSTRAINT transcript_summaries_type_check CHECK (report_type = 'transcript');
                EXCEPTION
                    WHEN duplicate_object THEN NULL;  -- Constraint already exists, ignore
                END $$;

                -- Migration: Ensure transcripts have fiscal_quarter and fiscal_year (not NULL)
                DO $$ BEGIN
                    ALTER TABLE transcript_summaries ADD CONSTRAINT transcript_summaries_quarter_year_check CHECK (fiscal_quarter IS NOT NULL AND fiscal_year IS NOT NULL);
                EXCEPTION
                    WHEN duplicate_object THEN NULL;  -- Constraint already exists, ignore
                END $$;

                -- NOTE: press_releases table DROPPED (Dec 2025) - replaced by company_releases
                -- Data migrated, table no longer needed

                -- SEC 8-K FILINGS: Store AI-generated 8-K summaries from SEC Edgar
                CREATE TABLE IF NOT EXISTS sec_8k_filings (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(20) NOT NULL,
                    company_name VARCHAR(255),

                    -- SEC Edgar metadata
                    cik VARCHAR(20),                         -- SEC CIK number (cached for future lookups)
                    accession_number VARCHAR(50) NOT NULL,   -- SEC unique ID (e.g., "0001193125-25-012345")

                    -- Filing identification
                    filing_date DATE NOT NULL,
                    filing_title VARCHAR(200) NOT NULL,      -- "Results of Operations | Apple announces Q1 2024 results"
                    item_codes VARCHAR(100),                 -- "2.02, 9.01"
                    sec_html_url TEXT NOT NULL,              -- Direct link to 8-K HTML on SEC.gov

                    -- Exhibit-level tracking (Dec 2025 - supports multiple exhibits per 8-K)
                    exhibit_number VARCHAR(20),              -- '99.1', '99.2', 'MAIN', etc.
                    exhibit_description TEXT,                -- Description from filing documents page
                    exhibit_type VARCHAR(20),                -- 'EX-99.1', 'EXHIBIT 10.1', etc.

                    -- Raw content (Exhibit 99.1 or main body)
                    raw_content TEXT,                        -- Raw extracted content before AI processing
                    char_count INTEGER,                      -- Character count of raw_content

                    -- AI Summary
                    summary_text TEXT NOT NULL,              -- Gemini-generated summary (noise filtered, 90% retention)

                    -- AI metadata
                    ai_provider VARCHAR(20) NOT NULL,        -- 'gemini'
                    ai_model VARCHAR(50),                    -- 'gemini-2.5-flash'

                    -- Job tracking
                    job_id VARCHAR(50),
                    processing_duration_seconds INTEGER,

                    -- Future automation support
                    monitored BOOLEAN DEFAULT FALSE,
                    last_checked_at TIMESTAMPTZ,

                    -- Timestamps
                    generated_at TIMESTAMPTZ DEFAULT NOW(),

                    -- Unique constraint: ticker + accession + exhibit (allows multiple exhibits per 8-K)
                    CONSTRAINT sec_8k_unique UNIQUE(ticker, accession_number, exhibit_number)
                );

                -- Migration: Add new columns to existing sec_8k_filings table (Dec 2025)
                DO $$ BEGIN
                    ALTER TABLE sec_8k_filings ADD COLUMN exhibit_number VARCHAR(20);
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE sec_8k_filings ADD COLUMN exhibit_description TEXT;
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE sec_8k_filings ADD COLUMN exhibit_type VARCHAR(20);
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE sec_8k_filings ADD COLUMN char_count INTEGER;
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                -- Migration: Widen exhibit_number and exhibit_type columns if they exist but are too small (Dec 2025)
                ALTER TABLE sec_8k_filings ALTER COLUMN exhibit_number TYPE VARCHAR(20);
                ALTER TABLE sec_8k_filings ALTER COLUMN exhibit_type TYPE VARCHAR(20);

                -- Migration: Update UNIQUE constraint to include exhibit_number (Dec 2025)
                -- This allows multiple exhibits per 8-K filing
                DO $$
                BEGIN
                    -- Drop old constraint if it exists with only 2 columns (ticker + accession)
                    -- and re-add with 3 columns (ticker + accession + exhibit_number)
                    IF EXISTS (
                        SELECT 1 FROM pg_constraint
                        WHERE conname = 'sec_8k_unique'
                        AND array_length(conkey, 1) = 2
                    ) THEN
                        ALTER TABLE sec_8k_filings DROP CONSTRAINT sec_8k_unique;
                        ALTER TABLE sec_8k_filings ADD CONSTRAINT sec_8k_unique UNIQUE(ticker, accession_number, exhibit_number);
                    END IF;
                END $$;

                -- Indexes for sec_8k_filings
                CREATE INDEX IF NOT EXISTS idx_8k_ticker ON sec_8k_filings(ticker);
                CREATE INDEX IF NOT EXISTS idx_8k_date ON sec_8k_filings(filing_date DESC);
                CREATE INDEX IF NOT EXISTS idx_8k_cik ON sec_8k_filings(cik);
                CREATE INDEX IF NOT EXISTS idx_8k_ticker_date ON sec_8k_filings(ticker, filing_date DESC);

                -- COMPANY RELEASES: Store AI-generated summaries from 8-K and FMP press releases
                CREATE TABLE IF NOT EXISTS company_releases (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(20) NOT NULL,
                    company_name VARCHAR(255),
                    release_type VARCHAR(50) NOT NULL,  -- '8k' or 'fmp_press_release'
                    filing_date DATE NOT NULL,
                    report_title TEXT,  -- From JSON metadata.report_title (TEXT for long titles)

                    -- Source tracking
                    source_id INTEGER,  -- Links to sec_8k_filings.id or press_releases.id
                    source_type VARCHAR(50),  -- '8k_exhibit' or 'fmp_press_release'

                    -- AI-generated content
                    summary_json JSONB NOT NULL,  -- Full Gemini JSON output
                    summary_html TEXT,  -- Pre-rendered HTML for display
                    summary_markdown TEXT,  -- Markdown text for article integration

                    -- AI metadata
                    ai_provider VARCHAR(20) NOT NULL DEFAULT 'gemini',
                    ai_model VARCHAR(50),
                    processing_duration_seconds INTEGER,
                    token_count_input INTEGER,
                    token_count_output INTEGER,

                    -- Job tracking
                    job_id VARCHAR(50),
                    generated_at TIMESTAMPTZ DEFAULT NOW(),

                    -- Fiscal period (for 8-K earnings releases only, NULL for FMP)
                    fiscal_year INTEGER,        -- 2024, 2025, etc. NULL for non-earnings or FMP
                    fiscal_quarter VARCHAR(5),  -- 'Q1', 'Q2', 'Q3', 'Q4', 'FY', NULL for non-earnings or FMP

                    -- 8-K specific fields (NULL for FMP)
                    exhibit_number VARCHAR(10),  -- '99.1', '99.2', etc. NULL for FMP
                    item_codes VARCHAR(100),     -- '2.02', '8.01', etc. NULL for FMP

                    -- Unique constraint
                    CONSTRAINT company_releases_unique UNIQUE(ticker, filing_date, report_title)
                );

                -- Migration: Add new columns to existing company_releases table
                -- This ensures existing production databases get the new columns
                DO $$ BEGIN
                    ALTER TABLE company_releases ADD COLUMN fiscal_year INTEGER;
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE company_releases ADD COLUMN fiscal_quarter VARCHAR(5);
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE company_releases ADD COLUMN exhibit_number VARCHAR(10);
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE company_releases ADD COLUMN item_codes VARCHAR(100);
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                DO $$ BEGIN
                    ALTER TABLE company_releases ADD COLUMN summary_markdown TEXT;
                EXCEPTION
                    WHEN duplicate_column THEN NULL;
                END $$;

                -- Indexes for company_releases (created AFTER migrations)
                CREATE INDEX IF NOT EXISTS idx_company_releases_ticker ON company_releases(ticker);
                CREATE INDEX IF NOT EXISTS idx_company_releases_filing_date ON company_releases(filing_date DESC);
                CREATE INDEX IF NOT EXISTS idx_company_releases_type ON company_releases(release_type);

                -- Fiscal period index (partial - only 8-K earnings, matches sec_filings pattern)
                CREATE INDEX IF NOT EXISTS idx_company_releases_fiscal_period
                ON company_releases(ticker, fiscal_year DESC, fiscal_quarter DESC)
                WHERE fiscal_year IS NOT NULL;

                -- COMPANY PROFILES: Store AI-generated 10-K company profiles
                -- SEC FILINGS: Unified table for 10-K, 10-Q, and investor presentations
                CREATE TABLE IF NOT EXISTS sec_filings (
                    id SERIAL PRIMARY KEY,
                    ticker VARCHAR(20) NOT NULL,

                    -- Filing identification
                    filing_type VARCHAR(20) NOT NULL,      -- '10-K', '10-Q', 'PRESENTATION'
                    fiscal_year INTEGER,                   -- Required for 10-K/10-Q, optional for presentations
                    fiscal_quarter VARCHAR(5),             -- 'Q1', 'Q2', 'Q3', 'Q4', NULL for 10-K/annual presentations
                    filing_date DATE,
                    period_end_date DATE,                  -- Actual quarter/year end date
                    presentation_date DATE,                -- Date of presentation (for PRESENTATION filing_type)

                    -- Content (same structure for all filing types)
                    profile_markdown TEXT NOT NULL,
                    profile_summary TEXT,                  -- AI-generated executive summary
                    key_metrics JSONB,

                    -- Source tracking
                    source_type VARCHAR(20),               -- 'fmp_sec', 'file_upload', 'gemini_multimodal'
                    source_file VARCHAR(500),              -- Original filename or 'SEC.gov via FMP'
                    sec_html_url TEXT,                     -- SEC.gov direct link (for 10-K/10-Q)

                    -- Company info (denormalized for convenience)
                    company_name VARCHAR(255),
                    industry VARCHAR(255),

                    -- AI metadata
                    ai_provider VARCHAR(20),               -- 'gemini', 'claude', 'openai'
                    ai_model VARCHAR(100),                 -- 'gemini-2.5-flash', 'gemini-2.5-pro', etc.
                    generation_time_seconds INTEGER,
                    token_count_input INTEGER,
                    token_count_output INTEGER,

                    -- Presentation-specific fields (NULL for 10-K/10-Q)
                    presentation_title VARCHAR(500),
                    presentation_type VARCHAR(50),         -- 'earnings', 'investor_day', 'analyst_day', 'conference'
                    page_count INTEGER,
                    file_size_bytes BIGINT,

                    -- Status
                    status VARCHAR(50) DEFAULT 'active',   -- 'active', 'stale', 'error'
                    error_message TEXT,
                    generated_at TIMESTAMPTZ DEFAULT NOW()
                );

                -- Indexes for performance
                CREATE INDEX IF NOT EXISTS idx_sec_filings_ticker ON sec_filings(ticker);
                CREATE INDEX IF NOT EXISTS idx_sec_filings_type ON sec_filings(filing_type);
                CREATE INDEX IF NOT EXISTS idx_sec_filings_ticker_type ON sec_filings(ticker, filing_type);
                CREATE INDEX IF NOT EXISTS idx_sec_filings_period ON sec_filings(fiscal_year DESC, fiscal_quarter DESC);
                CREATE INDEX IF NOT EXISTS idx_sec_filings_status ON sec_filings(status);
                CREATE INDEX IF NOT EXISTS idx_sec_filings_presentation_date ON sec_filings(presentation_date DESC);

                -- Partial UNIQUE constraint for 10-K and 10-Q filings
                CREATE UNIQUE INDEX IF NOT EXISTS uniq_sec_filings_10k_10q
                    ON sec_filings(ticker, filing_type, fiscal_year, fiscal_quarter)
                    WHERE filing_type IN ('10-K', '10-Q');

                -- Partial UNIQUE constraint for investor presentations
                CREATE UNIQUE INDEX IF NOT EXISTS uniq_sec_filings_presentations
                    ON sec_filings(ticker, filing_type, presentation_date, presentation_type)
                    WHERE filing_type = 'PRESENTATION';

                -- BACKWARD COMPATIBILITY VIEW: Maps old company_profiles queries to sec_filings
                -- This allows existing code to work during migration period
                -- Usage: SELECT * FROM company_profiles WHERE ticker='AAPL'
                CREATE OR REPLACE VIEW company_profiles AS
                SELECT
                    id,
                    ticker,
                    company_name,
                    industry,
                    fiscal_year,
                    filing_date,
                    profile_markdown,
                    profile_summary,
                    key_metrics,
                    source_file,
                    ai_provider,
                    ai_model AS gemini_model,              -- Alias for backward compatibility
                    NULL::INTEGER AS thinking_budget,      -- Deprecated field (always NULL)
                    generation_time_seconds,
                    token_count_input,
                    token_count_output,
                    status,
                    error_message,
                    generated_at
                FROM sec_filings
                WHERE filing_type = '10-K';

                -- NOTE: parsed_press_releases table DROPPED (Dec 2025) - replaced by company_releases
                -- Data migrated, table no longer needed

                -- ============================================================
                -- USERS TABLE (Dec 2025 - replaces beta_users)
                -- One row per user, email is UNIQUE
                -- ============================================================
                CREATE TABLE IF NOT EXISTS users (
                    id SERIAL PRIMARY KEY,
                    email VARCHAR(255) UNIQUE NOT NULL,
                    name VARCHAR(255) NOT NULL,
                    user_type VARCHAR(20) NOT NULL DEFAULT 'beta',  -- 'admin', 'beta', 'paid'
                    ticker_limit INTEGER DEFAULT 3,                  -- NULL = unlimited (for admin)
                    status VARCHAR(50) NOT NULL DEFAULT 'pending',   -- 'pending', 'active', 'paused', 'cancelled'
                    terms_version VARCHAR(10) DEFAULT '1.0',
                    terms_accepted_at TIMESTAMPTZ,
                    privacy_version VARCHAR(10) DEFAULT '1.0',
                    privacy_accepted_at TIMESTAMPTZ,
                    cancelled_at TIMESTAMPTZ,                        -- When user unsubscribed (NULL = never)
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_at TIMESTAMPTZ DEFAULT NOW()
                );

                CREATE INDEX IF NOT EXISTS idx_users_status ON users(status);
                CREATE INDEX IF NOT EXISTS idx_users_created_at ON users(created_at DESC);
                CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);
                CREATE INDEX IF NOT EXISTS idx_users_user_type ON users(user_type);

                -- ============================================================
                -- USER TICKERS TABLE (many-to-many: user can have multiple tickers)
                -- ============================================================
                CREATE TABLE IF NOT EXISTS user_tickers (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
                    ticker VARCHAR(10) NOT NULL,
                    added_at TIMESTAMPTZ DEFAULT NOW(),
                    UNIQUE(user_id, ticker)  -- No duplicate tickers per user
                );

                CREATE INDEX IF NOT EXISTS idx_user_tickers_user_id ON user_tickers(user_id);
                CREATE INDEX IF NOT EXISTS idx_user_tickers_ticker ON user_tickers(ticker);

                -- ============================================================
                -- UNSUBSCRIBE TOKENS TABLE (linked to user_id, not email)
                -- ============================================================
                CREATE TABLE IF NOT EXISTS unsubscribe_tokens (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                    user_email VARCHAR(255),  -- Kept for backward compat during migration
                    token VARCHAR(64) UNIQUE NOT NULL,
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    used_at TIMESTAMPTZ,
                    ip_address VARCHAR(45),
                    user_agent TEXT
                );

                -- ============================================================
                -- MIGRATION: Add user_id column to unsubscribe_tokens if missing
                -- MUST RUN BEFORE INDEX CREATION (existing tables may lack this column)
                -- ============================================================
                DO $$
                BEGIN
                    IF NOT EXISTS (SELECT 1 FROM information_schema.columns
                                 WHERE table_name='unsubscribe_tokens' AND column_name='user_id') THEN
                        ALTER TABLE unsubscribe_tokens ADD COLUMN user_id INTEGER REFERENCES users(id) ON DELETE CASCADE;
                    END IF;
                END $$;

                -- Indexes for unsubscribe_tokens (user_id column now guaranteed to exist)
                CREATE INDEX IF NOT EXISTS idx_unsubscribe_tokens_token ON unsubscribe_tokens(token);
                CREATE INDEX IF NOT EXISTS idx_unsubscribe_tokens_user_id ON unsubscribe_tokens(user_id);
                CREATE INDEX IF NOT EXISTS idx_unsubscribe_tokens_used ON unsubscribe_tokens(used_at) WHERE used_at IS NULL;

                -- ============================================================
                -- MIGRATION: Add cancelled_at column to users if missing
                -- ============================================================
                DO $$
                BEGIN
                    IF NOT EXISTS (SELECT 1 FROM information_schema.columns
                                 WHERE table_name='users' AND column_name='cancelled_at') THEN
                        ALTER TABLE users ADD COLUMN cancelled_at TIMESTAMPTZ;
                    END IF;
                END $$;

                -- ============================================================
                -- MIGRATION: Drop old beta_users table if it exists
                -- (Only after data has been migrated - safe because user cleared all test accounts)
                -- ============================================================
                DROP TABLE IF EXISTS beta_users CASCADE;

                -- Daily Email Queue: Workflow for beta user emails
                CREATE TABLE IF NOT EXISTS email_queue (
                    ticker VARCHAR(10) PRIMARY KEY,
                    company_name VARCHAR(255),
                    recipients TEXT[],  -- PostgreSQL array of email addresses
                    email_html TEXT,    -- Contains {{UNSUBSCRIBE_TOKEN}} placeholder
                    email_subject VARCHAR(500),
                    article_count INTEGER,
                    status VARCHAR(50) NOT NULL DEFAULT 'queued',
                    previous_status VARCHAR(50),  -- Tracks status before cancellation (for smart restore)
                    error_message TEXT,
                    is_production BOOLEAN DEFAULT TRUE,
                    heartbeat TIMESTAMPTZ,
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_at TIMESTAMPTZ DEFAULT NOW(),
                    sent_at TIMESTAMPTZ,
                    CONSTRAINT valid_status CHECK (status IN ('queued', 'processing', 'ready', 'failed', 'sent', 'cancelled'))
                );

                CREATE INDEX IF NOT EXISTS idx_email_queue_status ON email_queue(status);
                CREATE INDEX IF NOT EXISTS idx_email_queue_sent_at ON email_queue(sent_at);
                CREATE INDEX IF NOT EXISTS idx_email_queue_created_at ON email_queue(created_at);
                CREATE INDEX IF NOT EXISTS idx_email_queue_is_production ON email_queue(is_production);
                CREATE INDEX IF NOT EXISTS idx_email_queue_heartbeat ON email_queue(heartbeat) WHERE status = 'processing';

                -- Add Email #1 and Email #2 snapshot columns (for admin dashboard previews)
                ALTER TABLE email_queue ADD COLUMN IF NOT EXISTS email_1_html TEXT;
                ALTER TABLE email_queue ADD COLUMN IF NOT EXISTS email_2_html TEXT;

                -- System Configuration: UI-configurable settings
                CREATE TABLE IF NOT EXISTS system_config (
                    key VARCHAR(100) PRIMARY KEY,
                    value TEXT NOT NULL,
                    description TEXT,
                    updated_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_by VARCHAR(100)
                );

                -- Initialize default lookback window (1 day = 1440 minutes)
                INSERT INTO system_config (key, value, description, updated_by)
                VALUES ('lookback_minutes', '1440', 'Article lookback window in minutes (1 day default)', 'system')
                ON CONFLICT (key) DO NOTHING;

                CREATE INDEX IF NOT EXISTS idx_system_config_key ON system_config(key);

                -- ============================================================
                -- DAILY VS WEEKLY REPORTS (Nov 20, 2025)
                -- ============================================================

                -- Add lookback window settings for daily/weekly reports
                INSERT INTO system_config (key, value, description, updated_by)
                VALUES ('daily_lookback_minutes', '1440', 'Lookback window for daily reports (Tuesday-Sunday)', 'system')
                ON CONFLICT (key) DO NOTHING;

                INSERT INTO system_config (key, value, description, updated_by)
                VALUES ('weekly_lookback_minutes', '10080', 'Lookback window for weekly reports (Monday)', 'system')
                ON CONFLICT (key) DO NOTHING;

                -- Phase 1.5 Known Information Filter toggle
                INSERT INTO system_config (key, value, description, updated_by)
                VALUES ('phase_1_5_enabled', 'false', 'Enable Phase 1.5 Known Information Filter to remove claims already in SEC filings', 'system')
                ON CONFLICT (key) DO NOTHING;

                -- Add report_type and summary_date columns to email_queue
                ALTER TABLE email_queue
                ADD COLUMN IF NOT EXISTS report_type VARCHAR(10) DEFAULT 'daily';

                ALTER TABLE email_queue
                ADD COLUMN IF NOT EXISTS summary_date DATE;

                -- Create index on report_type for faster filtering
                CREATE INDEX IF NOT EXISTS idx_email_queue_report_type ON email_queue(report_type);

                -- ============================================================
                -- SCHEDULER SYSTEM (Nov 26, 2025)
                -- Timezone-aware scheduling with admin UI configuration
                -- ============================================================

                -- Schedule configuration per day of week
                -- Times stored in Toronto timezone (auto-handles EST/EDT)
                CREATE TABLE IF NOT EXISTS schedule_config (
                    day_of_week INTEGER PRIMARY KEY,  -- 0=Monday, 6=Sunday (Python weekday())
                    report_type VARCHAR(10) NOT NULL DEFAULT 'none',  -- 'daily', 'weekly', 'none'
                    process_time TIME,  -- When to start processing (NULL if report_type='none')
                    send_time TIME,     -- When to send emails (NULL if report_type='none')
                    cleanup_offset_minutes INTEGER DEFAULT 60,  -- Minutes before process_time
                    filings_offset_minutes INTEGER DEFAULT 60,  -- Minutes before process_time
                    updated_at TIMESTAMPTZ DEFAULT NOW(),
                    CONSTRAINT schedule_config_report_type_check CHECK (report_type IN ('daily', 'weekly', 'none'))
                );

                -- Insert default schedule configuration
                -- Monday: Weekly report at 2am, send at 7:30am
                -- Tue-Fri: Daily report at 7am, send at 8:30am
                -- Sat-Sun: No processing
                INSERT INTO schedule_config (day_of_week, report_type, process_time, send_time, cleanup_offset_minutes, filings_offset_minutes)
                VALUES
                    (0, 'weekly', '02:00', '07:30', 60, 60),  -- Monday
                    (1, 'daily',  '07:00', '08:30', 60, 60),  -- Tuesday
                    (2, 'daily',  '07:00', '08:30', 60, 60),  -- Wednesday
                    (3, 'daily',  '07:00', '08:30', 60, 60),  -- Thursday
                    (4, 'daily',  '07:00', '08:30', 60, 60),  -- Friday
                    (5, 'none',   NULL,    NULL,    60, 60),  -- Saturday
                    (6, 'none',   NULL,    NULL,    60, 60)   -- Sunday
                ON CONFLICT (day_of_week) DO NOTHING;

                CREATE INDEX IF NOT EXISTS idx_schedule_config_day ON schedule_config(day_of_week);
                """)

                LOG.info("âœ… Complete database schema created successfully with NEW ARCHITECTURE + JOB QUEUE + BETA USERS + DAILY/WEEKLY REPORTS + SCHEDULER")

            except Exception as e:
                # If DDL fails, rollback to clear the aborted transaction state
                # This allows us to release the advisory lock in finally block
                LOG.error(f"âŒ Schema DDL failed: {e}")
                conn.rollback()
                raise
            finally:
                # STEP 4: CRITICAL - Always release the advisory lock
                # Note: Advisory locks are connection-level, not transaction-level
                # We must release before connection returns to pool
                try:
                    cur.execute("SELECT pg_advisory_unlock(%s)", (SCHEMA_LOCK_ID,))
                    LOG.info("ðŸ”“ Schema initialization lock released")
                except Exception as unlock_error:
                    # If unlock fails (shouldn't happen after rollback), log but don't mask original error
                    LOG.warning(f"âš ï¸ Failed to release schema lock (non-fatal): {unlock_error}")

# Helper Functions for New Schema
def insert_article_if_new(url_hash: str, url: str, title: str, description: str,
                          domain: str, published_at: datetime, resolved_url: str = None,
                          ingestion_source: str = None) -> Optional[int]:
    """Insert article if it doesn't exist, return article_id

    Args:
        ingestion_source: Source of ingestion ('daily_workflow', 'hourly_alert', 'test_run', etc.)
                         Only set on first insertion, never overwritten on duplicates.
    """
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            INSERT INTO articles (url_hash, url, resolved_url, title, description, domain, published_at, ingestion_source)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (url_hash) DO UPDATE SET updated_at = NOW()
            RETURNING id
        """, (url_hash, url, resolved_url, title, description, domain, published_at, ingestion_source))
        result = cur.fetchone()

        if result:
            return result['id']
        else:
            # Handle UPDATE case - RETURNING id is NULL for ON CONFLICT DO UPDATE
            cur.execute("SELECT id FROM articles WHERE url_hash = %s", (url_hash,))
            result = cur.fetchone()
            return result['id'] if result else None

@with_deadlock_retry()
def link_article_to_ticker(article_id: int, ticker: str, feed_id: int) -> None:
    """Create relationship between article and ticker (NORMALIZED - Nov 24, 2025)

    All metadata (category, search_keyword, feed_ticker, value_chain_type) is derived
    via JOIN to ticker_feeds and feeds tables using feed_id.

    Args:
        article_id: Article ID
        ticker: Ticker symbol
        feed_id: Feed ID (links to feeds + ticker_feeds for metadata)
    """
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            INSERT INTO ticker_articles (ticker, article_id, feed_id)
            VALUES (%s, %s, %s)
            ON CONFLICT (ticker, article_id) DO UPDATE SET
                feed_id = EXCLUDED.feed_id
        """, (ticker, article_id, feed_id))

@with_deadlock_retry()
def update_article_content(article_id: int, scraped_content: str = None, ai_summary: str = None,
                          ai_model: str = None, scraping_failed: bool = False, scraping_error: str = None) -> None:
    """Update article with scraped content and error status (ai_summary/ai_model params ignored, kept for backward compatibility)"""
    with db() as conn, conn.cursor() as cur:
        updates = []
        params = []

        if scraped_content is not None:
            updates.append("scraped_content = %s")
            params.append(scraped_content)
            updates.append("content_scraped_at = NOW()")

        # NOTE: ai_summary and ai_model now stored in ticker_articles (POV-specific)
        # These parameters kept for backward compatibility but ignored

        if scraping_failed:
            updates.append("scraping_failed = %s")
            params.append(scraping_failed)

        if scraping_error is not None:
            updates.append("scraping_error = %s")
            params.append(scraping_error)

        updates.append("updated_at = NOW()")
        params.append(article_id)

        if updates:
            cur.execute(f"""
                UPDATE articles SET {', '.join(updates)} WHERE id = %s
            """, params)

@with_deadlock_retry()
def update_domain_quality_stats(domain: str) -> None:
    """Recalculate domain quality stats from articles table"""
    if not domain:
        return

    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            UPDATE domain_names SET
                quality_count = (
                    SELECT COUNT(*)
                    FROM articles
                    WHERE domain = %s AND quality_score IS NOT NULL
                ),
                quality_score_avg = (
                    SELECT AVG(quality_score)
                    FROM articles
                    WHERE domain = %s AND quality_score IS NOT NULL
                ),
                updated_at = NOW()
            WHERE domain = %s
        """, (domain, domain, domain))

@with_deadlock_retry()
def track_scrape_attempt(domain: str, success: bool) -> None:
    """Track scrape attempt and update domain stats"""
    if not domain:
        return

    with db() as conn, conn.cursor() as cur:
        # Increment attempts
        cur.execute("""
            UPDATE domain_names
            SET scrape_attempts = scrape_attempts + 1,
                scrape_failures = scrape_failures + CASE WHEN %s THEN 0 ELSE 1 END,
                scrape_success_rate =
                    CASE
                        WHEN (scrape_attempts + 1) > 0
                        THEN ((scrape_attempts - scrape_failures + CASE WHEN %s THEN 1 ELSE 0 END) * 100.0 / (scrape_attempts + 1))
                        ELSE 0
                    END,
                updated_at = NOW()
            WHERE domain = %s
        """, (success, success, domain))

def update_ticker_article_summary(ticker: str, article_id: int, ai_summary: str, ai_model: str, quality_score: float = None, domain: str = None) -> None:
    """Update ticker-specific AI summary and article quality score"""
    with db() as conn, conn.cursor() as cur:
        # Update ticker_articles with AI summary
        cur.execute("""
            UPDATE ticker_articles
            SET ai_summary = %s, ai_model = %s
            WHERE ticker = %s AND article_id = %s
        """, (ai_summary, ai_model, ticker, article_id))

        # Update articles with quality score
        if quality_score is not None:
            cur.execute("""
                UPDATE articles
                SET quality_score = %s
                WHERE id = %s
            """, (quality_score, article_id))

            # Update domain aggregation stats
            if domain:
                update_domain_quality_stats(domain)

@with_deadlock_retry()
def save_executive_summary(
    ticker: str,
    summary_text: str,
    ai_provider: str,
    article_ids: List[int],
    company_count: int,
    industry_count: int,
    competitor_count: int,
    value_chain_count: int = 0,  # NEW: Value chain articles count
    summary_json: Dict = None,  # NEW: Structured JSON (Phase 1)
    prompt_tokens: int = 0,     # NEW: Metadata
    completion_tokens: int = 0, # NEW: Metadata
    generation_time_ms: int = 0, # NEW: Metadata
    ai_models: Dict = None      # NEW: AI models used per phase
) -> None:
    """
    Store/update executive summary for ticker on current date.
    Overwrites if run multiple times same day.

    Args:
        ticker: Target company ticker (e.g., "NVDA")
        summary_text: Generated executive summary (JSON string for Phase 1)
        ai_provider: "claude" or "openai" (legacy field)
        article_ids: List of article IDs included in summary
        company_count: Number of company articles analyzed
        industry_count: Number of industry articles analyzed
        competitor_count: Number of competitor articles analyzed
        value_chain_count: Number of value chain articles analyzed (upstream/downstream)
        summary_json: Structured JSON output (Phase 1) - stored in JSONB column
        prompt_tokens: Token count for prompt
        completion_tokens: Token count for completion
        generation_time_ms: Generation time in milliseconds
        ai_models: Dict with phase1, phase2, phase3 model identifiers (e.g., {"phase1": "claude-sonnet-4-5-20250929", "phase2": "gemini-2.5-pro", "phase3": None})
    """
    with db() as conn, conn.cursor() as cur:
        article_ids_json = json.dumps(article_ids)
        ai_models_json = json.dumps(ai_models) if ai_models else None

        cur.execute("""
            INSERT INTO executive_summaries
                (ticker, summary_date, summary_text, summary_json, ai_provider, ai_models, article_ids,
                 company_articles_count, industry_articles_count, competitor_articles_count, value_chain_articles_count,
                 generation_phase, prompt_tokens, completion_tokens, generation_time_ms)
            VALUES (%s, CURRENT_DATE, %s, %s, %s, %s, %s, %s, %s, %s, %s, 'phase1', %s, %s, %s)
            ON CONFLICT (ticker, summary_date)
            DO UPDATE SET
                summary_text = EXCLUDED.summary_text,
                summary_json = EXCLUDED.summary_json,
                ai_provider = EXCLUDED.ai_provider,
                ai_models = EXCLUDED.ai_models,
                article_ids = EXCLUDED.article_ids,
                company_articles_count = EXCLUDED.company_articles_count,
                industry_articles_count = EXCLUDED.industry_articles_count,
                competitor_articles_count = EXCLUDED.competitor_articles_count,
                value_chain_articles_count = EXCLUDED.value_chain_articles_count,
                generation_phase = EXCLUDED.generation_phase,
                prompt_tokens = EXCLUDED.prompt_tokens,
                completion_tokens = EXCLUDED.completion_tokens,
                generation_time_ms = EXCLUDED.generation_time_ms,
                generated_at = NOW()
        """, (ticker, summary_text, json.dumps(summary_json) if summary_json else None,
              ai_provider, ai_models_json, article_ids_json,
              company_count, industry_count, competitor_count, value_chain_count,
              prompt_tokens, completion_tokens, generation_time_ms))

        models_str = f" (models: {ai_models})" if ai_models else ""
        LOG.info(f"âœ… Saved executive summary for {ticker} on {datetime.now().date()} ({ai_provider}, Phase 1, {len(article_ids)} articles{models_str})")

def get_latest_summary_date(ticker: str) -> date:
    """
    Get the summary_date of the most recently generated executive summary.
    Used by Regen and Quality Review to target the latest production run.

    Returns:
        date: The summary_date of the most recent executive summary

    Raises:
        ValueError: If no executive summary found for ticker
    """
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT summary_date
            FROM executive_summaries
            WHERE ticker = %s
            ORDER BY generated_at DESC
            LIMIT 1
        """, (ticker,))
        row = cur.fetchone()
        if not row:
            raise ValueError(f"No executive summary found for {ticker}")
        return row['summary_date']

def fetch_company_releases_for_digest(tickers: List[str], hours: int) -> List[Dict]:
    """
    Fetch company releases (8-K and FMP press releases) within lookback window.

    Args:
        tickers: List of tickers to fetch releases for
        hours: Lookback window in hours (e.g., 1440 for daily, 10080 for weekly)

    Returns:
        List of company_releases rows as dicts
    """
    if not tickers:
        return []

    cutoff_date = (datetime.now(timezone.utc) - timedelta(hours=hours)).date()

    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT id, ticker, company_name, filing_date, report_title,
                   summary_markdown, release_type, source_type
            FROM company_releases
            WHERE ticker = ANY(%s)
              AND filing_date >= %s
              AND source_type = '8k_exhibit'
            ORDER BY filing_date DESC
        """, (tickers, cutoff_date))

        return cur.fetchall()

def map_company_release_to_article_dict(release_row: Dict) -> Dict | None:
    """
    Map company_releases row to article dict format for digest emails and Phase 1.

    This is a simple field mapping - no complex conversion needed.

    Args:
        release_row: Row from company_releases table

    Returns:
        Dict with article-like structure for Phase 1 integration, or None if validation fails
    """
    try:
        # VALIDATION: Check critical fields
        release_id = release_row.get("id")
        ticker = release_row.get("ticker", "UNKNOWN")

        # Validate summary_markdown (critical - Phase 1 requires ai_summary)
        summary_markdown = release_row.get("summary_markdown")
        if not summary_markdown or not summary_markdown.strip():
            LOG.error(f"[{ticker}] Company release {release_id} has empty summary_markdown - skipping integration")
            return None

        # Fallback for NULL report_title (better UX than showing "None")
        report_title = release_row.get("report_title")
        if not report_title or not report_title.strip():
            report_title = f"{ticker} Company Release"
            LOG.warning(f"[{ticker}] Company release {release_id} has empty report_title - using fallback: {report_title}")

        # Convert filing_date to DATETIME (Phase 1 expects datetime for sorting)
        # Keep NAIVE (no timezone) to match articles.published_at format
        filing_date = release_row.get("filing_date")
        if not filing_date:
            LOG.error(f"[{ticker}] Company release {release_id} missing filing_date - skipping integration")
            return None

        # Handle both datetime and date types - keep naive like articles
        if isinstance(filing_date, datetime):
            # If already datetime, remove timezone to match articles (naive)
            filing_datetime = filing_date.replace(tzinfo=None) if filing_date.tzinfo else filing_date
        else:  # date type
            # Convert date to datetime at midnight (naive, UTC by convention)
            filing_datetime = datetime.combine(filing_date, datetime.min.time())

        # Get SEC.gov URL if available (8-K exhibits have URLs, FMP press releases don't)
        sec_html_url = release_row.get("sec_html_url")
        exhibit_number = release_row.get("exhibit_number")

        # Defensive: Strip whitespace and handle empty strings
        if exhibit_number:
            exhibit_number = exhibit_number.strip()
            if not exhibit_number:  # Empty after strip
                exhibit_number = None

        # Build descriptive domain label
        if sec_html_url and exhibit_number:
            # Special case: MAIN body fallback (when 8-K has no exhibits)
            # Use case-insensitive comparison for robustness
            if exhibit_number.upper() == 'MAIN':
                domain_label = "8-K Filing"
            else:
                domain_label = f"8-K Exhibit {exhibit_number}"
        elif sec_html_url:
            domain_label = "SEC Filing"
        else:
            domain_label = "Company Release"

        return {
            # REQUIRED by Phase 1 executive summary
            "ai_summary": summary_markdown.strip(),  # Already formatted markdown
            "title": report_title.strip(),  # "Q3 2024 Earnings Release"
            "domain": domain_label,  # "8-K Exhibit 99.1" or "Company Release"
            "published_at": filing_datetime,  # For chronological sorting

            # METADATA (useful for tracking and display)
            "id": -release_id,  # Negative integer to distinguish from articles (positive)
            "ticker": ticker,
            "company_name": release_row.get("company_name"),
            "release_type": release_row.get("release_type"),  # '8k' or 'fmp_press_release'
            "source_type": release_row.get("source_type"),  # '8k_exhibit' or 'fmp_press_release'
            "is_company_release": True,  # Flag for special badge rendering

            # OPTIONAL (Phase 1 doesn't use, but won't break if present)
            "category": "company",  # Always company category
            "resolved_url": sec_html_url,  # SEC.gov link for 8-Ks, None for FMP
            "scraped_content": None,  # Summary already generated
            "url": sec_html_url or "#",  # SEC.gov link or placeholder
        }

    except KeyError as e:
        LOG.error(f"[{ticker}] Company release {release_id} missing required field: {e} - skipping integration")
        return None
    except Exception as e:
        LOG.error(f"[{ticker}] Error mapping company release {release_id}: {e} - skipping integration")
        return None

@with_deadlock_retry()
def update_executive_summary_json(
    ticker: str,
    summary_json: Dict,
    phase: str = 'phase3',
    model: str = None
) -> bool:
    """
    Update existing executive summary with new JSON content.

    Used after Phase 3 (to save Phase 1+2+3) and after Phase 4 (to save Phase 1+2+3+4).
    Overwrites summary_text and summary_json fields in database.

    Args:
        ticker: Stock ticker
        summary_json: Complete merged JSON to save
        phase: Phase name for tracking ('phase3' or 'phase4')
        model: Model used for this phase (e.g., 'claude-sonnet-4-5', 'gemini-2.5-pro')

    Returns:
        bool: True if update successful, False otherwise
    """
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            UPDATE executive_summaries
            SET summary_text = %s,
                summary_json = %s,
                generation_phase = %s,
                ai_models = jsonb_set(COALESCE(ai_models, '{}'::jsonb), %s, %s),
                generated_at = NOW()
            WHERE ticker = %s AND summary_date = CURRENT_DATE
        """, (
            json.dumps(summary_json),   # summary_text (JSON string)
            json.dumps(summary_json),   # summary_json (JSONB)
            phase,                      # generation_phase ('phase3' or 'phase4')
            '{' + phase + '}',          # JSON path for ai_models ('{phase3}' or '{phase4}')
            json.dumps(model),          # model name as JSON string
            ticker
        ))

        rows_updated = cur.rowcount

        if rows_updated > 0:
            LOG.info(f"âœ… Updated executive summary for {ticker} with {phase} content")
            return True
        else:
            LOG.warning(f"âš ï¸ No executive summary found for {ticker} on CURRENT_DATE to update")
            return False

# ------------------------------------------------------------------------------
# Feed Name Utilities
# ------------------------------------------------------------------------------

def strip_legal_suffixes(name: str) -> str:
    """
    Strip common legal entity suffixes from company names for news feed queries.

    Used when creating Google News RSS feeds to match journalist usage patterns.
    Journalists rarely use legal suffixes (Inc., Corp., etc.) in articles.

    Examples:
        "Apple Inc." â†’ "Apple"
        "UnitedHealth Group Incorporated" â†’ "UnitedHealth Group"
        "NVIDIA Corporation" â†’ "NVIDIA"
        "JPMorgan Chase & Co." â†’ "JPMorgan Chase"

    Args:
        name: Company name (legal or brand name)

    Returns:
        Company name with legal suffix removed
    """
    if not name:
        return name

    # Common legal entity suffixes (order matters - check longer suffixes first)
    suffixes = [
        ', Incorporated', ' Incorporated',
        ', Inc.', ' Inc.', ' Inc',
        ', Corporation', ' Corporation',
        ', Corp.', ' Corp.', ' Corp',
        ', Limited', ' Limited',
        ', Ltd.', ' Ltd.', ' Ltd',
        ', LLC', ' LLC',
        ' & Co.', ' Company',
        ', Co.', ' Co.'
    ]

    result = name
    for suffix in suffixes:
        if result.endswith(suffix):
            result = result[:-len(suffix)]
            break  # Only remove first matching suffix

    return result.strip()

# NEW FEED ARCHITECTURE V2 - Category per Relationship Functions
def upsert_feed_new_architecture(url: str, name: str, search_keyword: str = None,
                                competitor_ticker: str = None, company_name: str = None, retain_days: int = 90,
                                value_chain_type: str = None) -> int:
    """Insert/update feed in new architecture - NO CATEGORY (category is per-relationship)

    Args:
        value_chain_type: DEPRECATED - Kept for backward compatibility, but ignored.
                         This field now lives in ticker_feeds table, not feeds table.
                         Moved to ticker_feeds on Nov 24, 2025 to fix feed contamination.

    Feed Immutability:
        Feeds are immutable once created. ON CONFLICT only reactivates the feed.
        This prevents contamination when multiple tickers share the same feed.
    """
    with db() as conn, conn.cursor() as cur:
        try:
            # Insert or get existing feed - Feeds are IMMUTABLE except for fixing NULL feed_ticker
            # COALESCE allows fixing feeds that were created without feed_ticker (bug fix Nov 29, 2025)
            cur.execute("""
                INSERT INTO feeds (url, name, search_keyword, feed_ticker, company_name, retain_days)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (url) DO UPDATE SET
                    active = TRUE,
                    updated_at = NOW(),
                    feed_ticker = COALESCE(feeds.feed_ticker, EXCLUDED.feed_ticker)
                RETURNING id;
            """, (url, name, search_keyword, competitor_ticker, company_name, retain_days))

            result = cur.fetchone()
            if result:
                feed_id = result['id']
                LOG.info(f"âœ… Feed upserted: {name} (ID: {feed_id})")
                return feed_id
            else:
                raise Exception(f"Failed to upsert feed: {name}")

        except Exception as e:
            # Handle race condition
            if "duplicate key" in str(e).lower() or "unique constraint" in str(e).lower():
                LOG.warning(f"âš ï¸ Concurrent feed creation detected for {url}, retrieving existing feed")
                try:
                    conn.rollback()
                    cur.execute("SELECT id FROM feeds WHERE url = %s", (url,))
                    result = cur.fetchone()
                    if result:
                        feed_id = result['id']
                        LOG.info(f"âœ… Retrieved existing feed: {name} (ID: {feed_id})")
                        return feed_id
                except Exception as recovery_error:
                    LOG.error(f"âŒ Recovery attempt failed: {recovery_error}")
            raise e

def associate_ticker_with_feed_new_architecture(ticker: str, feed_id: int, category: str,
                                               value_chain_type: str = None) -> bool:
    """Associate a ticker with a feed with SPECIFIC CATEGORY for this relationship

    Args:
        ticker: Ticker symbol
        feed_id: Feed ID to associate
        category: 'company', 'industry', 'competitor', or 'value_chain'
        value_chain_type: 'upstream', 'downstream', or None
                         - Required when category='value_chain'
                         - Should be None for other categories

    Note:
        This is where per-ticker relationship metadata is stored.
        The same feed can have different value_chain_type for different tickers.
        This fixes the feed contamination issue (Nov 24, 2025).
    """
    with db() as conn, conn.cursor() as cur:
        try:
            cur.execute("""
                INSERT INTO ticker_feeds (ticker, feed_id, category, value_chain_type)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (ticker, feed_id) DO UPDATE SET
                    category = EXCLUDED.category,
                    value_chain_type = EXCLUDED.value_chain_type,
                    active = TRUE,
                    updated_at = NOW()
            """, (ticker, feed_id, category, value_chain_type))

            # Enhanced logging to show value_chain_type
            if value_chain_type:
                LOG.info(f"âœ… Associated ticker {ticker} with feed {feed_id} as category '{category}' (value_chain_type='{value_chain_type}')")
            else:
                LOG.info(f"âœ… Associated ticker {ticker} with feed {feed_id} as category '{category}'")
            return True
        except Exception as e:
            LOG.error(f"âŒ Failed to associate ticker {ticker} with feed {feed_id}: {e}")
            return False

def create_feeds_for_ticker_new_architecture(ticker: str, metadata: dict) -> list:
    """Create feeds using new architecture with per-relationship categories

    Supports fallback configs (no full ticker data):
    - Always creates Google News feeds (works with any text)
    - Only creates Yahoo Finance feeds if has_full_config=True
    """
    feeds_created = []
    company_name = metadata.get("company_name", ticker)
    use_google_only = metadata.get("use_google_only", False)
    has_full_config = metadata.get("has_full_config", True)  # Default to True for backward compatibility

    if use_google_only or not has_full_config:
        LOG.info(f"ðŸ”„ Creating feeds for {ticker} using GOOGLE NEWS ONLY (no full config available)")
    else:
        LOG.info(f"ðŸ”„ Creating feeds for {ticker} using NEW ARCHITECTURE (Google News + Yahoo Finance)")

    # 1. Company feeds - ALWAYS create Google News, optionally create Yahoo Finance
    # Strip legal suffixes for Google News query (journalists rarely use Inc., Corp., etc.)
    feed_query_name = strip_legal_suffixes(company_name)

    company_feeds = [
        {
            "url": f"https://news.google.com/rss/search?q=\"{feed_query_name.replace(' ', '%20')}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
            "name": f"Google News: {company_name}",  # Display name uses legal name
            "search_keyword": feed_query_name,  # Query uses stripped name
            "source": "google"
        }
    ]

    # Only add Yahoo Finance if we have full config
    if not use_google_only and has_full_config:
        company_feeds.append({
            "url": f"https://finance.yahoo.com/rss/headline?s={ticker}",
            "name": f"Yahoo Finance: {ticker}",
            "search_keyword": ticker,
            "source": "yahoo"
        })
    else:
        LOG.info(f"â­ï¸ Skipping Yahoo Finance feed for {ticker} (using Google News only)")

    for feed_config in company_feeds:
        try:
            feed_id = upsert_feed_new_architecture(
                url=feed_config["url"],
                name=feed_config["name"],
                search_keyword=feed_config["search_keyword"],
                competitor_ticker=ticker  # Feed is ABOUT this ticker (needed for reuse as competitor/value_chain)
            )

            # Associate this feed with this ticker as "company" category
            if associate_ticker_with_feed_new_architecture(ticker, feed_id, "company", value_chain_type=None):
                feeds_created.append({
                    "feed_id": feed_id,
                    "config": {"category": "company", "name": feed_config["name"], "source": feed_config["source"]}
                })

        except Exception as e:
            LOG.error(f"âŒ Failed to create company feed for {ticker}: {e}")

    # 2. Industry feeds - will be associated with category="industry"
    industry_keywords = metadata.get("industry_keywords", [])[:3]
    for keyword in industry_keywords:
        try:
            feed_id = upsert_feed_new_architecture(
                url=f"https://news.google.com/rss/search?q=\"{keyword.replace(' ', '%20')}\"+when:7d&hl=en-US&gl=US&ceid=US:en",
                name=f"Industry: {keyword}",
                search_keyword=keyword
            )

            # Associate this feed with this ticker as "industry" category
            if associate_ticker_with_feed_new_architecture(ticker, feed_id, "industry", value_chain_type=None):
                feeds_created.append({
                    "feed_id": feed_id,
                    "config": {"category": "industry", "keyword": keyword}
                })

        except Exception as e:
            LOG.error(f"âŒ Failed to create industry feed for {ticker}: {e}")

    # 3. Competitor feeds - will be associated with category="competitor"
    # Supports competitors WITHOUT tickers (private companies) - Google News only
    competitors = metadata.get("competitors", [])[:3]
    for comp in competitors:
        if isinstance(comp, dict) and comp.get('name'):
            comp_name = comp['name']
            comp_ticker = comp.get('ticker')  # May be None for private companies

            # Strip legal suffixes for Google News query
            comp_feed_query_name = strip_legal_suffixes(comp_name)

            try:
                # ALWAYS create Google News feed (works for any company name)
                feed_id = upsert_feed_new_architecture(
                    url=f"https://news.google.com/rss/search?q=\"{comp_feed_query_name.replace(' ', '%20')}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
                    name=f"Google News: {comp_name}",  # Display name uses legal name (no "Competitor:" prefix)
                    search_keyword=comp_feed_query_name,  # Query uses stripped name
                    competitor_ticker=comp_ticker,  # Can be None
                    company_name=comp_name  # Full company name for display
                )

                # Associate this feed with this ticker as "competitor" category
                if associate_ticker_with_feed_new_architecture(ticker, feed_id, "competitor", value_chain_type=None):
                    feeds_created.append({
                        "feed_id": feed_id,
                        "config": {"category": "competitor", "name": comp_name, "source": "google"}
                    })

                # ONLY create Yahoo Finance feed if competitor has a ticker (public company)
                if comp_ticker:
                    feed_id = upsert_feed_new_architecture(
                        url=f"https://finance.yahoo.com/rss/headline?s={comp_ticker}",
                        name=f"Yahoo Finance: {comp_ticker}",  # Neutral name (no "Competitor:" prefix)
                        search_keyword=comp_name,
                        competitor_ticker=comp_ticker,
                        company_name=comp_name  # Full company name for display
                    )

                    # Associate this feed with this ticker as "competitor" category
                    if associate_ticker_with_feed_new_architecture(ticker, feed_id, "competitor", value_chain_type=None):
                        feeds_created.append({
                            "feed_id": feed_id,
                            "config": {"category": "competitor", "name": comp_name, "source": "yahoo"}
                        })
                else:
                    LOG.info(f"â­ï¸ Competitor {comp_name} has no ticker - using Google News only (private company)")

            except Exception as e:
                LOG.error(f"âŒ Failed to create competitor feeds for {comp_name}: {e}")

    # 4. Upstream (Supplier) Value Chain feeds - will be associated with category="value_chain"
    value_chain = metadata.get("value_chain", {})
    upstream_companies = value_chain.get("upstream", [])[:2]

    for upstream_comp in upstream_companies:
        if isinstance(upstream_comp, dict) and upstream_comp.get('name'):
            comp_name = upstream_comp['name']
            comp_ticker = upstream_comp.get('ticker')  # May be None for private suppliers

            # Strip legal suffixes for Google News query
            comp_feed_query_name = strip_legal_suffixes(comp_name)

            try:
                # ALWAYS create Google News feed (works for any company name)
                feed_id = upsert_feed_new_architecture(
                    url=f"https://news.google.com/rss/search?q=\"{comp_feed_query_name.replace(' ', '%20')}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
                    name=f"Google News: {comp_name}",
                    search_keyword=comp_feed_query_name,
                    competitor_ticker=comp_ticker,  # Can be None
                    company_name=comp_name,
                    value_chain_type='upstream'  # NEW FIELD
                )

                # Associate this feed with this ticker as "value_chain" category
                if associate_ticker_with_feed_new_architecture(ticker, feed_id, "value_chain", value_chain_type='upstream'):
                    feeds_created.append({
                        "feed_id": feed_id,
                        "config": {"category": "value_chain", "type": "upstream", "name": comp_name, "source": "google"}
                    })

                # ONLY create Yahoo Finance feed if supplier has a ticker (public company)
                if comp_ticker:
                    feed_id = upsert_feed_new_architecture(
                        url=f"https://finance.yahoo.com/rss/headline?s={comp_ticker}",
                        name=f"Yahoo Finance: {comp_ticker}",
                        search_keyword=comp_name,
                        competitor_ticker=comp_ticker,
                        company_name=comp_name,
                        value_chain_type='upstream'  # DEPRECATED - Ignored, but kept for backward compatibility
                    )

                    # Associate this feed with this ticker as "value_chain" category
                    if associate_ticker_with_feed_new_architecture(ticker, feed_id, "value_chain", value_chain_type='upstream'):
                        feeds_created.append({
                            "feed_id": feed_id,
                            "config": {"category": "value_chain", "type": "upstream", "name": comp_name, "source": "yahoo"}
                        })
                else:
                    LOG.info(f"â­ï¸ Upstream supplier {comp_name} has no ticker - using Google News only (private company)")

            except Exception as e:
                LOG.error(f"âŒ Failed to create upstream value chain feeds for {comp_name}: {e}")

    # 5. Downstream (Customer) Value Chain feeds - will be associated with category="value_chain"
    downstream_companies = value_chain.get("downstream", [])[:2]

    for downstream_comp in downstream_companies:
        if isinstance(downstream_comp, dict) and downstream_comp.get('name'):
            comp_name = downstream_comp['name']
            comp_ticker = downstream_comp.get('ticker')  # May be None for private customers

            # Strip legal suffixes for Google News query
            comp_feed_query_name = strip_legal_suffixes(comp_name)

            try:
                # ALWAYS create Google News feed (works for any company name)
                feed_id = upsert_feed_new_architecture(
                    url=f"https://news.google.com/rss/search?q=\"{comp_feed_query_name.replace(' ', '%20')}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
                    name=f"Google News: {comp_name}",
                    search_keyword=comp_feed_query_name,
                    competitor_ticker=comp_ticker,  # Can be None
                    company_name=comp_name,
                    value_chain_type='downstream'  # NEW FIELD
                )

                # Associate this feed with this ticker as "value_chain" category
                if associate_ticker_with_feed_new_architecture(ticker, feed_id, "value_chain", value_chain_type='downstream'):
                    feeds_created.append({
                        "feed_id": feed_id,
                        "config": {"category": "value_chain", "type": "downstream", "name": comp_name, "source": "google"}
                    })

                # ONLY create Yahoo Finance feed if customer has a ticker (public company)
                if comp_ticker:
                    feed_id = upsert_feed_new_architecture(
                        url=f"https://finance.yahoo.com/rss/headline?s={comp_ticker}",
                        name=f"Yahoo Finance: {comp_ticker}",
                        search_keyword=comp_name,
                        competitor_ticker=comp_ticker,
                        company_name=comp_name,
                        value_chain_type='downstream'  # DEPRECATED - Ignored, but kept for backward compatibility
                    )

                    # Associate this feed with this ticker as "value_chain" category
                    if associate_ticker_with_feed_new_architecture(ticker, feed_id, "value_chain", value_chain_type='downstream'):
                        feeds_created.append({
                            "feed_id": feed_id,
                            "config": {"category": "value_chain", "type": "downstream", "name": comp_name, "source": "yahoo"}
                        })
                else:
                    LOG.info(f"â­ï¸ Downstream customer {comp_name} has no ticker - using Google News only (private company)")

            except Exception as e:
                LOG.error(f"âŒ Failed to create downstream value chain feeds for {comp_name}: {e}")

    LOG.info(f"âœ… Created {len(feeds_created)} feed associations for {ticker} using NEW ARCHITECTURE (category-per-relationship)")
    return feeds_created


# ------------------------------------------------------------------------------
# FEED REFRESH COMPARISON HELPERS (Dec 2025)
# These functions enable smart feed refresh - only update tickers whose
# feed-relevant fields have changed, rather than refreshing all on every restart.
# ------------------------------------------------------------------------------

def get_feed_relevant_fields(tickers: List[str]) -> Dict[str, Dict[str, str]]:
    """
    Get feed-relevant fields from ticker_reference for comparison.

    Args:
        tickers: List of ticker symbols to query

    Returns:
        Dict mapping ticker to its feed-relevant fields:
        {
            'AAPL': {
                'industry_keyword_1': 'smartphones',
                'industry_keyword_2': 'consumer electronics',
                'industry_keyword_3': 'tech hardware',
                'competitor_1_name': 'Samsung', 'competitor_1_ticker': '005930.KS',
                'competitor_2_name': 'Google', 'competitor_2_ticker': 'GOOGL',
                'competitor_3_name': 'Microsoft', 'competitor_3_ticker': 'MSFT',
                'upstream_1_name': 'TSMC', 'upstream_1_ticker': 'TSM',
                'upstream_2_name': 'Foxconn', 'upstream_2_ticker': '2317.TW',
                'downstream_1_name': 'Best Buy', 'downstream_1_ticker': 'BBY',
                'downstream_2_name': 'Amazon', 'downstream_2_ticker': 'AMZN',
            },
            ...
        }
    """
    if not tickers:
        return {}

    result = {}

    try:
        with db() as conn, conn.cursor() as cur:
            # Query all feed-relevant fields for the given tickers
            placeholders = ','.join(['%s'] * len(tickers))
            cur.execute(f"""
                SELECT ticker,
                       industry_keyword_1, industry_keyword_2, industry_keyword_3,
                       competitor_1_name, competitor_1_ticker,
                       competitor_2_name, competitor_2_ticker,
                       competitor_3_name, competitor_3_ticker,
                       upstream_1_name, upstream_1_ticker,
                       upstream_2_name, upstream_2_ticker,
                       downstream_1_name, downstream_1_ticker,
                       downstream_2_name, downstream_2_ticker
                FROM ticker_reference
                WHERE ticker IN ({placeholders})
            """, tickers)

            rows = cur.fetchall()

            for row in rows:
                ticker = row['ticker']
                result[ticker] = {
                    'industry_keyword_1': row.get('industry_keyword_1') or '',
                    'industry_keyword_2': row.get('industry_keyword_2') or '',
                    'industry_keyword_3': row.get('industry_keyword_3') or '',
                    'competitor_1_name': row.get('competitor_1_name') or '',
                    'competitor_1_ticker': row.get('competitor_1_ticker') or '',
                    'competitor_2_name': row.get('competitor_2_name') or '',
                    'competitor_2_ticker': row.get('competitor_2_ticker') or '',
                    'competitor_3_name': row.get('competitor_3_name') or '',
                    'competitor_3_ticker': row.get('competitor_3_ticker') or '',
                    'upstream_1_name': row.get('upstream_1_name') or '',
                    'upstream_1_ticker': row.get('upstream_1_ticker') or '',
                    'upstream_2_name': row.get('upstream_2_name') or '',
                    'upstream_2_ticker': row.get('upstream_2_ticker') or '',
                    'downstream_1_name': row.get('downstream_1_name') or '',
                    'downstream_1_ticker': row.get('downstream_1_ticker') or '',
                    'downstream_2_name': row.get('downstream_2_name') or '',
                    'downstream_2_ticker': row.get('downstream_2_ticker') or '',
                }

        LOG.info(f"ðŸ“Š Retrieved feed-relevant fields for {len(result)}/{len(tickers)} tickers")
        return result

    except Exception as e:
        LOG.error(f"âŒ Failed to get feed-relevant fields: {e}")
        return {}


def normalize_field_value(value: str) -> str:
    """Normalize a field value for comparison (lowercase, strip whitespace)."""
    if not value:
        return ''
    return value.strip().lower()


def feed_fields_changed(before: Dict[str, str], after: Dict[str, str]) -> bool:
    """
    Compare before/after feed-relevant fields for a single ticker.

    Args:
        before: Dict of field values before CSV sync
        after: Dict of field values after CSV sync

    Returns:
        True if any feed-relevant field changed, False otherwise
    """
    # All fields that affect feed creation
    fields_to_compare = [
        'industry_keyword_1', 'industry_keyword_2', 'industry_keyword_3',
        'competitor_1_name', 'competitor_1_ticker',
        'competitor_2_name', 'competitor_2_ticker',
        'competitor_3_name', 'competitor_3_ticker',
        'upstream_1_name', 'upstream_1_ticker',
        'upstream_2_name', 'upstream_2_ticker',
        'downstream_1_name', 'downstream_1_ticker',
        'downstream_2_name', 'downstream_2_ticker',
    ]

    for field in fields_to_compare:
        before_val = normalize_field_value(before.get(field, ''))
        after_val = normalize_field_value(after.get(field, ''))

        if before_val != after_val:
            return True

    return False


def get_tickers_with_changed_feeds(
    before_state: Dict[str, Dict[str, str]],
    after_state: Dict[str, Dict[str, str]],
    all_active_tickers: List[str]
) -> List[str]:
    """
    Determine which tickers have changed feed-relevant fields.

    Args:
        before_state: Feed fields before CSV sync
        after_state: Feed fields after CSV sync
        all_active_tickers: List of all active tickers (in case some are new)

    Returns:
        List of tickers that need feed refresh
    """
    changed_tickers = []

    for ticker in all_active_tickers:
        before = before_state.get(ticker, {})
        after = after_state.get(ticker, {})

        # Case 1: Ticker is new (not in before_state) - needs refresh
        if not before and after:
            LOG.info(f"[{ticker}] ðŸ†• New ticker detected - will refresh feeds")
            changed_tickers.append(ticker)
            continue

        # Case 2: Ticker exists in both - compare fields
        if before and after:
            if feed_fields_changed(before, after):
                # Log which fields changed for debugging
                changed_fields = []
                for field in before.keys():
                    if normalize_field_value(before.get(field, '')) != normalize_field_value(after.get(field, '')):
                        changed_fields.append(field)
                LOG.info(f"[{ticker}] ðŸ“ Fields changed: {', '.join(changed_fields)} - will refresh feeds")
                changed_tickers.append(ticker)
            # else: no changes, skip this ticker
            continue

        # Case 3: Ticker in before but not in after (removed from CSV)
        # This shouldn't happen for active tickers, but handle gracefully
        if before and not after:
            LOG.warning(f"[{ticker}] âš ï¸ Ticker missing from ticker_reference after sync - skipping")
            continue

        # Case 4: Ticker not in either before or after (not in ticker_reference at all)
        # This means the ticker is in beta_users but has no CSV configuration
        if not before and not after:
            LOG.warning(f"[{ticker}] âš ï¸ Ticker not in ticker_reference - skipping (add to CSV first)")
            continue

    return changed_tickers


def refresh_feeds_for_active_tickers(before_state: Dict[str, Dict[str, str]] = None) -> Dict[str, Any]:
    """
    Refresh feed associations for active user tickers that have changed.

    Called at startup AFTER CSV sync to ensure feeds match current ticker_reference data.

    Smart Refresh Logic (Dec 2025):
    - If before_state is provided, only refresh tickers whose feed-relevant fields changed
    - If before_state is None (fallback), refresh ALL active tickers (legacy behavior)

    For each ticker that needs refresh:
    1. Delete existing ticker_feeds associations
    2. Get config from ticker_reference (just synced from CSV)
    3. Recreate feeds based on current config

    Feed IDs (in feeds table) are stable because they're keyed by URL.
    Only ticker_feeds associations are recreated.

    Args:
        before_state: Dict of feed-relevant fields BEFORE CSV sync (from get_feed_relevant_fields)
                      If None, falls back to refreshing all active tickers.

    Returns:
        Dict with refresh results: {status, tickers_refreshed, tickers_skipped, feeds_created, errors}
    """
    results = {
        "status": "success",
        "tickers_refreshed": 0,
        "tickers_skipped": 0,
        "total_feeds_created": 0,
        "errors": []
    }

    try:
        # Get unique tickers from active users
        ticker_recipients = load_active_users()
        unique_tickers = list(ticker_recipients.keys())

        if not unique_tickers:
            LOG.warning("âš ï¸ No active user tickers found - skipping feed refresh")
            results["status"] = "skipped"
            results["message"] = "No active user tickers"
            return results

        LOG.info(f"ðŸ“‹ Found {len(unique_tickers)} unique active tickers: {unique_tickers}")

        # Determine which tickers need refresh
        if before_state is not None:
            # Smart refresh: only refresh changed tickers
            LOG.info("ðŸ” Comparing feed-relevant fields to detect changes...")

            # Get current state (after CSV sync)
            after_state = get_feed_relevant_fields(unique_tickers)

            if not after_state:
                # Fallback: if we can't get after_state, refresh all
                LOG.warning("âš ï¸ Failed to get current feed fields - falling back to full refresh")
                tickers_to_refresh = unique_tickers
            else:
                # Compare and get list of changed tickers
                tickers_to_refresh = get_tickers_with_changed_feeds(before_state, after_state, unique_tickers)

                if not tickers_to_refresh:
                    LOG.info(f"âœ… No feed-relevant changes detected - skipping refresh for all {len(unique_tickers)} tickers")
                    results["status"] = "skipped"
                    results["message"] = "No feed-relevant changes detected"
                    results["tickers_skipped"] = len(unique_tickers)
                    return results

                LOG.info(f"ðŸ“ {len(tickers_to_refresh)}/{len(unique_tickers)} tickers have changed feeds: {tickers_to_refresh}")
                results["tickers_skipped"] = len(unique_tickers) - len(tickers_to_refresh)
        else:
            # Legacy behavior: refresh all tickers
            LOG.info("ðŸ”„ No before_state provided - refreshing ALL active tickers (legacy mode)")
            tickers_to_refresh = unique_tickers

        # Refresh only the tickers that need it
        for ticker in tickers_to_refresh:
            try:
                # Step 1: Delete existing ticker_feeds associations for this ticker
                with db() as conn, conn.cursor() as cur:
                    cur.execute("DELETE FROM ticker_feeds WHERE ticker = %s", (ticker,))
                    deleted_count = cur.rowcount
                    conn.commit()
                    LOG.info(f"[{ticker}] ðŸ—‘ï¸ Deleted {deleted_count} old feed associations")

                # Step 2: Get current config from ticker_reference (just synced from CSV)
                config = get_ticker_config(ticker)

                if not config:
                    LOG.warning(f"[{ticker}] âš ï¸ No config found in ticker_reference - using fallback")
                    config = {
                        'ticker': ticker,
                        'company_name': ticker,
                        'industry_keywords': [],
                        'competitors': [],
                        'value_chain': {'upstream': [], 'downstream': []},
                        'has_full_config': False,
                        'use_google_only': True
                    }

                # Step 3: Recreate feeds based on current config
                feeds_created = create_feeds_for_ticker_new_architecture(ticker, config)

                results["tickers_refreshed"] += 1
                results["total_feeds_created"] += len(feeds_created)

                LOG.info(f"[{ticker}] âœ… Refreshed: {len(feeds_created)} feed associations created")

            except Exception as e:
                error_msg = f"[{ticker}] Failed to refresh feeds: {e}"
                LOG.error(f"âŒ {error_msg}")
                results["errors"].append(error_msg)

        # Summary
        if results["errors"]:
            results["status"] = "partial"

        skipped_msg = f", {results['tickers_skipped']} unchanged" if results['tickers_skipped'] > 0 else ""
        LOG.info(f"âœ… Feed refresh complete: {results['tickers_refreshed']}/{len(unique_tickers)} tickers refreshed{skipped_msg}, "
                 f"{results['total_feeds_created']} total feed associations created")

        if results["errors"]:
            LOG.warning(f"âš ï¸ {len(results['errors'])} errors during feed refresh")

        return results

    except Exception as e:
        LOG.error(f"âŒ Feed refresh failed: {e}")
        results["status"] = "failed"
        results["error"] = str(e)
        return results


def get_feeds_for_ticker_new_architecture(ticker: str) -> list:
    """Get all active feeds for a ticker with their per-relationship categories"""
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT
                f.id, f.url, f.name, f.search_keyword, f.feed_ticker,
                tf.category, tf.active as association_active, tf.created_at as associated_at
            FROM feeds f
            JOIN ticker_feeds tf ON f.id = tf.feed_id
            WHERE tf.ticker = %s AND f.active = TRUE AND tf.active = TRUE
            ORDER BY tf.category, f.name
        """, (ticker,))

        return cur.fetchall()

def get_articles_for_ticker(ticker: str, hours: int = 24, sent_in_digest: bool = None) -> List[Dict]:
    """Get articles for a specific ticker within time window"""
    with db() as conn, conn.cursor() as cur:
        query = """
            SELECT a.*, tf.category, ta.sent_in_digest, ta.found_at, f.feed_ticker
            FROM articles a
            JOIN ticker_articles ta ON a.id = ta.article_id
            JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
            JOIN feeds f ON ta.feed_id = f.id
            WHERE ta.ticker = %s
            AND ta.found_at >= NOW() - INTERVAL '%s hours'
        """
        params = [ticker, hours]

        if sent_in_digest is not None:
            query += " AND ta.sent_in_digest = %s"
            params.append(sent_in_digest)

        query += " ORDER BY a.published_at DESC"
        cur.execute(query, params)
        return cur.fetchall()

def mark_articles_sent_in_digest(ticker: str, article_ids: List[int]) -> None:
    """Mark articles as sent in digest for a specific ticker"""
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            UPDATE ticker_articles
            SET sent_in_digest = TRUE
            WHERE ticker = %s AND article_id = ANY(%s)
        """, (ticker, article_ids))

# Core Ticker Reference Functions
# 1. UPDATED SCHEMA - With 3 industry keyword columns and 6 competitor columns

# Helper function for backward compatibility
def ensure_ticker_reference_schema():
    """Backward compatibility wrapper - schema is now created in ensure_schema()"""
    pass  # Schema already created in ensure_schema()

def create_ticker_reference_table():
    """Backward compatibility wrapper - table is now created in ensure_schema()"""
    pass  # Table already created in ensure_schema()

# 2. INTERNATIONAL TICKER FORMAT VALIDATION
def validate_ticker_format(ticker: str) -> bool:
    """
    Validate ticker format - supports international tickers.
    Used for internal processing (competitors, value chain, CSV import).
    For user-facing signup, use validate_ticker_format_us_only().
    """
    if not ticker or len(ticker) > 15:
        return False

    # Comprehensive regex patterns for global tickers
    patterns = [
        # Standard US tickers (1-5 letters, some up to 8)
        r'^[A-Z]{1,8}$',                          # US: MSFT, AAPL, GOOGL, BERKSHIRE

        # Canadian exchanges
        r'^[A-Z]{1,6}\.TO$',                      # Toronto: RY.TO, TD.TO
        r'^[A-Z]{1,6}-UN\.TO$',                   # Toronto units: REI-UN.TO
        r'^[A-Z]{1,6}-[A-Z]\.TO$',                # Toronto class: CTC-A.TO
        r'^[A-Z]{1,6}\.V$',                       # TSX Venture: ABC.V
        r'^[A-Z]{1,6}\.CN$',                      # CSE: WEED.CN

        # UK exchanges
        r'^[A-Z]{2,4}\.L$',                       # London: BP.L, ULVR.L
        r'^[A-Z0-9]{2,6}\.L$',                    # London with numbers

        # Australian exchanges
        r'^[A-Z]{3}\.AX$',                        # ASX: BHP.AX, CBA.AX
        r'^[A-Z0-9]{3,6}\.AX$',                   # ASX with numbers

        # Hong Kong exchanges
        r'^[0-9]{4}\.HK$',                        # HKEX: 0005.HK, 9988.HK
        r'^[0-9]{4,5}\.HK$',                      # Extended HKEX

        # European exchanges
        r'^[A-Z]{2,6}\.DE$',                      # Germany: SAP.DE, BMW.DE
        r'^[A-Z]{2,6}\.PA$',                      # Paris: MC.PA, OR.PA
        r'^[A-Z]{2,6}\.AS$',                      # Amsterdam: ASML.AS
        r'^[A-Z]{2,6}\.SW$',                      # Swiss: NESN.SW
        r'^[A-Z]{2,6}\.MI$',                      # Milan: ENI.MI
        r'^[A-Z]{2,6}\.ST$',                      # Stockholm
        r'^[A-Z]{2,6}\.OL$',                      # Oslo
        r'^[A-Z]{2,6}\.CO$',                      # Copenhagen
        r'^[A-Z]{2,6}\.BR$',                      # Brussels

        # Asian exchanges
        r'^[0-9]{4,6}\.KS$',                      # Korea: 005930.KS (Samsung)
        r'^[0-9]{4,6}\.KQ$',                      # Korea KOSDAQ
        r'^[0-9]{4,6}\.T$',                       # Tokyo: 7203.T (Toyota)
        r'^[0-9]{4,6}\.TW$',                      # Taiwan: 2330.TW (TSMC)
        r'^[A-Z0-9]{2,10}\.NS$',                  # India NSE
        r'^[A-Z0-9]{2,10}\.BO$',                  # India BSE
        r'^[A-Z0-9]{2,6}\.SI$',                   # Singapore
        r'^[A-Z0-9]{2,6}\.BK$',                   # Bangkok
        r'^[A-Z0-9]{2,6}\.JK$',                   # Jakarta

        # China exchanges
        r'^[0-9]{6}\.SS$',                        # Shanghai: 600000.SS
        r'^[0-9]{6}\.SZ$',                        # Shenzhen: 000001.SZ

        # Latin America
        r'^[A-Z]{2,10}\.MX$',                     # Mexico
        r'^[A-Z0-9]{4,10}\.SA$',                  # Brazil

        # Other formats
        r'^[A-Z0-9]{2,6}\.TA$',                   # Tel Aviv
        r'^[A-Z0-9]{2,6}\.IS$',                   # Istanbul
        r'^[A-Z0-9]{2,6}\.JO$',                   # Johannesburg

        # Crypto pairs (Yahoo Finance format)
        r'^[A-Z0-9]{2,10}-USD$',                 # Crypto to USD: BTC-USD, ETH-USD, BNB-USD
        r'^[A-Z0-9]{2,10}-[A-Z]{3}$',           # Crypto pairs: BTC-EUR, ETH-GBP

        # Forex pairs (Yahoo Finance format)
        r'^[A-Z]{6}=X$',                         # Forex: EURUSD=X, GBPUSD=X, CADJPY=X
        r'^[A-Z]{3}=X$',                         # Single currency to USD: CAD=X, EUR=X

        # Market indices (Yahoo Finance format)
        r'^\^[A-Z0-9]{2,8}$',                    # Indices: ^GSPC, ^DJI, ^IXIC, ^FTSE

        # Class/series shares (US and international)
        r'^[A-Z]{1,6}-[A-Z]$',                   # Class shares: BRK-A, BRK-B
        r'^[A-Z]{1,6}-[A-Z]{2}$',               # Extended class: BRK-PA, TECK-B

        # Rights, warrants, units
        r'^[A-Z]{1,6}\.R$',                      # Rights
        r'^[A-Z]{1,6}\.W$',                      # Warrants
        r'^[A-Z]{1,6}\.U$',                      # Units
    ]

    ticker_upper = ticker.upper().strip()

    # Check against all patterns
    for pattern in patterns:
        if re.match(pattern, ticker_upper):
            return True

    return False


def validate_ticker_format_us_only(ticker: str) -> bool:
    """
    Validate ticker format for US-listed companies only.
    Used for user-facing signup on landing page.
    """
    if not ticker or len(ticker) > 15:
        return False

    # US-only regex patterns
    patterns = [
        # Standard US tickers
        r'^[A-Z]{1,8}$',                          # US: MSFT, AAPL, GOOGL, BERKSHIRE

        # Crypto pairs (Yahoo Finance format)
        r'^[A-Z0-9]{2,10}-USD$',                 # Crypto to USD: BTC-USD, ETH-USD, BNB-USD
        r'^[A-Z0-9]{2,10}-[A-Z]{3}$',           # Crypto pairs: BTC-EUR, ETH-GBP

        # Forex pairs (Yahoo Finance format)
        r'^[A-Z]{6}=X$',                         # Forex: EURUSD=X, GBPUSD=X, CADJPY=X
        r'^[A-Z]{3}=X$',                         # Single currency to USD: CAD=X, EUR=X

        # US Market indices (Yahoo Finance format)
        r'^\^[A-Z0-9]{2,8}$',                    # Indices: ^GSPC, ^DJI, ^IXIC

        # US class/series shares
        r'^[A-Z]{1,6}-[A-Z]$',                   # Class shares: BRK-A, BRK-B
        r'^[A-Z]{1,6}-[A-Z]{2}$',               # Extended class: BRK-PA

        # US rights, warrants, units
        r'^[A-Z]{1,6}\.R$',                      # Rights: AAPL.R
        r'^[A-Z]{1,6}\.W$',                      # Warrants: AAPL.W
        r'^[A-Z]{1,6}\.U$',                      # Units: AAPL.U
    ]

    ticker_upper = ticker.upper().strip()

    # Check against all patterns
    for pattern in patterns:
        if re.match(pattern, ticker_upper):
            return True

    return False

# 3. TICKER FORMAT NORMALIZATION
def normalize_ticker_format(ticker: str) -> str:
    """Normalize ticker to consistent format for storage and lookup"""
    if not ticker:
        return ""
    
    # Convert to uppercase and strip whitespace
    normalized = ticker.upper().strip()

    # CRITICAL FIX: Convert colon format to dot format BEFORE character filtering
    # Bloomberg/Reuters uses "ULVR:L", Yahoo Finance uses "ULVR.L"
    colon_to_dot_mappings = {
        ':L': '.L',      # London: ULVR:L â†’ ULVR.L
        ':TO': '.TO',    # Toronto: RY:TO â†’ RY.TO
        ':AX': '.AX',    # Australia: BHP:AX â†’ BHP.AX
        ':HK': '.HK',    # Hong Kong: 0005:HK â†’ 0005.HK
        ':DE': '.DE',    # Germany: SAP:DE â†’ SAP.DE
        ':PA': '.PA',    # Paris: MC:PA â†’ MC.PA
        ':AS': '.AS',    # Amsterdam: ASML:AS â†’ ASML.AS
        ':KS': '.KS',    # Korea: 005930:KS â†’ 005930.KS
        ':T': '.T',      # Tokyo: 7203:T â†’ 7203.T
    }

    # Apply colon-to-dot conversions
    for colon_suffix, dot_suffix in colon_to_dot_mappings.items():
        if normalized.endswith(colon_suffix):
            normalized = normalized[:-len(colon_suffix)] + dot_suffix
            break

    # Remove quotes and invalid characters (keep alphanumeric, dots, dashes, carets, equals)
    # Keep: letters, numbers, dot (.), dash (-), caret (^), equals (=)
    normalized = re.sub(r'[^A-Z0-9.\-\^=]', '', normalized)
    
    # Handle common exchange variations and map to Yahoo Finance standard
    exchange_mappings = {
        # Toronto Stock Exchange variations
        '.TSX': '.TO',
        '.TSE': '.TO', 
        '.TOR': '.TO',
        
        # Vancouver variations  
        '.VAN': '.V',
        '.VSE': '.V',
        
        # London variations
        '.LSE': '.L',
        '.LON': '.L',
        
        # Australian variations
        '.ASX': '.AX',
        '.AUS': '.AX',
        
        # German variations
        '.FRA': '.DE',
        '.XETRA': '.DE',
        
        # US exchange suffixes to remove
        '.NYSE': '',
        '.NASDAQ': '',
        '.OTC': '',
    }
    
    # Apply exchange mappings
    for old_suffix, new_suffix in exchange_mappings.items():
        if normalized.endswith(old_suffix):
            normalized = normalized[:-len(old_suffix)] + new_suffix
            break
    
    # Handle edge cases
    edge_case_mappings = {
        # Berkshire Hathaway
        'BRKA': 'BRK-A',
        'BRKB': 'BRK-B',
        
        # Common Canadian bank shortcuts
        'ROYALBANK.TO': 'RY.TO',
        'SCOTIABANK.TO': 'BNS.TO',
    }
    
    # Apply edge case mappings
    if normalized in edge_case_mappings:
        normalized = edge_case_mappings[normalized]
    
    # Replace underscores with dashes in base ticker only
    if '.' in normalized:
        parts = normalized.split('.')
        parts[0] = parts[0].replace('_', '-')
        normalized = '.'.join(parts)
    else:
        normalized = normalized.replace('_', '-')

    return normalized


def validate_and_normalize_ticker(ticker_raw: str) -> tuple[str, dict]:
    """
    Validate and normalize ticker, ensuring it exists in ticker_reference.

    Args:
        ticker_raw: Raw ticker input from user/API

    Returns:
        Tuple of (normalized_ticker, ticker_config)

    Raises:
        ValueError: If ticker is invalid or not found in ticker_reference
    """
    if not ticker_raw or not ticker_raw.strip():
        raise ValueError("Ticker cannot be empty")

    # Normalize to standard format
    ticker = normalize_ticker_format(ticker_raw)

    if not ticker:
        raise ValueError(f"Invalid ticker format: '{ticker_raw}'")

    # Validate against ticker_reference
    config = get_ticker_config(ticker)

    if not config or not config.get('has_full_config', True):
        raise ValueError(
            f"Ticker '{ticker}' (normalized from '{ticker_raw}') not found in ticker_reference. "
            f"Please add it via /admin endpoint first."
        )

    return (ticker, config)

# 4. TICKER VALIDATION HELPER FUNCTIONS
def get_ticker_exchange_info(ticker: str) -> Dict[str, str]:
    """Extract exchange and country information from ticker format"""
    normalized = normalize_ticker_format(ticker)
    
    exchange_info = {
        'ticker': normalized,
        'exchange': 'UNKNOWN',
        'country': 'UNKNOWN',
        'currency': 'UNKNOWN'
    }
    
    # Exchange mappings based on ticker suffix
    if '.TO' in normalized:
        exchange_info.update({'exchange': 'TSX', 'country': 'CA', 'currency': 'CAD'})
    elif '.V' in normalized:
        exchange_info.update({'exchange': 'TSXV', 'country': 'CA', 'currency': 'CAD'})
    elif '.L' in normalized:
        exchange_info.update({'exchange': 'LSE', 'country': 'UK', 'currency': 'GBP'})
    elif '.AX' in normalized:
        exchange_info.update({'exchange': 'ASX', 'country': 'AU', 'currency': 'AUD'})
    elif '.DE' in normalized:
        exchange_info.update({'exchange': 'XETRA', 'country': 'DE', 'currency': 'EUR'})
    elif '.PA' in normalized:
        exchange_info.update({'exchange': 'EPA', 'country': 'FR', 'currency': 'EUR'})
    elif '.HK' in normalized:
        exchange_info.update({'exchange': 'HKEX', 'country': 'HK', 'currency': 'HKD'})
    elif '.AS' in normalized:
        exchange_info.update({'exchange': 'AEX', 'country': 'NL', 'currency': 'EUR'})
    elif '.KS' in normalized:
        exchange_info.update({'exchange': 'KRX', 'country': 'KR', 'currency': 'KRW'})
    elif '.T' in normalized:
        exchange_info.update({'exchange': 'TSE', 'country': 'JP', 'currency': 'JPY'})
    else:
        # Assume US market for tickers without suffix
        exchange_info.update({'exchange': 'NASDAQ/NYSE', 'country': 'US', 'currency': 'USD'})
    
    return exchange_info

# 5. COMPREHENSIVE TICKER TESTING FUNCTION
def test_ticker_validation():
    """Test function to verify ticker validation works correctly"""
    test_cases = [
        # US Markets
        ('AAPL', True), ('MSFT', True), ('GOOGL', True), ('BRK-A', True), ('BRK-B', True),
        
        # Canadian Markets
        ('RY.TO', True), ('TD.TO', True), ('BNS.TO', True), ('BMO.TO', True), ('TECK-A.TO', True),
        ('SHOP.TO', True), ('CNQ.TO', True), ('ENB.TO', True), ('BCE.TO', True),
        
        # International Markets  
        ('VOD.L', True), ('BP.L', True), ('BHP.AX', True), ('SAP.DE', True),
        ('ASML.AS', True), ('MC.PA', True), ('0700.HK', True),
        
        # Special formats
        ('BRK-PA', True), ('TECK-A.TO', True), ('RCI-B.TO', True),
        
        # Invalid formats
        ('', False), ('TOOLONG12345', False), ('12345', False), ('ABC.INVALID', False),
        ('SPECIAL!@#', False), ('A', False),  # Too short single letter
    ]
    
    passed = 0
    failed = 0
    
    LOG.info("Testing ticker validation...")
    
    for ticker, expected in test_cases:
        result = validate_ticker_format(ticker)
        if result == expected:
            passed += 1
        else:
            failed += 1
            LOG.warning(f"VALIDATION TEST FAILED: {ticker} - Expected: {expected}, Got: {result}")
    
    LOG.info(f"Ticker validation tests: {passed} passed, {failed} failed")
    
    # Test normalization
    normalization_tests = [
        ('ry.to', 'RY.TO'),
        ('BRK A', 'BRK-A'),  
        ('TECK_A.TSX', 'TECK-A.TO'),
        ('  AAPL  ', 'AAPL'),
        ('brk-b', 'BRK-B'),
    ]
    
    LOG.info("Testing ticker normalization...")
    norm_passed = 0
    norm_failed = 0
    
    for input_ticker, expected_output in normalization_tests:
        result = normalize_ticker_format(input_ticker)
        if result == expected_output:
            norm_passed += 1
        else:
            norm_failed += 1
            LOG.warning(f"NORMALIZATION TEST FAILED: '{input_ticker}' - Expected: '{expected_output}', Got: '{result}'")
    
    LOG.info(f"Ticker normalization tests: {norm_passed} passed, {norm_failed} failed")
    
    return {"validation": {"passed": passed, "failed": failed}, "normalization": {"passed": norm_passed, "failed": norm_failed}}

# Optional: Add this function to test the implementation
def debug_ticker_processing(ticker: str):
    """Debug function to see how a ticker gets processed through the system"""
    LOG.info(f"=== DEBUGGING TICKER: {ticker} ===")
    LOG.info(f"Original: '{ticker}'")
    
    normalized = normalize_ticker_format(ticker)
    LOG.info(f"Normalized: '{normalized}'")
    
    is_valid = validate_ticker_format(normalized)
    LOG.info(f"Valid: {is_valid}")
    
    exchange_info = get_ticker_exchange_info(normalized)
    LOG.info(f"Exchange Info: {exchange_info}")
    
    return {
        'original': ticker,
        'normalized': normalized,
        'valid': is_valid,
        'exchange_info': exchange_info
    }

# Ticker Reference Data Management Functions
# 1. GET TICKER REFERENCE - Enhanced lookup with fallback logic
def get_ticker_reference(ticker: str):
    """Get ticker reference data from database with US/Canadian fallback logic"""
    if not ticker:
        return None
        
    normalized_ticker = normalize_ticker_format(ticker)
    
    with db() as conn, conn.cursor() as cur:
        # First try exact match
        cur.execute("""
            SELECT ticker, country, company_name, industry, sector, sub_industry,
                   exchange, currency, market_cap_category, yahoo_ticker, active, is_etf,
                   industry_keyword_1, industry_keyword_2, industry_keyword_3,
                   competitor_1_name, competitor_1_ticker,
                   competitor_2_name, competitor_2_ticker,
                   competitor_3_name, competitor_3_ticker,
                   upstream_1_name, upstream_1_ticker,
                   upstream_2_name, upstream_2_ticker,
                   downstream_1_name, downstream_1_ticker,
                   downstream_2_name, downstream_2_ticker,
                   ai_generated, ai_enhanced_at, created_at, updated_at, data_source
            FROM ticker_reference
            WHERE ticker = %s AND active = TRUE
        """, (normalized_ticker,))
        result = cur.fetchone()
        
        if result:
            return dict(result)
        
        # Fallback logic for Canadian tickers
        if not normalized_ticker.endswith('.TO') and len(normalized_ticker) <= 5:
            canadian_ticker = f"{normalized_ticker}.TO"
            cur.execute("""
                SELECT ticker, country, company_name, industry, sector, sub_industry,
                       exchange, currency, market_cap_category, yahoo_ticker, active, is_etf,
                       industry_keyword_1, industry_keyword_2, industry_keyword_3,
                       competitor_1_name, competitor_1_ticker,
                       competitor_2_name, competitor_2_ticker,
                       competitor_3_name, competitor_3_ticker,
                       upstream_1_name, upstream_1_ticker,
                       upstream_2_name, upstream_2_ticker,
                       downstream_1_name, downstream_1_ticker,
                       downstream_2_name, downstream_2_ticker,
                       ai_generated, ai_enhanced_at, created_at, updated_at, data_source
                FROM ticker_reference
                WHERE ticker = %s AND active = TRUE
            """, (canadian_ticker,))
            result = cur.fetchone()
            if result:
                return dict(result)
        
        # Fallback logic for US tickers
        if '.TO' in normalized_ticker:
            us_ticker = normalized_ticker.replace('.TO', '')
            cur.execute("""
                SELECT ticker, country, company_name, industry, sector, sub_industry,
                       exchange, currency, market_cap_category, yahoo_ticker, active, is_etf,
                       industry_keyword_1, industry_keyword_2, industry_keyword_3,
                       competitor_1_name, competitor_1_ticker,
                       competitor_2_name, competitor_2_ticker,
                       competitor_3_name, competitor_3_ticker,
                       upstream_1_name, upstream_1_ticker,
                       upstream_2_name, upstream_2_ticker,
                       downstream_1_name, downstream_1_ticker,
                       downstream_2_name, downstream_2_ticker,
                       ai_generated, ai_enhanced_at, created_at, updated_at, data_source
                FROM ticker_reference
                WHERE ticker = %s AND active = TRUE
            """, (us_ticker,))
            result = cur.fetchone()
            if result:
                LOG.info(f"Found US listing {us_ticker} for Canadian ticker {normalized_ticker}")
                return dict(result)
    
    return None

# ============================================================================
# FINANCIAL DATA FUNCTIONS (yfinance integration)
# ============================================================================

def format_financial_number(num):
    """Format large numbers with B/M/K suffixes"""
    if num is None or num == 0:
        return None

    try:
        num = float(num)
        if num >= 1e12:
            return f"${num/1e12:.2f}T"
        elif num >= 1e9:
            return f"${num/1e9:.2f}B"
        elif num >= 1e6:
            return f"${num/1e6:.2f}M"
        elif num >= 1e3:
            return f"${num/1e3:.2f}K"
        else:
            return f"${num:.2f}"
    except:
        return None

def format_financial_volume(num):
    """Format volume without dollar sign"""
    if num is None or num == 0:
        return None

    try:
        num = float(num)
        if num >= 1e9:
            return f"{num/1e9:.2f}B"
        elif num >= 1e6:
            return f"{num/1e6:.2f}M"
        elif num >= 1e3:
            return f"{num/1e3:.2f}K"
        else:
            return f"{num:,.0f}"
    except:
        return None

def format_financial_percent(value, include_plus=True):
    """Format percentage with + or - sign"""
    if value is None:
        return None

    try:
        value = float(value)
        if value > 0 and include_plus:
            return f"+{value:.2f}%"
        return f"{value:.2f}%"
    except:
        return None

def _wait_for_polygon_rate_limit():
    """
    Rate limiter for Polygon.io API (5 calls/minute for free tier).
    Blocks until safe to make next call.
    """
    now = time.time()

    # Remove calls older than 60 seconds
    while POLYGON_CALL_TIMES and now - POLYGON_CALL_TIMES[0] > 60:
        POLYGON_CALL_TIMES.popleft()

    # If at limit, wait until oldest call expires
    if len(POLYGON_CALL_TIMES) >= POLYGON_RATE_LIMIT:
        sleep_time = 60 - (now - POLYGON_CALL_TIMES[0]) + 1  # +1 second buffer
        if sleep_time > 0:
            LOG.info(f"â³ Polygon.io rate limit reached, waiting {sleep_time:.1f}s...")
            time.sleep(sleep_time)

    # Record this call
    POLYGON_CALL_TIMES.append(time.time())


def get_stock_data_fmp(ticker: str) -> Optional[Dict]:
    """
    Fetch financial data from FMP (Financial Modeling Prep) as PRIMARY source.

    Uses three endpoints:
    - /v3/quote/{ticker}: Current price, daily change, market cap, 52-week range, volume
    - /v3/stock-price-change/{ticker}: Pre-calculated YTD return
    - /v3/key-metrics-ttm/{ticker}: Enterprise Value

    Returns dict with financial_* keys matching yfinance format, or None on failure.
    """
    if not FMP_API_KEY:
        LOG.warning("FMP_API_KEY not set, skipping FMP")
        return None

    try:
        LOG.info(f"ðŸ“Š Fetching stock data from FMP for {ticker}")

        # Get quote data (price, daily change, market cap, 52-week range, volume)
        quote_url = f"https://financialmodelingprep.com/api/v3/quote/{ticker}"
        quote_resp = requests.get(quote_url, params={"apikey": FMP_API_KEY}, timeout=10)

        if quote_resp.status_code != 200:
            LOG.warning(f"FMP quote API error {quote_resp.status_code} for {ticker}")
            return None

        quote_data = quote_resp.json()
        if not quote_data or len(quote_data) == 0:
            LOG.warning(f"FMP returned no quote data for {ticker}")
            return None

        quote = quote_data[0]
        current_price = quote.get('price')

        if not current_price:
            LOG.warning(f"FMP missing price data for {ticker}")
            return None

        # Get price change data (YTD, 1D, etc.)
        change_url = f"https://financialmodelingprep.com/api/v3/stock-price-change/{ticker}"
        change_resp = requests.get(change_url, params={"apikey": FMP_API_KEY}, timeout=10)

        ytd_return = None
        if change_resp.status_code == 200:
            change_data = change_resp.json()
            if change_data and len(change_data) > 0:
                ytd_return = change_data[0].get('ytd')

        # Get Enterprise Value from key-metrics-ttm endpoint
        enterprise_value = None
        try:
            metrics_url = f"https://financialmodelingprep.com/api/v3/key-metrics-ttm/{ticker}"
            metrics_resp = requests.get(metrics_url, params={"apikey": FMP_API_KEY}, timeout=10)
            if metrics_resp.status_code == 200:
                metrics_data = metrics_resp.json()
                if metrics_data and len(metrics_data) > 0:
                    enterprise_value = metrics_data[0].get('enterpriseValueTTM')
        except Exception as e:
            LOG.warning(f"FMP key-metrics-ttm failed for {ticker}: {e}")

        # Build financial data dict (matching yfinance format)
        daily_change = quote.get('changesPercentage')

        financial_data = {
            'financial_last_price': float(current_price),
            'financial_price_change_pct': float(daily_change) if daily_change is not None else None,
            'financial_yesterday_return_pct': float(daily_change) if daily_change is not None else None,
            'financial_ytd_return_pct': float(ytd_return) if ytd_return is not None else None,
            'financial_market_cap': float(quote.get('marketCap')) if quote.get('marketCap') else None,
            'financial_enterprise_value': float(enterprise_value) if enterprise_value else None,
            'financial_volume': float(quote.get('volume')) if quote.get('volume') else None,
            'financial_avg_volume': float(quote.get('avgVolume')) if quote.get('avgVolume') else None,
            'financial_year_high': float(quote.get('yearHigh')) if quote.get('yearHigh') else None,
            'financial_year_low': float(quote.get('yearLow')) if quote.get('yearLow') else None,
            'financial_analyst_target': None,
            'financial_analyst_range_low': None,
            'financial_analyst_range_high': None,
            'financial_analyst_count': None,
            'financial_analyst_recommendation': None,
            'financial_snapshot_date': datetime.now(pytz.timezone('America/Toronto')).strftime('%Y-%m-%d')
        }

        change_str = f"{daily_change:+.2f}%" if daily_change is not None else "N/A"
        ytd_str = f", YTD={ytd_return:+.2f}%" if ytd_return is not None else ""
        LOG.info(f"âœ… FMP data retrieved for {ticker}: Price=${current_price:.2f}, Change={change_str}{ytd_str}")
        return financial_data

    except requests.exceptions.Timeout:
        LOG.warning(f"FMP timeout for {ticker}")
        return None
    except Exception as e:
        LOG.error(f"âŒ FMP failed for {ticker}: {e}")
        return None


def get_stock_context_polygon(ticker: str) -> Optional[Dict]:
    """
    Fetch financial data from Polygon.io as fallback when yfinance fails.
    Free tier: 5 API calls/minute.

    Returns dict with price and yesterday's return, or None on failure.
    """
    if not POLYGON_API_KEY:
        LOG.warning("POLYGON_API_KEY not set, skipping Polygon.io fallback")
        return None

    try:
        _wait_for_polygon_rate_limit()

        # Get previous close (snapshot endpoint)
        url = f"https://api.polygon.io/v2/aggs/ticker/{ticker}/prev"
        params = {"apiKey": POLYGON_API_KEY}

        LOG.info(f"ðŸ“Š Fetching from Polygon.io: {ticker}")
        response = requests.get(url, params=params, timeout=10)

        if response.status_code != 200:
            LOG.warning(f"Polygon.io API error {response.status_code}: {response.text[:200]}")
            return None

        data = response.json()

        if data.get("status") != "OK" or not data.get("results"):
            LOG.warning(f"Polygon.io no data for {ticker}")
            return None

        result = data["results"][0]

        # Extract price data
        close_price = result.get("c")  # Close price
        open_price = result.get("o")   # Open price

        if not close_price:
            LOG.warning(f"Polygon.io missing price data for {ticker}")
            return None

        # Calculate yesterday's return (close vs open)
        yesterday_return = None
        if close_price and open_price:
            yesterday_return = ((close_price - open_price) / open_price) * 100

        # Build financial data dict (minimal - just what Email #3 needs)
        financial_data = {
            'financial_last_price': float(close_price),
            'financial_price_change_pct': float(yesterday_return) if yesterday_return else None,
            'financial_yesterday_return_pct': float(yesterday_return) if yesterday_return else None,
            'financial_ytd_return_pct': None,  # Not available from Polygon free tier
            'financial_market_cap': None,
            'financial_enterprise_value': None,
            'financial_volume': float(result.get("v")) if result.get("v") else None,
            'financial_avg_volume': None,
            'financial_analyst_target': None,
            'financial_analyst_range_low': None,
            'financial_analyst_range_high': None,
            'financial_analyst_count': None,
            'financial_analyst_recommendation': None,
            'financial_snapshot_date': datetime.now(pytz.timezone('America/Toronto')).strftime('%Y-%m-%d')
        }

        LOG.info(f"âœ… Polygon.io data retrieved for {ticker}: Price=${close_price:.2f}, Return={yesterday_return:.2f}%")
        return financial_data

    except Exception as e:
        LOG.error(f"âŒ Polygon.io failed for {ticker}: {e}")
        return None

def fetch_company_name_from_polygon(ticker: str) -> Optional[str]:
    """
    Fetch company name from Polygon.io ticker details API.
    Free tier: 5 API calls/minute.
    Returns company name or None if unavailable.
    """
    if not POLYGON_API_KEY:
        LOG.warning("POLYGON_API_KEY not set, skipping Polygon.io company name fetch")
        return None

    try:
        _wait_for_polygon_rate_limit()

        # Ticker details endpoint provides company name
        url = f"https://api.polygon.io/v3/reference/tickers/{ticker}"
        params = {"apiKey": POLYGON_API_KEY}

        LOG.info(f"ðŸ“Š Fetching company name from Polygon.io for {ticker}...")
        response = requests.get(url, params=params, timeout=10)

        if response.status_code == 200:
            data = response.json()
            results = data.get('results', {})
            company_name = results.get('name')

            if company_name and company_name != ticker:
                LOG.info(f"âœ… Polygon.io company name for {ticker}: {company_name}")
                return company_name
            else:
                LOG.warning(f"âš ï¸ Polygon.io returned no valid company name for {ticker}")
                return None
        else:
            LOG.warning(f"âš ï¸ Polygon.io company name fetch failed for {ticker}: HTTP {response.status_code}")
            return None

    except Exception as e:
        LOG.error(f"âŒ Polygon.io company name fetch failed for {ticker}: {e}")
        return None

def fetch_company_name_from_yfinance(ticker: str, timeout: int = 10) -> Optional[str]:
    """
    Fetch company name from yfinance as a lightweight fallback.
    Returns longName or shortName, or None if unavailable.
    """
    try:
        LOG.info(f"Fetching company name from yfinance for {ticker}...")
        result = {'data': None, 'error': None}

        def fetch_data():
            try:
                ticker_obj = yf.Ticker(ticker)
                info = ticker_obj.info
                result['data'] = info
            except Exception as e:
                result['error'] = e

        # Start fetch in thread with timeout
        fetch_thread = threading.Thread(target=fetch_data)
        fetch_thread.daemon = True
        fetch_thread.start()
        fetch_thread.join(timeout=timeout)

        if fetch_thread.is_alive():
            LOG.warning(f"yfinance company name fetch timeout for {ticker} after {timeout}s")
            return None

        if result['error']:
            LOG.warning(f"yfinance company name fetch error for {ticker}: {result['error']}")
            return None

        if not result['data']:
            return None

        info = result['data']
        company_name = info.get('longName') or info.get('shortName') or info.get('name')

        if company_name and company_name != ticker:
            LOG.info(f"âœ… yfinance company name for {ticker}: {company_name}")
            return company_name
        else:
            LOG.warning(f"âš ï¸ yfinance returned ticker symbol as company name for {ticker}")
            return None

    except Exception as e:
        LOG.error(f"âŒ Failed to fetch company name from yfinance for {ticker}: {e}")
        return None

def get_stock_context(ticker: str, retries: int = 3, timeout: int = 10) -> Optional[Dict]:
    """
    Fetch financial data with 3-tier fallback: FMP (primary) â†’ yfinance â†’ Polygon.io

    Priority order:
    1. FMP (Financial Modeling Prep) - Fast, reliable, has pre-calculated YTD
    2. yfinance - Full data but unreliable (timeouts, rate limits)
    3. Polygon.io - Emergency fallback, price + daily return only (no YTD)

    Returns dict with price + daily return + YTD (required for emails).
    Other fields (market cap, analysts) are optional extras.

    Validation: Only requires price data (not market cap).
    This supports forex (EURUSD=X), indices (^GSPC), crypto (BTC-USD), stocks (AAPL).
    """

    # Tier 1: FMP (Primary) - Fast, reliable, has YTD
    fmp_data = get_stock_data_fmp(ticker)
    if fmp_data and fmp_data.get('financial_last_price'):
        return fmp_data

    LOG.info(f"[{ticker}] FMP failed, trying yfinance fallback...")

    # Tier 2: yfinance (Fallback) - Full data but unreliable
    for attempt in range(retries):
        try:
            LOG.info(f"Fetching financial data for {ticker} (attempt {attempt + 1}/{retries})")

            # Create a wrapper with timeout using threading
            result = {'data': None, 'error': None}

            def fetch_data():
                try:
                    ticker_obj = yf.Ticker(ticker)
                    info = ticker_obj.info
                    hist = ticker_obj.history(period="ytd")
                    result['data'] = (info, hist)
                except Exception as e:
                    result['error'] = e

            # Start fetch in thread with timeout
            fetch_thread = threading.Thread(target=fetch_data)
            fetch_thread.daemon = True
            fetch_thread.start()
            fetch_thread.join(timeout=timeout)

            if fetch_thread.is_alive():
                LOG.warning(f"yfinance timeout for {ticker} after {timeout}s (attempt {attempt + 1})")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                else:
                    # Last attempt timed out - break to try Polygon
                    break

            # Check for errors
            if result['error']:
                raise result['error']

            if not result['data']:
                raise ValueError("No data returned from yfinance")

            info, hist = result['data']

            # Validate we got real data
            if not info or not isinstance(info, dict):
                raise ValueError(f"Invalid info data for {ticker}")

            # Get price data
            current_price = info.get('currentPrice') or info.get('regularMarketPrice') or info.get('previousClose')
            if not current_price:
                raise ValueError(f"No price data available for {ticker}")

            # Calculate price change percentage
            regular_market_change = info.get('regularMarketChangePercent', 0.0)

            # Calculate yesterday's return
            previous_close = info.get('previousClose')
            regular_market_previous_close = info.get('regularMarketPreviousClose')
            yesterday_return = None

            if previous_close and regular_market_previous_close:
                yesterday_return = ((previous_close - regular_market_previous_close) / regular_market_previous_close) * 100

            # Calculate YTD return
            ytd_return = None
            try:
                if not hist.empty and len(hist) > 0:
                    ytd_start = hist['Close'].iloc[0]
                    ytd_current = previous_close if previous_close else current_price
                    ytd_return = ((ytd_current - ytd_start) / ytd_start) * 100
            except Exception as e:
                LOG.warning(f"YTD calculation failed for {ticker}: {e}")

            # Get financial data
            market_cap = info.get('marketCap')
            enterprise_value = info.get('enterpriseValue')

            # Get volume data
            volume = info.get('volume')
            avg_volume = info.get('averageVolume')

            # Get 52-week range
            year_high = info.get('fiftyTwoWeekHigh')
            year_low = info.get('fiftyTwoWeekLow')

            # Get analyst data
            target_mean = info.get('targetMeanPrice')
            target_low = info.get('targetLowPrice')
            target_high = info.get('targetHighPrice')
            num_analysts = info.get('numberOfAnalystOpinions')
            recommendation = info.get('recommendationKey', '').capitalize() if info.get('recommendationKey') else None

            # Validate critical fields (RELAXED: only require price, not market cap)
            # This allows forex (EURUSD=X), indices (^GSPC), crypto to work
            if not current_price:
                raise ValueError(f"Missing price data for {ticker}")

            # Build financial data dict
            financial_data = {
                'financial_last_price': float(current_price) if current_price else None,
                'financial_price_change_pct': float(regular_market_change) if regular_market_change else None,
                'financial_yesterday_return_pct': float(yesterday_return) if yesterday_return else None,
                'financial_ytd_return_pct': float(ytd_return) if ytd_return else None,
                'financial_market_cap': float(market_cap) if market_cap else None,
                'financial_enterprise_value': float(enterprise_value) if enterprise_value else None,
                'financial_volume': float(volume) if volume else None,
                'financial_avg_volume': float(avg_volume) if avg_volume else None,
                'financial_year_high': float(year_high) if year_high else None,
                'financial_year_low': float(year_low) if year_low else None,
                'financial_analyst_target': float(target_mean) if target_mean else None,
                'financial_analyst_range_low': float(target_low) if target_low else None,
                'financial_analyst_range_high': float(target_high) if target_high else None,
                'financial_analyst_count': int(num_analysts) if num_analysts else None,
                'financial_analyst_recommendation': recommendation,
                'financial_snapshot_date': datetime.now(pytz.timezone('America/Toronto')).strftime('%Y-%m-%d')
            }

            mcap_str = format_financial_number(market_cap) if market_cap else "N/A"
            LOG.info(f"âœ… yfinance data retrieved for {ticker}: Price=${current_price:.2f}, MCap={mcap_str}")
            return financial_data

        except Exception as e:
            LOG.warning(f"yfinance attempt {attempt + 1}/{retries} failed for {ticker}: {e}")
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff: 1s, 2s, 4s
            else:
                LOG.warning(f"[{ticker}] yfinance failed after {retries} attempts, trying Polygon.io...")

    # Tier 3: Polygon.io (Emergency fallback) - Price + daily return only, no YTD
    LOG.info(f"[{ticker}] ðŸ”„ Trying Polygon.io emergency fallback...")
    polygon_data = get_stock_context_polygon(ticker)

    if polygon_data:
        LOG.info(f"[{ticker}] âœ… Polygon.io fallback succeeded (note: no YTD data)")
        return polygon_data
    else:
        LOG.error(f"[{ticker}] âŒ All stock data sources failed (FMP, yfinance, Polygon)")
        return None

# Backwards compatability ticker_reference to ticker_config
def get_ticker_config(ticker: str) -> Optional[Dict]:
    """Get ticker configuration from ticker_reference table with proper field conversion"""
    LOG.info(f"[DB_DEBUG] get_ticker_config() called with ticker='{ticker}'")

    with db() as conn, conn.cursor() as cur:
        # Check if ticker_reference table exists first (safe for fresh database)
        try:
            cur.execute("SELECT 1 FROM ticker_reference LIMIT 1")
        except Exception as e:
            LOG.warning(f"âš ï¸ ticker_reference table doesn't exist yet, using fallback config for {ticker}")
            # Return fallback config instead of None
            return {
                'ticker': ticker,
                'name': ticker,
                'company_name': ticker,
                'industry_keywords': [],
                'competitors': [],
                'value_chain': {'upstream': [], 'downstream': []},
                'sector': 'Unknown',
                'industry': 'Unknown',
                'sub_industry': '',
                'has_full_config': False,
                'use_google_only': True
            }

        # Add debug query first
        cur.execute("SELECT COUNT(*) as count FROM ticker_reference WHERE ticker = %s", (ticker,))
        count_result = cur.fetchone()
        LOG.info(f"[DB_DEBUG] Found {count_result['count'] if count_result else 0} records for ticker '{ticker}'")
        
        cur.execute("""
            SELECT ticker, company_name, country,
                   industry_keyword_1, industry_keyword_2, industry_keyword_3,
                   competitor_1_name, competitor_1_ticker,
                   competitor_2_name, competitor_2_ticker,
                   competitor_3_name, competitor_3_ticker,
                   upstream_1_name, upstream_1_ticker,
                   upstream_2_name, upstream_2_ticker,
                   downstream_1_name, downstream_1_ticker,
                   downstream_2_name, downstream_2_ticker,
                   sector, industry, sub_industry,
                   financial_last_price, financial_price_change_pct,
                   financial_yesterday_return_pct, financial_ytd_return_pct,
                   financial_market_cap, financial_enterprise_value,
                   financial_volume, financial_avg_volume,
                   financial_analyst_target, financial_analyst_range_low,
                   financial_analyst_range_high, financial_analyst_count,
                   financial_analyst_recommendation, financial_snapshot_date,
                   geographic_markets, subsidiaries
            FROM ticker_reference
            WHERE ticker = %s
        """, (ticker,))

        # Comprehensive error logging for coroutine debugging
        try:
            result = cur.fetchone()
            LOG.info(f"[DB_DEBUG] Raw database result for '{ticker}': {result}")
            LOG.info(f"[DB_DEBUG] Result type: {type(result)}")
        except Exception as e:
            LOG.error(f"[DB_DEBUG] âŒ fetchone() failed with {type(e).__name__}: {e}")
            LOG.error(f"[DB_DEBUG] Cursor type: {type(cur)}")
            LOG.error(f"[DB_DEBUG] Connection type: {type(conn)}")
            LOG.error(f"[DB_DEBUG] Traceback:\n{traceback.format_exc()}")
            raise  # Re-raise to surface the error

        if not result:
            LOG.warning(f"âš ï¸ No config found for {ticker} - using fallback config (Google News only)")
            # Return fallback config instead of None to prevent crashes
            return {
                'ticker': ticker,
                'name': ticker,
                'company_name': ticker,  # Use ticker as display name
                'industry_keywords': [],
                'competitors': [],
                'value_chain': {'upstream': [], 'downstream': []},
                'sector': 'Unknown',
                'industry': 'Unknown',
                'sub_industry': '',
                'has_full_config': False,  # Flag for downstream logic
                'use_google_only': True    # Skip Yahoo Finance feeds
            }
        
        # Convert 3 separate keyword fields back to array format
        industry_keywords = []
        for i in range(1, 4):
            keyword = result.get(f"industry_keyword_{i}")
            if keyword and keyword.strip():
                industry_keywords.append(keyword.strip())
        
        # Convert 6 separate competitor fields back to structured format
        competitors = []
        for i in range(1, 4):
            name = result.get(f"competitor_{i}_name")
            ticker_field = result.get(f"competitor_{i}_ticker")
            if name and name.strip():
                comp = {"name": name.strip()}
                if ticker_field and ticker_field.strip():
                    comp["ticker"] = ticker_field.strip()
                competitors.append(comp)

        # Convert 4 separate value chain fields back to structured format
        value_chain = {"upstream": [], "downstream": []}

        # Upstream companies (2 max)
        for i in range(1, 3):
            name = result.get(f"upstream_{i}_name")
            ticker_field = result.get(f"upstream_{i}_ticker")
            if name and name.strip():
                vc = {"name": name.strip()}
                if ticker_field and ticker_field.strip():
                    vc["ticker"] = ticker_field.strip()
                value_chain["upstream"].append(vc)

        # Downstream companies (2 max)
        for i in range(1, 3):
            name = result.get(f"downstream_{i}_name")
            ticker_field = result.get(f"downstream_{i}_ticker")
            if name and name.strip():
                vc = {"name": name.strip()}
                if ticker_field and ticker_field.strip():
                    vc["ticker"] = ticker_field.strip()
                value_chain["downstream"].append(vc)

        config = {
            "name": result["company_name"],
            "company_name": result["company_name"],  # Some functions expect this field name
            "country": result.get("country", ""),  # Country code (US, CA, etc.)
            "industry_keywords": industry_keywords,
            "competitors": competitors,
            "value_chain": value_chain,
            "sector": result.get("sector", ""),
            "industry": result.get("industry", ""),
            "sub_industry": result.get("sub_industry", ""),
            "geographic_markets": result.get("geographic_markets", ""),
            "subsidiaries": result.get("subsidiaries", ""),
            "has_full_config": True  # Flag indicating this is a real ticker from database
        }

        # Add financial data if available
        if result.get("financial_snapshot_date"):
            config.update({
                "financial_last_price": float(result["financial_last_price"]) if result.get("financial_last_price") else None,
                "financial_price_change_pct": float(result["financial_price_change_pct"]) if result.get("financial_price_change_pct") else None,
                "financial_yesterday_return_pct": float(result["financial_yesterday_return_pct"]) if result.get("financial_yesterday_return_pct") else None,
                "financial_ytd_return_pct": float(result["financial_ytd_return_pct"]) if result.get("financial_ytd_return_pct") else None,
                "financial_market_cap": float(result["financial_market_cap"]) if result.get("financial_market_cap") else None,
                "financial_enterprise_value": float(result["financial_enterprise_value"]) if result.get("financial_enterprise_value") else None,
                "financial_volume": float(result["financial_volume"]) if result.get("financial_volume") else None,
                "financial_avg_volume": float(result["financial_avg_volume"]) if result.get("financial_avg_volume") else None,
                "financial_analyst_target": float(result["financial_analyst_target"]) if result.get("financial_analyst_target") else None,
                "financial_analyst_range_low": float(result["financial_analyst_range_low"]) if result.get("financial_analyst_range_low") else None,
                "financial_analyst_range_high": float(result["financial_analyst_range_high"]) if result.get("financial_analyst_range_high") else None,
                "financial_analyst_count": int(result["financial_analyst_count"]) if result.get("financial_analyst_count") else None,
                "financial_analyst_recommendation": result.get("financial_analyst_recommendation"),
                "financial_snapshot_date": str(result["financial_snapshot_date"]) if result.get("financial_snapshot_date") else None
            })

        return config

# 2. STORE TICKER REFERENCE - With 6 competitor fields
def store_ticker_reference(ticker_data: dict) -> bool:
    """Store or update ticker reference data with 3 industry keywords + 6 competitor fields + 8 value chain fields"""
    try:
        # Validate required fields
        required_fields = ['ticker', 'country', 'company_name']
        for field in required_fields:
            if not ticker_data.get(field):
                LOG.warning(f"Missing required field '{field}' for ticker reference")
                return False
        
        # Normalize ticker format
        ticker_data['ticker'] = normalize_ticker_format(ticker_data['ticker'])
        
        # Validate ticker format
        if not validate_ticker_format(ticker_data['ticker']):
            LOG.warning(f"Invalid ticker format: {ticker_data['ticker']}")
            return False
        
        # Set yahoo_ticker if not provided
        if not ticker_data.get('yahoo_ticker'):
            ticker_data['yahoo_ticker'] = ticker_data['ticker']
        
        # Clean text fields to remove NULL bytes
        text_fields = [
            'company_name', 'industry', 'sector', 'sub_industry', 'exchange',
            'industry_keyword_1', 'industry_keyword_2', 'industry_keyword_3',
            'competitor_1_name', 'competitor_2_name', 'competitor_3_name',
            'competitor_1_ticker', 'competitor_2_ticker', 'competitor_3_ticker',
            'upstream_1_name', 'upstream_1_ticker', 'upstream_2_name', 'upstream_2_ticker',
            'downstream_1_name', 'downstream_1_ticker', 'downstream_2_name', 'downstream_2_ticker',
            'geographic_markets', 'subsidiaries'
        ]
        for field in text_fields:
            if ticker_data.get(field):
                ticker_data[field] = clean_null_bytes(str(ticker_data[field]))
        
        # Normalize competitor tickers (no validation - international tickers allowed for feed creation)
        competitor_ticker_fields = ['competitor_1_ticker', 'competitor_2_ticker', 'competitor_3_ticker']
        for field in competitor_ticker_fields:
            if ticker_data.get(field):
                ticker_data[field] = normalize_ticker_format(ticker_data[field])

        # Normalize upstream/downstream tickers (no validation - international tickers allowed for feed creation)
        value_chain_ticker_fields = ['upstream_1_ticker', 'upstream_2_ticker', 'downstream_1_ticker', 'downstream_2_ticker']
        for field in value_chain_ticker_fields:
            if ticker_data.get(field):
                ticker_data[field] = normalize_ticker_format(ticker_data[field])
        
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                INSERT INTO ticker_reference (
                    ticker, country, company_name, industry, sector, sub_industry,
                    exchange, currency, market_cap_category, yahoo_ticker, active, is_etf,
                    industry_keyword_1, industry_keyword_2, industry_keyword_3,
                    competitor_1_name, competitor_1_ticker,
                    competitor_2_name, competitor_2_ticker,
                    competitor_3_name, competitor_3_ticker,
                    upstream_1_name, upstream_1_ticker,
                    upstream_2_name, upstream_2_ticker,
                    downstream_1_name, downstream_1_ticker,
                    downstream_2_name, downstream_2_ticker,
                    geographic_markets, subsidiaries,
                    ai_generated, data_source
                ) VALUES (
                    %(ticker)s, %(country)s, %(company_name)s, %(industry)s, %(sector)s, %(sub_industry)s,
                    %(exchange)s, %(currency)s, %(market_cap_category)s, %(yahoo_ticker)s, %(active)s, %(is_etf)s,
                    %(industry_keyword_1)s, %(industry_keyword_2)s, %(industry_keyword_3)s,
                    %(competitor_1_name)s, %(competitor_1_ticker)s,
                    %(competitor_2_name)s, %(competitor_2_ticker)s,
                    %(competitor_3_name)s, %(competitor_3_ticker)s,
                    %(upstream_1_name)s, %(upstream_1_ticker)s,
                    %(upstream_2_name)s, %(upstream_2_ticker)s,
                    %(downstream_1_name)s, %(downstream_1_ticker)s,
                    %(downstream_2_name)s, %(downstream_2_ticker)s,
                    %(geographic_markets)s, %(subsidiaries)s,
                    %(ai_generated)s, %(data_source)s
                )
                ON CONFLICT (ticker) DO UPDATE SET
                    country = EXCLUDED.country,
                    company_name = EXCLUDED.company_name,
                    industry = EXCLUDED.industry,
                    sector = EXCLUDED.sector,
                    sub_industry = EXCLUDED.sub_industry,
                    exchange = EXCLUDED.exchange,
                    currency = EXCLUDED.currency,
                    market_cap_category = EXCLUDED.market_cap_category,
                    yahoo_ticker = EXCLUDED.yahoo_ticker,
                    active = EXCLUDED.active,
                    is_etf = EXCLUDED.is_etf,
                    industry_keyword_1 = EXCLUDED.industry_keyword_1,
                    industry_keyword_2 = EXCLUDED.industry_keyword_2,
                    industry_keyword_3 = EXCLUDED.industry_keyword_3,
                    competitor_1_name = EXCLUDED.competitor_1_name,
                    competitor_1_ticker = EXCLUDED.competitor_1_ticker,
                    competitor_2_name = EXCLUDED.competitor_2_name,
                    competitor_2_ticker = EXCLUDED.competitor_2_ticker,
                    competitor_3_name = EXCLUDED.competitor_3_name,
                    competitor_3_ticker = EXCLUDED.competitor_3_ticker,
                    upstream_1_name = EXCLUDED.upstream_1_name,
                    upstream_1_ticker = EXCLUDED.upstream_1_ticker,
                    upstream_2_name = EXCLUDED.upstream_2_name,
                    upstream_2_ticker = EXCLUDED.upstream_2_ticker,
                    downstream_1_name = EXCLUDED.downstream_1_name,
                    downstream_1_ticker = EXCLUDED.downstream_1_ticker,
                    downstream_2_name = EXCLUDED.downstream_2_name,
                    downstream_2_ticker = EXCLUDED.downstream_2_ticker,
                    geographic_markets = EXCLUDED.geographic_markets,
                    subsidiaries = EXCLUDED.subsidiaries,
                    ai_generated = EXCLUDED.ai_generated,
                    data_source = EXCLUDED.data_source,
                    updated_at = NOW()
            """, {
                'ticker': ticker_data['ticker'],
                'country': ticker_data['country'],
                'company_name': ticker_data['company_name'],
                'industry': ticker_data.get('industry'),
                'sector': ticker_data.get('sector'),
                'sub_industry': ticker_data.get('sub_industry'),
                'exchange': ticker_data.get('exchange'),
                'currency': ticker_data.get('currency'),
                'market_cap_category': ticker_data.get('market_cap_category'),
                'yahoo_ticker': ticker_data.get('yahoo_ticker', ticker_data['ticker']),
                'active': ticker_data.get('active', True),
                'is_etf': ticker_data.get('is_etf', False),
                'industry_keyword_1': ticker_data.get('industry_keyword_1'),
                'industry_keyword_2': ticker_data.get('industry_keyword_2'),
                'industry_keyword_3': ticker_data.get('industry_keyword_3'),
                'competitor_1_name': ticker_data.get('competitor_1_name'),
                'competitor_1_ticker': ticker_data.get('competitor_1_ticker'),
                'competitor_2_name': ticker_data.get('competitor_2_name'),
                'competitor_2_ticker': ticker_data.get('competitor_2_ticker'),
                'competitor_3_name': ticker_data.get('competitor_3_name'),
                'competitor_3_ticker': ticker_data.get('competitor_3_ticker'),
                'upstream_1_name': ticker_data.get('upstream_1_name'),
                'upstream_1_ticker': ticker_data.get('upstream_1_ticker'),
                'upstream_2_name': ticker_data.get('upstream_2_name'),
                'upstream_2_ticker': ticker_data.get('upstream_2_ticker'),
                'downstream_1_name': ticker_data.get('downstream_1_name'),
                'downstream_1_ticker': ticker_data.get('downstream_1_ticker'),
                'downstream_2_name': ticker_data.get('downstream_2_name'),
                'downstream_2_ticker': ticker_data.get('downstream_2_ticker'),
                'geographic_markets': ticker_data.get('geographic_markets'),
                'subsidiaries': ticker_data.get('subsidiaries'),
                'ai_generated': ticker_data.get('ai_generated', False),
                'data_source': ticker_data.get('data_source', 'api')
            })
            
            LOG.info(f"Successfully stored ticker reference: {ticker_data['ticker']} - {ticker_data['company_name']}")
            return True
            
    except Exception as e:
        LOG.error(f"Failed to store ticker reference for {ticker_data.get('ticker')}: {e}")
        return False

# ==============================================================================
# FMP API INTEGRATION - Transcripts & Press Releases
# ==============================================================================

# 3. CSV IMPORT - With 6 competitor fields support
def import_ticker_reference_from_csv_content(csv_content: str):
    """Import ticker reference data from CSV with 3 industry keywords + 6 competitor fields - BULK OPTIMIZED"""
    ensure_ticker_reference_schema()
    
    imported = 0
    updated = 0
    errors = []
    skipped = 0
    
    try:
        LOG.debug(f"[CSV_IMPORT] Starting CSV import, content length: {len(csv_content) if csv_content else 'None'}")

        if not csv_content:
            LOG.error("[CSV_IMPORT] csv_content is None or empty!")
            return {
                "status": "error",
                "message": "CSV content is empty or None",
                "imported": 0, "updated": 0, "errors": []
            }

        csv_reader = csv.DictReader(io.StringIO(csv_content))

        # Validate CSV headers
        required_headers = ['ticker', 'country', 'company_name']
        missing_headers = [h for h in required_headers if h not in csv_reader.fieldnames]
        if missing_headers:
            LOG.error(f"[CSV_IMPORT] Missing headers: {missing_headers}")
            return {
                "status": "error",
                "message": f"Missing required CSV columns: {missing_headers}",
                "imported": 0, "updated": 0, "errors": []
            }

        LOG.info(f"[CSV_IMPORT] CSV headers found: {csv_reader.fieldnames}")
        LOG.debug(f"[CSV_IMPORT] Has legacy 'competitors' column: {'competitors' in csv_reader.fieldnames}")
        LOG.debug(f"[CSV_IMPORT] Has legacy 'industry_keywords' column: {'industry_keywords' in csv_reader.fieldnames}")
        
        # Collect all ticker data for bulk processing
        ticker_data_batch = []
        
        for row_num, row in enumerate(csv_reader, start=2):
            try:
                LOG.debug(f"[CSV_IMPORT] Processing row {row_num}, row type: {type(row)}")

                # Skip empty rows
                ticker_value = row.get('ticker', '')
                company_value = row.get('company_name', '')

                LOG.debug(f"[CSV_IMPORT] Row {row_num} - ticker type: {type(ticker_value)}, company type: {type(company_value)}")

                if not ticker_value or not str(ticker_value).strip() or not company_value or not str(company_value).strip():
                    skipped += 1
                    LOG.debug(f"[CSV_IMPORT] Row {row_num} skipped (empty ticker or company)")
                    continue

                # Build ticker data from CSV row
                ticker = str(ticker_value).strip()
                LOG.debug(f"[CSV_IMPORT] Row {row_num} processing ticker: {ticker}")
                
                # Build ticker_data with defensive string handling
                try:
                    country_val = row.get('country', '')
                    if country_val is None:
                        LOG.warning(f"[CSV_IMPORT] Row {row_num} has None country value")
                        country_val = ''

                    ticker_data = {
                        'ticker': ticker,
                        'country': str(country_val).strip().upper(),
                        'company_name': str(row.get('company_name', '')).strip(),
                        'industry': str(row.get('industry', '')).strip() or None,
                        'sector': str(row.get('sector', '')).strip() or None,
                        'sub_industry': str(row.get('sub_industry', '')).strip() or None,
                        'exchange': str(row.get('exchange', '')).strip() or None,
                        'currency': str(row.get('currency', '')).strip().upper() or None,
                        'market_cap_category': str(row.get('market_cap_category', '')).strip() or None,
                        'yahoo_ticker': str(row.get('yahoo_ticker', '')).strip() or ticker,
                        'active': str(row.get('active', 'true')).lower() in ('true', '1', 'yes', 'y', 't'),
                        'is_etf': str(row.get('is_etf', 'FALSE')).upper() in ('TRUE', '1', 'YES', 'Y'),
                        'geographic_markets': str(row.get('geographic_markets', '')).strip() or None,
                        'subsidiaries': str(row.get('subsidiaries', '')).strip() or None,
                        'data_source': 'csv_import',
                        'ai_generated': str(row.get('ai_generated', 'FALSE')).upper() in ('TRUE', '1', 'YES', 'Y')
                    }
                    LOG.debug(f"[CSV_IMPORT] Row {row_num} ticker_data built successfully")
                except Exception as e:
                    LOG.error(f"[CSV_IMPORT] Row {row_num} failed building ticker_data: {e}, row keys: {row.keys()}")
                    raise
                
                # Handle 3 industry keyword fields (with None safety)
                try:
                    ticker_data['industry_keyword_1'] = str(row.get('industry_keyword_1', '') or '').strip() or None
                    ticker_data['industry_keyword_2'] = str(row.get('industry_keyword_2', '') or '').strip() or None
                    ticker_data['industry_keyword_3'] = str(row.get('industry_keyword_3', '') or '').strip() or None
                    LOG.debug(f"[CSV_IMPORT] Row {row_num} industry keywords processed")
                except Exception as e:
                    LOG.error(f"[CSV_IMPORT] Row {row_num} failed processing industry keywords: {e}")
                    raise

                # Handle 6 competitor fields (with None safety)
                try:
                    ticker_data['competitor_1_name'] = str(row.get('competitor_1_name', '') or '').strip() or None
                    ticker_data['competitor_1_ticker'] = str(row.get('competitor_1_ticker', '') or '').strip() or None
                    ticker_data['competitor_2_name'] = str(row.get('competitor_2_name', '') or '').strip() or None
                    ticker_data['competitor_2_ticker'] = str(row.get('competitor_2_ticker', '') or '').strip() or None
                    ticker_data['competitor_3_name'] = str(row.get('competitor_3_name', '') or '').strip() or None
                    ticker_data['competitor_3_ticker'] = str(row.get('competitor_3_ticker', '') or '').strip() or None
                    LOG.debug(f"[CSV_IMPORT] Row {row_num} competitor fields processed")
                except Exception as e:
                    LOG.error(f"[CSV_IMPORT] Row {row_num} failed processing competitor fields: {e}")
                    raise

                # Handle 8 value chain fields (with None safety) - Oct 31, 2025
                try:
                    ticker_data['upstream_1_name'] = str(row.get('upstream_1_name', '') or '').strip() or None
                    ticker_data['upstream_1_ticker'] = str(row.get('upstream_1_ticker', '') or '').strip() or None
                    ticker_data['upstream_2_name'] = str(row.get('upstream_2_name', '') or '').strip() or None
                    ticker_data['upstream_2_ticker'] = str(row.get('upstream_2_ticker', '') or '').strip() or None
                    ticker_data['downstream_1_name'] = str(row.get('downstream_1_name', '') or '').strip() or None
                    ticker_data['downstream_1_ticker'] = str(row.get('downstream_1_ticker', '') or '').strip() or None
                    ticker_data['downstream_2_name'] = str(row.get('downstream_2_name', '') or '').strip() or None
                    ticker_data['downstream_2_ticker'] = str(row.get('downstream_2_ticker', '') or '').strip() or None
                    LOG.debug(f"[CSV_IMPORT] Row {row_num} value chain fields processed")
                except Exception as e:
                    LOG.error(f"[CSV_IMPORT] Row {row_num} failed processing value chain fields: {e}")
                    raise

                # LEGACY SUPPORT: Handle old "competitors" field format
                competitors_field = row.get('competitors', '') or ''  # Handle None explicitly
                LOG.debug(f"[CSV_DEBUG] Row {row_num} competitors field type: {type(competitors_field)}, value: {repr(competitors_field)[:100]}")

                if (competitors_field and competitors_field.strip() and
                    not any(ticker_data.get(f'competitor_{i}_name') for i in range(1, 4))):
                    legacy_competitors = competitors_field.split(',')
                    for i, comp in enumerate(legacy_competitors[:3], 1):
                        comp = comp.strip()
                        if comp:
                            # Try to parse "Name (TICKER)" format
                            match = re.search(r'^(.+?)\s*\(([^)]+)\)$', comp)
                            if match:
                                name = match.group(1).strip()
                                ticker_comp = match.group(2).strip()
                                # Validate competitor ticker format
                                normalized_ticker = normalize_ticker_format(ticker_comp)
                                if validate_ticker_format(normalized_ticker):
                                    ticker_data[f'competitor_{i}_name'] = name
                                    ticker_data[f'competitor_{i}_ticker'] = normalized_ticker
                                else:
                                    LOG.warning(f"Invalid competitor ticker format: {ticker_comp}")
                                    ticker_data[f'competitor_{i}_name'] = name
                            else:
                                # Just a name without ticker
                                ticker_data[f'competitor_{i}_name'] = comp
                
                # LEGACY SUPPORT: Handle old "industry_keywords" field
                keywords_field = row.get('industry_keywords', '') or ''  # Handle None explicitly
                LOG.debug(f"[CSV_DEBUG] Row {row_num} industry_keywords field type: {type(keywords_field)}, value: {repr(keywords_field)[:100]}")

                if (keywords_field and keywords_field.strip() and
                    not any(ticker_data.get(f'industry_keyword_{i}') for i in range(1, 4))):
                    legacy_keywords = [kw.strip() for kw in keywords_field.split(',') if kw.strip()]
                    for i, keyword in enumerate(legacy_keywords[:3], 1):
                        ticker_data[f'industry_keyword_{i}'] = keyword
                
                # Clean NULL bytes from text fields
                text_fields = [
                    'company_name', 'industry', 'sector', 'sub_industry', 'exchange',
                    'industry_keyword_1', 'industry_keyword_2', 'industry_keyword_3',
                    'competitor_1_name', 'competitor_2_name', 'competitor_3_name',
                    'competitor_1_ticker', 'competitor_2_ticker', 'competitor_3_ticker',
                    'upstream_1_name', 'upstream_1_ticker', 'upstream_2_name', 'upstream_2_ticker',
                    'downstream_1_name', 'downstream_1_ticker', 'downstream_2_name', 'downstream_2_ticker',
                    'geographic_markets', 'subsidiaries'
                ]
                for field in text_fields:
                    if ticker_data.get(field):
                        ticker_data[field] = clean_null_bytes(str(ticker_data[field]))
                
                # Normalize competitor tickers (no validation - international tickers allowed for feed creation)
                competitor_ticker_fields = ['competitor_1_ticker', 'competitor_2_ticker', 'competitor_3_ticker']
                for field in competitor_ticker_fields:
                    if ticker_data.get(field):
                        ticker_data[field] = normalize_ticker_format(ticker_data[field])

                # Normalize value chain tickers (no validation - international tickers allowed for feed creation)
                value_chain_ticker_fields = ['upstream_1_ticker', 'upstream_2_ticker', 'downstream_1_ticker', 'downstream_2_ticker']
                for field in value_chain_ticker_fields:
                    if ticker_data.get(field):
                        ticker_data[field] = normalize_ticker_format(ticker_data[field])

                ticker_data_batch.append(ticker_data)
                    
            except Exception as e:
                errors.append(f"Row {row_num}: {str(e)}")
                continue
        
        # Bulk insert all valid data in single transaction
        if ticker_data_batch:
            try:
                with db() as conn, conn.cursor() as cur:
                   
                    # Prepare data tuples for bulk insert (fresh start, so everything is imported)
                    insert_data = []
                    imported = len(ticker_data_batch)
                    updated = 0
                    
                    for ticker_data in ticker_data_batch:

                        insert_data.append((
                            ticker_data['ticker'], ticker_data['country'], ticker_data['company_name'],
                            ticker_data.get('industry'), ticker_data.get('sector'), ticker_data.get('sub_industry'),
                            ticker_data.get('exchange'), ticker_data.get('currency'), ticker_data.get('market_cap_category'),
                            ticker_data.get('yahoo_ticker'), ticker_data.get('active', True), ticker_data.get('is_etf', False),
                            ticker_data.get('industry_keyword_1'), ticker_data.get('industry_keyword_2'), ticker_data.get('industry_keyword_3'),
                            ticker_data.get('competitor_1_name'), ticker_data.get('competitor_1_ticker'),
                            ticker_data.get('competitor_2_name'), ticker_data.get('competitor_2_ticker'),
                            ticker_data.get('competitor_3_name'), ticker_data.get('competitor_3_ticker'),
                            ticker_data.get('upstream_1_name'), ticker_data.get('upstream_1_ticker'),
                            ticker_data.get('upstream_2_name'), ticker_data.get('upstream_2_ticker'),
                            ticker_data.get('downstream_1_name'), ticker_data.get('downstream_1_ticker'),
                            ticker_data.get('downstream_2_name'), ticker_data.get('downstream_2_ticker'),
                            ticker_data.get('geographic_markets'), ticker_data.get('subsidiaries'),
                            ticker_data.get('ai_generated', False), ticker_data.get('data_source', 'csv_import')
                        ))
                    
                    # Single bulk INSERT with ON CONFLICT handling
                    cur.executemany("""
                        INSERT INTO ticker_reference (
                            ticker, country, company_name, industry, sector, sub_industry,
                            exchange, currency, market_cap_category, yahoo_ticker, active, is_etf,
                            industry_keyword_1, industry_keyword_2, industry_keyword_3,
                            competitor_1_name, competitor_1_ticker,
                            competitor_2_name, competitor_2_ticker,
                            competitor_3_name, competitor_3_ticker,
                            upstream_1_name, upstream_1_ticker,
                            upstream_2_name, upstream_2_ticker,
                            downstream_1_name, downstream_1_ticker,
                            downstream_2_name, downstream_2_ticker,
                            geographic_markets, subsidiaries,
                            ai_generated, data_source
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (ticker) DO UPDATE SET
                            country = EXCLUDED.country,
                            company_name = EXCLUDED.company_name,
                            industry = EXCLUDED.industry,
                            sector = EXCLUDED.sector,
                            sub_industry = EXCLUDED.sub_industry,
                            exchange = EXCLUDED.exchange,
                            currency = EXCLUDED.currency,
                            market_cap_category = EXCLUDED.market_cap_category,
                            yahoo_ticker = EXCLUDED.yahoo_ticker,
                            active = EXCLUDED.active,
                            is_etf = EXCLUDED.is_etf,
                            industry_keyword_1 = EXCLUDED.industry_keyword_1,
                            industry_keyword_2 = EXCLUDED.industry_keyword_2,
                            industry_keyword_3 = EXCLUDED.industry_keyword_3,
                            competitor_1_name = EXCLUDED.competitor_1_name,
                            competitor_1_ticker = EXCLUDED.competitor_1_ticker,
                            competitor_2_name = EXCLUDED.competitor_2_name,
                            competitor_2_ticker = EXCLUDED.competitor_2_ticker,
                            competitor_3_name = EXCLUDED.competitor_3_name,
                            competitor_3_ticker = EXCLUDED.competitor_3_ticker,
                            upstream_1_name = EXCLUDED.upstream_1_name,
                            upstream_1_ticker = EXCLUDED.upstream_1_ticker,
                            upstream_2_name = EXCLUDED.upstream_2_name,
                            upstream_2_ticker = EXCLUDED.upstream_2_ticker,
                            downstream_1_name = EXCLUDED.downstream_1_name,
                            downstream_1_ticker = EXCLUDED.downstream_1_ticker,
                            downstream_2_name = EXCLUDED.downstream_2_name,
                            downstream_2_ticker = EXCLUDED.downstream_2_ticker,
                            geographic_markets = EXCLUDED.geographic_markets,
                            subsidiaries = EXCLUDED.subsidiaries,
                            ai_generated = EXCLUDED.ai_generated,
                            data_source = EXCLUDED.data_source,
                            updated_at = NOW()
                    """, insert_data)
                    
                    LOG.info(f"BULK INSERT COMPLETED: {len(insert_data)} records processed in single transaction")
                    
            except Exception as e:
                LOG.error(f"Bulk insert failed: {e}")
                return {
                    "status": "error", 
                    "message": f"Bulk insert failed: {str(e)}",
                    "imported": 0,
                    "updated": 0,
                    "errors": [str(e)]
                }
        else:
            LOG.warning("No valid ticker data found to import")
        
        LOG.info(f"CSV Import completed: {imported} imported, {updated} updated, {len(errors)} errors, {skipped} skipped")
        
        return {
            "status": "completed",
            "imported": imported,
            "updated": updated,
            "skipped": skipped,
            "errors": errors[:10],
            "total_errors": len(errors),
            "message": f"Successfully processed {imported + updated} ticker references ({imported} new, {updated} updated)"
        }
        
    except Exception as e:
        full_trace = traceback.format_exc()
        LOG.error(f"[CSV_IMPORT] CSV parsing failed with exception: {e}")
        LOG.error(f"[CSV_IMPORT] Full traceback:\n{full_trace}")
        return {
            "status": "error",
            "message": f"CSV parsing failed: {str(e)}",
            "imported": 0,
            "updated": 0,
            "errors": [str(e)]
        }

# 4. HELPER FUNCTIONS for data management
def get_all_ticker_references(limit: int = None, offset: int = 0, country_filter: str = None):
    """Get all ticker references with pagination and filtering"""
    with db() as conn, conn.cursor() as cur:
        # Build query based on filters
        where_clause = "WHERE active = TRUE"
        params = []
        
        if country_filter:
            where_clause += " AND country = %s"
            params.append(country_filter.upper())
        
        # Add pagination
        limit_clause = ""
        if limit:
            limit_clause = f" LIMIT {limit} OFFSET {offset}"
        
        cur.execute(f"""
            SELECT ticker, country, company_name, industry, sector, exchange, 
                   currency, market_cap_category, 
                   industry_keyword_1, industry_keyword_2, industry_keyword_3,
                   competitor_1_name, competitor_1_ticker,
                   competitor_2_name, competitor_2_ticker,
                   competitor_3_name, competitor_3_ticker,
                   ai_generated, ai_enhanced_at, created_at, updated_at
            FROM ticker_reference
            {where_clause}
            ORDER BY ticker
            {limit_clause}
        """, params)
        
        return list(cur.fetchall())

def count_ticker_references(country_filter: str = None) -> int:
    """Count total ticker references"""
    with db() as conn, conn.cursor() as cur:
        where_clause = "WHERE active = TRUE"
        params = []
        
        if country_filter:
            where_clause += " AND country = %s"
            params.append(country_filter.upper())
        
        cur.execute(f"""
            SELECT COUNT(*) as count
            FROM ticker_reference
            {where_clause}
        """, params)
        
        result = cur.fetchone()
        return result["count"] if result else 0

def delete_ticker_reference(ticker: str) -> bool:
    """Delete (deactivate) a ticker reference"""
    normalized_ticker = normalize_ticker_format(ticker)
    
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            UPDATE ticker_reference 
            SET active = FALSE, updated_at = NOW()
            WHERE ticker = %s
        """, (normalized_ticker,))
        
        if cur.rowcount > 0:
            LOG.info(f"Deactivated ticker reference: {normalized_ticker}")
            return True
        else:
            LOG.warning(f"Ticker reference not found: {normalized_ticker}")
            return False

# GitHub Integration Functions
# 1. FETCH CSV FROM GITHUB - Download latest version
def fetch_csv_from_github():
    """Download the latest ticker reference CSV from GitHub repository"""
    if not GITHUB_TOKEN or not GITHUB_REPO:
        return {
            "status": "error",
            "message": "GitHub integration not configured. Set GITHUB_TOKEN and GITHUB_REPO environment variables."
        }
    
    try:
        # GitHub API URL for file content
        api_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{GITHUB_CSV_PATH}"
        
        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        LOG.info(f"Fetching CSV from GitHub: {GITHUB_REPO}/{GITHUB_CSV_PATH}")
        
        response = requests.get(api_url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            file_data = response.json()

            # Get file metadata
            file_info = {
                "sha": file_data["sha"],
                "size": file_data["size"],
                "last_modified": file_data.get("last_modified"),
                "download_url": file_data["download_url"]
            }

            LOG.info(f"[GITHUB_FETCH] File size: {file_info['size']} bytes, SHA: {file_info['sha'][:8]}")

            # For files >1MB, GitHub's content field is empty - use download_url instead
            raw_content = file_data.get("content", "")

            if not raw_content or file_info["size"] > 1000000:
                LOG.info(f"[GITHUB_FETCH] Using download_url for large file ({file_info['size']} bytes)")

                # Fetch raw content directly via download_url
                download_response = requests.get(file_info["download_url"], timeout=30)

                if download_response.status_code == 200:
                    csv_content = download_response.text
                    LOG.info(f"[GITHUB_FETCH] Successfully downloaded {len(csv_content)} characters via download_url")
                else:
                    LOG.error(f"[GITHUB_FETCH] Failed to download from download_url: HTTP {download_response.status_code}")
                    return {
                        "status": "error",
                        "message": f"Failed to download file content: HTTP {download_response.status_code}",
                        "csv_content": None
                    }
            else:
                # Small file - decode from base64 content field
                LOG.info(f"[GITHUB_FETCH] Using base64 content field for small file")
                csv_content = base64.b64decode(raw_content).decode('utf-8')
            
            return {
                "status": "success",
                "csv_content": csv_content,
                "file_info": file_info,
                "message": f"Successfully downloaded {len(csv_content)} characters from GitHub"
            }
            
        elif response.status_code == 404:
            return {
                "status": "error",
                "message": f"CSV file not found at {GITHUB_REPO}/{GITHUB_CSV_PATH}. Please verify the path."
            }
        elif response.status_code == 401:
            return {
                "status": "error",
                "message": "GitHub authentication failed. Please check your GITHUB_TOKEN."
            }
        elif response.status_code == 403:
            return {
                "status": "error",
                "message": "GitHub API rate limit exceeded or insufficient permissions."
            }
        else:
            return {
                "status": "error",
                "message": f"GitHub API error {response.status_code}: {response.text}"
            }
            
    except requests.RequestException as e:
        LOG.error(f"Network error fetching CSV from GitHub: {e}")
        return {
            "status": "error",
            "message": f"Network error: {str(e)}"
        }
    except Exception as e:
        LOG.error(f"Unexpected error fetching CSV from GitHub: {e}")
        return {
            "status": "error",
            "message": f"Unexpected error: {str(e)}"
        }

# 2. EXPORT SQL TO CSV FORMAT - Generate CSV from current database
def export_ticker_references_to_csv():
    LOG.info("ðŸš¨ EXPORT FUNCTION CALLED - UPDATED VERSION WITH DEBUG ðŸš¨")
    """Export all ticker references from database to CSV format with new structure"""
    try:
        with db() as conn, conn.cursor() as cur:
            # Get all ticker references with all fields
            cur.execute("""
                SELECT ticker, country, company_name, industry, sector, sub_industry,
                       exchange, currency, market_cap_category, active, is_etf, yahoo_ticker,
                       industry_keyword_1, industry_keyword_2, industry_keyword_3,
                       ai_generated, ai_enhanced_at,
                       competitor_1_name, competitor_1_ticker,
                       competitor_2_name, competitor_2_ticker,
                       competitor_3_name, competitor_3_ticker,
                       upstream_1_name, upstream_1_ticker,
                       upstream_2_name, upstream_2_ticker,
                       downstream_1_name, downstream_1_ticker,
                       downstream_2_name, downstream_2_ticker,
                       geographic_markets, subsidiaries,
                       financial_last_price, financial_price_change_pct,
                       financial_yesterday_return_pct, financial_ytd_return_pct,
                       financial_market_cap, financial_enterprise_value,
                       financial_volume, financial_avg_volume,
                       financial_analyst_target, financial_analyst_range_low,
                       financial_analyst_range_high, financial_analyst_count,
                       financial_analyst_recommendation, financial_snapshot_date,
                       created_at, updated_at, data_source
                FROM ticker_reference
                ORDER BY ticker
            """)
            
            rows = cur.fetchall()
            LOG.info(f"DEBUG: Retrieved {len(rows)} rows from database")
            
            if not rows:
                LOG.error("DEBUG: No rows returned from database query")
                return {
                    "status": "error",
                    "message": "No ticker references found in database"
                }
            
            # Debug: Show first row structure
            LOG.info(f"DEBUG: First row sample: {dict(rows[0]) if rows else 'No rows'}")
            LOG.info(f"DEBUG: Row has {len(rows[0])} columns")
            
            # Build CSV content
            csv_buffer = io.StringIO()
            
            # Define CSV headers (matching your GitHub CSV structure)
            headers = [
                'ticker', 'country', 'company_name', 'industry', 'sector', 'sub_industry',
                'exchange', 'currency', 'market_cap_category', 'active', 'is_etf', 'yahoo_ticker',
                'industry_keyword_1', 'industry_keyword_2', 'industry_keyword_3',
                'ai_generated', 'ai_enhanced_at',
                'competitor_1_name', 'competitor_1_ticker',
                'competitor_2_name', 'competitor_2_ticker',
                'competitor_3_name', 'competitor_3_ticker',
                'upstream_1_name', 'upstream_1_ticker',
                'upstream_2_name', 'upstream_2_ticker',
                'downstream_1_name', 'downstream_1_ticker',
                'downstream_2_name', 'downstream_2_ticker',
                'geographic_markets', 'subsidiaries',
                'financial_last_price', 'financial_price_change_pct',
                'financial_yesterday_return_pct', 'financial_ytd_return_pct',
                'financial_market_cap', 'financial_enterprise_value',
                'financial_volume', 'financial_avg_volume',
                'financial_analyst_target', 'financial_analyst_range_low',
                'financial_analyst_range_high', 'financial_analyst_count',
                'financial_analyst_recommendation', 'financial_snapshot_date',
                'created_at', 'updated_at', 'data_source'
            ]
            
            LOG.info(f"DEBUG: Header has {len(headers)} columns")
            
            writer = csv.writer(csv_buffer)
            writer.writerow(headers)
            
            # Write data rows with debug info
            rows_written = 0
            for i, row in enumerate(rows):
                try:
                    csv_row = []
                    
                    # Debug first few rows
                    if i < 3:
                        LOG.info(f"DEBUG: Processing row {i}: ticker={row.get('ticker', 'NO_TICKER')}")
                    
                    for header in headers:
                        value = row[header]
                        if value is None:
                            csv_row.append('')
                        elif isinstance(value, bool):
                            csv_row.append('TRUE' if value else 'FALSE')
                        elif isinstance(value, datetime):
                            csv_row.append(value.isoformat())
                        else:
                            csv_row.append(str(value))
                    
                    # Debug row length
                    if i < 3:
                        LOG.info(f"DEBUG: Row {i} has {len(csv_row)} values")
                    
                    writer.writerow(csv_row)
                    rows_written += 1
                    
                except Exception as row_error:
                    LOG.error(f"DEBUG: Error processing row {i}: {row_error}")
                    continue
            
            csv_content = csv_buffer.getvalue()
            csv_buffer.close()
            
            LOG.info(f"DEBUG: Wrote {rows_written} data rows to CSV")
            LOG.info(f"DEBUG: CSV content length: {len(csv_content)} characters")
            LOG.info(f"DEBUG: CSV starts with: {csv_content[:200]}...")
            
            return {
                "status": "success",
                "csv_content": csv_content,
                "ticker_count": len(rows),
                "message": f"Successfully exported {len(rows)} ticker references"
            }
            
    except Exception as e:
        LOG.error(f"Failed to export ticker references to CSV: {e}")
        return {
            "status": "error",
            "message": f"Export failed: {str(e)}"
        }

# 3. COMMIT CSV TO GITHUB - Push updated CSV back to repository
def commit_csv_to_github(csv_content: str, commit_message: str = None, file_path: str = None):
    """
    Push updated CSV content back to GitHub repository.

    Args:
        csv_content: The CSV file content as a string
        commit_message: Optional commit message
        file_path: Optional file path in repo (e.g., "data/users/user_tickers.csv")
                   If not provided, uses GITHUB_CSV_PATH environment variable
    """
    if not GITHUB_TOKEN or not GITHUB_REPO:
        return {
            "status": "error",
            "message": "GitHub integration not configured. Set GITHUB_TOKEN and GITHUB_REPO environment variables."
        }
    
    if not csv_content:
        return {
            "status": "error",
            "message": "No CSV content provided"
        }
    
    try:
        # Determine which file path to use
        target_path = file_path or GITHUB_CSV_PATH

        # First, get the current file to obtain its SHA (required for updates)
        api_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{target_path}"
        
        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28"
        }
        
        # Get current file info
        LOG.info(f"Getting current file info from GitHub: {GITHUB_REPO}/{target_path}")
        response = requests.get(api_url, headers=headers, timeout=30)
        
        file_sha = None
        if response.status_code == 200:
            current_file = response.json()
            file_sha = current_file["sha"]
            LOG.info(f"Found existing file, SHA: {file_sha[:8]}")
        elif response.status_code == 404:
            LOG.info("File doesn't exist, will create new file")
        else:
            return {
                "status": "error",
                "message": f"Failed to get current file info: {response.status_code} - {response.text}"
            }
        
        # Prepare commit data
        if not commit_message:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
            commit_message = f"[skip render] Update ticker reference data - {timestamp}"
        
        # Encode content to base64
        encoded_content = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')
        
        commit_data = {
            "message": commit_message,
            "content": encoded_content,
        }
        
        # Include SHA for updates (not for new files)
        if file_sha:
            commit_data["sha"] = file_sha
        
        # Commit the file
        LOG.info(f"Committing CSV to GitHub: {len(csv_content)} characters")
        # Add retry logic for network timeouts AND SHA conflicts
        max_retries = 3
        sha_retry_count = 0
        max_sha_retries = 2  # Allow 2 SHA conflict retries

        for attempt in range(max_retries):
            try:
                LOG.info(f"GitHub commit attempt {attempt + 1}/{max_retries}")
                commit_response = requests.put(api_url, headers=headers, json=commit_data, timeout=120)

                # Handle 409 Conflict (SHA mismatch) - someone else committed
                if commit_response.status_code == 409 and sha_retry_count < max_sha_retries:
                    sha_retry_count += 1
                    LOG.warning(f"âš ï¸ GitHub SHA conflict detected (attempt {sha_retry_count}/{max_sha_retries})")
                    LOG.warning("   Another commit was made between GET and PUT. Re-fetching current SHA...")

                    # Re-fetch current file SHA
                    time.sleep(2)  # Brief pause before retry
                    refetch_response = requests.get(api_url, headers=headers, timeout=30)

                    if refetch_response.status_code == 200:
                        current_file = refetch_response.json()
                        new_sha = current_file["sha"]
                        LOG.info(f"   Refetched SHA: {new_sha[:8]} (was: {file_sha[:8] if file_sha else 'None'})")

                        # Update commit_data with new SHA
                        commit_data["sha"] = new_sha
                        file_sha = new_sha

                        # Retry the commit with new SHA
                        LOG.info(f"   Retrying commit with updated SHA...")
                        continue
                    else:
                        LOG.error(f"   Failed to refetch SHA: {refetch_response.status_code}")
                        return {
                            "status": "error",
                            "message": f"SHA conflict: failed to refetch file ({refetch_response.status_code})"
                        }

                # If not a retryable 409, break out of loop
                break

            except requests.Timeout as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 10  # 10s, 20s, 30s
                    LOG.warning(f"GitHub commit timeout (attempt {attempt + 1}), retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    LOG.error(f"GitHub commit failed after {max_retries} timeout attempts")
                    return {
                        "status": "error",
                        "message": f"GitHub commit timed out after {max_retries} attempts"
                    }
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # 5s, 10s, 15s
                    LOG.warning(f"GitHub commit network error (attempt {attempt + 1}), retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    raise

        if commit_response.status_code in [200, 201]:
            commit_result = commit_response.json()

            LOG.info(f"âœ… Successfully committed CSV to GitHub: {commit_result['commit']['sha'][:8]}")

            return {
                "status": "success",
                "commit_sha": commit_result['commit']['sha'],
                "file_sha": commit_result['content']['sha'],
                "commit_url": commit_result['commit']['html_url'],
                "message": f"Successfully updated {target_path} in {GITHUB_REPO}",
                "csv_size": len(csv_content),
                "sha_retries": sha_retry_count
            }
        else:
            error_msg = commit_response.text
            LOG.error(f"âŒ Failed to commit CSV to GitHub: {commit_response.status_code} - {error_msg}")

            return {
                "status": "error",
                "message": f"GitHub commit failed ({commit_response.status_code}): {error_msg}",
                "response_body": error_msg[:500]  # Include partial response for debugging
            }
            
    except requests.RequestException as e:
        LOG.error(f"Network error committing CSV to GitHub: {e}")
        return {
            "status": "error",
            "message": f"Network error: {str(e)}"
        }
    except Exception as e:
        LOG.error(f"Unexpected error committing CSV to GitHub: {e}")
        return {
            "status": "error",
            "message": f"Unexpected error: {str(e)}"
        }

# 4. FULL SYNC OPERATIONS - High-level workflow functions
def sync_ticker_references_from_github():
    """Complete workflow: Fetch CSV from GitHub and update database"""
    LOG.info("=== Starting full sync from GitHub ===")
    
    # Step 1: Fetch CSV from GitHub
    fetch_result = fetch_csv_from_github()
    if fetch_result["status"] != "success":
        return fetch_result
    
    # Step 2: Import CSV into database
    import_result = import_ticker_reference_from_csv_content(fetch_result["csv_content"])
    
    # Combine results - Fix status logic
    sync_status = "success" if import_result["status"] == "completed" else "error"
    success_message = f"GitHub sync successful: {import_result.get('imported', 0)} imported, {import_result.get('updated', 0)} updated"
    
    return {
        "status": sync_status,
        "github_fetch": {
            "csv_size": len(fetch_result["csv_content"]),
            "file_sha": fetch_result["file_info"]["sha"][:8]
        },
        "database_import": {
            "imported": import_result.get("imported", 0),
            "updated": import_result.get("updated", 0),
            "skipped": import_result.get("skipped", 0),
            "errors": import_result.get("total_errors", 0)
        },
        "message": success_message if sync_status == "success" else f"GitHub sync failed: {import_result.get('message', 'Unknown error')}"
    }

def sync_ticker_references_to_github(commit_message: str = None):
    """Export database to CSV format (no GitHub commit)"""
    LOG.info("=== Exporting ticker references to CSV format ===")

    # Export database to CSV
    export_result = export_ticker_references_to_csv()

    return {
        "status": export_result["status"],
        "database_export": {
            "ticker_count": export_result.get("ticker_count", 0),
            "csv_size": len(export_result.get("csv_content", ""))
        },
        "message": f"Exported {export_result.get('ticker_count', 0)} ticker references to CSV format"
    }

# DISABLED (Nov 30, 2025): CSV is source of truth - never write DB back to ticker_reference.csv
# def commit_ticker_reference_to_github():
#     """
#     Daily cron job function: Export ticker_reference from database and commit to GitHub.
#     Called by: python app.py commit (Render cron at 6:30 AM EST)
#
#     Unlike incremental commits during job runs, this triggers a Render deployment.
#     """
#     LOG.info("ðŸ”„ ============================================")
#     LOG.info("ðŸ”„ DAILY GITHUB COMMIT - ticker_reference.csv")
#     LOG.info("ðŸ”„ ============================================")
#
#     try:
#         # Step 1: Export ticker_reference from database to CSV
#         LOG.info("ðŸ“¤ Step 1: Exporting ticker_reference from database...")
#         export_result = export_ticker_references_to_csv()
#
#         if export_result["status"] != "success":
#             LOG.error(f"âŒ Export failed: {export_result.get('message', 'Unknown error')}")
#             return {
#                 "status": "error",
#                 "message": f"CSV export failed: {export_result.get('message')}",
#                 "rows": 0
#             }
#
#         csv_content = export_result["csv_content"]
#         ticker_count = export_result.get("ticker_count", 0)
#         LOG.info(f"âœ… Exported {ticker_count} tickers ({len(csv_content)} chars)")
#
#         # Step 2: Commit to GitHub (WITHOUT [skip render])
#         LOG.info("ðŸ“¤ Step 2: Committing to GitHub (triggers deployment)...")
#         from datetime import datetime
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#
#         commit_message = f"Daily ticker reference update - {timestamp}"
#
#         commit_result = commit_csv_to_github(csv_content, commit_message)
#
#         if commit_result["status"] != "success":
#             error_msg = commit_result.get('message', 'Unknown error')
#
#             # Check if it's a missing env vars error (not a critical failure)
#             if "not configured" in error_msg or "GITHUB_TOKEN" in error_msg:
#                 LOG.warning(f"âš ï¸ GitHub commit skipped: {error_msg}")
#                 LOG.warning(f"   To enable: Set GITHUB_TOKEN and GITHUB_REPO in Render environment")
#                 LOG.info(f"âœ… CSV export successful (GitHub backup skipped)")
#                 return {
#                     "status": "success",
#                     "message": "CSV export successful (GitHub backup skipped - env vars not configured)",
#                     "rows": ticker_count,
#                     "timestamp": timestamp,
#                     "github_skipped": True
#                 }
#             else:
#                 # Real error (network, permission, etc)
#                 LOG.error(f"âŒ GitHub commit failed: {error_msg}")
#                 return {
#                     "status": "error",
#                     "message": f"GitHub commit failed: {error_msg}",
#                     "rows": ticker_count
#                 }
#
#         LOG.info(f"âœ… GitHub commit successful")
#         LOG.info(f"   Commit SHA: {commit_result.get('commit_sha', 'N/A')[:8]}")
#         LOG.info(f"   Tickers: {ticker_count}")
#         LOG.info(f"   CSV size: {len(csv_content)} chars")
#         LOG.info(f"âš ï¸  Render deployment will start in ~10 seconds")
#
#         return {
#             "status": "success",
#             "message": "Ticker reference committed to GitHub successfully",
#             "rows": ticker_count,
#             "commit_sha": commit_result.get("commit_sha"),
#             "commit_url": commit_result.get("commit_url"),
#             "timestamp": timestamp
#         }
#
#     except Exception as e:
#         LOG.error(f"âŒ Daily GitHub commit failed: {e}")
#         LOG.error(traceback.format_exc())
#         return {
#             "status": "error",
#             "message": f"Unexpected error: {str(e)}",
#             "rows": 0
#         }

# 5. SELECTIVE TICKER UPDATE - Update specific tickers only
def update_specific_tickers_on_github(tickers: list, commit_message: str = None):
    """Update specific tickers in CSV format (no GitHub commit)"""
    if not tickers:
        return {"status": "error", "message": "No tickers specified"}
    
    LOG.info(f"=== Updating specific tickers in CSV format: {tickers} ===")
    
    try:
        # Get current CSV from database export
        export_result = export_ticker_references_to_csv()
        if export_result["status"] != "success":
            return export_result
        
        # Parse and validate the updated data
        updated_csv_content = export_result["csv_content"]
        
        return {
            "status": "success", 
            "updated_tickers": len(tickers),
            "csv_content": updated_csv_content,
            "message": f"Updated {len(tickers)} tickers in CSV format"
        }
        
    except Exception as e:
        LOG.error(f"Failed to update specific tickers: {e}")
        return {"status": "error", "message": f"Update failed: {str(e)}"}

def extract_article_content(url: str, domain: str) -> Tuple[Optional[str], Optional[str]]:
    """
    Enhanced article extraction with intelligent delay management and content cleaning
    """
    try:
        # Check for known paywall domains first (reduced list)
        if normalize_domain(domain) in PAYWALL_DOMAINS:
            return None, f"Paywall domain: {domain}"
        
        # Get domain-specific strategy
        strategy = get_domain_strategy(domain)
        
        # Add domain-specific headers
        headers = strategy.get('headers', {})
        headers['Referer'] = get_referrer_for_domain(url, domain)
        headers['User-Agent'] = get_random_user_agent()
        
        # Apply headers to session
        scraping_session.headers.update(headers)
        
        # INTELLIGENT DELAY - only delay when necessary
        delay, reason = calculate_intelligent_delay(domain)
        if delay > 0.5:  # Only log significant delays
            LOG.info(f"Waiting {delay:.1f}s before scraping {domain} ({reason})")
            time.sleep(delay)
        else:
            LOG.debug(f"Quick scrape of {domain} ({reason})")
        
        # Try enhanced scraping with backoff
        response = scrape_with_backoff(url)
        if not response:
            return None, "Failed to fetch URL after retries"
        
        # Handle cookies and JavaScript redirects
        if 'Set-Cookie' in response.headers:
            scraping_session.cookies.update(response.cookies)
        
        # Check for JavaScript redirects
        if 'window.location' in response.text or 'document.location' in response.text:
            return None, "JavaScript redirect detected"
        
        # Use newspaper3k to parse the HTML
        config = newspaper.Config()
        config.browser_user_agent = headers.get('User-Agent')
        config.request_timeout = 15
        config.fetch_images = False
        config.memoize_articles = False
        
        article = newspaper.Article(url, config=config)
        article.set_html(response.text)
        article.parse()
        
        # Get the main text content
        raw_content = article.text.strip()
        
        if not raw_content:
            return None, "No content extracted"
        
        # ENHANCED CONTENT CLEANING
        cleaned_content = clean_scraped_content(raw_content, url, domain)
        
        if not cleaned_content or len(cleaned_content.strip()) < 100:
            return None, "Content too short after cleaning"
        
        # Enhanced cookie banner detection on cleaned content
        content_lower = cleaned_content.lower()
        cookie_indicators = [
            "we use cookies", "accept all cookies", "cookie policy",
            "privacy policy and terms of service", "consent to the use"
        ]
        
        cookie_count = sum(1 for indicator in cookie_indicators if indicator in content_lower)
        
        # If multiple cookie indicators and content is short, likely a cookie page
        if cookie_count >= 2 and len(cleaned_content) < 500:
            return None, "Cookie consent page detected"
        
        # Enhanced paywall indicators (check cleaned content)
        paywall_indicators = [
            "subscribe to continue", "unlock this story", "premium content",
            "sign up to read", "become a member", "subscription required"
        ]
        
        if any(indicator in content_lower for indicator in paywall_indicators):
            return None, "Paywall content detected"
        
        # Validate content quality on cleaned content
        is_valid, validation_msg = validate_scraped_content(cleaned_content, url, domain)
        if not is_valid:
            return None, validation_msg
        
        LOG.info(f"Successfully extracted and cleaned {len(cleaned_content)} chars from {domain}")
        return cleaned_content, None
        
    except newspaper.article.ArticleException as e:
        error_msg = f"Newspaper article error: {str(e)}"
        LOG.warning(f"Failed to extract content from {url}: {error_msg}")
        return None, error_msg
    except Exception as e:
        error_msg = f"Content extraction failed: {str(e)}"
        LOG.warning(f"Failed to extract content from {url}: {error_msg}")
        return None, error_msg

def create_scraping_session():
    session = requests.Session()
    
    # Retry strategy
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # Comprehensive headers
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0',
    })
    
    return session

# ============================================================================
# PLAYWRIGHT REMOVED (Oct 2025) - Replaced with 3-Tier Scrapfly Architecture
# ============================================================================
# Playwright caused indefinite hangs and required 100MB+ overhead per ticker.
# NEW: 3-tier fallback: Scrapfly scrape+extract â†’ Scrapfly HTML â†’ Free
# ============================================================================

# Helper functions for tier implementations
def _host(u: str) -> str:
    """Extract hostname from URL"""
    h = (urlparse(u).hostname or "").lower()
    if h.startswith("www."):
        h = h[4:]
    return h

def _matches(host: str, dom: str) -> bool:
    """Check if host matches domain (exact or subdomain)"""
    return host == dom or host.endswith("." + dom)

# Domains requiring anti-bot protection (used in Tier 1 and Tier 2)
ANTIBOT_DOMAINS = {
    "simplywall.st", "seekingalpha.com", "zacks.com", "benzinga.com",
    "cnbc.com", "investing.com", "gurufocus.com", "fool.com",
    "insidermonkey.com", "nasdaq.com", "markets.financialcontent.com",
    "thefly.com", "streetinsider.com", "accesswire.com",
    "247wallst.com", "barchart.com", "telecompaper.com",
    "news.stocktradersdaily.com", "sharewise.com"
}


async def scrape_with_scrapfly_html_only(url: str, domain: str, max_retries: int = 2) -> Tuple[Optional[str], Optional[str]]:
    """
    TIER 2: Scrapfly Web Scraping API + newspaper3k parsing
    Uses GET /scrape endpoint to fetch HTML (bypasses anti-bot), then we parse with newspaper3k.
    Cost: ~$0.0003 per article (1 API credit for basic scrape)
    Success rate: ~85% (Scrapfly handles anti-bot, newspaper3k parses)

    This tier is ONLY called when free scraping fails (anti-bot protection, JavaScript sites, etc.)
    """
    global scrapfly_stats, enhanced_scraping_stats

    if "video.media.yql.yahoo.com" in url:
        LOG.warning(f"SCRAPFLY: Rejecting video URL: {url}")
        return None, "Video URL not supported"

    for attempt in range(max_retries + 1):
        try:
            if not SCRAPFLY_API_KEY:
                return None, "Scrapfly API key not configured"

            if normalize_domain(domain) in PAYWALL_DOMAINS:
                return None, f"Paywall domain: {domain}"

            if attempt > 0:
                delay = 2 ** attempt
                LOG.info(f"SCRAPFLY RETRY {attempt}/{max_retries} for {domain} after {delay}s delay")
                await asyncio.sleep(delay)

            LOG.info(f"SCRAPFLY: Attempting {domain} (attempt {attempt + 1})")

            # Update usage stats (1 credit = ~$0.0003)
            scrapfly_stats["requests_made"] += 1
            scrapfly_stats["cost_estimate"] += 0.0003
            scrapfly_stats["by_domain"][domain]["attempts"] += 1
            enhanced_scraping_stats["by_method"]["scrapfly"]["attempts"] += 1

            # Build params for Web Scraping API without extraction
            params = {
                "key": SCRAPFLY_API_KEY,
                "url": url,
                "country": "us"
            }

            # Enable anti-bot protection for known tough domains
            host = _host(url)
            if any(_matches(host, d) for d in ANTIBOT_DOMAINS) and "video.media" not in host:
                params["asp"] = "true"
                params["render_js"] = "true"

            session = get_http_session()
            async with session.get("https://api.scrapfly.io/scrape", params=params, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        try:
                            result = await response.json()
                            html_content = result.get("result", {}).get("content", "")

                            if not html_content or len(html_content) < 500:
                                LOG.warning(f"SCRAPFLY: Insufficient HTML for {domain} ({len(html_content)} bytes)")
                                if attempt < max_retries:
                                    continue
                                return None, "Insufficient HTML"

                            # Parse with newspaper3k
                            article = newspaper.Article(url)
                            article.set_html(html_content)
                            article.parse()

                            text = article.text.strip()

                            if not text or len(text) < 100:
                                LOG.warning(f"SCRAPFLY: newspaper3k extraction empty for {domain} ({len(text)} chars)")
                                if attempt < max_retries:
                                    continue
                                return None, "Extraction returned insufficient content"

                            # Minimal cleaning
                            cleaned_content = clean_scraped_content(text, url, domain)

                            if not cleaned_content or len(cleaned_content) < 100:
                                LOG.warning(f"SCRAPFLY: Content too short after cleaning for {domain}")
                                if attempt < max_retries:
                                    continue
                                return None, "Content too short after cleaning"

                            # Validation
                            is_valid, validation_msg = validate_scraped_content(cleaned_content, url, domain)
                            if not is_valid:
                                LOG.warning(f"SCRAPFLY: Validation failed for {domain}: {validation_msg}")
                                if attempt < max_retries:
                                    continue
                                return None, f"Validation failed: {validation_msg}"

                            # Track tier success
                            scrapfly_stats["successful"] += 1
                            scrapfly_stats["by_domain"][domain]["successes"] += 1
                            enhanced_scraping_stats["by_method"]["scrapfly"]["successes"] += 1
                            enhanced_scraping_stats["scrapfly_success"] += 1

                            LOG.info(f"âœ… SCRAPFLY SUCCESS: {domain} -> {len(cleaned_content)} chars")
                            return cleaned_content, None

                        except Exception as e:
                            LOG.warning(f"SCRAPFLY: Error processing for {domain}: {e}")
                            if attempt < max_retries:
                                continue
                            return None, str(e)

                    elif response.status == 402:
                        LOG.error(f"SCRAPFLY: Payment required for {domain}")
                        return None, "Scrapfly quota exceeded"

                    elif response.status == 422:
                        error_text = await response.text()
                        LOG.warning(f"SCRAPFLY: 422 for {domain}: {error_text[:500]}")
                        return None, "Invalid parameters"

                    elif response.status == 429:
                        LOG.warning(f"SCRAPFLY: Rate limited for {domain}")
                        if attempt < max_retries:
                            await asyncio.sleep(5)
                            continue
                        return None, "Rate limited"

                    else:
                        error_text = await response.text()
                        LOG.warning(f"SCRAPFLY: HTTP {response.status} for {domain}: {error_text[:500]}")
                        if attempt < max_retries:
                            continue
                        return None, f"HTTP {response.status}"

        except Exception as e:
            LOG.warning(f"SCRAPFLY: Request error for {domain} (attempt {attempt + 1}): {e}")
            if attempt < max_retries:
                continue
            return None, str(e)

    # If we got here, all retries failed
    scrapfly_stats["failed"] += 1
    return None, f"Scrapfly HTML fetch failed after {max_retries + 1} attempts"


async def scrape_with_requests_free(url: str, domain: str) -> Tuple[Optional[str], Optional[str]]:
    """
    TIER 1: Free scraping (FIRST tier - tries this before paid Scrapfly)
    Uses direct HTTP request + newspaper3k parser.
    Cost: $0
    Success rate: ~70% (works for simple sites without anti-bot)
    """
    global enhanced_scraping_stats

    try:
        LOG.info(f"FREE SCRAPER: Starting for {domain}")

        # Track tier attempts
        enhanced_scraping_stats["by_method"]["requests"]["attempts"] += 1

        # Check paywall domains
        if normalize_domain(domain) in PAYWALL_DOMAINS:
            return None, f"Paywall domain: {domain}"

        session = get_http_session()
        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Referer": "https://www.google.com/"
        }

        async with session.get(url, headers=headers, allow_redirects=True, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status != 200:
                    LOG.warning(f"FREE: HTTP {response.status} for {domain}")
                    return None, f"HTTP {response.status}"

                html = await response.text()

                if not html or len(html) < 500:
                    LOG.warning(f"FREE: Insufficient HTML for {domain} ({len(html)} bytes)")
                    return None, "Insufficient HTML"

                # Use newspaper3k to parse
                article = newspaper.Article(url)
                article.set_html(html)
                article.parse()

                text = article.text.strip()

                if not text or len(text) < 100:
                    LOG.warning(f"FREE: newspaper3k extraction empty for {domain} ({len(text)} chars)")
                    return None, "Extraction returned insufficient content"

                # Minimal cleaning
                cleaned_content = clean_scraped_content(text, url, domain)

                if not cleaned_content or len(cleaned_content) < 100:
                    LOG.warning(f"FREE: Content too short after cleaning for {domain}")
                    return None, "Content too short after cleaning"

                # Validation
                is_valid, validation_msg = validate_scraped_content(cleaned_content, url, domain)
                if not is_valid:
                    LOG.warning(f"FREE: Validation failed for {domain}: {validation_msg}")
                    return None, f"Validation failed: {validation_msg}"

                # Track tier success
                enhanced_scraping_stats["by_method"]["requests"]["successes"] += 1
                enhanced_scraping_stats["requests_success"] += 1

                LOG.info(f"âœ… FREE SUCCESS: {domain} -> {len(cleaned_content)} chars")
                return cleaned_content, None

    except Exception as e:
        LOG.warning(f"FREE: Error for {domain}: {e}")
        return None, str(e)


async def scrape_with_scrapfly_async(url: str, domain: str, max_retries: int = 2) -> Tuple[Optional[str], Optional[str]]:
    """
    2-TIER SCRAPING ARCHITECTURE (REVERTED TO OLD STRUCTURE):

    Tier 1: Free requests + newspaper3k (handles ~70% of sites, $0 cost)
      â†“ If fails
    Tier 2: Scrapfly Web Scraping API + newspaper3k (anti-bot bypass, $0.0003 cost)

    This minimizes Scrapfly API calls and stays well under the 5/min concurrency limit.

    Returns: (content, error_message)
    """
    global enhanced_scraping_stats

    LOG.info(f"SCRAPFLY: Starting extraction for {domain}")

    # Track total scraping attempt
    enhanced_scraping_stats["total_attempts"] += 1

    # Try Tier 1: Free scraping FIRST
    content, error1 = await scrape_with_requests_free(url, domain)

    if content:
        return content, None

    LOG.warning(f"âš ï¸ TIER 1 (Free) failed for {domain}: {error1}")
    LOG.info(f"ðŸ”„ Falling back to TIER 2 (Scrapfly) for {domain}")

    # Try Tier 2: Scrapfly (only for tough sites)
    if SCRAPFLY_API_KEY:
        content, error2 = await scrape_with_scrapfly_html_only(url, domain, max_retries)

        if content:
            return content, None

        LOG.warning(f"âš ï¸ TIER 2 (Scrapfly) failed for {domain}: {error2}")

        # Smart categorization: ERROR for our infrastructure issues, WARNING for external failures
        if error2 and "429" in str(error2) and "scrapfly" in str(error2).lower():
            # ScrapFly rate limited - OUR quota/concurrency issue (SERIOUS)
            LOG.error(f"âŒ SCRAPFLY RATE LIMITED for {domain} - Free: {error1}, Scrapfly: {error2}")
            LOG.error(f"   âš ï¸ Consider upgrading plan or adding request delays")
        elif error2 and "422" in str(error2):
            # ScrapFly config error - OUR mistake
            LOG.error(f"âŒ SCRAPFLY CONFIG ERROR for {domain} - Free: {error1}, Scrapfly: {error2}")
        else:
            # All other failures (403, 401, insufficient content, etc.) - expected external failures
            LOG.warning(f"âš ï¸ ALL TIERS FAILED for {domain} - Free: {error1}, Scrapfly: {error2}")

        enhanced_scraping_stats["total_failures"] += 1
        return None, f"All methods failed (Free: {error1}, Scrapfly: {error2})"
    else:
        LOG.warning(f"âš ï¸ Scraping failed for {domain} - Free tier failed and no Scrapfly key configured")
        enhanced_scraping_stats["total_failures"] += 1
        return None, f"Free scraping failed: {error1}"


def get_random_user_agent():
    return random.choice(USER_AGENTS)

def get_referrer_for_domain(url, domain):
    """Add realistic referrer headers"""
    referrers = {
        'default': [
            'https://www.google.com/',
            'https://duckduckgo.com/',
            'https://www.bing.com/',
        ],
        'news_sites': [
            'https://news.google.com/',
            'https://finance.yahoo.com/',
            'https://www.reddit.com/r/investing/',
        ]
    }
    
    if any(news in domain for news in ['news', 'finance', 'investing']):
        return random.choice(referrers['news_sites'])
    return random.choice(referrers['default'])

def clean_scraped_content(content: str, url: str = "", domain: str = "") -> str:
    """
    Minimal content cleaning for Scrapfly Extraction API output.
    Scrapfly already removes HTML, ads, navigation - we just need light sanitization.

    IMPORTANT: Preserves financial data (numbers, percentages, financial codes).
    """
    if not content:
        return ""

    original_length = len(content)

    # Stage 1: Remove NULL bytes and control characters
    content = clean_null_bytes(content)
    content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]+', '', content)

    # Stage 2: Remove encoding artifacts (rarely needed with Scrapfly)
    content = re.sub(r'[Ã‚Â¿Ã‚Â½]{3,}', '', content)  # Binary encoding artifacts

    # Stage 3: Clean up any leftover HTML entities (Scrapfly usually handles this)
    content = re.sub(r'&[a-zA-Z0-9#]+;', ' ', content)

    # Stage 4: Remove obvious boilerplate phrases (minimal)
    boilerplate_patterns = [
        r'Advertisement\s*',
        r'Sponsored Content\s*',
        r'Continue Reading\s*',
    ]

    for pattern in boilerplate_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)

    # Stage 5: Normalize whitespace (preserve paragraphs)
    content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)  # Max 2 consecutive line breaks
    content = re.sub(r'[ \t]+', ' ', content)  # Normalize spaces/tabs to single space
    content = re.sub(r'\n +', '\n', content)  # Remove leading spaces
    content = re.sub(r' +\n', '\n', content)  # Remove trailing spaces

    # Final cleanup
    content = content.strip()
    content = clean_null_bytes(content)  # Final NULL byte check

    # Log cleaning effectiveness
    final_length = len(content)
    reduction = ((original_length - final_length) / original_length * 100) if original_length > 0 else 0

    if reduction > 20:  # Only log if significant reduction
        LOG.debug(f"Minimal cleaning: {original_length} â†’ {final_length} chars ({reduction:.1f}% reduction)")

    return content

def get_domain_strategy(domain):
    return DOMAIN_STRATEGIES.get(domain, {
        'delay_range': (4, 8),
        'headers': {},
    })

def calculate_intelligent_delay(domain):
    """
    Calculate delay based on domain access patterns:
    - No delay for first-time domains
    - Short delay (1-2s) if different from last domain
    - Full delay (4-8s) if same as last domain or recent access
    """
    global last_scraped_domain, domain_last_accessed
    
    current_time = time.time()
    strategy = get_domain_strategy(domain)
    
    # Check if we've accessed this domain recently (within last 10 seconds)
    last_access_time = domain_last_accessed.get(domain, 0)
    time_since_last_access = current_time - last_access_time
    
    # Determine delay strategy
    if domain == last_scraped_domain:
        # Same domain as last scrape - use full delay
        delay_min, delay_max = strategy.get('delay_range', (4, 8))
        delay = random.uniform(delay_min, delay_max)
        reason = "same domain as previous"
    elif time_since_last_access < 10:
        # Recently accessed this domain - use medium delay
        delay = random.uniform(2, 4)
        reason = f"accessed {time_since_last_access:.1f}s ago"
    elif domain not in domain_last_accessed:
        # First time accessing this domain - minimal delay
        delay = random.uniform(0.5, 1.5)
        reason = "first access"
    else:
        # Different domain, not recently accessed - short delay
        delay = random.uniform(1, 2)
        reason = "different domain"
    
    # Update tracking
    domain_last_accessed[domain] = current_time
    last_scraped_domain = domain
    
    return delay, reason

def scrape_with_backoff(url, max_retries=3):
    """Enhanced scraping with better timeout handling for slow domains"""
    domain = normalize_domain(urlparse(url).netloc.lower())
    
    # Longer timeout for known slow domains
    timeout = 30 if domain in ["businesswire.com", "globenewswire.com"] else 15
    
    for attempt in range(max_retries):
        try:
            response = scraping_session.get(url, timeout=timeout)
            if response.status_code == 200:
                return response
            elif response.status_code in [429, 500, 503, 504]:  # Added 504
                delay = (2 ** attempt) + random.uniform(0, 1)
                LOG.info(f"HTTP {response.status_code} error, retrying in {delay:.1f}s (attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)
                if attempt == max_retries - 1:
                    LOG.warning(f"Max retries reached for {url} after {response.status_code} errors")
                    return None
            else:
                LOG.warning(f"HTTP {response.status_code} for {url}, not retrying")
                return None
        except requests.RequestException as e:
            if attempt < max_retries - 1:
                delay = (2 ** attempt) + random.uniform(0, 1)
                LOG.warning(f"Request failed, retrying in {delay:.1f}s: {e}")
                time.sleep(delay)
            else:
                LOG.warning(f"Final retry failed for {url}: {e}")
    
    return None

def log_scraping_success_rates():
    """Log success rates for all scraping methods in a prominent format"""
    total_attempts = enhanced_scraping_stats["total_attempts"]
    if total_attempts == 0:
        LOG.info("SCRAPING SUCCESS RATES: No attempts made")
        return
    
    # Calculate overall success rate
    total_success = (enhanced_scraping_stats["requests_success"] +
                    enhanced_scraping_stats["playwright_success"] +
                    enhanced_scraping_stats.get("scrapfly_success", 0))
    overall_rate = (total_success / total_attempts) * 100
    
    # Calculate individual success rates
    requests_attempts = enhanced_scraping_stats["by_method"]["requests"]["attempts"]
    requests_success = enhanced_scraping_stats["by_method"]["requests"]["successes"]
    requests_rate = (requests_success / requests_attempts * 100) if requests_attempts > 0 else 0
    
    playwright_attempts = enhanced_scraping_stats["by_method"]["playwright"]["attempts"]
    playwright_success = enhanced_scraping_stats["by_method"]["playwright"]["successes"]
    playwright_rate = (playwright_success / playwright_attempts * 100) if playwright_attempts > 0 else 0
    
    scrapfly_attempts = enhanced_scraping_stats["by_method"].get("scrapfly", {}).get("attempts", 0)
    scrapfly_success = enhanced_scraping_stats["by_method"].get("scrapfly", {}).get("successes", 0)
    scrapfly_rate = (scrapfly_success / scrapfly_attempts * 100) if scrapfly_attempts > 0 else 0
    
    # Log prominent success rate summary with per-tier percentages
    LOG.info("=" * 60)
    LOG.info("SCRAPING SUCCESS RATES (2-Tier Architecture)")
    LOG.info("=" * 60)
    LOG.info(f"OVERALL SUCCESS: {overall_rate:.1f}% ({total_success}/{total_attempts} total articles)")
    LOG.info(f"TIER 1 (Free):     {requests_rate:.1f}% ({requests_success}/{requests_attempts} attempted)")
    LOG.info(f"TIER 2 (Scrapfly): {scrapfly_rate:.1f}% ({scrapfly_success}/{scrapfly_attempts} attempted)")
    if scrapfly_attempts > 0:
        LOG.info(f"Scrapfly Cost: ${scrapfly_stats['cost_estimate']:.3f}")

    LOG.info("=" * 60)

def validate_scraped_content(content, url, domain):
    """Enhanced content validation with relaxed requirements for quality domains"""
    if not content or len(content.strip()) < 100:
        return False, "Content too short"
    
    # RELAXED REQUIREMENTS for quality domains
    domain_normalized = normalize_domain(domain)
    is_quality_domain = domain_normalized in QUALITY_DOMAINS
    
    # Check content-to-boilerplate ratio
    sentences = [s.strip() for s in content.split('.') if s.strip()]
    min_sentences = 2 if is_quality_domain else 3  # RELAXED for quality domains
    
    if len(sentences) < min_sentences:
        return False, f"Insufficient sentences (need {min_sentences}, got {len(sentences)})"
    
    # Check for repetitive content
    words = content.lower().split()
    if len(words) > 0:
        unique_ratio = len(set(words)) / len(words)
        min_ratio = 0.25 if is_quality_domain else 0.3  # RELAXED for quality domains
        if unique_ratio < min_ratio:
            return False, "Repetitive content detected"
    
    # Check if content is mostly technical/code-like
    technical_chars = len(re.findall(r'[{}();:=<>]', content))
    max_technical = 0.15 if is_quality_domain else 0.1  # RELAXED for quality domains
    if technical_chars > len(content) * max_technical:
        return False, "Content appears to be technical/code data"
    
    return True, "Valid content"

# Create global session
scraping_session = create_scraping_session()

def is_insider_trading_article(title: str) -> bool:
    """
    Detect insider trading/institutional flow articles that are typically low-value
    Enhanced to catch more patterns
    """
    title_lower = title.lower()
    
    # Executive transactions
    insider_patterns = [
        # Existing patterns...
        r"\w+\s+(ceo|cfo|coo|president|director|officer|executive)\s+\w+\s+(sells?|buys?|purchases?)",
        r"(sells?|buys?|purchases?)\s+\$[\d,]+\.?\d*[km]?\s+(in\s+shares?|worth\s+of)",
        r"(insider|executive|officer|director|ceo|cfo)\s+(selling|buying|sold|bought)",
        
        # Institutional flow patterns  
        r"\w+\s+(capital|management|advisors?|investments?|llc|inc\.?)\s+(buys?|sells?|invests?|increases?|decreases?)",
        r"(invests?|buys?)\s+\$[\d,]+\.?\d*[km]?\s+in",
        r"shares?\s+(sold|bought)\s+by\s+",
        r"(increases?|decreases?|trims?|adds?\s+to)\s+(stake|position|holdings?)\s+in",
        
        # NEW: More aggressive patterns
        r"(acquires?|sells?)\s+\d+\s+shares?",
        r"(boosts?|cuts?|trims?)\s+(stake|holdings?|position)",
        r"(takes?|builds?)\s+\$[\d,]+\.?\d*[km]?\s+(stake|position)",
        r"(hedge fund|institutional|mutual fund)\s+.{0,20}(buys?|sells?|adds?|cuts?)",
        r"(13f|form\s*4|schedule\s*13d)\s+(filing|report)",
        r"(quarterly\s+)?(holdings?|portfolio)\s+(report|update|filing)",
        
        # Specific low-value phrases
        r"buys?\s+\d+\s+shares?",
        r"sells?\s+\d+\s+shares?", 
        r"shares?\s+(sold|purchased|bought)\s+by",
        r"\$[\d,]+\.?\d*[km]?\s+stake",
        r"(quarterly|q\d)\s+holdings?\s+(report|filing)",
        r"13f\s+filing",
        r"form\s+4\s+filing"
    ]
    
    # Check against patterns
    for pattern in insider_patterns:
        if re.search(pattern, title_lower):
            LOG.debug(f"INSIDER PATTERN MATCH: '{pattern}' in '{title[:50]}...'")
            return True
    
    # Enhanced small dollar amount detection
    small_amount_pattern = r"\$(\d+(?:,\d{3})*(?:\.\d+)?)\s*([km])?"
    matches = re.findall(small_amount_pattern, title_lower)
    if matches and any(word in title_lower for word in ["sells", "buys", "purchases", "sold", "bought", "stake", "position"]):
        for amount_str, unit in matches:
            try:
                amount = float(amount_str.replace(",", ""))
                if unit.lower() == 'k':
                    amount *= 1000
                elif unit.lower() == 'm':
                    amount *= 1000000
                
                # Flag transactions under $100M as likely insider trading (raised threshold)
                if amount < 100000000:
                    LOG.debug(f"SMALL AMOUNT DETECTED: ${amount:,.0f} in '{title[:50]}...'")
                    return True
            except:
                continue
    
    return False

def safe_content_scraper(url: str, domain: str, scraped_domains: set) -> Tuple[Optional[str], str]:
    """
    Safe content scraper using requests only (no Playwright)
    """
    try:
        # Remove domain deduplication - scrape every URL
        # scraped_domains.add(domain)  # Optional: still track but don't block
        
        # Use the existing extract_article_content function
        content, error = extract_article_content(url, domain)
        
        if content:
            return content, f"Successfully scraped {len(content)} chars"
        else:
            return None, error or "Failed to extract content"
            
    except Exception as e:
        return None, f"Scraping error: {str(e)}"

async def safe_content_scraper_with_scrapfly_only_async(url: str, domain: str, category: str, keyword: str, scraped_domains: set) -> Tuple[Optional[str], str]:
    """
    Scrapfly-only content scraper with Extraction API.
    No Tier 1 requests, no Playwright - Scrapfly handles everything.
    """
    global enhanced_scraping_stats

    # Check limits first
    if not _check_scraping_limit(category, keyword):
        if category == "company":
            return None, f"Company limit reached ({scraping_stats['company_scraped']}/{scraping_stats['limits']['company']})"
        elif category == "industry":
            keyword_count = scraping_stats["industry_scraped_by_keyword"].get(keyword, 0)
            return None, f"Industry keyword '{keyword}' limit reached ({keyword_count}/{scraping_stats['limits']['industry_per_keyword']})"
        elif category == "competitor":
            keyword_count = scraping_stats["competitor_scraped_by_keyword"].get(keyword, 0)
            return None, f"Competitor '{keyword}' limit reached ({keyword_count}/{scraping_stats['limits']['competitor_per_keyword']})"

    enhanced_scraping_stats["total_attempts"] += 1

    # SCRAPFLY-ONLY: No tier fallback needed
    if not SCRAPFLY_API_KEY:
        LOG.error("SCRAPFLY: API key not configured")
        enhanced_scraping_stats["total_failures"] += 1
        return None, "Scrapfly API key not configured"

    LOG.info(f"SCRAPFLY: Starting extraction for {domain}")

    try:
        scrapfly_content, scrapfly_error = await scrape_with_scrapfly_async(url, domain)

        if scrapfly_content:
            enhanced_scraping_stats["scrapfly_success"] += 1
            update_scraping_stats(category, keyword, True)
            return scrapfly_content, f"SCRAPFLY SUCCESS: {len(scrapfly_content)} chars (extraction API)"
        else:
            # Scrapfly failed
            enhanced_scraping_stats["total_failures"] += 1
            return None, f"SCRAPFLY FAILED: {scrapfly_error}"

    except Exception as e:
        LOG.error(f"SCRAPFLY: Unexpected error for {domain}: {e}")
        enhanced_scraping_stats["total_failures"] += 1
        return None, f"SCRAPFLY ERROR: {str(e)}"

def log_enhanced_scraping_stats():
    """Log Scrapfly-only scraping statistics"""
    total = enhanced_scraping_stats["total_attempts"]
    if total == 0:
        LOG.info("SCRAPING STATS: No attempts made")
        return

    scrapfly_success = enhanced_scraping_stats["scrapfly_success"]
    success_rate = (scrapfly_success / total) * 100

    LOG.info("=" * 60)
    LOG.info("SCRAPFLY SCRAPING STATS (Extraction API)")
    LOG.info("=" * 60)
    LOG.info(f"SUCCESS RATE: {success_rate:.1f}% ({scrapfly_success}/{total})")
    LOG.info(f"COST ESTIMATE: ${scrapfly_stats['cost_estimate']:.2f}")
    LOG.info("=" * 60)


def log_scrapfly_stats():
    """Scrapfly statistics logging"""
    if scrapfly_stats["requests_made"] == 0:
        LOG.info("SCRAPFLY: No requests made this run")
        return
    
    success_rate = (scrapfly_stats["successful"] / scrapfly_stats["requests_made"]) * 100
    LOG.info(f"SCRAPFLY FINAL: {success_rate:.1f}% success rate ({scrapfly_stats['successful']}/{scrapfly_stats['requests_made']})")
    LOG.info(f"SCRAPFLY COST: ${scrapfly_stats['cost_estimate']:.3f} estimated")
    
    if scrapfly_stats["failed"] > 0:
        LOG.warning(f"SCRAPFLY: {scrapfly_stats['failed']} requests failed")
    
    # Log top performing and failing domains
    successful_domains = [(domain, stats) for domain, stats in scrapfly_stats["by_domain"].items() if stats["successes"] > 0]
    failed_domains = [(domain, stats) for domain, stats in scrapfly_stats["by_domain"].items() if stats["successes"] == 0 and stats["attempts"] > 0]
    
    if successful_domains:
        LOG.info(f"SCRAPFLY SUCCESS DOMAINS: {len(successful_domains)} domains working")
    if failed_domains:
        LOG.info(f"SCRAPFLY FAILED DOMAINS: {len(failed_domains)} domains blocked/failed")

def reset_enhanced_scraping_stats():
    """Reset enhanced scraping stats for new run"""
    global enhanced_scraping_stats, scrapfly_stats
    enhanced_scraping_stats = {
        "total_attempts": 0,
        "requests_success": 0,
        "playwright_success": 0,
        "scrapfly_success": 0,
        "total_failures": 0,
        "by_method": {
            "requests": {"attempts": 0, "successes": 0},
            "playwright": {"attempts": 0, "successes": 0},
            "scrapfly": {"attempts": 0, "successes": 0},
        }
    }
    # Reset Scrapfly stats
    scrapfly_stats = {
        "requests_made": 0,
        "successful": 0,
        "failed": 0,
        "cost_estimate": 0.0,
        "by_domain": defaultdict(lambda: {"attempts": 0, "successes": 0})
    }

async def process_article_batch_async(articles_batch: List[Dict], categories: Union[str, List[str]], metadata: Dict, analysis_ticker: str) -> List[Dict]:
    """
    Process a batch of articles concurrently: scraping â†’ AI summarization â†’ database update
    Now supports per-article categories for POV-agnostic summarization
    Returns list of results for each article in the batch
    """
    batch_size = len(articles_batch)
    LOG.info(f"BATCH START: Processing {batch_size} articles from {analysis_ticker}'s perspective")

    # Normalize categories to list
    if isinstance(categories, str):
        categories = [categories] * batch_size

    # Build competitor name cache from metadata
    competitor_name_cache = {}
    for comp in metadata.get("competitors", []):
        if comp.get("ticker") and comp.get("name"):
            competitor_name_cache[comp["ticker"]] = comp["name"]

    target_company_name = metadata.get("company_name", analysis_ticker)

    results = []

    # Phase 1: Concurrent scraping for all articles in batch
    LOG.info(f"BATCH PHASE 1: Concurrent scraping of {batch_size} articles")

    scraping_tasks = []
    for i, article in enumerate(articles_batch):
        # Use individual article's category
        article_category = categories[i] if i < len(categories) else categories[0]
        task = scrape_single_article_async(article, article_category, metadata, analysis_ticker, i)
        scraping_tasks.append(task)

    # Execute all scraping tasks concurrently
    scraping_results = await asyncio.gather(*scraping_tasks, return_exceptions=True)

    # Phase 1.5: Industry article relevance scoring (NEW - Oct 2025)
    # For industry articles only, score relevance after scraping but before AI summarization
    relevance_scores = {}  # {article_index: {"score": float, "reason": str, "is_rejected": bool, "provider": str}}

    for i, result in enumerate(scraping_results):
        if isinstance(result, Exception) or not result["success"]:
            continue

        article_category = categories[i] if i < len(categories) else categories[0]

        # Only score industry articles
        if article_category == "industry" and result.get("scraped_content"):
            article = articles_batch[i]
            industry_keyword = article.get("search_keyword", "unknown industry")

            # âœ… NEW: Check for retail platform content BEFORE relevance gate
            is_retail, keyword = is_retail_platform_content(result["scraped_content"])
            if is_retail:
                LOG.info(f"[{analysis_ticker}] ðŸš« SKIP (Retail): '{keyword}' found in industry article - {article.get('title', 'No title')[:60]}...")
                relevance_scores[i] = {
                    "score": 0.0,
                    "reason": f"Retail platform content: {keyword}",
                    "is_rejected": True,
                    "provider": "retail_filter"
                }
                continue  # Skip relevance gate and move to next article

            try:
                LOG.info(f"[{analysis_ticker}] ðŸ“Š Scoring relevance for industry article {i}: {article.get('title', 'No title')[:60]}...")

                relevance_result = await score_industry_article_relevance(
                    ticker=analysis_ticker,
                    company_name=target_company_name,
                    industry_keyword=industry_keyword,
                    title=article.get("title", ""),
                    scraped_content=result["scraped_content"]
                )

                relevance_scores[i] = relevance_result

                # Log score
                score = relevance_result["score"]
                is_rejected = relevance_result["is_rejected"]
                provider = relevance_result["provider"]
                status = "REJECTED" if is_rejected else "ACCEPTED"

                LOG.info(f"[{analysis_ticker}] {'âœ—' if is_rejected else 'âœ“'} Article {i} scored {score:.1f}/10 [{status}] via {provider}")
                LOG.debug(f"   Reason: {relevance_result['reason']}")

                # Store relevance score in database immediately
                article_id = article.get("id")
                if article_id:
                    with db() as conn, conn.cursor() as cur:
                        cur.execute("""
                            UPDATE ticker_articles
                            SET relevance_score = %s,
                                relevance_reason = %s,
                                is_rejected = %s
                            WHERE ticker = %s AND article_id = %s
                        """, (
                            score,
                            relevance_result["reason"],
                            is_rejected,
                            analysis_ticker,
                            article_id
                        ))
                        LOG.debug(f"   Saved relevance score to database for article {article_id}")

            except Exception as e:
                LOG.error(f"[{analysis_ticker}] âŒ Relevance scoring failed for article {i}: {e}")
                # Don't block processing on scoring failure - treat as accepted
                relevance_scores[i] = {
                    "score": 5.0,
                    "reason": f"Scoring failed: {str(e)}",
                    "is_rejected": False,
                    "provider": "error"
                }

    # Phase 2: Concurrent AI summarization with category-specific prompts
    successful_scrapes = []
    for i, result in enumerate(scraping_results):
        if isinstance(result, Exception):
            LOG.error(f"BATCH SCRAPING ERROR: Article {i} failed: {result}")
            results.append({
                "article_id": articles_batch[i]["id"],
                "success": False,
                "error": f"Scraping failed: {str(result)}",
                "scraped_content": None,
                "ai_summary": None,
                "ai_model": None
            })
        elif result["success"]:
            # Check if article was rejected by relevance gate OR retail filter
            if i in relevance_scores and relevance_scores[i]["is_rejected"]:
                article_category = categories[i] if i < len(categories) else categories[0]

                # Distinguish between retail filter vs AI relevance scoring rejection
                if relevance_scores[i].get("provider") == "retail_filter":
                    # Retail platform content - use "filtered" tag (same as non-industry articles)
                    ai_model_tag = "filtered"
                    LOG.info(f"[{analysis_ticker}] ðŸš« Skipping AI summary for retail-filtered {article_category} article {i}")
                else:
                    # AI relevance scoring rejected - use "low_relevance" tag
                    ai_model_tag = "low_relevance"
                    LOG.info(f"[{analysis_ticker}] â­ï¸ Skipping AI summary for rejected {article_category} article {i} (score: {relevance_scores[i]['score']:.1f}/10)")

                # Add to results as successful scrape but no AI summary
                results.append({
                    "article_id": articles_batch[i]["id"],
                    "article_idx": i,
                    "success": True,
                    "scraped_content": result["scraped_content"],
                    "ai_summary": None,  # Rejected articles get no summary
                    "ai_model": ai_model_tag,
                    "content_scraped_at": result["content_scraped_at"],
                    "scraping_error": None
                })
            else:
                # âœ… Check for retail platform content BEFORE adding to AI queue
                # Industry articles already checked at Line 5081 (before relevance gate)
                # Check company/competitor/value_chain articles here
                article_category = categories[i] if i < len(categories) else categories[0]

                if article_category in ("company", "competitor", "value_chain"):
                    is_retail, keyword = is_retail_platform_content(result["scraped_content"])

                    if is_retail:
                        LOG.info(f"[{analysis_ticker}] ðŸš« SKIP (Retail): '{keyword}' found in {article_category} article - {articles_batch[i].get('title', 'No title')[:60]}...")
                        # Add to results as filtered (no AI summary)
                        results.append({
                            "article_id": articles_batch[i]["id"],
                            "article_idx": i,
                            "success": True,
                            "scraped_content": result["scraped_content"],
                            "ai_summary": None,  # Filtered - no AI analysis
                            "ai_model": "filtered",  # Mark as filtered by Python
                            "content_scraped_at": result["content_scraped_at"],
                            "scraping_error": None
                        })
                    else:
                        # Pass to AI queue for analysis
                        successful_scrapes.append((i, result))
                else:
                    # Industry article - already passed retail check at Line 5081
                    # Pass directly to AI queue
                    successful_scrapes.append((i, result))

    if successful_scrapes:
        LOG.info(f"BATCH PHASE 2: AI summarization of {len(successful_scrapes)} successful scrapes with category-specific prompts")

        ai_tasks = []
        for i, scrape_result in successful_scrapes:
            article_category = categories[i] if i < len(categories) else categories[0]
            task = generate_article_summary(
                scrape_result["scraped_content"],
                articles_batch[i]["title"],
                analysis_ticker,
                articles_batch[i].get("description", ""),
                article_category,
                articles_batch[i],  # article_metadata
                target_company_name,
                competitor_name_cache
            )
            ai_tasks.append((i, task))

        # Execute AI summarization concurrently
        ai_results = await asyncio.gather(*[task for _, task in ai_tasks], return_exceptions=True)

        # Combine scraping and AI results
        for j, (original_idx, _) in enumerate(ai_tasks):
            scrape_result = next(result for i, result in successful_scrapes if i == original_idx)
            ai_result = ai_results[j]

            if isinstance(ai_result, Exception):
                LOG.error(f"BATCH AI ERROR: Article {original_idx} failed: {ai_result}")
                ai_summary = None
                ai_model = "error"  # Mark as AI analysis error
            else:
                # ai_result is tuple (summary, model_used)
                ai_summary, ai_model = ai_result if isinstance(ai_result, tuple) else (ai_result, "unknown")

            results.append({
                "article_id": articles_batch[original_idx]["id"],
                "article_idx": original_idx,
                "success": True,
                "scraped_content": scrape_result["scraped_content"],
                "ai_summary": ai_summary,
                "ai_model": ai_model,
                "content_scraped_at": scrape_result["content_scraped_at"],
                "scraping_error": None
            })
    
    # Add failed scraping results
    for i, result in enumerate(scraping_results):
        if isinstance(result, Exception) or not result["success"]:
            if not any(r["article_id"] == articles_batch[i]["id"] for r in results):
                results.append({
                    "article_id": articles_batch[i]["id"],
                    "article_idx": i,
                    "success": False,
                    "error": str(result) if isinstance(result, Exception) else result.get("error", "Unknown error"),
                    "scraped_content": None,
                    "ai_summary": None
                })
    
    # Phase 3: Batch database update
    LOG.info(f"BATCH PHASE 3: Database update for {len(results)} articles")
    successful_updates = 0
    
    try:
        with db() as conn, conn.cursor() as cur:
            for result in results:
                if result["success"]:
                    article = articles_batch[result["article_idx"]]

                    clean_content = clean_null_bytes(result["scraped_content"]) if result["scraped_content"] else None
                    raw_summary = clean_null_bytes(result["ai_summary"]) if result["ai_summary"] else None

                    # Parse quality score from AI summary
                    clean_summary, quality_score = parse_quality_score(raw_summary) if raw_summary else (None, None)

                    # Articles already exist - use their IDs directly
                    article_id = article.get("id")
                    domain = article.get("domain")

                    # Update article with scraped content and error status
                    if article_id:
                        update_article_content(
                            article_id, clean_content, None,
                            None, False, None
                        )

                        # Track successful scrape
                        if domain:
                            track_scrape_attempt(domain, success=True)

                        # Ensure ticker relationship exists
                        link_article_to_ticker(
                            article_id, analysis_ticker, article.get("feed_id")
                        )

                        # Update ticker-specific AI summary and quality score
                        # ALWAYS save ai_model, even if summary is None (for filtered/rejected/error articles)
                        if result.get("ai_model"):
                            update_ticker_article_summary(
                                analysis_ticker, article_id, clean_summary, result.get("ai_model"),
                                quality_score=quality_score, domain=domain
                            )

                        successful_updates += 1
                else:
                    # Update with scraping failure
                    article = articles_batch[result["article_idx"]]

                    # Articles already exist - use their IDs directly
                    article_id = article.get("id")
                    domain = article.get("domain")

                    # Update article with scraping failure
                    if article_id:
                        update_article_content(
                            article_id, None, None, None,
                            True, clean_null_bytes(result.get("error", ""))
                        )

                        # Track failed scrape
                        if domain:
                            track_scrape_attempt(domain, success=False)

                        # Ensure ticker relationship exists
                        link_article_to_ticker(
                            article_id, analysis_ticker, article.get("feed_id")
                        )
        
        LOG.info(f"BATCH COMPLETE: {successful_updates}/{len(results)} articles successfully updated in database")
        
    except Exception as e:
        LOG.error(f"BATCH DATABASE ERROR: Failed to update batch results: {e}")
    
    return results

async def scrape_single_article_async(article: Dict, category: str, metadata: Dict, analysis_ticker: str, article_idx: int) -> Dict:
    """Scrape a single article asynchronously, or reuse existing content if available"""
    try:
        # Check if article already has scraped content (from another ticker's processing)
        existing_content = article.get("scraped_content")
        if existing_content:
            LOG.info(f"[{analysis_ticker}] â™»ï¸ Reusing existing content for article {article_idx}: {article.get('title', 'No title')[:60]}...")
            return {
                "success": True,
                "scraped_content": existing_content,
                "content_scraped_at": article.get("content_scraped_at") or datetime.now(timezone.utc),
                "error": None,
                "reused": True  # Flag to track content reuse
            }

        resolved_url = article.get("resolved_url") or article.get("url")
        domain = article.get("domain", "unknown")
        title = article.get("title", "")
        
        # Get keyword for limit tracking
        if category == "company":
            keyword = analysis_ticker
        elif category == "competitor":
            keyword = article.get("feed_ticker", "unknown")
            if keyword == "unknown":
                keyword = article.get("search_keyword", "unknown")
        else:
            keyword = article.get("search_keyword", "unknown")
        
        if resolved_url and resolved_url.startswith(('http://', 'https://')):
            scrape_domain = normalize_domain(urlparse(resolved_url).netloc.lower())
            
            if scrape_domain not in PAYWALL_DOMAINS and scrape_domain not in PROBLEMATIC_SCRAPE_DOMAINS:
                content, status = await safe_content_scraper_with_scrapfly_only_async(
                    resolved_url, scrape_domain, category, keyword, set()
                )

                if content:
                    return {
                        "success": True,
                        "scraped_content": content,
                        "content_scraped_at": datetime.now(timezone.utc),
                        "error": None
                    }
                else:
                    return {
                        "success": False,
                        "error": status or "Failed to scrape content",
                        "scraped_content": None
                    }
            else:
                # Paywall or problematic domain - no scraping, no ai_summary
                return {
                    "success": False,
                    "error": f"Skipped problematic domain: {scrape_domain}",
                    "scraped_content": None
                }
        else:
            return {
                "success": False,
                "error": "Invalid or missing URL",
                "scraped_content": None
            }
            
    except Exception as e:
        LOG.error(f"Article scraping failed for article {article_idx}: {e}")
        return {
            "success": False,
            "error": f"Exception during scraping: {str(e)}",
            "scraped_content": None
        }

async def generate_ai_summary_for_scraped_article(scraped_content: str, title: str, ticker: str, description: str = "") -> Optional[str]:
    """Generate AI summary for a scraped article"""
    try:
        return await generate_ai_individual_summary_async(scraped_content, title, ticker, description)
    except Exception as e:
        LOG.error(f"AI summary generation failed: {e}")
        return None

def update_scraping_stats(category: str, keyword: str, success: bool):
    """Helper to update scraping statistics"""
    global scraping_stats
    
    if success:
        scraping_stats["successful_scrapes"] += 1
        
        if category == "company":
            scraping_stats["company_scraped"] += 1
            LOG.info(f"SCRAPING SUCCESS: Company {scraping_stats['company_scraped']}/{scraping_stats['limits']['company']} | Total: {scraping_stats['successful_scrapes']}")
        
        elif category == "industry":
            if keyword not in scraping_stats["industry_scraped_by_keyword"]:
                scraping_stats["industry_scraped_by_keyword"][keyword] = 0
            scraping_stats["industry_scraped_by_keyword"][keyword] += 1
            keyword_count = scraping_stats["industry_scraped_by_keyword"][keyword]
            LOG.info(f"SCRAPING SUCCESS: Industry '{keyword}' {keyword_count}/{scraping_stats['limits']['industry_per_keyword']} | Total: {scraping_stats['successful_scrapes']}")
        
        elif category == "competitor":
            # Use competitor_ticker as consolidation key
            if keyword not in scraping_stats["competitor_scraped_by_keyword"]:
                scraping_stats["competitor_scraped_by_keyword"][keyword] = 0
            scraping_stats["competitor_scraped_by_keyword"][keyword] += 1
            keyword_count = scraping_stats["competitor_scraped_by_keyword"][keyword]
            LOG.info(f"SCRAPING SUCCESS: Competitor '{keyword}' {keyword_count}/{scraping_stats['limits']['competitor_per_keyword']} | Total: {scraping_stats['successful_scrapes']}")
    
def _check_scraping_limit(category: str, keyword: str) -> bool:
    """Check if we can scrape more articles for this category/keyword"""
    global scraping_stats
    
    if category == "company":
        return scraping_stats["company_scraped"] < scraping_stats["limits"]["company"]
    
    elif category == "industry":
        keyword_count = scraping_stats["industry_scraped_by_keyword"].get(keyword, 0)
        return keyword_count < scraping_stats["limits"]["industry_per_keyword"]
    
    elif category == "competitor":
        # Use competitor_ticker as consolidation key
        keyword_count = scraping_stats["competitor_scraped_by_keyword"].get(keyword, 0)
        return keyword_count < scraping_stats["limits"]["competitor_per_keyword"]
    
    return False

def calculate_dynamic_scraping_limits(ticker: str) -> Dict[str, int]:
    """Calculate dynamic scraping limits based on actual keywords/competitors"""
    config = get_ticker_config(ticker)
    if not config:
        LOG.warning(f"âš ï¸ No config found for {ticker} - using minimal limits")
        return {"company": 20, "industry_total": 0, "competitor_total": 0}

    # Get actual counts
    industry_keywords = config.get("industry_keywords", [])
    competitors = config.get("competitors", [])

    # DIAGNOSTIC: Log what we actually found
    LOG.info(f"ðŸ“Š METADATA READ for {ticker}:")
    LOG.info(f"   AI Generated: {config.get('ai_generated', False)}")
    LOG.info(f"   Industry Keywords: {industry_keywords}")
    LOG.info(f"   Competitors: {competitors}")

    # Calculate totals: 8 articles per industry keyword, 5 per competitor
    industry_total = len(industry_keywords) * 8
    competitor_total = len(competitors) * 5

    LOG.info(f"DYNAMIC SCRAPING LIMITS for {ticker}:")
    LOG.info(f"  Company: 20")
    LOG.info(f"  Industry: {len(industry_keywords)} keywords Ã— 8 = {industry_total}")
    LOG.info(f"  Competitor: {len(competitors)} competitors Ã— 5 = {competitor_total}")
    LOG.info(f"  TOTAL: {20 + industry_total + competitor_total} articles max")
    
    return {
        "company": 20,
        "industry_total": industry_total,
        "competitor_total": competitor_total,
        "total_possible": 20 + industry_total + competitor_total
    }

# Update the ingest function to include AI summary generation
def ingest_feed_with_content_scraping(feed: Dict, category: str = "company", keywords: List[str] = None, 
                                       enable_ai_scoring: bool = True, max_ai_articles: int = None) -> Dict[str, int]:
    """
    Enhanced feed processing with AI summaries for scraped content and comprehensive Yahoo Finance resolution
    Flow: Check scraping limits -> Scrape resolved URLs -> AI analysis only on successful scrapes
    """
    global scraping_stats
    
    stats = {
        "processed": 0, "inserted": 0, "duplicates": 0, "blocked_spam": 0, "blocked_non_latin": 0,
        "content_scraped": 0, "content_failed": 0, "scraping_skipped": 0, "ai_reanalyzed": 0,
        "ai_scored": 0, "basic_scored": 0, "ai_summaries_generated": 0, "blocked_insider_trading": 0
    }
    
    scraped_domains = set()
    ai_processed_count = 0
    
    # Get the keyword for this feed (for limit tracking)
    feed_keyword = feed.get("search_keyword", "unknown")
    
    try:
        parsed = feedparser.parse(feed["url"])
        LOG.info(f"Processing feed [{category}]: {feed['name']} - {len(parsed.entries)} entries (AI: {'enabled' if enable_ai_scoring else 'disabled'})")
        
        # Sort entries by publication date (newest first) if available
        entries_with_dates = []
        for entry in parsed.entries:
            pub_date = None
            if hasattr(entry, "published_parsed"):
                pub_date = parse_datetime(entry.published_parsed)
            entries_with_dates.append((entry, pub_date))
        
        entries_with_dates.sort(key=lambda x: (x[1] or datetime.min.replace(tzinfo=timezone.utc)), reverse=True)
        
        for entry, _ in entries_with_dates:
            stats["processed"] += 1
            
            url = getattr(entry, "link", None)
            title = getattr(entry, "title", "") or "No Title"
            raw_description = getattr(entry, "summary", "") if hasattr(entry, "summary") else ""
            
            # Filter description
            description = ""
            if raw_description and is_description_valuable(title, raw_description):
                description = raw_description
            
            # Quick spam checks
            if not url or contains_non_latin_script(title):
                stats["blocked_non_latin"] += 1
                continue
                
            if any(spam in title.lower() for spam in ["marketbeat", "newser", "khodrobank"]):
                stats["blocked_spam"] += 1
                continue
            
            # NEW: Insider trading check - skip these articles entirely
            if is_insider_trading_article(title):
                stats["blocked_insider_trading"] += 1
                LOG.debug(f"INSIDER TRADING BLOCKED: {title[:50]}...")
                continue

            # FEED-TYPE-AWARE URL RESOLUTION
            final_resolved_url = None
            final_domain = None
            final_source_url = None

            # Determine feed type by checking feed URL (once per feed, not per article)
            feed_url = feed.get("url", "")
            is_yahoo_feed = "finance.yahoo.com" in feed_url
            is_google_feed = "news.google.com" in feed_url

            if is_yahoo_feed:
                # YAHOO FEED: Resolve immediately (existing logic)
                resolved_url, domain, source_url = domain_resolver._handle_yahoo_finance(url)

                if not resolved_url or not domain:
                    stats["blocked_spam"] += 1
                    continue

                final_resolved_url = resolved_url
                final_domain = domain
                final_source_url = source_url
                LOG.info(f"YAHOO FEED RESOLVED: {url[:60]} â†’ {final_domain}")

            elif is_google_feed:
                # GOOGLE FEED: Try FREE resolution first (Tier 1: Advanced API only)
                resolved_url = domain_resolver._resolve_google_news_url_advanced(url)

                if resolved_url:
                    # âœ… RESOLUTION SUCCEEDED

                    # YAHOO CHAIN DETECTION (exact same logic as Phase 1.5)
                    is_yahoo_finance = any(yahoo_domain in resolved_url for yahoo_domain in [
                        "finance.yahoo.com", "ca.finance.yahoo.com", "uk.finance.yahoo.com"
                    ])

                    if is_yahoo_finance:
                        yahoo_original = extract_yahoo_finance_source_optimized(resolved_url)
                        if yahoo_original and yahoo_original != resolved_url:
                            final_resolved_url = yahoo_original
                            final_domain = normalize_domain(urlparse(yahoo_original).netloc.lower())
                            LOG.info(f"GOOGLEâ†’YAHOOâ†’DIRECT: {title[:60]} â†’ {final_domain}")
                        else:
                            final_resolved_url = resolved_url
                            final_domain = normalize_domain(urlparse(resolved_url).netloc.lower())
                            if yahoo_original == resolved_url:
                                LOG.warning(f"Yahoo extraction failed (no providerContentUrl): {resolved_url[:80]}")
                            LOG.info(f"GOOGLEâ†’YAHOO: {title[:60]} â†’ {final_domain}")
                    else:
                        final_resolved_url = resolved_url
                        final_domain = normalize_domain(urlparse(resolved_url).netloc.lower())
                        LOG.info(f"GOOGLE RESOLVED (Tier 1): {title[:60]} â†’ {final_domain}")

                    # SPAM CHECK (same as Yahoo feeds - block article entirely)
                    if domain_resolver._is_spam_domain(final_domain):
                        stats["blocked_spam"] += 1
                        LOG.info(f"SPAM REJECTED: Google News â†’ {final_domain} (from resolution)")
                        continue

                    final_source_url = None

                else:
                    # âŒ RESOLUTION FAILED - Fall back to current workflow (title extraction)
                    LOG.warning(f"âš ï¸ ADVANCED API FAILED: {title[:60]} (falling back to title extraction)")
                    clean_title, source_name = extract_source_from_title_smart(title)

                    if source_name and not domain_resolver._is_spam_source(source_name):
                        domain = domain_resolver._resolve_publication_to_domain(source_name)

                        if domain and domain_resolver._is_spam_domain(domain):
                            stats["blocked_spam"] += 1
                            LOG.info(f"SPAM REJECTED: Google News â†’ {domain} (from title: {title[:40]})")
                            continue
                    else:
                        # Could not extract valid domain from title - skip this article
                        stats["blocked_spam"] += 1
                        LOG.warning(f"GOOGLE NEWS: Could not extract domain from title: {title[:60]}")
                        continue

                    # Store with NULL resolved_url (will resolve in Phase 1.5)
                    final_resolved_url = None  # Deferred resolution
                    final_domain = domain
                    final_source_url = None
                    LOG.info(f"GOOGLE FEED DEFERRED: {title[:60]} â†’ {domain}")

            else:
                # Direct URL or unknown feed type - use standard resolution
                resolved_url, domain, source_url = domain_resolver._handle_direct_url(url)

                if not resolved_url or not domain:
                    stats["blocked_spam"] += 1
                    continue

                final_resolved_url = resolved_url
                final_domain = domain
                final_source_url = source_url

            # Generate hash for deduplication (supports both resolved URLs and Google News title-based)
            url_hash = get_url_hash(url, final_resolved_url, final_domain, title)
            
            try:
                with db() as conn, conn.cursor() as cur:
                    # Check if article already exists
                    cur.execute("""
                        SELECT a.id FROM articles a
                        JOIN ticker_articles ta ON a.id = ta.article_id
                        WHERE a.url_hash = %s AND ta.ticker = %s
                    """, (url_hash, feed["ticker"]))
                    existing_article = cur.fetchone()

                    if existing_article:
                        # Article already exists for this ticker
                        stats["duplicates"] += 1
                        # Enhanced logging to show deduplication strategy
                        is_google_news = 'news.google.com' in url
                        dedup_method = "domain+title" if is_google_news and final_domain else "URL"
                        LOG.info(f"âœ“ DUPLICATE BLOCKED ({dedup_method}): {final_domain or 'unknown'} - {title[:50]}...")
                        continue
                    
                    # Parse publish date
                    published_at = None
                    if hasattr(entry, "published_parsed"):
                        published_at = parse_datetime(entry.published_parsed)
                    
                    # Initialize content scraping variables
                    scraped_content = None
                    scraping_error = None
                    content_scraped_at = None
                    scraping_failed = False
                    ai_summary = None
                    should_use_ai = False
                    
                    # Attempt content scraping (respects 20/5/5 limits)
                    if final_resolved_url and final_resolved_url.startswith(('http://', 'https://')):
                        scrape_domain = normalize_domain(urlparse(final_resolved_url).netloc.lower())
                        
                        if scrape_domain in PAYWALL_DOMAINS:
                            stats["scraping_skipped"] += 1
                            LOG.info(f"Skipping paywall domain: {scrape_domain}")
                        else:
                            # Check scraping limits and attempt scraping
                            content, status = safe_content_scraper_with_3tier_fallback(
                                final_resolved_url, scrape_domain, category, feed_keyword, scraped_domains
                            )
                            
                            if content:
                                # Scraping successful - enable AI processing
                                scraped_content = content
                                content_scraped_at = datetime.now(timezone.utc)
                                stats["content_scraped"] += 1
                                should_use_ai = True
                                
                                # Generate AI summary from scraped content
                                ai_summary = generate_ai_individual_summary(scraped_content, title, feed["ticker"])
                                if ai_summary:
                                    stats["ai_summaries_generated"] += 1
                            else:
                                scraping_failed = True
                                scraping_error = status
                                stats["content_failed"] += 1
                    else:
                        stats["scraping_skipped"] += 1
                    
                    # Calculate quality score (AI only runs if scraping was successful)
                    if should_use_ai:
                        quality_score, ai_impact, ai_reasoning, components = calculate_quality_score(
                            title=title, domain=final_domain, ticker=feed["ticker"],
                            description=description, category=category, keywords=keywords
                        )
                        
                        source_tier = components.get('source_tier') if components else None
                        event_multiplier = components.get('event_multiplier') if components else None
                        event_multiplier_reason = components.get('event_multiplier_reason') if components else None
                        relevance_boost = components.get('relevance_boost') if components else None
                        relevance_boost_reason = components.get('relevance_boost_reason') if components else None
                        numeric_bonus = components.get('numeric_bonus') if components else None
                        penalty_multiplier = components.get('penalty_multiplier') if components else None
                        penalty_reason = components.get('penalty_reason') if components else None
                        
                        ai_processed_count += 1
                        stats["ai_scored"] += 1
                    else:
                        # No QB fallback - triage handles selection
                        quality_score = None
                        ai_impact = None
                        ai_reasoning = None
                        source_tier = None
                        event_multiplier = None
                        event_multiplier_reason = None
                        relevance_boost = None
                        relevance_boost_reason = None
                        numeric_bonus = None
                        penalty_multiplier = None
                        penalty_reason = None
                        stats["basic_scored"] += 1
                    
                    display_content = scraped_content if scraped_content else description
                    
                    # Insert article and link to ticker
                    article_id = insert_article_if_new(
                        url_hash, url, title, display_content,
                        final_domain, published_at, final_resolved_url
                    )

                    if article_id:
                        # Update with scraped content and AI summary if available
                        if scraped_content or ai_summary:
                            update_article_content(
                                article_id, scraped_content, ai_summary,
                                scraping_failed, scraping_error
                            )

                        # Link article to ticker
                        link_article_to_ticker(
                            article_id, feed["ticker"], feed["id"]
                        )

                    if article_id:
                        stats["inserted"] += 1
                        processing_type = "AI analysis" if should_use_ai else "basic processing"
                        content_info = f"with content + summary" if scraped_content and ai_summary else f"with content" if scraped_content else "no content"
                        
                        # Enhanced logging
                        resolution_info = ""
                        if final_source_url:
                            resolution_info = f" (via {get_or_create_formal_domain_name(normalize_domain(urlparse(final_source_url).netloc))})"
                        elif not final_resolved_url and is_google_feed:
                            resolution_info = f" (Google News - deferred resolution)"

                        LOG.info(f"Inserted [{category}] from {get_or_create_formal_domain_name(final_domain)}: {title[:60]}... ({processing_type}, {content_info}){resolution_info}")
                        
            except Exception as e:
                LOG.error(f"Database error for '{title[:50]}': {e}")
                continue
                
    except Exception as e:
        LOG.error(f"Feed processing error for {feed['name']}: {e}")
    
    # At the very end of the function, update the return statement:
    return {
        "processed": stats["processed"],
        "inserted": stats["inserted"],
        "duplicates": stats["duplicates"], 
        "blocked_spam": stats["blocked_spam"],
        "blocked_non_latin": stats["blocked_non_latin"],
        "content_scraped": stats["content_scraped"],
        "content_failed": stats["content_failed"],
        "scraping_skipped": stats["scraping_skipped"],
        "ai_reanalyzed": stats["ai_reanalyzed"],
        "ai_scored": stats["ai_scored"],
        "basic_scored": stats["basic_scored"],
        "ai_summaries_generated": stats["ai_summaries_generated"],
        "blocked_insider_trading": stats["blocked_insider_trading"]  # ADD THIS LINE
    }

def ingest_feed_basic_only(feed: Dict, mode: str = 'production', quota: int = None, ingestion_source: str = None) -> Dict[str, int]:
    """Basic feed ingestion with FIXED deduplication count tracking

    Args:
        feed: Feed configuration dict
        mode: 'production' or 'hourly' - controls Google title filtering
        quota: Max articles to insert from this feed (after spam filtering). None = unlimited.
        ingestion_source: Source of ingestion ('daily_workflow', 'hourly_alert', etc.)

    Returns:
        Stats dict with processed, inserted, duplicates, blocked counts
    """
    stats = {
        "processed": 0, "inserted": 0, "duplicates": 0, "blocked_spam": 0,
        "blocked_non_latin": 0, "limit_reached": 0, "blocked_insider_trading": 0,
        "yahoo_rejected": 0
    }

    # Track successful insertions for quota enforcement (counts AFTER spam filtering)
    articles_inserted_this_feed = 0

    category = feed.get("category", "company")
    
    # Use feed_ticker for competitor feeds, search_keyword for others - FIXED: Consistent logic
    if category == "competitor":
        feed_keyword = feed.get("feed_ticker")
        if not feed_keyword:
            feed_keyword = feed.get("search_keyword", "unknown")
    else:
        feed_keyword = feed.get("search_keyword", "unknown")
    
    # Track processed URLs within this feed run to avoid counting duplicates
    processed_hashes = set()
    processed_content_hashes = set()  # Domain + title deduplication

    try:
        parsed = feedparser.parse(feed["url"])
        
        # Check if feed parsed successfully
        if not hasattr(parsed, 'entries') or not parsed.entries:
            LOG.warning(f"No entries found in feed: {feed['name']}")
            return stats
        
        # Sort entries by publication date (newest first) if available
        entries_with_dates = []
        for entry in parsed.entries:
            pub_date = None
            if hasattr(entry, "published_parsed"):
                pub_date = parse_datetime(entry.published_parsed)
            entries_with_dates.append((entry, pub_date))
        
        entries_with_dates.sort(key=lambda x: (x[1] or datetime.min.replace(tzinfo=timezone.utc)), reverse=True)
        
        for entry, _ in entries_with_dates:
            try:
                stats["processed"] += 1
                
                url = getattr(entry, "link", None)
                title = getattr(entry, "title", "") or "No Title"
                raw_description = getattr(entry, "summary", "") if hasattr(entry, "summary") else ""
                
                # Filter description
                description = ""
                if raw_description and is_description_valuable(title, raw_description):
                    description = raw_description
                
                # Quick spam checks
                if not url or contains_non_latin_script(title):
                    stats["blocked_non_latin"] += 1
                    continue
                    
                if any(spam in title.lower() for spam in ["marketbeat", "newser", "khodrobank"]):
                    stats["blocked_spam"] += 1
                    continue
                
                # NEW: Insider trading check - skip these articles entirely
                if is_insider_trading_article(title):
                    stats["blocked_insider_trading"] += 1
                    LOG.debug(f"INSIDER TRADING BLOCKED: {title[:50]}...")
                    continue

                # HOURLY ALERTS: Google articles MUST have company name in title
                # EXCEPTION: Skip filter for industry feeds (keywords like "5G deployment", not company names)
                # Determine feed type early for filter check
                feed_url = feed.get("url", "")
                is_yahoo_feed = "finance.yahoo.com" in feed_url
                is_google_feed = "news.google.com" in feed_url

                if mode == 'hourly' and is_google_feed and category != 'industry':
                    search_keyword = feed.get('search_keyword', '')  # Stripped company name (no Inc., Corp., etc.)

                    # Check if company name in title (case-insensitive)
                    title_lower = title.lower()
                    keyword_lower = search_keyword.lower() if search_keyword else ''

                    if not (keyword_lower and keyword_lower in title_lower):
                        stats["blocked_spam"] += 1
                        LOG.info(f"HOURLY GOOGLE FILTER BLOCKED ({category}): '{title[:60]}' (no '{search_keyword}' in title)")
                        continue

                # FEED-TYPE-AWARE URL RESOLUTION (same as main ingestion)
                final_resolved_url = None
                final_domain = None
                final_source_url = None

                if is_yahoo_feed:
                    # YAHOO FEED: Resolve immediately
                    try:
                        resolved_url, domain, source_url = domain_resolver._handle_yahoo_finance(url)

                        if not resolved_url or not domain:
                            stats["yahoo_rejected"] += 1
                            continue

                        final_resolved_url = resolved_url
                        final_domain = domain
                        final_source_url = source_url
                    except Exception as e:
                        LOG.warning(f"Yahoo resolution failed for {url}: {e}")
                        stats["yahoo_rejected"] += 1
                        continue

                elif is_google_feed:
                    # GOOGLE FEED: Try FREE resolution first (Tier 1: Advanced API only)
                    resolved_url = domain_resolver._resolve_google_news_url_advanced(url)

                    if resolved_url:
                        # âœ… RESOLUTION SUCCEEDED

                        # YAHOO CHAIN DETECTION (exact same logic as Phase 1.5)
                        is_yahoo_finance = any(yahoo_domain in resolved_url for yahoo_domain in [
                            "finance.yahoo.com", "ca.finance.yahoo.com", "uk.finance.yahoo.com"
                        ])

                        if is_yahoo_finance:
                            yahoo_original = extract_yahoo_finance_source_optimized(resolved_url)
                            if yahoo_original and yahoo_original != resolved_url:
                                final_resolved_url = yahoo_original
                                final_domain = normalize_domain(urlparse(yahoo_original).netloc.lower())
                                LOG.info(f"GOOGLEâ†’YAHOOâ†’DIRECT: {title[:60]} â†’ {final_domain}")
                            else:
                                final_resolved_url = resolved_url
                                final_domain = normalize_domain(urlparse(resolved_url).netloc.lower())
                                if yahoo_original == resolved_url:
                                    LOG.warning(f"Yahoo extraction failed (no providerContentUrl): {resolved_url[:80]}")
                                LOG.info(f"GOOGLEâ†’YAHOO: {title[:60]} â†’ {final_domain}")
                        else:
                            final_resolved_url = resolved_url
                            final_domain = normalize_domain(urlparse(resolved_url).netloc.lower())
                            LOG.info(f"GOOGLE RESOLVED (Tier 1): {title[:60]} â†’ {final_domain}")

                        # SPAM CHECK (same as Yahoo feeds - block article entirely)
                        if domain_resolver._is_spam_domain(final_domain):
                            stats["blocked_spam"] += 1
                            LOG.info(f"SPAM REJECTED: Google News â†’ {final_domain} (from resolution)")
                            continue

                        final_source_url = None

                    else:
                        # âŒ RESOLUTION FAILED - Fall back to current workflow (title extraction)
                        LOG.warning(f"âš ï¸ ADVANCED API FAILED: {title[:60]} (falling back to title extraction)")
                        clean_title, source_name = extract_source_from_title_smart(title)

                        if source_name and not domain_resolver._is_spam_source(source_name):
                            domain = domain_resolver._resolve_publication_to_domain(source_name)

                            if domain and domain_resolver._is_spam_domain(domain):
                                stats["blocked_spam"] += 1
                                continue
                        else:
                            stats["blocked_spam"] += 1
                            continue

                        final_resolved_url = None  # Deferred
                        final_domain = domain
                        final_source_url = None

                else:
                    # Direct URL
                    try:
                        resolved_url, domain, source_url = domain_resolver._handle_direct_url(url)

                        if not resolved_url or not domain:
                            stats["blocked_spam"] += 1
                            continue

                        final_resolved_url = resolved_url
                        final_domain = domain
                        final_source_url = source_url
                    except Exception as e:
                        LOG.warning(f"URL resolution failed for {url}: {e}")
                        stats["blocked_spam"] += 1
                        continue

                # CHECK SPAM DOMAINS AFTER RESOLUTION
                if final_domain and final_domain in SPAM_DOMAINS:
                    stats["blocked_spam"] += 1
                    LOG.debug(f"SPAM DOMAIN BLOCKED: {final_domain} - {title[:50]}...")
                    continue

                # Generate hash for deduplication (supports both resolved URLs and Google News)
                url_hash = get_url_hash(url, final_resolved_url, final_domain, title)

                # Generate content hash for domain + title deduplication (safety net)
                content_hash = None
                if final_domain and title:
                    content_key = f"{final_domain}||{title.lower().strip()}"
                    content_hash = hashlib.md5(content_key.encode()).hexdigest()

                # CHECK FOR DUPLICATES WITHIN THIS RUN FIRST (URL or Content)
                if url_hash in processed_hashes:
                    stats["duplicates"] += 1
                    # Enhanced logging to show deduplication strategy
                    is_google_news = 'news.google.com' in url
                    dedup_method = "domain+title" if is_google_news and final_domain else "URL"
                    LOG.info(f"âœ“ DUPLICATE BLOCKED (URL): {final_domain or 'unknown'} - {title[:50]}...")
                    continue

                if content_hash and content_hash in processed_content_hashes:
                    stats["duplicates"] += 1
                    LOG.info(f"âœ“ DUPLICATE BLOCKED (Content): {final_domain or 'unknown'} - {title[:50]}...")
                    continue

                # Add to processed sets
                processed_hashes.add(url_hash)
                if content_hash:
                    processed_content_hashes.add(content_hash)

                # NOW check ingestion limits with FIXED count logic
                with db() as conn, conn.cursor() as cur:
                    try:
                        # Check if article already exists (URL hash OR domain+title)
                        cur.execute("""
                            SELECT a.id FROM articles a
                            JOIN ticker_articles ta ON a.id = ta.article_id
                            WHERE ta.ticker = %s
                            AND (
                                a.url_hash = %s
                                OR (a.domain = %s AND LOWER(TRIM(a.title)) = %s)
                            )
                            LIMIT 1
                        """, (feed["ticker"], url_hash, final_domain, title.lower().strip()))
                        if cur.fetchone():
                            stats["duplicates"] += 1
                            LOG.info(f"âœ“ DUPLICATE BLOCKED (DB): {final_domain or 'unknown'} - {title[:50]}...")
                            continue
                        
                        # HOURLY ALERTS: No ingestion limits (lightweight, no scraping/AI)
                        # Articles are processed quickly and cumulative display is expected
                        
                        # Parse publish date
                        published_at = None
                        if hasattr(entry, "published_parsed"):
                            published_at = parse_datetime(entry.published_parsed)
                        
                        # Use basic scoring
                        domain_tier = _get_domain_tier(final_domain, title, description)
                        basic_quality_score = 50.0 + (domain_tier - 0.5) * 20
                        
                        # Add ticker mention bonus
                        if feed["ticker"].upper() in title.upper():
                            basic_quality_score += 10
                        
                        # Clamp to reasonable range
                        basic_quality_score = max(20.0, min(80.0, basic_quality_score))
                        
                        display_content = description
                        
                        # Clean all text fields to remove NULL bytes
                        clean_url = clean_null_bytes(url or "")
                        clean_resolved_url = clean_null_bytes(final_resolved_url) if final_resolved_url else None
                        clean_title = clean_null_bytes(title or "")
                        clean_description = clean_null_bytes(display_content or "")
                        clean_search_keyword = clean_null_bytes(feed.get("search_keyword") or "")
                        clean_source_url = clean_null_bytes(final_source_url) if final_source_url else None
                        clean_feed_ticker = clean_null_bytes(feed.get("feed_ticker") or "")
                        
                        # Insert article if new, then link to ticker
                        article_id = insert_article_if_new(
                            url_hash, clean_url, clean_title, clean_description,
                            final_domain, published_at, clean_resolved_url,
                            ingestion_source
                        )

                        if article_id:
                            # DEBUG: Log feed dict for value_chain articles
                            if feed.get("search_keyword") in ["ON Semiconductor", "Flex", "Texas Instruments", "Tesla", "Sunrun", "Sunnova Energy"]:
                                LOG.info(f"[DEBUG VALUE_CHAIN] Article: {title[:60]}")
                                LOG.info(f"[DEBUG VALUE_CHAIN] Feed keys: {list(feed.keys())}")
                                LOG.info(f"[DEBUG VALUE_CHAIN] value_chain_type raw: {repr(feed.get('value_chain_type'))}")
                                LOG.info(f"[DEBUG VALUE_CHAIN] feed_id: {feed.get('id')}, feed_name: {feed.get('name')}")
                                LOG.info(f"[DEBUG VALUE_CHAIN] category from ticker_feeds: {feed.get('category')}")
                                LOG.info(f"[DEBUG VALUE_CHAIN] category variable: {category}")

                            # Link article to ticker (all metadata derived via feed_id)
                            link_article_to_ticker(
                                article_id, feed["ticker"], feed["id"]
                            )
                            stats["inserted"] += 1

                            # Increment quota counter (only after successful insert, post-spam filtering)
                            articles_inserted_this_feed += 1

                            # Check quota limit (enforced per feed, after all spam/duplicate filtering)
                            if quota is not None and articles_inserted_this_feed >= quota:
                                stats["limit_reached"] += 1
                                LOG.info(f"âœ… QUOTA REACHED: {articles_inserted_this_feed}/{quota} clean articles for {feed.get('name')} [{category}]")
                                break  # Exit entry loop, stop processing this feed

                            # Get current count after insertion
                            limit_key = 'company' if category == 'company' else f'{category}_per_keyword'
                            current_limit = ingestion_stats['limits'][limit_key]
                            LOG.info(f"INSERTED [{category}]: Article inserted (limit: {current_limit}) - {title[:50]}...")
                        else:
                            LOG.info(f"DUPLICATE SKIPPED: {title[:30]}")
                            
                    except Exception as db_e:
                        LOG.error(f"Database error processing article '{title[:30]}...': {type(db_e).__name__}: {str(db_e)}")
                        LOG.error(f"Full traceback: {traceback.format_exc()}")
                        continue
                            
            except Exception as entry_e:
                LOG.error(f"Error processing feed entry: {entry_e}")
                continue
                        
    except Exception as e:
        LOG.error(f"Feed processing error for {feed['name']}: {e}")
    
    # At the very end of the function, update the return statement:
    return {
        "processed": stats["processed"],
        "inserted": stats["inserted"], 
        "duplicates": stats["duplicates"],
        "blocked_spam": stats["blocked_spam"],
        "blocked_non_latin": stats["blocked_non_latin"],
        "limit_reached": stats["limit_reached"],
        "blocked_insider_trading": stats["blocked_insider_trading"],
        "yahoo_rejected": stats["yahoo_rejected"]  # ADD THIS LINE
    }

def _check_ingestion_limit_with_existing_count(category: str, keyword: str, existing_count: int) -> bool:
    """Check if we can ingest more articles considering existing count in database"""
    global ingestion_stats
    
    if category == "company":
        total_count = existing_count + ingestion_stats["company_ingested"]
        return total_count < ingestion_stats["limits"]["company"]
    
    elif category == "industry":
        current_keyword_count = ingestion_stats["industry_ingested_by_keyword"].get(keyword, 0)
        total_count = existing_count + current_keyword_count
        return total_count < ingestion_stats["limits"]["industry_per_keyword"]
    
    elif category == "competitor":
        current_keyword_count = ingestion_stats["competitor_ingested_by_keyword"].get(keyword, 0)
        total_count = existing_count + current_keyword_count
        return total_count < ingestion_stats["limits"]["competitor_per_keyword"]

    elif category == "value_chain":
        current_keyword_count = ingestion_stats["value_chain_ingested_by_keyword"].get(keyword, 0)
        total_count = existing_count + current_keyword_count
        return total_count < ingestion_stats["limits"]["value_chain_per_keyword"]

    return False

# Updated article formatting function
def _format_article_html_with_ai_summary(article: Dict, category: str, ticker_metadata_cache: Dict = None,
                                         show_ai_analysis: bool = True, show_descriptions: bool = True) -> str:
    """
    Enhanced article HTML formatting with AI summaries and proper left-side headers

    Args:
        article: Article dictionary
        category: Article category (company/industry/competitor)
        ticker_metadata_cache: Cache of ticker metadata
        show_ai_analysis: If True, show AI analysis boxes (default True)
        show_descriptions: If True, show article descriptions (default True)
    """
    import html
    
    # Format timestamp for individual articles
    if article["published_at"]:
        pub_date = format_timestamp_est(article["published_at"])
    else:
        pub_date = "N/A"
    
    original_title = article["title"] or "No Title"
    resolved_domain = article["domain"] or "unknown"
    
    # Determine source and clean title based on domain type
    if "news.google.com" in resolved_domain or resolved_domain == "google-news-unresolved":
        title_result = extract_source_from_title_smart(original_title)
        
        if title_result[0] is None:
            return ""
        
        title, extracted_source = title_result
        
        if extracted_source:
            display_source = get_or_create_formal_domain_name(extracted_source)
        else:
            display_source = "Google News"
    else:
        title = original_title
        display_source = get_or_create_formal_domain_name(resolved_domain)
    
    # Additional title cleanup
    title = re.sub(r'\s*\$[A-Z]+\s*-?\s*', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    
    # Determine the actual link URL
    link_url = article["resolved_url"] or article.get("original_source_url") or article["url"]
    
    # Get company name for company articles
    ticker = article.get("ticker", "")
    company_name = ticker
    if ticker_metadata_cache and ticker in ticker_metadata_cache:
        company_name = ticker_metadata_cache[ticker].get("company_name", ticker)
    
    # Build header badges - LEFT SIDE POSITIONING
    header_badges = []
    
    # 1. FIRST BADGE: Category-specific (LEFT SIDE)
    if category == "company":
        header_badges.append(f'<span class="company-name-badge">ðŸŽ¯ {company_name}</span>')
    elif category == "competitor":
        comp_name = get_competitor_display_name(article.get('search_keyword'), article.get('feed_ticker'))
        header_badges.append(f'<span class="competitor-badge">ðŸ¢ {comp_name}</span>')
    elif category == "upstream":
        # Show upstream supplier company name
        vc_name = article.get('search_keyword', 'Unknown')
        header_badges.append(f'<span class="upstream-badge">ðŸ¢ {vc_name}</span>')
    elif category == "downstream":
        # Show downstream customer company name
        vc_name = article.get('search_keyword', 'Unknown')
        header_badges.append(f'<span class="downstream-badge">ðŸ¢ {vc_name}</span>')
    elif category == "value_chain":
        # Fallback for old value_chain category (should not be used anymore)
        vc_name = article.get('search_keyword', 'Unknown')
        vc_type = article.get('value_chain_type', '')
        vc_icon = "â¬†ï¸" if vc_type == "upstream" else "â¬‡ï¸" if vc_type == "downstream" else "ðŸ”—"
        vc_label = "Upstream" if vc_type == "upstream" else "Downstream" if vc_type == "downstream" else "Value Chain"
        header_badges.append(f'<span class="value-chain-badge">{vc_icon} {vc_label}: {vc_name}</span>')
    elif category == "industry" and article.get('search_keyword'):
        header_badges.append(f'<span class="industry-badge">ðŸ­ {article["search_keyword"]}</span>')
    
    # 2. SECOND BADGE: Source name
    header_badges.append(f'<span class="source-badge">ðŸ“° {display_source}</span>')

    # 2.5 OFFICIAL BADGE: For company releases (NEW - Nov 2025)
    if article.get("is_company_release"):
        header_badges.append('<span class="official-badge">ðŸ›ï¸ Official</span>')

    # 3. STATUS BADGES - Mutually exclusive statuses based on ai_model and processing state
    ai_model = article.get('ai_model') or ''
    ai_summary = article.get('ai_summary')
    scraped_content = article.get('scraped_content')
    scraping_failed = article.get('scraping_failed', False)

    # Determine status and render appropriate badge
    if ai_model == 'spam':
        # Spam article (resolved to spam domain)
        header_badges.append('<span class="spam-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fee; color: #c53030; border: 1px solid #fc8181;">ðŸ—‘ï¸ Spam</span>')
    elif normalize_domain(resolved_domain) in PAYWALL_DOMAINS:
        # Paywalled article - scraping skipped
        header_badges.append('<span class="paywall-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fef5e7; color: #b7791f; border: 1px solid #f6e05e;">ðŸ“° Paywall</span>')
    elif ai_model == 'filtered':
        # Filtered by retail platform detection
        header_badges.append('<span class="filtered-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #f7fafc; color: #718096; border: 1px solid #cbd5e0;">ðŸš« Skipped (Retail)</span>')
    elif ai_model == 'low_relevance':
        # Rejected by relevance gate (badge handled by relevance_badge_html below at Line 6240+)
        # No additional badge needed here - relevance section shows score and reason
        pass
    elif ai_model == 'error':
        # AI analysis failed (technical error)
        header_badges.append('<span class="error-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fee; color: #c53030; border: 1px solid #fc8181;">âŒ Failed (AI Error)</span>')
    elif ai_model == 'short_content':
        # Article too short for AI analysis (< 200 chars)
        header_badges.append('<span class="short-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fee; color: #c53030; border: 1px solid #fc8181;">âŒ Failed (Too Short)</span>')
    elif ai_model == 'api_failed':
        # Both Gemini and Claude API calls failed
        header_badges.append('<span class="api-failed-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fee; color: #c53030; border: 1px solid #fc8181;">âŒ Failed (AI)</span>')
    elif ai_model == 'metadata_missing':
        # Missing feed_ticker or value_chain_type for competitor/value_chain articles
        header_badges.append('<span class="metadata-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fee; color: #c53030; border: 1px solid #fc8181;">âŒ Failed (Metadata)</span>')
    elif ai_summary and ai_model and ai_model not in ('none', 'spam', 'filtered', 'low_relevance', 'error', 'short_content', 'api_failed', 'metadata_missing'):
        # Successfully analyzed article - show which AI model was used
        header_badges.append(f'<span class="ai-model-badge">ðŸ¤– {ai_model}</span>')
    elif scraping_failed:
        # Scraping failed
        header_badges.append('<span class="failed-badge" style="display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fee; color: #c53030; border: 1px solid #fc8181;">âŒ Failed (Scraping)</span>')

    # 4. Quality badge for quality domains
    if normalize_domain(resolved_domain) in QUALITY_DOMAINS:
        header_badges.append('<span class="quality-badge">â­ Quality</span>')

    # 6. Relevance score badge for industry articles (NEW - Oct 2025)
    relevance_badge_html = ""
    relevance_reason_html = ""
    if category == "industry" and article.get('relevance_score') is not None:
        score = article.get('relevance_score')
        is_rejected = article.get('is_rejected', False)
        reason = article.get('relevance_reason', 'No reason provided')

        # Color and label based on rejection status
        if is_rejected:
            badge_style = "background-color: #fee; color: #c53030; border: 1px solid #fc8181;"
            badge_icon = "âœ—"
            badge_label = f"REJECTED ({score:.1f}/10)"
        else:
            badge_style = "background-color: #e6ffed; color: #22543d; border: 1px solid #9ae6b4;"
            badge_icon = "âœ“"
            badge_label = f"RELEVANCE ({score:.1f}/10)"

        relevance_badge_html = f'<span style="display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; {badge_style}">{badge_icon} {badge_label}</span>'
        header_badges.append(relevance_badge_html)

        # Relevance reason shown below badges, above AI summary
        reason_escaped = html.escape(reason)
        relevance_reason_html = f"<br><div style='color: #718096; font-size: 11px; font-style: italic; margin-top: 4px; padding: 6px 8px; background-color: #f7fafc; border-left: 3px solid {'#fc8181' if is_rejected else '#9ae6b4'}; border-radius: 3px;'><strong>Relevance:</strong> {reason_escaped}</div>"

    # 7. Quality score badge (NEW - Oct 2025)
    quality_badge_html = ""
    if article.get('quality_score') is not None:
        score = article.get('quality_score')

        # Color based on score
        if score >= 7.0:
            badge_color = "#22543d"  # Green
            bg_color = "#e6ffed"
            border_color = "#9ae6b4"
        elif score >= 5.0:
            badge_color = "#744210"  # Yellow
            bg_color = "#fefcbf"
            border_color = "#f6e05e"
        else:
            badge_color = "#c53030"  # Red
            bg_color = "#fee"
            border_color = "#fc8181"

        quality_badge_html = f'<span style="display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: {bg_color}; color: {badge_color}; border: 1px solid {border_color};">QUALITY ({score:.1f}/10)</span>'
        header_badges.append(quality_badge_html)

    # AI Summary section - check for ai_summary field (conditional based on show_ai_analysis)
    ai_summary_html = ""
    if show_ai_analysis and article.get("ai_summary"):
        clean_summary = html.escape(article["ai_summary"].strip())
        ai_summary_html = f"<br><div class='ai-summary'><strong>ðŸ“Š Analysis:</strong> {clean_summary}</div>"

    # Get description and format it (only if no AI summary shown, conditional based on show_descriptions)
    description_html = ""
    if show_descriptions and not article.get("ai_summary") and article.get("description"):
        description = article["description"].strip()
        description = html.unescape(description)
        description = re.sub(r'<[^>]+>', '', description)
        description = re.sub(r'\s+', ' ', description).strip()

        if len(description) > 500:
            description = description[:500] + "..."

        description = html.escape(description)
        description_html = f"<br><div class='description'>{description}</div>"
    
    return f"""
    <div class='article {category}'>
        <div class='article-header'>
            {' '.join(header_badges)}
        </div>
        <div class='article-content'>
            <a href='{link_url}' target='_blank'>{title}</a>
            <span class='meta'> | {pub_date}</span>
            {relevance_reason_html}
            {ai_summary_html}
            {description_html}
        </div>
    </div>
    """

def get_or_create_ticker_metadata(ticker: str) -> Dict:
    """Wrapper for backward compatibility - delegates to TickerManager"""
    return ticker_manager.get_or_create_metadata(ticker)

def build_feed_urls(ticker: str, keywords: Dict) -> List[Dict]:
    """NEW ARCHITECTURE: Build feed URLs based on ticker metadata (without creating feeds)"""
    feeds = []
    company_name = keywords.get("company_name", ticker)

    # 1. Company feeds (2 feeds)
    feeds.extend([
        {
            "url": f"https://news.google.com/rss/search?q=\"{company_name.replace(' ', '%20')}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
            "name": f"Google News: {company_name}",
            "category": "company",
            "search_keyword": company_name
        },
        {
            "url": f"https://finance.yahoo.com/rss/headline?s={ticker}",
            "name": f"Yahoo Finance: {ticker}",
            "category": "company",
            "search_keyword": ticker
        }
    ])

    # 2. Industry feeds (up to 3)
    industry_keywords = keywords.get("industry_keywords", [])[:3]
    for keyword in industry_keywords:
        feeds.append({
            "url": f"https://news.google.com/rss/search?q=\"{keyword.replace(' ', '%20')}\"+when:7d&hl=en-US&gl=US&ceid=US:en",
            "name": f"Industry: {keyword}",
            "category": "industry",
            "search_keyword": keyword
        })

    # 3. Competitor feeds (up to 3)
    competitors = keywords.get("competitors", [])[:3]
    for comp in competitors:
        if isinstance(comp, dict) and comp.get('name') and comp.get('ticker'):
            comp_name = comp['name']
            comp_ticker = comp['ticker']

            feeds.extend([
                {
                    "url": f"https://news.google.com/rss/search?q=\"{comp_name.replace(' ', '%20')}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
                    "name": f"Competitor: {comp_name}",
                    "category": "competitor",
                    "search_keyword": comp_name,
                    "competitor_ticker": comp_ticker
                },
                {
                    "url": f"https://finance.yahoo.com/rss/headline?s={comp_ticker}",
                    "name": f"Yahoo Competitor: {comp_name} ({comp_ticker})",
                    "category": "competitor",
                    "search_keyword": comp_name,
                    "competitor_ticker": comp_ticker
                }
            ])

    return feeds
    
def upsert_feed(url: str, name: str, ticker: str, category: str = "company",
                retain_days: int = 90, search_keyword: str = None,
                competitor_ticker: str = None) -> int:
    """NEW ARCHITECTURE V2: Feed upsert with category per ticker-feed relationship"""
    LOG.info(f"DEBUG: Upserting feed (NEW ARCHITECTURE V2) - ticker: {ticker}, name: {name}, category: {category}, search_keyword: {search_keyword}")

    try:
        # Use NEW ARCHITECTURE V2 functions directly (no import needed)
        # Create/get feed in new architecture (NO CATEGORY in feed itself)
        feed_id = upsert_feed_new_architecture(
            url=url,
            name=name,
            search_keyword=search_keyword,
            competitor_ticker=competitor_ticker,
            retain_days=retain_days
        )

        # Associate ticker with feed WITH SPECIFIC CATEGORY for this relationship
        if associate_ticker_with_feed_new_architecture(ticker, feed_id, category):
            LOG.info(f"DEBUG: Feed upsert SUCCESS (NEW ARCHITECTURE V2) - ID: {feed_id}, category: {category}")
            return feed_id
        else:
            LOG.error(f"Failed to associate ticker {ticker} with feed {feed_id}")
            return -1

    except Exception as e:
        LOG.error(f"Error upserting feed for {ticker} (NEW ARCHITECTURE): {e}")
        return -1

def list_active_feeds(tickers: List[str] = None) -> List[Dict]:
    """NEW ARCHITECTURE V2: Get all active feeds with per-relationship categories"""
    with db() as conn, conn.cursor() as cur:
        if tickers:
            cur.execute("""
                SELECT f.id, f.url, f.name, tf.ticker, f.retain_days, tf.category, f.search_keyword, f.feed_ticker
                FROM feeds f
                JOIN ticker_feeds tf ON f.id = tf.feed_id
                WHERE f.active = TRUE AND tf.active = TRUE AND tf.ticker = ANY(%s)
                ORDER BY tf.ticker, tf.category, f.id
            """, (tickers,))
        else:
            cur.execute("""
                SELECT f.id, f.url, f.name, tf.ticker, f.retain_days, tf.category, f.search_keyword, f.feed_ticker
                FROM feeds f
                JOIN ticker_feeds tf ON f.id = tf.feed_id
                WHERE f.active = TRUE AND tf.active = TRUE
                ORDER BY tf.ticker, tf.category, f.id
            """)
        return list(cur.fetchall())

# ------------------------------------------------------------------------------
# URL Resolution and Quality Scoring
# ------------------------------------------------------------------------------
# FIX #6: Add function to detect non-Latin script
def contains_non_latin_script(text: str) -> bool:
    """Detect if text contains non-Latin script characters (Arabic, Chinese, etc.)"""
    if not text:
        return False
    
    # Unicode ranges for non-Latin scripts that commonly appear in spam
    non_latin_ranges = [
        # Arabic and Arabic Supplement
        (0x0600, 0x06FF),  # Arabic
        (0x0750, 0x077F),  # Arabic Supplement
        (0x08A0, 0x08FF),  # Arabic Extended-A
        (0xFB50, 0xFDFF),  # Arabic Presentation Forms-A
        (0xFE70, 0xFEFF),  # Arabic Presentation Forms-B
        
        # Chinese, Japanese, Korean
        (0x4E00, 0x9FFF),  # CJK Unified Ideographs
        (0x3400, 0x4DBF),  # CJK Extension A
        (0x3040, 0x309F),  # Hiragana
        (0x30A0, 0x30FF),  # Katakana
        (0xAC00, 0xD7AF),  # Hangul Syllables
        
        # Cyrillic
        (0x0400, 0x04FF),  # Cyrillic
        (0x0500, 0x052F),  # Cyrillic Supplement
        
        # Hebrew
        (0x0590, 0x05FF),  # Hebrew
        
        # Thai
        (0x0E00, 0x0E7F),  # Thai
        
        # Devanagari (Hindi, Sanskrit)
        (0x0900, 0x097F),  # Devanagari
    ]
    
    for char in text:
        char_code = ord(char)
        for start, end in non_latin_ranges:
            if start <= char_code <= end:
                return True
    
    return False

def extract_yahoo_finance_source_optimized(url: str) -> Optional[str]:
    """Enhanced Yahoo Finance source extraction - handles all Yahoo Finance domains and filters out video URLs"""
    try:
        # Expand the domain check to include regional Yahoo Finance
        if not any(domain in url for domain in ["finance.yahoo.com", "ca.finance.yahoo.com", "uk.finance.yahoo.com"]):
            return None
        
        # Skip Yahoo author pages, video files, media.zenfs.com redirects, AND video.media.yql.yahoo.com
        skip_patterns = [
            "/author/", "yahoo-finance-video", ".mp4", ".avi", ".mov",
            "video.media.yql.yahoo.com",  # Block video URLs
            "media.zenfs.com"  # Block media.zenfs.com redirects
        ]
        if any(skip_pattern in url for skip_pattern in skip_patterns):
            LOG.info(f"Skipping Yahoo video/author/zenfs page: {url}")
            return None
            
        LOG.info(f"Extracting Yahoo Finance source from: {url}")

        response = requests.get(url, timeout=15, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Encoding': 'identity'  # Prevent gzip decompression errors from Yahoo
        })
        
        if response.status_code != 200:
            LOG.warning(f"HTTP {response.status_code} when fetching Yahoo URL: {url}")
            return None
        
        html_content = response.text
        LOG.debug(f"Yahoo page content length: {len(html_content)}")
        
        # Try the most reliable patterns in order of preference
        extraction_patterns = [
            # Pattern 1: Standard providerContentUrl
            r'"providerContentUrl"\s*:\s*"([^"]*)"',
            # Pattern 2: Escaped JSON patterns
            r'\\+"providerContentUrl\\+"\s*:\s*\\+"([^\\]*?)\\+"'
        ]
        
        for i, pattern in enumerate(extraction_patterns):
            matches = re.findall(pattern, html_content)
            LOG.debug(f"Pattern {i+1} found {len(matches)} matches")
            
            for match in matches:
                try:
                    # Try different unescaping methods
                    candidate_urls = []
                    
                    # Method 1: JSON unescape
                    try:
                        unescaped_url = json.loads(f'"{match}"')
                        candidate_urls.append(unescaped_url)
                    except json.JSONDecodeError:
                        pass
                    
                    # Method 2: Simple replace
                    simple_unescaped = match.replace('\\/', '/').replace('\\"', '"')
                    candidate_urls.append(simple_unescaped)
                    
                    # Method 3: Raw match
                    candidate_urls.append(match)
                    
                    # Test each candidate
                    for candidate_url in candidate_urls:
                        try:
                            parsed = urlparse(candidate_url)
                            if (parsed.scheme in ['http', 'https'] and
                                parsed.netloc and
                                len(candidate_url) > 20 and
                                'finance.yahoo.com' not in candidate_url and
                                'ca.finance.yahoo.com' not in candidate_url and
                                'video.media.yql.yahoo.com' not in candidate_url and  # Block video URLs
                                'media.zenfs.com' not in candidate_url and  # Block media.zenfs.com redirects
                                not candidate_url.startswith('//') and
                                '.' in parsed.netloc and
                                # Enhanced validation to exclude problematic URLs
                                not candidate_url.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.mp4', '.avi', '.mov')) and
                                not '/api/' in candidate_url.lower() and
                                not 'yimg.com' in candidate_url.lower() and
                                not '/author/' in candidate_url.lower() and
                                not 'yahoo-finance-video' in candidate_url.lower()):

                                LOG.info(f"Successfully extracted Yahoo source: {candidate_url}")
                                return candidate_url
                        except Exception as e:
                            LOG.debug(f"URL validation failed for {candidate_url}: {e}")
                            continue
                            
                except Exception as e:
                    LOG.debug(f"Processing match failed: {e}")
                    continue

        # If no redirect found, keep the original Yahoo URL
        LOG.info(f"No redirect found, keeping original Yahoo URL: {url}")
        return url
        
    except Exception as e:
        LOG.error(f"Yahoo Finance source extraction failed for {url}: {e}")
        return None

def extract_source_from_title_smart(title: str) -> Tuple[str, Optional[str]]:
    """Extract source from title with simple regex"""
    if not title:
        return title, None
    
    # Simple patterns for common formats
    patterns = [
        r'\s*-\s*([^-]+?)(?:\s*\([^)]*\.gov[^)]*\))?$',  # " - Source" (ignore .gov suffix)
        r'\s*-\s*([^-]+)$',  # " - Source"
        r'\s*\|\s*([^|]+)$'   # " | Source"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, title)
        if match:
            source = match.group(1).strip()
            clean_title = re.sub(pattern, '', title).strip()
            
            # FIXED: Allow 3 characters (for MSN, CNN, etc.) and basic validation
            if 2 < len(source) < 50 and not any(spam in source.lower() for spam in ["marketbeat", "newser", "khodrobank"]):
                return clean_title, source
    
    return title, None
    
def _get_domain_tier(domain: str, title: str = "", description: str = "") -> float:
    """Get domain authority tier with potential upgrades from content - FIXED minimum tier"""
    if not domain:
        return 0.5
    
    normalized_domain = normalize_domain(domain)
    base_tier = DOMAIN_TIERS.get(normalized_domain, 0.5)  # This is correct
    
    # Upgrade tier if aggregator content reveals higher-quality source
    if any(agg in normalized_domain for agg in ["yahoo", "msn", "google"]):
        combined_text = f"{title} || {description}".lower()
        
        for pattern, tier in SOURCE_TIER_HINTS:
            if re.search(pattern, combined_text, re.IGNORECASE):
                return max(base_tier, tier)
    
    # FIXED: Ensure Tier C and below get 0.5 minimum for scoring
    # Tier C (0.4) and below should be treated as 0.5 for scoring purposes
    return max(base_tier, 0.5) if base_tier < 0.5 else base_tier

def _is_spam_content(title: str, domain: str, description: str = "") -> bool:
    """Check if content appears to be spam - reuse existing spam logic"""
    if not title:
        return True
    
    # Use existing spam domain check
    if any(spam in domain.lower() for spam in SPAM_DOMAINS):
        return True
    
    # Use existing non-Latin script check
    if contains_non_latin_script(title):
        return True
    
    # Additional spam indicators
    spam_phrases = [
        "marketbeat", "newser", "khodrobank", "should you buy", "top stocks to",
        "best stocks", "stock picks", "hot stocks", "penny stocks"
    ]
    
    combined_text = f"{title} {description}".lower()
    return any(phrase in combined_text for phrase in spam_phrases)

# Retail platform keywords for content filtering
RETAIL_PLATFORM_KEYWORDS = [
    # Platform names
    "zacks investment research", "zacks.com", "zacks rank",
    "tipranks", "tipranks.com", "smart score",
    "simply wall st", "simplywall", "snowflake rating", "snowflake score",
    "gurufocus", "gurufocus.com", "gf value", "peter lynch fair value",
    "motley fool", "fool.com", "stock advisor", "rule breakers",
    "marketbeat", "marketbeat.com",
    "finviz", "finviz.com", "finviz screener",
    "fintel",
    "insider monkey", "insidermonkey",
    "24/7 wall st", "247wallst",
    "stockanalysis.com", "stockrover", "stockcharts.com",
    "stockinvest.us", "wallstreetzen", "stockopedia",
    "sharewise", "stockstory.org",

    # Disclaimers (high confidence smoking guns)
    "this article by simply wall st",
    "see our full analysis for",  # Simply Wall St signature call-to-action
    "the motley fool has a disclosure policy",
    "zacks investment research disclaimer",
    "gurufocus disclaimer",

    # Service/product references
    "zacks premium", "zacks elite",
    "tipranks premium", "tipranks gold",
    "stock advisor subscription", "rule breakers subscription",

    # SimplyWall.St content patterns (highly specific signatures)
    "estimated discount to fair value",       # Their signature metric format
    "stocks from our screener",               # Their screener article signature
    "based on cash flows screener",           # Their screener naming convention
    "discounted cash flow as at",             # Their chart caption format
    "fair value (est)",                       # Their table column header
    "discount (est)",                         # Their table column header
    "operations: the company generates",      # Their template structure
    "most popular narrative",                 # Their proprietary valuation term (NEW - Oct 2025)
    "read the complete narrative",            # Their standard CTA with proprietary term (NEW - Oct 2025)
    "see our latest analysis for",            # Variant of "see our full analysis for" (NEW - Oct 2025)
    "view our latest analysis for",           # Another Simply Wall St CTA variant (NEW - Dec 2025)
    "free analyst report for",                # Simply Wall St upsell CTA (NEW - Dec 2025)
]

def is_retail_platform_content(content: str) -> Tuple[bool, Optional[str]]:
    """
    Detect retail stock screener platform content via keyword search.

    This function performs Python-based filtering BEFORE AI analysis to:
    - Eliminate AI interpretation variability
    - Save API costs (skip expensive Claude/OpenAI calls)
    - Provide deterministic, debuggable filtering

    Args:
        content: Article scraped content (full text)

    Returns:
        Tuple of (is_retail: bool, keyword: str | None)
        - (True, "keyword") if retail content detected
        - (False, None) if legitimate article

    Examples:
        >>> is_retail_platform_content("...Zacks Rank #2...")
        (True, "zacks rank")

        >>> is_retail_platform_content("Reuters reports earnings beat")
        (False, None)
    """
    if not content:
        return False, None

    content_lower = content.lower()

    # Simple substring search - fast and catches variations
    for keyword in RETAIL_PLATFORM_KEYWORDS:
        if keyword in content_lower:
            return True, keyword

    return False, None

def calculate_quality_score(
    title: str, 
    domain: str, 
    ticker: str,
    description: str = "",
    category: str = "company",
    keywords: List[str] = None
) -> Tuple[float, Optional[str], Optional[str], Optional[Dict]]:
    """No individual article scoring - return None values"""
    return None, None, None, None

# QB fallback scoring removed - triage now handles all article selection

async def generate_ai_individual_summary_async(scraped_content: str, title: str, ticker: str, description: str = "") -> Optional[str]:
    """Async version of AI summary generation with semaphore control"""
    if not OPENAI_API_KEY or not scraped_content or len(scraped_content.strip()) < 200:
        LOG.warning(f"AI summary generation skipped - API key: {bool(OPENAI_API_KEY)}, content length: {len(scraped_content) if scraped_content else 0}")
        return None

    # SEMAPHORE DISABLED: Prevents threading deadlock with concurrent tickers
    # with OPENAI_SEM:
    if True:  # Maintain indentation
        try:
            config = get_ticker_config(ticker)
            company_name = config.get("name", ticker) if config else ticker
            sector = config.get("sector", "") if config else ""
            
            prompt = f"""You are a hedge-fund analyst. Write a 5â€“7 sentence summary that is 100% EXTRACTIVE.

Rules (hard):
- Use ONLY facts explicitly present in the provided text (no outside knowledge, no inference, no estimates).
- Include every stated number, date, percentage, price, share count, unit, and named entity relevant to the main event.
- If a detail is not stated in the text, do NOT mention it or imply it.
- Forbidden words/hedges: likely, may, could, should, appears, expect, estimate, assume, infer, suggests, catalysts, risks, second-order.
- No bullets, no labels, no quotes, no headlines; 5â€“7 sentences; each â‰¤ 28 words.

If the text lacks enough information for 5 sentences, write only as many factual sentences as the text supports (minimum 3), still â‰¤ 28 words each.

Article Title: {title}
Content Snippet: {description[:1000] if description else ""}
Full Content: {scraped_content[:10000]}

"""

            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": OPENAI_MODEL,
                "input": prompt,
                "max_output_tokens": 15000,
                "reasoning": {"effort": "medium"},
                "text": {"verbosity": "low"},
                "truncation": "auto"
            }
            
            LOG.info(f"Generating AI summary for {ticker} - Content: {len(scraped_content)} chars, Title: {title[:50]}...")
            
            # Use asyncio-compatible HTTP client
            session = get_http_session()
            async with session.post(OPENAI_API_URL, headers=headers, json=data, timeout=aiohttp.ClientTimeout(total=180)) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        u = result.get("usage", {}) or {}
                        LOG.info("AI Enhanced Summary usage â€“ input:%s output:%s (cap:%s) status:%s reason:%s",
                                 u.get("input_tokens"), u.get("output_tokens"),
                                 result.get("max_output_tokens"),
                                 result.get("status"),
                                 (result.get("incomplete_details") or {}).get("reason"))
                        
                        summary = extract_text_from_responses(result)
                        if summary and len(summary.strip()) > 10:
                            LOG.info(f"Generated AI summary for {ticker}: {len(summary)} chars - '{summary[:100]}...'")
                            return summary.strip()
                        else:
                            LOG.warning(f"AI summary empty or too short for {ticker}: '{summary}'")
                            return None
                    else:
                        error_text = await response.text()
                        LOG.error(f"AI summary API error {response.status} for {ticker}: {error_text}")
                        return None
                        
        except Exception as e:
            LOG.error(f"AI enhanced summary generation failed for {ticker}: {e}")
            return None

# ===== CATEGORIZED AI SUMMARIZATION WITH CLAUDE PRIMARY, OPENAI FALLBACK =====

# Content character limit for AI summarization
CONTENT_CHAR_LIMIT = 10000


# ============================================================================
# ARTICLE SUMMARY WRAPPERS (Nov 2025 - Gemini primary, Claude fallback)
# ============================================================================

async def generate_article_summary_company(company_name: str, ticker: str, title: str, scraped_content: str) -> Tuple[Optional[str], str]:
    """Generate article summary for company article

    Primary: Gemini Flash 2.5
    Fallback: Claude Sonnet 4.5

    Returns:
        Tuple[Optional[str], str]: (summary_with_quality_json, provider)
    """
    # Try Gemini first
    if GEMINI_API_KEY:
        summary, provider, usage = await article_summaries.generate_gemini_article_summary_company(
            company_name, ticker, title, scraped_content, GEMINI_API_KEY
        )
        if provider == "Gemini" and usage:
            calculate_gemini_api_cost(usage, "article_summary_company", model="flash")
            return summary, provider
        if provider == "short_content":
            return None, "short_content"  # Don't try Claude - same check will fail
        LOG.warning(f"[{ticker}] Gemini company summary failed, falling back to Claude")

    # Fallback to Claude
    if ANTHROPIC_API_KEY:
        session = get_http_session()
        summary, provider, usage = await article_summaries.generate_claude_article_summary_company(
            company_name, ticker, title, scraped_content,
            ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_API_URL, session
        )
        if provider == "Sonnet" and usage:
            calculate_claude_api_cost(usage, "article_summary_company")
            return summary, provider

    return None, "api_failed"


async def generate_article_summary_competitor(competitor_name: str, competitor_ticker: str, target_company: str, target_ticker: str, title: str, scraped_content: str) -> Tuple[Optional[str], str]:
    """Generate article summary for competitor article"""
    if GEMINI_API_KEY:
        summary, provider, usage = await article_summaries.generate_gemini_article_summary_competitor(
            competitor_name, competitor_ticker, target_company, target_ticker,
            title, scraped_content, GEMINI_API_KEY
        )
        if provider == "Gemini" and usage:
            calculate_gemini_api_cost(usage, "article_summary_competitor", model="flash")
            return summary, provider
        if provider == "short_content":
            return None, "short_content"  # Don't try Claude - same check will fail
        LOG.warning(f"[{target_ticker}] Gemini competitor summary failed, falling back to Claude")

    if ANTHROPIC_API_KEY:
        session = get_http_session()
        summary, provider, usage = await article_summaries.generate_claude_article_summary_competitor(
            competitor_name, competitor_ticker, target_company, target_ticker,
            title, scraped_content, ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_API_URL, session
        )
        if provider == "Sonnet" and usage:
            calculate_claude_api_cost(usage, "article_summary_competitor")
            return summary, provider

    return None, "api_failed"


async def generate_article_summary_upstream(value_chain_company: str, value_chain_ticker: str, target_company: str, target_ticker: str, title: str, scraped_content: str) -> Tuple[Optional[str], str]:
    """Generate article summary for upstream supplier article"""
    if GEMINI_API_KEY:
        summary, provider, usage = await article_summaries.generate_gemini_article_summary_upstream(
            value_chain_company, value_chain_ticker, target_company, target_ticker,
            title, scraped_content, GEMINI_API_KEY
        )
        if provider == "Gemini" and usage:
            calculate_gemini_api_cost(usage, "article_summary_upstream", model="flash")
            return summary, provider
        if provider == "short_content":
            return None, "short_content"  # Don't try Claude - same check will fail
        LOG.warning(f"[{target_ticker}] Gemini upstream summary failed, falling back to Claude")

    if ANTHROPIC_API_KEY:
        session = get_http_session()
        summary, provider, usage = await article_summaries.generate_claude_article_summary_upstream(
            value_chain_company, value_chain_ticker, target_company, target_ticker,
            title, scraped_content, ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_API_URL, session
        )
        if provider == "Sonnet" and usage:
            calculate_claude_api_cost(usage, "article_summary_upstream")
            return summary, provider

    return None, "api_failed"


async def generate_article_summary_downstream(value_chain_company: str, value_chain_ticker: str, target_company: str, target_ticker: str, title: str, scraped_content: str) -> Tuple[Optional[str], str]:
    """Generate article summary for downstream customer article"""
    if GEMINI_API_KEY:
        summary, provider, usage = await article_summaries.generate_gemini_article_summary_downstream(
            value_chain_company, value_chain_ticker, target_company, target_ticker,
            title, scraped_content, GEMINI_API_KEY
        )
        if provider == "Gemini" and usage:
            calculate_gemini_api_cost(usage, "article_summary_downstream", model="flash")
            return summary, provider
        if provider == "short_content":
            return None, "short_content"  # Don't try Claude - same check will fail
        LOG.warning(f"[{target_ticker}] Gemini downstream summary failed, falling back to Claude")

    if ANTHROPIC_API_KEY:
        session = get_http_session()
        summary, provider, usage = await article_summaries.generate_claude_article_summary_downstream(
            value_chain_company, value_chain_ticker, target_company, target_ticker,
            title, scraped_content, ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_API_URL, session
        )
        if provider == "Sonnet" and usage:
            calculate_claude_api_cost(usage, "article_summary_downstream")
            return summary, provider

    return None, "api_failed"


async def generate_article_summary_industry(industry_keyword: str, target_company: str, target_ticker: str, title: str, scraped_content: str) -> Tuple[Optional[str], str]:
    """Generate article summary for industry/fundamental driver article"""
    # Get ticker config for geographic metadata
    config = get_ticker_config(target_ticker) or {}
    geographic_markets = (config.get('geographic_markets') or '').strip()

    if GEMINI_API_KEY:
        summary, provider, usage = await article_summaries.generate_gemini_article_summary_industry(
            industry_keyword, target_company, target_ticker,
            title, scraped_content, GEMINI_API_KEY, geographic_markets
        )
        if provider == "Gemini" and usage:
            calculate_gemini_api_cost(usage, "article_summary_industry", model="flash")
            return summary, provider
        if provider == "short_content":
            return None, "short_content"  # Don't try Claude - same check will fail
        LOG.warning(f"[{target_ticker}] Gemini industry summary failed, falling back to Claude")

    if ANTHROPIC_API_KEY:
        session = get_http_session()
        summary, provider, usage = await article_summaries.generate_claude_article_summary_industry(
            industry_keyword, target_company, target_ticker,
            title, scraped_content, ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_API_URL, session, geographic_markets
        )
        if provider == "Sonnet" and usage:
            calculate_claude_api_cost(usage, "article_summary_industry")
            return summary, provider

    return None, "api_failed"


async def score_industry_article_relevance(ticker: str, company_name: str, industry_keyword: str, title: str, scraped_content: str, threshold: float = 5.0) -> Dict:
    """Score industry article relevance using Gemini (primary) with Claude (fallback)

    Returns: {"score": float, "reason": str, "is_rejected": bool, "provider": str}
    """
    # Get ticker config for geographic metadata
    config = get_ticker_config(ticker) or {}
    geographic_markets = (config.get('geographic_markets') or '').strip()

    # Try Gemini first
    if GEMINI_API_KEY:
        result = await article_summaries.score_industry_relevance_gemini(
            ticker, company_name, industry_keyword,
            title, scraped_content, GEMINI_API_KEY, geographic_markets
        )
        if result:
            result["is_rejected"] = result["score"] <= threshold
            return result
        LOG.warning(f"[{ticker}] Gemini relevance gate failed, falling back to Claude")

    # Fallback to Claude
    if ANTHROPIC_API_KEY:
        session = get_http_session()
        result = await article_summaries.score_industry_relevance_claude(
            ticker, company_name, industry_keyword,
            title, scraped_content, ANTHROPIC_API_KEY, ANTHROPIC_MODEL, ANTHROPIC_API_URL, session, geographic_markets
        )
        if result:
            result["is_rejected"] = result["score"] <= threshold
            return result

    # If both failed
    return {"score": 0.0, "reason": "Both Gemini and Claude failed", "is_rejected": True, "provider": "none"}



async def route_article_summary_by_category(scraped_content: str, title: str, ticker: str, category: str,
                                             article_metadata: dict, target_company_name: str,
                                             competitor_name_cache: dict) -> Tuple[Optional[str], str]:
    """Route to category-specific summary function (Gemini primary, Claude fallback)

    Returns:
        Tuple[Optional[str], str]: (summary, provider) where provider is:
            - "Gemini": Generated by Gemini
            - "Sonnet": Generated by Claude Sonnet (fallback)
            - "short_content": Article too short (< 200 chars)
            - "api_failed": Both Gemini and Claude API calls failed
            - "metadata_missing": Missing feed_ticker or value_chain_type for competitor/value_chain
    """
    # Debug: Validate metadata before routing
    LOG.debug(f"[{ticker}] ðŸŽ¯ Routing {category} article: {title[:50]}...")
    LOG.debug(f"[{ticker}]    Metadata keys: {list(article_metadata.keys())}")

    if category == "company":
        return await generate_article_summary_company(target_company_name, ticker, title, scraped_content)
    elif category == "competitor":
        competitor_ticker = article_metadata.get("feed_ticker")
        if not competitor_ticker:
            return None, "metadata_missing"
        competitor_name = competitor_name_cache.get(competitor_ticker, competitor_ticker)
        return await generate_article_summary_competitor(competitor_name, competitor_ticker, target_company_name, ticker, title, scraped_content)
    elif category == "value_chain":
        value_chain_ticker = article_metadata.get("feed_ticker")  # Feed ticker (competitor, supplier, or customer)
        value_chain_type = article_metadata.get("value_chain_type")  # upstream or downstream
        if not value_chain_ticker or not value_chain_type:
            return None, "metadata_missing"
        value_chain_name = competitor_name_cache.get(value_chain_ticker, value_chain_ticker)
        # Call appropriate value chain function
        if value_chain_type == "upstream":
            return await generate_article_summary_upstream(value_chain_name, value_chain_ticker, target_company_name, ticker, title, scraped_content)
        elif value_chain_type == "downstream":
            return await generate_article_summary_downstream(value_chain_name, value_chain_ticker, target_company_name, ticker, title, scraped_content)
        else:
            return None, "metadata_missing"  # Invalid value_chain_type
    elif category == "industry":
        industry_keyword = article_metadata.get("search_keyword", "this industry")
        return await generate_article_summary_industry(industry_keyword, target_company_name, ticker, title, scraped_content)
    return None, "api_failed"  # Unknown category

async def generate_article_summary(scraped_content: str, title: str, ticker: str, description: str,
                                   category: str, article_metadata: dict, target_company_name: str,
                                   competitor_name_cache: dict) -> tuple[Optional[str], str]:
    """Main entry point: Generate article summary with Geminiâ†’Claude fallback

    Architecture: Routes by category â†’ Gemini primary â†’ Claude fallback

    Returns (summary, model_used) where model_used is:
        - "Gemini": Successfully generated by Gemini
        - "Sonnet": Successfully generated by Claude Sonnet
        - "filtered": Article intentionally skipped by AI (retail analysis)
        - "short_content": Article too short (< 200 chars)
        - "api_failed": Both Gemini and Claude API calls failed
        - "metadata_missing": Missing feed_ticker or value_chain_type
    """
    # Try AI summarization (if enabled and API key available)
    if USE_CLAUDE_FOR_SUMMARIES and ANTHROPIC_API_KEY:
        try:
            summary, status = await route_article_summary_by_category(
                scraped_content, title, ticker, category,
                article_metadata, target_company_name, competitor_name_cache
            )

            if status in ["Gemini", "Sonnet"]:
                # Gemini or Claude succeeded - return immediately
                LOG.info(f"[{ticker}] âœ… {status} generated summary successfully")
                return summary, status
            elif status == "filtered":
                # Article intentionally filtered by retail analysis
                LOG.info(f"[{ticker}] ðŸš« Article filtered (retail content)")
                return None, "filtered"
            elif status == "short_content":
                # Article too short for AI analysis
                content_len = len(scraped_content.strip()) if scraped_content else 0
                LOG.warning(f"[{ticker}] â­ï¸ Article too short for AI summary ({content_len} chars < 200 min)")
                LOG.warning(f"[{ticker}]    Title: {title[:80]}...")
                return None, "short_content"
            elif status == "metadata_missing":
                # Missing feed_ticker or value_chain_type for competitor/value_chain articles
                category_name = category.upper() if category else "UNKNOWN"
                LOG.error(f"[{ticker}] âŒ Metadata missing for {category_name} article")
                LOG.error(f"[{ticker}]    Title: {title[:80]}...")
                LOG.error(f"[{ticker}]    Missing required field: feed_ticker or value_chain_type")
                LOG.debug(f"[{ticker}]    Available metadata keys: {list(article_metadata.keys())}")
                return None, "metadata_missing"
            else:  # status == "api_failed" or unknown
                # Both AI providers failed
                category_name = category.upper() if category else "UNKNOWN"
                LOG.error(f"[{ticker}] âŒ AI summary failed for {category_name} article (both Gemini and Claude failed)")
                LOG.error(f"[{ticker}]    Title: {title[:80]}...")
                return None, "api_failed"
        except Exception as e:
            LOG.error(f"[{ticker}] âŒ Summary generation exception: {e}")
            return None, "api_failed"

    return None, "api_failed"

def perform_ai_triage_batch(articles_by_category: Dict[str, List[Dict]], ticker: str) -> Dict[str, List[Dict]]:
    """
    Perform AI triage on batched articles to identify scraping candidates
    FIXED: Process industry/competitor articles by keyword/competitor separately
    """
    if not OPENAI_API_KEY:
        LOG.warning("OpenAI API key not configured - skipping triage")
        return {"company": [], "industry": [], "competitor": [], "value_chain": []}
    
    selected_results = {"company": [], "industry": [], "competitor": [], "value_chain": []}

    # Get ticker metadata for enhanced prompts
    config = get_ticker_config(ticker)
    company_name = config.get("name", ticker) if config else ticker
    sector = config.get("sector", "") if config else ""

    # Build peers list for industry triage
    competitors = [(config.get(f"competitor_{i}_name"), config.get(f"competitor_{i}_ticker")) for i in range(1, 4) if config.get(f"competitor_{i}_name")] if config else []
    peers = []
    for comp_name, comp_ticker in competitors:
        if comp_name and comp_ticker:
            peers.append(f"{comp_name} ({comp_ticker})")
        elif comp_name:
            peers.append(comp_name)

    # Company articles - process as single batch (limit: 20)
    company_articles = articles_by_category.get("company", [])
    if company_articles:
        LOG.info(f"Starting AI triage for company: {len(company_articles)} articles")
        try:
            selected = triage_company_articles_full(company_articles, ticker, company_name, {}, {})
            selected_results["company"] = selected
            LOG.info(f"AI triage company: selected {len(selected)} articles for scraping")
        except Exception as e:
            LOG.error(f"Company triage failed: {e}")
    
    # Industry articles - FIXED: Process by keyword separately (5 per keyword)
    industry_articles = articles_by_category.get("industry", [])
    if industry_articles:
        # Group by search_keyword
        industry_by_keyword = {}
        for idx, article in enumerate(industry_articles):
            keyword = article.get("search_keyword", "unknown")
            if keyword not in industry_by_keyword:
                industry_by_keyword[keyword] = []
            industry_by_keyword[keyword].append({"article": article, "original_idx": idx})
        
        LOG.info(f"Starting AI triage for industry: {len(industry_articles)} articles across {len(industry_by_keyword)} keywords")
        
        all_industry_selected = []
        for keyword, keyword_articles in industry_by_keyword.items():
            try:
                LOG.info(f"Processing industry keyword '{keyword}': {len(keyword_articles)} articles")
                triage_articles = [item["article"] for item in keyword_articles]
                selected = triage_industry_articles_full(triage_articles, ticker, company_name, sector, peers)
                
                # Map back to original indices
                for selected_item in selected:
                    original_idx = keyword_articles[selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                    all_industry_selected.append(selected_item)
                
                LOG.info(f"Industry keyword '{keyword}': selected {len(selected)} articles")
            except Exception as e:
                LOG.error(f"Industry triage failed for keyword '{keyword}': {e}")
        
        selected_results["industry"] = all_industry_selected
        LOG.info(f"AI triage industry: selected {len(all_industry_selected)} articles total")
    
    # Competitor articles - FIXED: Process by competitor separately (5 per competitor)
    competitor_articles = articles_by_category.get("competitor", [])
    if competitor_articles:
        # Group by feed_ticker (primary) or search_keyword (fallback)
        competitor_by_entity = {}
        for idx, article in enumerate(competitor_articles):
            entity_key = article.get("feed_ticker") or article.get("search_keyword", "unknown")
            if entity_key not in competitor_by_entity:
                competitor_by_entity[entity_key] = []
            competitor_by_entity[entity_key].append({"article": article, "original_idx": idx})
        
        LOG.info(f"Starting AI triage for competitor: {len(competitor_articles)} articles across {len(competitor_by_entity)} competitors")
        
        all_competitor_selected = []
        for entity_key, entity_articles in competitor_by_entity.items():
            try:
                LOG.info(f"Processing competitor '{entity_key}': {len(entity_articles)} articles")
                triage_articles = [item["article"] for item in entity_articles]
                selected = triage_competitor_articles_full(triage_articles, ticker, [], {})
                
                # Map back to original indices
                for selected_item in selected:
                    original_idx = entity_articles[selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                    all_competitor_selected.append(selected_item)
                
                LOG.info(f"Competitor '{entity_key}': selected {len(selected)} articles")
            except Exception as e:
                LOG.error(f"Competitor triage failed for entity '{entity_key}': {e}")
        
        selected_results["competitor"] = all_competitor_selected
        LOG.info(f"AI triage competitor: selected {len(all_competitor_selected)} articles total")
    
    return selected_results

def rule_based_triage_score_company(title: str, domain: str) -> Tuple[int, str, str]:
    """
    Company-specific rule-based scoring based on AI triage logic
    Returns (score, reasoning, qb_level)
    """
    score = 0
    reasons = []
    
    title_lower = title.lower()
    
    # HIGH PRIORITY - Hard business events (60-80 points)
    high_events = [
        r'\b(acquires?|acquisition|merger|divests?|divestiture|spin-?off)\b',
        r'\b(bankruptcy|chapter 11|delist|delisting|halt|halted)\b',
        r'\b(guidance|preannounce|beats?|misses?|earnings|results|q[1-4])\b',
        r'\b(margin|backlog|contract|long-?term agreement|supply deal)\b',
        r'\b(price increase|price cut|capacity add|closure|curtailment)\b',
        r'\b(buyback|tender|equity offering|convertible|refinanc)\b',
        r'\b(rating change|approval|license)\b',
        r'\b(tariff|quota|sanction|fine|settlement)\b',
        r'\b(doj|ftc|sec|fda|usda|epa|osha|nhtsa|faa|fcc)\b'
    ]
    
    for pattern in high_events:
        if re.search(pattern, title_lower):
            score += 70
            reasons.append("hard business event")
            break
    
    # MEDIUM PRIORITY - Strategic developments (40-55 points)
    if score == 0:  # Only if no high-priority event found
        medium_events = [
            r'\b(investment|expansion)\b.*\$[\d,]+',  # Investment with $ amount
            r'\b(technology|product)\b.*\b(launch|deployment|ship)\b',
            r'\b(ceo|cfo|president|director)\b.*\b(change|resign|appoint|hire)\b',
            r'\b(partnership|joint venture|collaboration)\b',
            r'\b(facility|plant|factory)\b.*\b(opening|closing)\b'
        ]
        
        for pattern in medium_events:
            if re.search(pattern, title_lower):
                score += 50
                reasons.append("strategic development")
                break
    
    # LOW PRIORITY - Routine coverage (25-35 points)
    if score == 0:  # Only if no higher priority found
        low_events = [
            r'\b(analyst|rating|target|upgrade|downgrade)\b',
            r'\b(corporate announcement|operational)\b'
        ]
        
        for pattern in low_events:
            if re.search(pattern, title_lower):
                score += 30
                reasons.append("routine coverage")
                break
    
    # FINANCIAL SPECIFICITY BOOSTERS (+10-25 points)
    if re.search(r'\$[\d,]+\.?\d*\s*(million|billion|m|b)\b', title_lower):
        score += 20
        reasons.append("dollar amount")
        
    if re.search(r'\b\d+\.?\d*%\b', title):
        score += 15
        reasons.append("percentage")
    
    if re.search(r'\b(q[1-4]|\d{4}|\d+\s*(year|month|day)s?)\b', title_lower):
        score += 10
        reasons.append("timeline specificity")
    
    # EXCLUDE/PENALIZE PATTERNS (-30 to -50 points)
    exclude_patterns = [
        r'\b(top\s+\d+|best|should you|right now|reasons|prediction)\b',
        r'\b(if you.d invested|what to know|how to|why|analysis|outlook)\b',
        r'\b(announces)\b(?!.*\$)(?!.*\d+%)',  # "Announces" without concrete numbers
        r'\b(market size|cagr|forecast 20\d{2}|to reach \$.*by 20\d{2})\b',
        r'\b(market report|press release)\b(?!.*\$)(?!.*\d+%)'
    ]
    
    for pattern in exclude_patterns:
        if re.search(pattern, title_lower):
            score -= 40
            reasons.append("excluded pattern")
            break
    
    # DOMAIN BONUS
    domain_tier = DOMAIN_TIERS.get(normalize_domain(domain), 0.3)
    score += int(domain_tier * 25)
    reasons.append(f"domain tier {domain_tier}")
    
    final_score = max(0, min(100, score))
    
    # Determine QB level for company
    if final_score >= 70:
        qb_level = "QB: High"
    elif final_score >= 40:
        qb_level = "QB: Medium"
    else:
        qb_level = "QB: Low"
    
    return final_score, " + ".join(reasons), qb_level

def rule_based_triage_score_industry(title: str, domain: str, industry_keywords: List[str] = None) -> Tuple[int, str, str]:
    """
    Industry-specific rule-based scoring based on AI triage logic
    Returns (score, reasoning, qb_level)
    """
    score = 0
    reasons = []
    
    title_lower = title.lower()
    keywords = industry_keywords or []
    
    # HIGH PRIORITY - Policy/regulatory shocks with quantified impact (65-85 points)
    high_events = [
        r'\b(tariff|ban|quota|price control|regulatory change)\b.*\b\d+',
        r'\b(supply shock|inventory)\b.*\b(draw|build)\b.*\b\d+',
        r'\b(price cap|price floor|standard adopted)\b',
        r'\b(subsidy|credit|reimbursement change)\b.*\$',
        r'\b(safety requirement|environmental standard)\b.*\b(effective|deadline)\b',
        r'\b(trade agreement|export control|import restriction)\b'
    ]
    
    for pattern in high_events:
        if re.search(pattern, title_lower):
            score += 75
            reasons.append("policy/regulatory shock")
            break
    
    # MEDIUM PRIORITY - Sector developments (45-60 points)
    if score == 0:
        medium_events = [
            r'\b(infrastructure investment)\b.*\$[\d,]+',
            r'\b(industry consolidation)\b.*\$[\d,]+',
            r'\b(technology standards adoption)\b.*\b(implementation|schedule)\b',
            r'\b(labor agreement)\b.*\b(wage|benefit|cost)\b',
            r'\b(supply chain)\b.*\b(volume|pricing)\b',
            r'\b(capacity)\b.*\b(addition|reduction)\b.*\b(production|impact)\b'
        ]
        
        for pattern in medium_events:
            if re.search(pattern, title_lower):
                score += 52
                reasons.append("sector development")
                break
    
    # LOW PRIORITY - Broad trends (30-40 points)
    if score == 0:
        low_events = [
            r'\b(government initiative)\b.*\b(budget|implementation)\b',
            r'\b(economic indicator)\b.*\b(sector|industry)\b',
            r'\b(research finding)\b.*\b(quantified|impact)\b'
        ]
        
        for pattern in low_events:
            if re.search(pattern, title_lower):
                score += 35
                reasons.append("broad trend")
                break
    
    # INDUSTRY KEYWORD RELEVANCE BOOST (+15-25 points)
    if keywords:
        keyword_matches = 0
        for keyword in keywords:
            if keyword.lower() in title_lower:
                keyword_matches += 1
        
        if keyword_matches > 0:
            boost = min(keyword_matches * 8, 25)
            score += boost
            reasons.append(f"{keyword_matches} keyword matches")
    
    # SPECIFICITY BOOSTERS
    if re.search(r'\b\d+\.?\d*%\b.*\b(up|down|increase|decrease)\b', title_lower):
        score += 20
        reasons.append("percentage change")
        
    if re.search(r'\b(effective|deadline|implementation)\b.*\b20\d{2}\b', title_lower):
        score += 15
        reasons.append("implementation date")
    
    # EXCLUDE PATTERNS - Industry specific
    exclude_patterns = [
        r'\b(market size|cagr|forecast 20\d{2}|analysis report)\b',
        r'\b(industry outlook|future of|trends|sustainability)\b(?!.*\b(regulation|compliance)\b)',
        r'\b(academic research|consumer preference)\b(?!.*\b(policy|demand shift)\b)'
    ]
    
    for pattern in exclude_patterns:
        if re.search(pattern, title_lower):
            score -= 35
            reasons.append("excluded industry pattern")
            break
    
    # DOMAIN BONUS
    domain_tier = DOMAIN_TIERS.get(normalize_domain(domain), 0.3)
    score += int(domain_tier * 20)
    reasons.append(f"domain tier {domain_tier}")
    
    final_score = max(0, min(100, score))
    
    # Industry-specific QB levels (slightly different thresholds)
    if final_score >= 75:
        qb_level = "QB: High"
    elif final_score >= 45:
        qb_level = "QB: Medium"
    else:
        qb_level = "QB: Low"
    
    return final_score, " + ".join(reasons), qb_level

def rule_based_triage_score_competitor(title: str, domain: str, competitors: List[str] = None) -> Tuple[int, str, str]:
    """
    Competitor-specific rule-based scoring based on AI triage logic
    Returns (score, reasoning, qb_level)
    """
    score = 0
    reasons = []
    
    title_lower = title.lower()
    competitor_names = []
    
    # Extract competitor names from list
    if competitors:
        for comp in competitors:
            if isinstance(comp, dict):
                competitor_names.append(comp.get('name', '').lower())
            else:
                # Handle "Name (TICKER)" format
                name = comp.split('(')[0].strip().lower()
                competitor_names.append(name)
    
    # Check if any competitor is mentioned
    competitor_mentioned = False
    mentioned_competitor = ""
    for comp_name in competitor_names:
        if comp_name and comp_name in title_lower:
            competitor_mentioned = True
            mentioned_competitor = comp_name
            break
    
    if not competitor_mentioned:
        # If no competitor mentioned, default to low score
        score = 15
        reasons.append("no competitor match")
    else:
        # HIGH PRIORITY - Hard competitive events (65-85 points)
        high_events = [
            r'\b(capacity expansion|capacity reduction)\b.*\b\d+',
            r'\b(pricing action|price increase|price cut)\b.*\b\d+%',
            r'\b(customer win|customer loss|major customer)\b',
            r'\b(plant opening|plant closing)\b.*\b(output|capacity)\b',
            r'\b(asset sale|acquisition)\b.*\$[\d,]+',
            r'\b(restructuring|bankruptcy|chapter 11)\b',
            r'\b(breakthrough|launch)\b.*\b(ship date|deployment)\b',
            r'\b(supply agreement)\b.*\b(volume|capacity)\b',
            r'\b(market entry|market exit)\b.*\$[\d,]+'
        ]
        
        for pattern in high_events:
            if re.search(pattern, title_lower):
                score += 75
                reasons.append(f"hard competitive event ({mentioned_competitor})")
                break
        
        # MEDIUM PRIORITY - Strategic moves (45-60 points)
        if score == 0:
            medium_events = [
                r'\b(acquisition|partnership)\b.*\b(deal value|strategic)\b',
                r'\b(technology)\b.*\b(deployment|timeline)\b.*\b(competitive)\b',
                r'\b(ceo|cfo|division head)\b.*\b(change|succession)\b',
                r'\b(geographic expansion)\b.*\b(investment|market entry)\b',
                r'\b(regulatory approval)\b.*\b(timeline|capability)\b',
                r'\b(supply chain)\b.*\b(cost|availability|contract)\b'
            ]
            
            for pattern in medium_events:
                if re.search(pattern, title_lower):
                    score += 52
                    reasons.append(f"strategic move ({mentioned_competitor})")
                    break
        
        # LOW PRIORITY - Routine competitive intel (30-40 points)
        if score == 0:
            low_events = [
                r'\b(earnings)\b.*\b(guidance|beat|miss)\b',
                r'\b(analyst)\b.*\b(rating change|target)\b',
                r'\b(product announcement)\b.*\b(launch|timeline)\b'
            ]
            
            for pattern in low_events:
                if re.search(pattern, title_lower):
                    score += 35
                    reasons.append(f"routine intel ({mentioned_competitor})")
                    break
    
    # COMPETITIVE IMPACT BOOSTERS
    if re.search(r'\b(market share|losing ground|gaining share)\b', title_lower):
        score += 20
        reasons.append("market share impact")
        
    if re.search(r'\b(pricing power|cost structure|competitive advantage)\b', title_lower):
        score += 15
        reasons.append("competitive positioning")
    
    # FINANCIAL SPECIFICITY
    if re.search(r'\$[\d,]+\.?\d*\s*(million|billion)\b', title_lower):
        score += 18
        reasons.append("deal value")
        
    if re.search(r'\b\d+\.?\d*%\b.*\b(capacity|price|margin)\b', title_lower):
        score += 15
        reasons.append("quantified impact")
    
    # EXCLUDE PATTERNS - Competitor specific
    exclude_patterns = [
        r'\b(generic analyst commentary|stock performance)\b(?!.*\b(operational|guidance)\b)',
        r'\b(historical retrospective)\b(?!.*\b(competitive|forward)\b)',
        r'\b(technical analysis|trading analysis)\b',
        r'\b(market commentary)\b(?!.*\b(competitive|share)\b)'
    ]
    
    for pattern in exclude_patterns:
        if re.search(pattern, title_lower):
            score -= 30
            reasons.append("excluded competitor pattern")
            break
    
    # DOMAIN BONUS
    domain_tier = DOMAIN_TIERS.get(normalize_domain(domain), 0.3)
    score += int(domain_tier * 20)
    reasons.append(f"domain tier {domain_tier}")
    
    final_score = max(0, min(100, score))
    
    # Competitor-specific QB levels
    if final_score >= 70:
        qb_level = "QB: High"
    elif final_score >= 40:
        qb_level = "QB: Medium"
    else:
        qb_level = "QB: Low"
    
    return final_score, " + ".join(reasons), qb_level

def _apply_tiered_backfill_to_limits(articles: List[Dict], ai_selected: List[Dict], category: str, low_quality_domains: Set[str], target_limit: int) -> List[Dict]:
    """
    Apply backfill using AI â†’ Quality â†’ Category-specific QB scoring (100â†’0)
    AND store QB scores in database for ALL articles
    """
    # Step 1: Start with AI selections
    combined_selected = list(ai_selected)
    selected_indices = {item["id"] for item in ai_selected}
    
    # Step 2: Add quality domains (not already AI selected)
    quality_selected = []
    for idx, article in enumerate(articles):
        if idx in selected_indices:
            continue
            
        domain = normalize_domain(article.get("domain", ""))
        title = article.get("title", "").lower()
        
        if domain in low_quality_domains or is_insider_trading_article(title):
            continue
            
        if domain in QUALITY_DOMAINS:
            quality_selected.append({
                "id": idx,
                "scrape_priority": "MEDIUM",  # Updated to use HIGH/MEDIUM/LOW
                "likely_repeat": False,
                "repeat_key": "",
                "why": f"Quality domain: {domain}",
                "confidence": 0.8,
                "selection_method": "quality_domain"
            })
            selected_indices.add(idx)
    
    combined_selected.extend(quality_selected)
    
    # Step 3: Category-specific QB scoring and selection
    current_count = len(combined_selected)
    backfill_selected = []
    
    # Score ALL articles for QB analysis (regardless of selection)
    for idx, article in enumerate(articles):
        domain = normalize_domain(article.get("domain", ""))
        title = article.get("title", "")
        
        if not (domain in low_quality_domains or is_insider_trading_article(title.lower())):
            # Calculate QB score for every article
            if category == "company":
                qb_score, qb_reasoning, qb_level = rule_based_triage_score_company(title, domain)
            elif category == "industry":
                keywords = [article.get('search_keyword')] if article.get('search_keyword') else []
                qb_score, qb_reasoning, qb_level = rule_based_triage_score_industry(title, domain, keywords)
            elif category == "competitor":
                qb_score, qb_reasoning, qb_level = rule_based_triage_score_competitor(title, domain, [])
            else:
                qb_score, qb_reasoning, qb_level = rule_based_triage_score(title, domain)
            
            # Store QB scores in article object for database update
            article['qb_score'] = qb_score
            article['qb_level'] = qb_level
            article['qb_reasoning'] = qb_reasoning
    
    if current_count < target_limit:
        remaining_slots = target_limit - current_count
        
        # Score remaining articles for backfill selection
        scored_candidates = []
        for idx, article in enumerate(articles):
            if idx in selected_indices:
                continue
                
            domain = normalize_domain(article.get("domain", ""))
            title = article.get("title", "")
            
            if domain in low_quality_domains or is_insider_trading_article(title.lower()):
                continue
            
            # Use the QB scores we just calculated
            qb_score = article.get('qb_score', 0)
            qb_level = article.get('qb_level', 'QB: Low')
            qb_reasoning = article.get('qb_reasoning', '')
            
            scored_candidates.append({
                "id": idx,
                "article": article,
                "qb_score": qb_score,
                "qb_reasoning": qb_reasoning,
                "qb_level": qb_level,
                "domain": domain,
                "published_at": article.get("published_at")
            })
        
        # Sort by QB score descending (100 â†’ 0), then by publication time
        scored_candidates.sort(key=lambda x: (
            -x["qb_score"],
            -(x["published_at"].timestamp() if x["published_at"] else 0)
        ))
        
        # Take top candidates up to remaining slots
        for candidate in scored_candidates[:remaining_slots]:
            # Convert QB level to scrape priority
            if candidate['qb_score'] >= 70:
                scrape_priority = "HIGH"
            elif candidate['qb_score'] >= 40:
                scrape_priority = "MEDIUM"
            else:
                scrape_priority = "LOW"
                
            backfill_selected.append({
                "id": candidate["id"],
                "scrape_priority": scrape_priority,  # Now using HIGH/MEDIUM/LOW strings
                "likely_repeat": False,
                "repeat_key": "",
                "why": f"{candidate['qb_level']}: {candidate['qb_reasoning']} (score: {candidate['qb_score']})",
                "confidence": 0.6,
                "selection_method": f"qb_score_{category}",
                "qb_score": candidate["qb_score"],
                "qb_level": candidate["qb_level"]
            })
        
        combined_selected.extend(backfill_selected)
        
        # Final sort by priority (HIGH=1, MEDIUM=2, LOW=3)
        priority_map = {"HIGH": 1, "MEDIUM": 2, "LOW": 3}
        combined_selected.sort(key=lambda x: priority_map.get(x.get("scrape_priority", "LOW"), 3))
    
    # Enhanced logging
    ai_count = len(ai_selected)
    quality_count = len(quality_selected)
    qb_count = len(backfill_selected)
    
    LOG.info(f"Category-specific QB backfill {category}: {ai_count} AI + {quality_count} Quality + {qb_count} QB-{category} = {len(combined_selected)}/{target_limit}")
    
    return combined_selected

def perform_ai_triage_batch_with_enhanced_selection(articles_by_category: Dict[str, List[Dict]], ticker: str, target_limits: Dict[str, int] = None) -> Dict[str, List[Dict]]:
    """
    Enhanced triage with detailed logging for render logs
    """
    if not OPENAI_API_KEY:
        LOG.warning("OpenAI API key not configured - using existing triage data and quality domains only")
        return {"company": [], "industry": [], "competitor": [], "value_chain": []}
    
    selected_results = {"company": [], "industry": [], "competitor": [], "value_chain": []}

    # Get ticker metadata
    config = get_ticker_config(ticker)
    company_name = config.get("name", ticker) if config else ticker
    sector = config.get("sector", "") if config else ""

    # Build peers list for industry triage
    competitors = [(config.get(f"competitor_{i}_name"), config.get(f"competitor_{i}_ticker")) for i in range(1, 4) if config.get(f"competitor_{i}_name")] if config else []
    peers = []
    for comp_name, comp_ticker in competitors:
        if comp_name and comp_ticker:
            peers.append(f"{comp_name} ({comp_ticker})")
        elif comp_name:
            peers.append(comp_name)

    # DETAILED TRIAGE LOGGING FOR RENDER
    LOG.info("=== DETAILED TRIAGE BREAKDOWN ===")
    LOG.info(f"Ticker: {ticker} ({company_name})")

    total_ai_selected = 0
    total_quality_selected = 0
    total_qb_backfill = 0

    # COMPANY: Process with smart reuse and detailed logging
    company_articles = articles_by_category.get("company", [])
    if company_articles:
        LOG.info(f"COMPANY TRIAGE: Processing {len(company_articles)} articles")
        
        # Separate articles into those that need triage vs those already triaged
        needs_triage = []
        already_triaged = []
        
        for idx, article in enumerate(company_articles):
            # Check if article already has triage data and good quality
            if (article.get('ai_triage_selected') and 
                article.get('triage_priority') and 
                article.get('quality_score', 0) >= 40):
                
                already_triaged.append({
                    "id": idx,
                    "scrape_priority": article.get('triage_priority', 'MEDIUM'),
                    "likely_repeat": False,
                    "repeat_key": "",
                    "why": f"Previously triaged - {article.get('triage_reasoning', 'existing selection')}",
                    "confidence": 0.9,
                    "selection_method": "existing_triage"
                })
            else:
                needs_triage.append((idx, article))
        
        LOG.info(f"  COMPANY REUSE: {len(already_triaged)} already triaged")
        LOG.info(f"  COMPANY NEW: {len(needs_triage)} need new triage")
        
        # Start with existing triaged articles
        company_selected = list(already_triaged)
        
        # Run AI triage on new articles if needed (NO LIMIT - process all AI selections)
        if needs_triage:
            try:
                triage_articles = [article for _, article in needs_triage]
                new_selected = triage_company_articles_full(triage_articles, ticker, company_name, {}, {})

                # Map back to original indices and add to selection (NO CAP - take all AI selections)
                for selected_item in new_selected:
                    original_idx = needs_triage[selected_item["id"]][0]
                    selected_item["id"] = original_idx
                    selected_item["selection_method"] = "new_ai_triage"
                    company_selected.append(selected_item)

                LOG.info(f"  COMPANY AI: {len(new_selected)} selected from new AI triage (no cap applied)")
                total_ai_selected += len(new_selected)
            except Exception as e:
                LOG.error(f"Company triage failed: {e}")
        
        # Apply enhanced selection (Quality + QB backfill)
        quality_domains_count = 0
        qb_backfill_count = 0
        
        for idx, article in enumerate(company_articles):
            if idx not in [item["id"] for item in company_selected]:
                domain = normalize_domain(article.get("domain", ""))
                if domain in QUALITY_DOMAINS:
                    quality_domains_count += 1
                else:
                    # This would be QB backfill
                    qb_backfill_count += 1
        
        total_quality_selected += quality_domains_count
        total_qb_backfill += min(qb_backfill_count, max(0, 20 - len(company_selected) - quality_domains_count))
        
        LOG.info(f"  COMPANY QUALITY: {quality_domains_count} quality domains")
        LOG.info(f"  COMPANY QB BACKFILL: {total_qb_backfill} QB score selections")
        LOG.info(f"  COMPANY TOTAL: {len(company_selected)} selected for scraping")
        
        selected_results["company"] = company_selected
    
    # INDUSTRY: Process EACH keyword separately with detailed logging
    industry_articles = articles_by_category.get("industry", [])
    if industry_articles:
        # Group by search_keyword
        industry_by_keyword = {}
        for idx, article in enumerate(industry_articles):
            keyword = article.get("search_keyword", "unknown")
            if keyword not in industry_by_keyword:
                industry_by_keyword[keyword] = []
            industry_by_keyword[keyword].append({"article": article, "original_idx": idx})
        
        LOG.info(f"INDUSTRY TRIAGE: Processing {len(industry_by_keyword)} keywords")
        
        industry_selected = []
        for keyword, keyword_articles in industry_by_keyword.items():
            if len(keyword_articles) == 0:
                continue
                
            LOG.info(f"  KEYWORD '{keyword}': {len(keyword_articles)} articles")
            
            # Separate into existing vs new for this keyword
            needs_triage = []
            already_triaged = []
            
            for item in keyword_articles:
                article = item["article"]
                original_idx = item["original_idx"]
                
                if (article.get('ai_triage_selected') and 
                    article.get('triage_priority') and 
                    article.get('quality_score', 0) >= 40):
                    
                    already_triaged.append({
                        "id": original_idx,
                        "scrape_priority": article.get('triage_priority', 'MEDIUM'),
                        "likely_repeat": False,
                        "repeat_key": "",
                        "why": f"Previously triaged - {article.get('triage_reasoning', 'existing selection')}",
                        "confidence": 0.9,
                        "selection_method": "existing_triage"
                    })
                else:
                    needs_triage.append(item)
            
            # Start with existing
            keyword_selected = list(already_triaged)

            # Run AI triage on new articles if needed (NO LIMIT - process all AI selections)
            if needs_triage:
                try:
                    triage_articles = [item["article"] for item in needs_triage]
                    new_selected = triage_industry_articles_full(triage_articles, ticker, company_name, sector, peers)

                    # Map back to original indices (NO CAP - take all AI selections)
                    for selected_item in new_selected:
                        original_idx = needs_triage[selected_item["id"]]["original_idx"]
                        selected_item["id"] = original_idx
                        selected_item["selection_method"] = "new_ai_triage"
                        keyword_selected.append(selected_item)
                    
                    LOG.info(f"    AI TRIAGE: {len(new_selected)} selected")
                    total_ai_selected += len(new_selected)
                except Exception as e:
                    LOG.error(f"Industry triage failed for keyword '{keyword}': {e}")
            
            industry_selected.extend(keyword_selected)
            LOG.info(f"    TOTAL: {len(keyword_selected)}/8 selected for '{keyword}'")
        
        selected_results["industry"] = industry_selected
        LOG.info(f"INDUSTRY TOTAL: {len(industry_selected)} articles selected across all keywords")
    
    # COMPETITOR: Process EACH competitor separately with detailed logging
    competitor_articles = articles_by_category.get("competitor", [])
    if competitor_articles:
        # Group by feed_ticker (primary) or search_keyword (fallback)
        competitor_by_entity = {}
        for idx, article in enumerate(competitor_articles):
            entity_key = article.get("feed_ticker") or article.get("search_keyword", "unknown")
            if entity_key not in competitor_by_entity:
                competitor_by_entity[entity_key] = []
            competitor_by_entity[entity_key].append({"article": article, "original_idx": idx})
        
        LOG.info(f"COMPETITOR TRIAGE: Processing {len(competitor_by_entity)} competitors")
        
        competitor_selected = []
        for entity_key, entity_articles in competitor_by_entity.items():
            if len(entity_articles) == 0:
                continue
                
            LOG.info(f"  COMPETITOR '{entity_key}': {len(entity_articles)} articles")
            
            # Separate into existing vs new for this competitor
            needs_triage = []
            already_triaged = []
            
            for item in entity_articles:
                article = item["article"]
                original_idx = item["original_idx"]
                
                if (article.get('ai_triage_selected') and 
                    article.get('triage_priority') and 
                    article.get('quality_score', 0) >= 40):
                    
                    already_triaged.append({
                        "id": original_idx,
                        "scrape_priority": article.get('triage_priority', 'MEDIUM'),
                        "likely_repeat": False,
                        "repeat_key": "",
                        "why": f"Previously triaged - {article.get('triage_reasoning', 'existing selection')}",
                        "confidence": 0.9,
                        "selection_method": "existing_triage"
                    })
                else:
                    needs_triage.append(item)
            
            # Start with existing
            entity_selected = list(already_triaged)

            # Run AI triage on new articles if needed (NO LIMIT - process all AI selections)
            if needs_triage:
                try:
                    triage_articles = [item["article"] for item in needs_triage]
                    new_selected = triage_competitor_articles_full(triage_articles, ticker, [], {})

                    # Map back to original indices (NO CAP - take all AI selections)
                    for selected_item in new_selected:
                        original_idx = needs_triage[selected_item["id"]]["original_idx"]
                        selected_item["id"] = original_idx
                        selected_item["selection_method"] = "new_ai_triage"
                        entity_selected.append(selected_item)

                    LOG.info(f"    AI TRIAGE: {len(new_selected)} selected (no cap applied)")
                    total_ai_selected += len(new_selected)
                except Exception as e:
                    LOG.error(f"Competitor triage failed for entity '{entity_key}': {e}")
            
            competitor_selected.extend(entity_selected)
            LOG.info(f"    TOTAL: {len(entity_selected)}/5 selected for '{entity_key}'")
        
        selected_results["competitor"] = competitor_selected
        LOG.info(f"COMPETITOR TOTAL: {len(competitor_selected)} articles selected across all competitors")
    
    # FINAL TRIAGE SUMMARY FOR RENDER LOGS
    total_selected = sum(len(items) for items in selected_results.values())
    LOG.info("=== TRIAGE SUMMARY ===")
    LOG.info(f"AI TRIAGE: {total_ai_selected} articles")
    LOG.info(f"QUALITY DOMAINS: {total_quality_selected} articles")
    LOG.info(f"QB BACKFILL: {total_qb_backfill} articles")
    LOG.info(f"TOTAL SELECTED: {total_selected} articles")
    LOG.info("=== END TRIAGE ===")
    
    return selected_results

def _safe_json_loads(s: str) -> Dict:
    """
    Robust JSON parsing:
      - strip code fences if any
      - find outermost JSON object
      - fallback to {} on failure
    """
    s = s.strip()
    
    # Strip ```json ... ``` fences if present
    if s.startswith("```"):
        s = s.strip("`")
        # after stripping backticks, content may start with 'json\n'
        if s.lower().startswith("json"):
            s = s[4:].lstrip()
    
    # Quick path - try to parse as-is
    try:
        return json.loads(s)
    except Exception:
        pass
    
    # Fallback: slice from first '{' to last '}' (common when trailing notes appear)
    try:
        start = s.find("{")
        end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            return json.loads(s[start:end + 1])
    except Exception:
        pass
    
    LOG.error("Failed to parse JSON. First 200 chars:\n" + s[:200])
    return {"selected_ids": [], "selected": [], "skipped": []}

async def merge_triage_scores(
    openai_results: List[Dict],
    claude_results: List[Dict],
    articles: List[Dict],
    target_cap: int,
    category_type: str,
    category_key: str
) -> List[Dict]:
    """
    Merge OpenAI and Claude triage results by combined score.
    Returns top N articles by combined score (openai_score + claude_score).
    Handles fallback if one API fails (use single API's selections).
    """
    # Build URL-based lookup for matching articles across APIs
    url_scores = {}  # url -> {"openai": score, "claude": score, "article": article_obj}

    # Process OpenAI results
    for result in openai_results:
        article_id = result["id"]
        if article_id < len(articles):
            article = articles[article_id]
            url = article.get("url", "")
            if url:
                if url not in url_scores:
                    url_scores[url] = {"openai": 0, "claude": 0, "article": article, "id": article_id}
                # Score: 1=high, 2=medium, 3=low -> convert to reverse (high=3, low=1)
                priority = result.get("scrape_priority", 2)
                url_scores[url]["openai"] = 4 - priority  # 3, 2, 1
                url_scores[url]["why_openai"] = result.get("why", "")

    # Process Claude results
    for result in claude_results:
        article_id = result["id"]
        if article_id < len(articles):
            article = articles[article_id]
            url = article.get("url", "")
            if url:
                if url not in url_scores:
                    url_scores[url] = {"openai": 0, "claude": 0, "article": article, "id": article_id}
                priority = result.get("scrape_priority", 2)
                url_scores[url]["claude"] = 4 - priority
                url_scores[url]["why_claude"] = result.get("why", "")

    # Track stats for logging
    total_unique_before_filter = len(url_scores)

    # Filter out problematic domains BEFORE ranking
    filtered_scores = {}
    blocked_count = 0
    for url, data in url_scores.items():
        article = data["article"]
        domain = article.get("domain", "")
        if domain in PROBLEMATIC_SCRAPE_DOMAINS:
            blocked_count += 1
            LOG.debug(f"Blocked {domain} from triage selection (problematic domain)")
            continue
        filtered_scores[url] = data

    # Calculate combined scores and sort
    scored_articles = []
    for url, data in filtered_scores.items():
        combined_score = data["openai"] + data["claude"]
        if combined_score > 0:  # At least one API selected it
            scored_articles.append({
                "url": url,
                "article": data["article"],
                "id": data["id"],
                "openai_score": data["openai"],
                "claude_score": data["claude"],
                "combined_score": combined_score,
                "why_openai": data.get("why_openai", ""),
                "why_claude": data.get("why_claude", "")
            })

    # Sort by combined score (descending), then by timestamp (recent first)
    scored_articles.sort(key=lambda x: (
        -x["combined_score"],
        -x["article"].get("published_at", datetime.min).timestamp() if x["article"].get("published_at") else 0
    ))

    # Take top N
    top_articles = scored_articles[:target_cap]

    # Enhanced logging with domain filter transparency
    if blocked_count > 0:
        LOG.info(f"  Dual scoring {category_type}/{category_key}: {len(openai_results)} OpenAI + {len(claude_results)} Claude = {total_unique_before_filter} unique ({blocked_count} blocked by domain filter) â†’ {len(scored_articles)} remaining â†’ top {len(top_articles)}")
    else:
        LOG.info(f"  Dual scoring {category_type}/{category_key}: {len(openai_results)} OpenAI + {len(claude_results)} Claude = {len(scored_articles)} unique â†’ top {len(top_articles)}")

    # Return in format expected by downstream code
    result = []
    for item in top_articles:
        result.append({
            "id": item["id"],
            "scrape_priority": 1 if item["combined_score"] >= 5 else (2 if item["combined_score"] >= 3 else 3),
            "why": f"OpenAI: {item['why_openai'][:50]}... Claude: {item['why_claude'][:50]}..." if item['why_openai'] and item['why_claude'] else (item['why_openai'] or item['why_claude']),
            "confidence": 0.9 if (item["openai_score"] > 0 and item["claude_score"] > 0) else 0.7,
            "likely_repeat": False,
            "repeat_key": "",
            "openai_score": item["openai_score"],
            "claude_score": item["claude_score"],
            "combined_score": item["combined_score"]
        })

    return result

async def perform_ai_triage_with_fallback_async(
    articles_by_category: Dict[str, List[Dict]],
    ticker: str,
    triage_batch_size: int = 5  # Updated default to 5
) -> Dict[str, List[Dict]]:
    """
    Dual AI triage with Claude + Gemini: Both run in parallel, results merged.
    Smart fallback: Use Gemini if Claude fails, Claude if Gemini fails.
    Replaces old sequential Claudeâ†’OpenAI fallback with parallel dual scoring.
    """
    selected_results = {"company": [], "industry": [], "competitor": [], "value_chain": []}

    if not ANTHROPIC_API_KEY and not GEMINI_API_KEY:
        LOG.warning("No AI API keys configured - skipping triage")
        return selected_results

    # Get ticker metadata
    config = get_ticker_config(ticker)
    company_name = config.get("name", ticker) if config else ticker
    sector = config.get("sector", "") if config else ""
    industry_keywords = [config.get(f"industry_keyword_{i}") for i in range(1, 4) if config.get(f"industry_keyword_{i}")]
    competitors = [(config.get(f"competitor_{i}_name"), config.get(f"competitor_{i}_ticker")) for i in range(1, 4) if config.get(f"competitor_{i}_name")]

    # Get value chain companies (upstream and downstream)
    value_chain_companies = []
    # Upstream companies
    for i in range(1, 3):  # upstream_1, upstream_2
        vc_name = config.get(f"upstream_{i}_name") if config else None
        vc_ticker = config.get(f"upstream_{i}_ticker") if config else None
        if vc_name:
            value_chain_companies.append((vc_name, vc_ticker, "upstream"))
    # Downstream companies
    for i in range(1, 3):  # downstream_1, downstream_2
        vc_name = config.get(f"downstream_{i}_name") if config else None
        vc_ticker = config.get(f"downstream_{i}_ticker") if config else None
        if vc_name:
            value_chain_companies.append((vc_name, vc_ticker, "downstream"))

    # Build peers list for industry triage
    peers = []
    for comp_name, comp_ticker in competitors:
        if comp_name and comp_ticker:
            peers.append(f"{comp_name} ({comp_ticker})")
        elif comp_name:
            peers.append(comp_name)

    # Get geographic markets for industry triage
    geographic_markets = (config.get('geographic_markets') or '').strip() if config else ''

    LOG.info(f"=== DUAL AI TRIAGE (Claude + Gemini parallel with smart fallback): batch_size={triage_batch_size} ===")

    # Collect ALL triage operations
    all_triage_operations = []

    # Company operations
    company_articles = articles_by_category.get("company", [])
    if company_articles:
        all_triage_operations.append({
            "type": "company",
            "key": "company",
            "articles": company_articles,
            "target_cap": min(20, len(company_articles)),
            "dual_func": triage_company_articles_dual,
            "dual_args": (company_articles, ticker, company_name, ANTHROPIC_API_KEY, GEMINI_API_KEY, PROBLEMATIC_SCRAPE_DOMAINS)
        })

    # Industry operations (one per keyword)
    industry_articles = articles_by_category.get("industry", [])
    if industry_articles:
        industry_by_keyword = {}
        for idx, article in enumerate(industry_articles):
            keyword = article.get("search_keyword", "unknown")
            if keyword not in industry_by_keyword:
                industry_by_keyword[keyword] = []
            industry_by_keyword[keyword].append({"article": article, "original_idx": idx})

        for keyword, keyword_articles in industry_by_keyword.items():
            triage_articles = [item["article"] for item in keyword_articles]
            all_triage_operations.append({
                "type": "industry",
                "key": keyword,
                "articles": triage_articles,
                "target_cap": min(8, len(triage_articles)),
                "dual_func": triage_industry_articles_dual,
                "dual_args": (triage_articles, ticker, company_name, sector, peers, industry_keywords, geographic_markets, ANTHROPIC_API_KEY, GEMINI_API_KEY, PROBLEMATIC_SCRAPE_DOMAINS),
                "index_mapping": keyword_articles
            })

    # Competitor operations (one per competitor)
    competitor_articles = articles_by_category.get("competitor", [])
    if competitor_articles:
        competitor_by_entity = {}
        for idx, article in enumerate(competitor_articles):
            entity_key = article.get("feed_ticker") or article.get("search_keyword", "unknown")
            if entity_key not in competitor_by_entity:
                competitor_by_entity[entity_key] = []
            competitor_by_entity[entity_key].append({"article": article, "original_idx": idx})

        for entity_key, entity_articles in competitor_by_entity.items():
            triage_articles = [item["article"] for item in entity_articles]
            # Get competitor name and ticker from article metadata
            competitor_ticker = entity_articles[0]["article"].get("feed_ticker", entity_key)
            competitor_name = entity_articles[0]["article"].get("search_keyword", entity_key)
            all_triage_operations.append({
                "type": "competitor",
                "key": entity_key,
                "articles": triage_articles,
                "target_cap": min(5, len(triage_articles)),
                "dual_func": triage_competitor_articles_dual,
                "dual_args": (triage_articles, ticker, competitor_name, competitor_ticker, ANTHROPIC_API_KEY, GEMINI_API_KEY, PROBLEMATIC_SCRAPE_DOMAINS),
                "index_mapping": entity_articles
            })

    # Value chain operations (one per value chain company)
    value_chain_articles = articles_by_category.get("value_chain", [])
    if value_chain_articles:
        # Group by value chain company (using feed_ticker field which stores vc ticker)
        value_chain_by_entity = {}
        for idx, article in enumerate(value_chain_articles):
            entity_key = article.get("feed_ticker") or article.get("search_keyword", "unknown")
            if entity_key not in value_chain_by_entity:
                value_chain_by_entity[entity_key] = []
            value_chain_by_entity[entity_key].append({"article": article, "original_idx": idx})

        for entity_key, entity_articles in value_chain_by_entity.items():
            triage_articles = [item["article"] for item in entity_articles]
            # Get value chain company name, ticker, and type from article metadata
            vc_ticker = entity_articles[0]["article"].get("feed_ticker", entity_key)
            vc_name = entity_articles[0]["article"].get("search_keyword", entity_key)
            # Get value_chain_type from first article (upstream or downstream)
            vc_type = None
            for article in triage_articles:
                if article.get("value_chain_type"):
                    vc_type = article.get("value_chain_type")
                    break

            if vc_type:  # Only add if we have value_chain_type
                # Choose appropriate dual function based on value_chain_type
                if vc_type == "upstream":
                    dual_func = triage_upstream_articles_dual
                    dual_args = (triage_articles, ticker, company_name, vc_name, vc_ticker, ANTHROPIC_API_KEY, GEMINI_API_KEY, PROBLEMATIC_SCRAPE_DOMAINS)
                elif vc_type == "downstream":
                    dual_func = triage_downstream_articles_dual
                    dual_args = (triage_articles, ticker, company_name, vc_name, vc_ticker, ANTHROPIC_API_KEY, GEMINI_API_KEY, PROBLEMATIC_SCRAPE_DOMAINS)
                else:
                    LOG.warning(f"Unknown value_chain_type: {vc_type}, skipping")
                    continue

                all_triage_operations.append({
                    "type": "value_chain",
                    "key": entity_key,
                    "articles": triage_articles,
                    "target_cap": min(5, len(triage_articles)),
                    "dual_func": dual_func,
                    "dual_args": dual_args,
                    "index_mapping": entity_articles,
                    "value_chain_type": vc_type
                })

    total_operations = len(all_triage_operations)
    LOG.info(f"Total triage operations: {total_operations}")

    if total_operations == 0:
        return selected_results

    # Track API usage statistics
    dual_success_count = 0
    claude_only_count = 0
    gemini_fallback_count = 0
    both_failed_count = 0

    # Process operations in batches
    for batch_start in range(0, total_operations, triage_batch_size):
        batch_end = min(batch_start + triage_batch_size, total_operations)
        batch = all_triage_operations[batch_start:batch_end]
        batch_num = (batch_start // triage_batch_size) + 1
        total_batches = (total_operations + triage_batch_size - 1) // triage_batch_size

        LOG.info(f"BATCH {batch_num}/{total_batches}: Processing {len(batch)} operations (Claude + Gemini dual):")
        for op in batch:
            LOG.info(f"  - {op['type']}: {op['key']} ({len(op['articles'])} articles)")

        # Create dual AI tasks for each operation
        batch_tasks = []
        for op in batch:
            task = op["dual_func"](*op["dual_args"])
            batch_tasks.append((op, task))

        # Execute batch concurrently
        batch_results = await asyncio.gather(*[task for _, task in batch_tasks], return_exceptions=True)

        # Process results and track API usage
        for i, result_tuple in enumerate(batch_results):
            op = batch_tasks[i][0]

            if isinstance(result_tuple, Exception):
                LOG.error(f"Triage failed for {op['type']}/{op['key']}: {result_tuple}")
                both_failed_count += 1
                continue

            # Unpack result: (results, provider_name, usage_metadata)
            result, provider_name, usage_metadata = result_tuple

            # Track API usage counts
            if provider_name == "dual":
                dual_success_count += 1
            elif provider_name == "claude_only":
                claude_only_count += 1
            elif provider_name == "gemini_fallback":
                gemini_fallback_count += 1
            else:
                both_failed_count += 1

            # Track triage costs for both providers
            if usage_metadata:
                # Determine triage type for function name
                triage_type = op['type']
                if triage_type == 'value_chain':
                    # Use upstream/downstream based on value_chain_type
                    triage_type = op.get('value_chain_type', 'upstream')

                # Track Claude costs
                if 'claude' in usage_metadata and usage_metadata['claude']:
                    calculate_claude_api_cost(
                        usage_metadata['claude'],
                        f"triage_{triage_type}_claude"
                    )

                # Track Gemini costs
                if 'gemini' in usage_metadata and usage_metadata['gemini']:
                    calculate_gemini_api_cost(
                        usage_metadata['gemini'],
                        f"triage_{triage_type}_gemini",
                        model="flash"
                    )

            # Score fields are already set by the merge function in the module
            # (claude_score and gemini_score are in the result items)

            # Map results back to original indices and add to selected_results
            if op["type"] == "company":
                selected_results["company"].extend(result)

            elif op["type"] == "industry":
                # Map back to original indices
                for selected_item in result:
                    original_idx = op["index_mapping"][selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                selected_results["industry"].extend(result)

            elif op["type"] == "competitor":
                # Map back to original indices
                for selected_item in result:
                    original_idx = op["index_mapping"][selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                selected_results["competitor"].extend(result)

            elif op["type"] == "value_chain":
                # Map back to original indices
                for selected_item in result:
                    original_idx = op["index_mapping"][selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                selected_results["value_chain"].extend(result)

        LOG.info(f"BATCH {batch_num} COMPLETE")

    LOG.info(f"DUAL AI TRIAGE COMPLETE:")
    LOG.info(f"  Company: {len(selected_results['company'])} selected")
    LOG.info(f"  Industry: {len(selected_results['industry'])} selected")
    LOG.info(f"  Competitor: {len(selected_results['competitor'])} selected")
    LOG.info(f"  Value Chain: {len(selected_results['value_chain'])} selected")
    LOG.info(f"  API Usage: Dual (both): {dual_success_count}/{total_operations} | Claude only: {claude_only_count}/{total_operations} | Gemini fallback: {gemini_fallback_count}/{total_operations} | Both failed: {both_failed_count}/{total_operations}")

    return selected_results

# ===== END FALLBACK TRIAGE LOGIC =====

async def perform_ai_triage_with_batching_async(
    articles_by_category: Dict[str, List[Dict]],
    ticker: str,
    triage_batch_size: int = 2
) -> Dict[str, List[Dict]]:
    """
    Perform AI triage with true cross-category async batching
    """
    if not OPENAI_API_KEY:
        LOG.warning("OpenAI API key not configured - skipping triage")
        return {"company": [], "industry": [], "competitor": [], "value_chain": []}

    selected_results = {"company": [], "industry": [], "competitor": [], "value_chain": []}

    # Get ticker metadata for enhanced prompts
    config = get_ticker_config(ticker)
    company_name = config.get("name", ticker) if config else ticker
    sector = config.get("sector", "") if config else ""

    # Build peers list for industry triage
    competitors = [(config.get(f"competitor_{i}_name"), config.get(f"competitor_{i}_ticker")) for i in range(1, 4) if config.get(f"competitor_{i}_name")] if config else []
    peers = []
    for comp_name, comp_ticker in competitors:
        if comp_name and comp_ticker:
            peers.append(f"{comp_name} ({comp_ticker})")
        elif comp_name:
            peers.append(comp_name)

    LOG.info(f"=== TRUE CROSS-CATEGORY ASYNC TRIAGE: batch_size={triage_batch_size} ===")
    
    # Collect ALL triage operations into a single list
    all_triage_operations = []
    
    # Add company operation
    company_articles = articles_by_category.get("company", [])
    if company_articles:
        all_triage_operations.append({
            "type": "company",
            "key": "company",
            "articles": company_articles,
            "task_func": triage_company_articles_full,
            "args": (company_articles, ticker, company_name, {}, {})
        })
    
    # Add industry operations (one per keyword)
    industry_articles = articles_by_category.get("industry", [])
    if industry_articles:
        # Group by search_keyword
        industry_by_keyword = {}
        for idx, article in enumerate(industry_articles):
            keyword = article.get("search_keyword", "unknown")
            if keyword not in industry_by_keyword:
                industry_by_keyword[keyword] = []
            industry_by_keyword[keyword].append({"article": article, "original_idx": idx})
        
        for keyword, keyword_articles in industry_by_keyword.items():
            triage_articles = [item["article"] for item in keyword_articles]
            all_triage_operations.append({
                "type": "industry",
                "key": keyword,
                "articles": triage_articles,
                "task_func": triage_industry_articles_full,
                "args": (triage_articles, ticker, company_name, sector, peers),
                "index_mapping": keyword_articles  # For mapping back to original indices
            })
    
    # Add competitor operations (one per competitor)
    competitor_articles = articles_by_category.get("competitor", [])
    if competitor_articles:
        # Group by feed_ticker (primary) or search_keyword (fallback)
        competitor_by_entity = {}
        for idx, article in enumerate(competitor_articles):
            entity_key = article.get("feed_ticker") or article.get("search_keyword", "unknown")
            if entity_key not in competitor_by_entity:
                competitor_by_entity[entity_key] = []
            competitor_by_entity[entity_key].append({"article": article, "original_idx": idx})
        
        for entity_key, entity_articles in competitor_by_entity.items():
            triage_articles = [item["article"] for item in entity_articles]
            all_triage_operations.append({
                "type": "competitor",
                "key": entity_key,
                "articles": triage_articles,
                "task_func": triage_competitor_articles_full,
                "args": (triage_articles, ticker, [], {}),
                "index_mapping": entity_articles  # For mapping back to original indices
            })
    
    total_operations = len(all_triage_operations)
    LOG.info(f"Total triage operations to process: {total_operations}")
    
    if total_operations == 0:
        return selected_results
    
    # Process operations in cross-category batches
    for batch_start in range(0, total_operations, triage_batch_size):
        batch_end = min(batch_start + triage_batch_size, total_operations)
        batch = all_triage_operations[batch_start:batch_end]
        batch_num = (batch_start // triage_batch_size) + 1
        total_batches = (total_operations + triage_batch_size - 1) // triage_batch_size
        
        LOG.info(f"BATCH {batch_num}/{total_batches}: Processing {len(batch)} operations concurrently:")
        for op in batch:
            LOG.info(f"  - {op['type']}: {op['key']} ({len(op['articles'])} articles)")
        
        # Create tasks for this batch with semaphore control
        batch_tasks = []
        for op in batch:
            async def run_with_semaphore(operation):
                # SEMAPHORE DISABLED: Prevents threading deadlock with concurrent tickers
                # with TRIAGE_SEM:
                if True:  # Maintain indentation
                    return await operation["task_func"](*operation["args"])

            task = run_with_semaphore(op)
            batch_tasks.append((op, task))
        
        # Execute batch concurrently
        batch_results = await asyncio.gather(*[task for _, task in batch_tasks], return_exceptions=True)
        
        # Process results
        for i, result in enumerate(batch_results):
            op = batch_tasks[i][0]
            
            if isinstance(result, Exception):
                LOG.error(f"Triage failed for {op['type']}/{op['key']}: {result}")
                continue
            
            # Map results back to original indices and add to selected_results
            if op["type"] == "company":
                selected_results["company"].extend(result)
                LOG.info(f"  âœ“ Company: selected {len(result)} articles")
            
            elif op["type"] == "industry":
                # Map back to original indices
                for selected_item in result:
                    original_idx = op["index_mapping"][selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                selected_results["industry"].extend(result)
                LOG.info(f"  âœ“ Industry '{op['key']}': selected {len(result)} articles")
            
            elif op["type"] == "competitor":
                # Map back to original indices
                for selected_item in result:
                    original_idx = op["index_mapping"][selected_item["id"]]["original_idx"]
                    selected_item["id"] = original_idx
                selected_results["competitor"].extend(result)
                LOG.info(f"  âœ“ Competitor '{op['key']}': selected {len(result)} articles")
        
        LOG.info(f"BATCH {batch_num} COMPLETE")
    
    LOG.info(f"CROSS-CATEGORY TRIAGE COMPLETE:")
    LOG.info(f"  Company: {len(selected_results['company'])} selected")
    LOG.info(f"  Industry: {len(selected_results['industry'])} selected") 
    LOG.info(f"  Competitor: {len(selected_results['competitor'])} selected")
    
    return selected_results

class TriageItem(BaseModel):
    id: int
    scrape_priority: int
    likely_repeat: bool
    repeat_key: str
    why: str
    confidence: float

class TriageResponse(BaseModel):
    selected_ids: List[int]
    selected: List[TriageItem]
    skipped: List[TriageItem]

def get_competitor_display_name(search_keyword: str, competitor_ticker: str = None) -> str:
    """
    Get standardized competitor display name using database lookup with proper fallback
    Priority: ticker_reference.company_name -> search_keyword -> fallback

    ALWAYS uses competitor_ticker to lookup company_name in ticker_reference table.
    Never relies on search_keyword for display (search_keyword is feed query parameter, not display name).
    """

    # Input validation (normalize only - no format validation to allow international tickers)
    if competitor_ticker:
        competitor_ticker = normalize_ticker_format(competitor_ticker)

    if search_keyword:
        search_keyword = search_keyword.strip()
        if len(search_keyword) > 100 or not search_keyword:
            LOG.warning(f"Invalid search keyword: {search_keyword}")
            search_keyword = None

    # STEP 1: Try ticker_reference table first (most comprehensive - 6,178 tickers)
    if competitor_ticker:
        try:
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    SELECT company_name FROM ticker_reference
                    WHERE ticker = %s
                    LIMIT 1
                """, (competitor_ticker,))
                result = cur.fetchone()

                if result and result["company_name"]:
                    return result["company_name"]
        except Exception as e:
            LOG.debug(f"ticker_reference lookup failed for competitor {competitor_ticker}: {e}")

    # STEP 2: Fallback to search_keyword ONLY if it looks like a company name (not ticker)
    if search_keyword and not search_keyword.isupper():  # Likely a company name, not ticker
        return search_keyword

    # STEP 3: Final fallback - use ticker if that's all we have
    return competitor_ticker or search_keyword or "Unknown Competitor"

def determine_article_category_for_ticker(article_row: dict, viewing_ticker: str) -> str:
    """
    Determine the correct article category based on the viewing ticker.
    Same article can be 'company' for one ticker and 'competitor' for another.
    """
    try:
        # Get ticker metadata to determine relationships
        ticker_config = get_ticker_reference(viewing_ticker)
        if not ticker_config:
            # Fallback to stored category if no metadata
            LOG.debug(f"CATEGORIZATION: No metadata for {viewing_ticker}, using original category")
            return article_row.get("category", "company")

        # Get company name for the viewing ticker
        viewing_company_name = ticker_config.get("company_name", viewing_ticker)

        # Get article's search keyword and feed ticker
        search_keyword = article_row.get("search_keyword", "")
        competitor_ticker = article_row.get("feed_ticker", "")
        original_category = article_row.get("category", "company")
        article_title = article_row.get("title", "")[:50]

        LOG.debug(f"CATEGORIZATION: {viewing_ticker} | Company: '{viewing_company_name}' | Article: '{article_title}' | Search: '{search_keyword}' | Original: '{original_category}'")

        # If this article mentions the viewing ticker's company, it's company news
        if search_keyword and viewing_company_name:
            # Check if search keyword matches company name (case insensitive)
            if viewing_company_name.lower() in search_keyword.lower() or \
               search_keyword.lower() in viewing_company_name.lower():
                LOG.debug(f"CATEGORIZATION: {viewing_ticker} | MATCH FOUND - Converting '{original_category}' â†’ 'company' for '{search_keyword}'")
                return "company"

        # If the competitor_ticker matches viewing ticker, this is company news
        if competitor_ticker == viewing_ticker:
            LOG.debug(f"CATEGORIZATION: {viewing_ticker} | COMPETITOR TICKER MATCH - Converting '{original_category}' â†’ 'company'")
            return "company"

        # If this is from a competitor feed but about viewing ticker's company, it's company news
        if original_category == "competitor":
            # Check if this competitor article is actually about the viewing ticker's company
            competitors_list = ticker_config.get("competitors", [])
            article_company_mentioned = False

            for comp in competitors_list:
                if isinstance(comp, dict):
                    comp_name = comp.get("name", "")
                    comp_ticker = comp.get("ticker", "")
                    # If article mentions a known competitor, keep it as competitor
                    if comp_name and (comp_name.lower() in search_keyword.lower()):
                        article_company_mentioned = True
                        break
                    if comp_ticker and comp_ticker == competitor_ticker:
                        article_company_mentioned = True
                        break

            # If competitor article doesn't mention known competitors, might be about viewing company
            if not article_company_mentioned and search_keyword:
                if viewing_company_name.lower() in search_keyword.lower():
                    LOG.debug(f"CATEGORIZATION: {viewing_ticker} | COMPETITOR ARTICLE ABOUT COMPANY - Converting 'competitor' â†’ 'company'")
                    return "company"

        # Default: use original category from database
        LOG.debug(f"CATEGORIZATION: {viewing_ticker} | NO MATCH - Keeping original category '{original_category}'")
        return original_category

    except Exception as e:
        LOG.warning(f"Error determining category for {viewing_ticker}: {e}")
        return article_row.get("category", "company")

def extract_title_words_normalized(title: str, num_words: int = 10) -> str:
    """
    Extract first N words from title, normalized and concatenated (no spaces)
    Used for Google News deduplication when resolved URL not available

    Example: "Tesla Stock Rises 5% After Earnings" â†’ "teslastockrises5afterearnings"
    """
    if not title:
        return ""

    # Remove all non-alphanumeric characters except spaces
    clean_title = re.sub(r'[^a-zA-Z0-9\s]', '', title.lower())

    # Split into words, take first N
    words = clean_title.split()[:num_words]

    # Join without spaces
    return ''.join(words)

def get_url_hash(url: str, resolved_url: str = None, domain: str = None, title: str = None) -> str:
    """
    Generate hash for URL deduplication with intelligent strategy selection.

    Deduplication strategies:
    1. Resolved URLs (Yahoo, Direct): Hash the final destination URL
    2. Google News URLs (unresolved): Hash domain + title to catch cross-feed duplicates
    3. Fallback: Hash the original URL

    Why domain+title for Google News:
    - Google generates unique redirect URLs per feed/context (news.google.com/rss/articles/ABC123...)
    - Same article appears with DIFFERENT Google URLs in company/industry/competitor feeds
    - Same article appears with DIFFERENT Google URLs in each hourly alert run
    - Domain + title is stable across all feeds and time periods
    - Prevents 10-15x duplication that was occurring hourly

    Edge case handling:
    - Two different articles with identical title from same publisher on same day: Rare in practice
    - RSS feed titles are normalized, so variations unlikely
    """
    # Strategy 1: Use resolved URL if available (Yahoo Finance URLs, Direct URLs)
    if resolved_url:
        # Strip tracking/analytics parameters (gaa_=Google Analytics Auth, fbclid=Facebook, etc.)
        url_clean = re.sub(r'[?&](utm_|ref=|source=|siteid=|cid=|\.tsrc=|gaa_|fbclid|msclkid|mc_|_ga).*', '', resolved_url.lower())
        url_clean = url_clean.rstrip('/')
        return hashlib.md5(url_clean.encode()).hexdigest()

    # Strategy 2: Google News - Use domain + title for deduplication
    # This catches the same article appearing with different Google News redirect URLs
    if domain and title and 'news.google.com' in url.lower():
        # Normalize domain and title for consistent hashing
        domain_normalized = domain.lower().strip()
        title_normalized = title.lower().strip()

        # Create composite key for deduplication
        dedup_key = f"{domain_normalized}||{title_normalized}"

        # Log for monitoring (can be removed after validation)
        LOG.debug(f"ðŸ”— Domain+Title dedup: {domain[:30]}... || {title[:50]}...")

        return hashlib.md5(dedup_key.encode()).hexdigest()

    # Strategy 3: Fallback - Use original URL (should rarely hit this for Google News)
    # Strip tracking/analytics parameters (same as Strategy 1)
    url_clean = re.sub(r'[?&](utm_|ref=|source=|siteid=|cid=|\.tsrc=|gaa_|fbclid|msclkid|mc_|_ga).*', '', url.lower())
    url_clean = url_clean.rstrip('/')
    return hashlib.md5(url_clean.encode()).hexdigest()

def normalize_domain(domain: str) -> str:
    """Enhanced domain normalization for consistent storage"""
    if not domain:
        return None
    
    # Handle special consolidation cases first
    domain_mappings = {
        'ca.finance.yahoo.com': 'finance.yahoo.com',
        'uk.finance.yahoo.com': 'finance.yahoo.com', 
        'sg.finance.yahoo.com': 'finance.yahoo.com',
        'www.finance.yahoo.com': 'finance.yahoo.com',
        'yahoo.com/finance': 'finance.yahoo.com',
        'www.bloomberg.com': 'bloomberg.com',
        'www.reuters.com': 'reuters.com',
        'www.wsj.com': 'wsj.com',
        'www.cnbc.com': 'cnbc.com',
        'www.marketwatch.com': 'marketwatch.com',
        'www.seekingalpha.com': 'seekingalpha.com',
        'www.fool.com': 'fool.com',
        'www.tipranks.com': 'tipranks.com',
        'www.benzinga.com': 'benzinga.com',
        'www.barrons.com': 'barrons.com'
    }
    
    # Check for direct mapping first
    domain_lower = domain.lower().strip()
    if domain_lower in domain_mappings:
        return domain_mappings[domain_lower]
    
    # Standard normalization
    normalized = domain_lower
    if normalized.startswith('www.') and normalized != 'www.':
        normalized = normalized[4:]
    
    # Remove trailing slash
    normalized = normalized.rstrip('/')
    
    # Final check for any remaining mappings after normalization
    if normalized in domain_mappings:
        return domain_mappings[normalized]
    
    return normalized

class DomainResolver:
    def __init__(self):
        self._cache = {}
        self._common_mappings = {
            'reuters.com': 'Reuters',
            'bloomberg.com': 'Bloomberg News',
            'wsj.com': 'The Wall Street Journal',
            'cnbc.com': 'CNBC',
            'marketwatch.com': 'MarketWatch',
            'seekingalpha.com': 'Seeking Alpha',
            'fool.com': 'The Motley Fool',
            'tipranks.com': 'TipRanks',
            'benzinga.com': 'Benzinga',
            'investors.com': "Investor's Business Daily",
            'barrons.com': "Barron's",
            'ft.com': 'Financial Times',
            'theglobeandmail.com': 'The Globe and Mail',
        }

    def _resolve_publication_to_domain(self, publication_name: str) -> Optional[str]:
        """Resolve publication name to domain using database first, then AI fallback"""
        if not publication_name:
            return None
        
        clean_name = publication_name.lower().strip()
        
        # First, check if we already have this publication mapped in database
        try:
            with db() as conn, conn.cursor() as cur:
                # Look for existing mapping by formal_name (case insensitive)
                cur.execute("""
                    SELECT domain FROM domain_names 
                    WHERE LOWER(formal_name) = %s
                    LIMIT 1
                """, (clean_name,))
                result = cur.fetchone()
                
                if result:
                    return result["domain"]
                    
                # Also try variations (without "the", etc.)
                variations = [
                    clean_name.replace("the ", ""),
                    clean_name.replace(" the", ""),
                    clean_name + " news",
                    clean_name.replace(" news", "")
                ]
                
                for variation in variations:
                    if variation != clean_name:
                        cur.execute("""
                            SELECT domain FROM domain_names 
                            WHERE LOWER(formal_name) = %s
                            LIMIT 1
                        """, (variation,))
                        result = cur.fetchone()
                        if result:
                            return result["domain"]
                            
        except Exception as e:
            LOG.warning(f"Database lookup failed for '{publication_name}': {e}")
        
        # If not found in database, try AI resolution
        ai_domain = self._resolve_publication_to_domain_with_ai(publication_name)
        
        # If AI found a domain, store the mapping for future use
        if ai_domain:
            try:
                self._store_in_database(ai_domain, publication_name, True)
                LOG.info(f"Stored new mapping: '{publication_name}' -> '{ai_domain}'")
                return ai_domain
            except Exception as e:
                LOG.warning(f"Failed to store domain mapping: {e}")
                return ai_domain
        
        # FIXED: If AI fails, create fallback domain instead of returning None
        LOG.warning(f"AI failed to resolve '{publication_name}', creating fallback domain")
        return self._create_fallback_domain(publication_name)

    def _create_fallback_domain(self, publication_name: str) -> str:
        """Create a fallback domain when AI resolution fails"""
        if not publication_name:
            return "unknown-publication.com"
        
        # Clean the publication name
        clean_name = publication_name.lower().strip()
        
        # Remove common words
        words_to_remove = ['the', 'news', 'magazine', 'journal', 'times', 'post', 'daily', 'weekly']
        words = clean_name.split()
        filtered_words = [word for word in words if word not in words_to_remove]
        
        if not filtered_words:
            # If all words were removed, use the original
            filtered_words = words
        
        # Join words and clean up
        domain_base = ''.join(filtered_words[:3])  # Limit to first 3 words
        domain_base = ''.join(c for c in domain_base if c.isalnum())  # Remove special chars
        
        if len(domain_base) < 3:
            domain_base = "unknown"
        
        fallback_domain = f"{domain_base}.com"
        
        # Store in database as a fallback (not AI generated)
        try:
            self._store_in_database(fallback_domain, publication_name, False)
        except Exception as e:
            LOG.warning(f"Failed to store fallback domain {fallback_domain}: {e}")
        
        return fallback_domain

    def _resolve_publication_to_domain_with_ai(self, publication_name: str) -> Optional[str]:
        """Use AI to convert publication name to domain"""
        if not OPENAI_API_KEY or not publication_name:
            return None
        
        try:
            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            prompt = f'What is the primary domain name for the publication "{publication_name}"? Respond with just the domain (e.g., "reuters.com").'
            
            data = {
                "model": OPENAI_MODEL,
                "input": prompt,
                "max_output_tokens": 30,
                "reasoning": {"effort": "low"},
                "text": {"verbosity": "low"},
                "truncation": "auto"
            }
            
            response = get_openai_session().post(OPENAI_API_URL, headers=headers, json=data, timeout=(10, 180))
            if response.status_code == 200:
                result = response.json()
                
                # Log usage details
                u = result.get("usage", {}) or {}
                LOG.debug("Domain resolution usage â€” input:%s output:%s (cap:%s) status:%s",
                          u.get("input_tokens"), u.get("output_tokens"),
                          result.get("max_output_tokens"),
                          result.get("status"))
                
                domain = extract_text_from_responses(result).strip().lower()
                
                # Clean up common AI response patterns
                domain = domain.replace('"', '').replace("'", "").replace("www.", "")
                
                # Validate it looks like a domain
                if '.' in domain and len(domain) > 4 and len(domain) < 50 and not ' ' in domain:
                    normalized = normalize_domain(domain)
                    if normalized:
                        LOG.info(f"AI resolved '{publication_name}' -> '{normalized}'")
                        return normalized
            
        except Exception as e:
            LOG.warning(f"AI domain resolution failed for '{publication_name}': {e}")
        
        return None
    
    def resolve_url_and_domain(self, url, title=None):
        """Single method to resolve any URL to (final_url, domain, source_url)"""
        try:
            if "news.google.com" in url:
                return self._handle_google_news(url, title)
            elif any(yahoo_domain in url for yahoo_domain in ["finance.yahoo.com", "ca.finance.yahoo.com", "uk.finance.yahoo.com", "yahoo.com"]):
                return self._handle_yahoo_finance(url)
            else:
                return self._handle_direct_url(url)
        except Exception as e:
            LOG.warning(f"URL resolution failed for {url}: {e}")
            fallback_domain = urlparse(url).netloc.lower() if url else None
            return url, normalize_domain(fallback_domain), None
    
    def _resolve_google_news_url_advanced(self, url: str) -> Optional[str]:
        """
        Advanced Google News URL resolution using internal API
        Falls back to existing method if this fails
        """
        try:
            # Only attempt for Google News URLs
            if "news.google.com" not in url:
                return None
                
            headers = {
                'content-type': 'application/x-www-form-urlencoded;charset=UTF-8',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36',
            }
            
            # Get the page content
            resp = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(resp.text, 'html.parser')

            # Find the c-wiz element with data-p attribute
            c_wiz = soup.select_one('c-wiz[data-p]')
            if not c_wiz:
                return None

            data_p = c_wiz.get('data-p')
            if not data_p:
                return None

            # Parse the embedded JSON
            obj = json.loads(data_p.replace('%.@.', '["garturlreq",'))

            # Prepare the payload for the internal API
            payload = {
                'f.req': json.dumps([[['Fbv4je', json.dumps(obj[:-6] + obj[-2:]), 'null', 'generic']]])
            }

            # Make the API call
            api_url = "https://news.google.com/_/DotsSplashUi/data/batchexecute"
            response = requests.post(api_url, headers=headers, data=payload, timeout=10)

            if response.status_code != 200:
                return None

            # Parse the response
            response_text = response.text.replace(")]}'", "")
            response_json = json.loads(response_text)

            array_string = response_json[0][2]
            article_url = json.loads(array_string)[1]

            if article_url and len(article_url) > 10:  # Basic validation
                return article_url

        except Exception as e:
            return None
        
        return None
    
    def get_formal_name(self, domain):
        """Get formal name for domain with caching"""
        if not domain:
            return "Unknown"
        
        clean_domain = normalize_domain(domain)
        if not clean_domain:
            return "Unknown"
        
        # Check cache
        if clean_domain in self._cache:
            return self._cache[clean_domain]
        
        # Check database
        formal_name = self._get_from_database(clean_domain)
        if formal_name:
            self._cache[clean_domain] = formal_name
            return formal_name
        
        # Check common mappings
        if clean_domain in self._common_mappings:
            formal_name = self._common_mappings[clean_domain]
            self._store_in_database(clean_domain, formal_name, False)
            self._cache[clean_domain] = formal_name
            return formal_name
        
        # Use AI as last resort
        if OPENAI_API_KEY:
            formal_name = self._get_from_ai(clean_domain)
            if formal_name:
                self._store_in_database(clean_domain, formal_name, True)
                self._cache[clean_domain] = formal_name
                return formal_name
        
        # Fallback
        fallback = clean_domain.replace('.com', '').replace('.org', '').title()
        self._store_in_database(clean_domain, fallback, False)
        self._cache[clean_domain] = fallback
        return fallback
    
    def _handle_google_news(self, url, title):
        """Handle Google News URL resolution with advanced and fallback methods"""

        LOG.info(f"ðŸ” [GOOGLE_NEWS] Attempting resolution for: {url[:100]}...")

        # Try advanced resolution first
        LOG.info(f"ðŸ”„ [GOOGLE_NEWS] Trying advanced API resolution...")
        advanced_url = self._resolve_google_news_url_advanced(url)
        if advanced_url:
            domain = normalize_domain(urlparse(advanced_url).netloc.lower())
            if not self._is_spam_domain(domain):
                LOG.info(f"âœ… [GOOGLE_NEWS] Advanced resolution: {url[:80]} -> {advanced_url}")
                return advanced_url, domain, None
            else:
                LOG.info(f"SPAM REJECTED: Advanced resolution found spam domain {domain}")
                return None, None, None  # Reject entirely, don't fall back
        else:
            LOG.info(f"âŒ [GOOGLE_NEWS] Advanced API resolution returned None")

        # Fall back to direct resolution method
        LOG.info(f"ðŸ”„ [GOOGLE_NEWS] Trying direct HTTP redirect...")
        try:
            response = requests.get(url, timeout=10, allow_redirects=True, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            final_url = response.url

            if final_url != url and "news.google.com" not in final_url:
                domain = normalize_domain(urlparse(final_url).netloc.lower())
                if not self._is_spam_domain(domain):
                    LOG.info(f"âœ… [GOOGLE_NEWS] Direct resolution: {url[:80]} -> {final_url}")
                    return final_url, domain, None
                else:
                    LOG.info(f"SPAM REJECTED: Direct resolution found spam domain {domain}")
                    return None, None, None  # Reject entirely
            else:
                LOG.info(f"âŒ [GOOGLE_NEWS] Direct HTTP didn't redirect (final_url still Google News)")
        except Exception as e:
            LOG.info(f"âŒ [GOOGLE_NEWS] Direct HTTP failed: {str(e)[:100]}")

        # Title extraction fallback - also check for spam
        LOG.info(f"ðŸ”„ [GOOGLE_NEWS] Trying title extraction from: '{title[:60] if title else 'NO TITLE'}...'")
        if title and not contains_non_latin_script(title):
            clean_title, source = extract_source_from_title_smart(title)
            LOG.info(f"   Title parser extracted source: '{source}'")
            if source and not self._is_spam_source(source):
                resolved_domain = self._resolve_publication_to_domain(source)
                LOG.info(f"   Domain resolver returned: '{resolved_domain}'")
                if resolved_domain:
                    if not self._is_spam_domain(resolved_domain):
                        LOG.info(f"âœ… [GOOGLE_NEWS] Title resolution: {source} -> {resolved_domain}")
                        return url, resolved_domain, None
                    else:
                        LOG.info(f"SPAM REJECTED: Title resolution found spam domain {resolved_domain}")
                        return None, None, None
                else:
                    LOG.warning(f"âŒ [GOOGLE_NEWS] Could not resolve publication '{source}' to domain")
                    return url, "google-news-unresolved", None
            else:
                LOG.info(f"âŒ [GOOGLE_NEWS] Title extraction: No valid source extracted or spam source")
        else:
            LOG.info(f"âŒ [GOOGLE_NEWS] Title extraction: Title is empty or contains non-Latin script")

        LOG.warning(f"âŒ [GOOGLE_NEWS] ALL 3 RESOLUTION METHODS FAILED for: {url[:100]}")
        return url, "google-news-unresolved", None
    
    def _handle_yahoo_finance(self, url):
        """Handle Yahoo Finance URL resolution - keep Yahoo URLs when resolution fails"""
        original_source = extract_yahoo_finance_source_optimized(url)
        if original_source:
            domain = normalize_domain(urlparse(original_source).netloc.lower())
            if not self._is_spam_domain(domain):
                LOG.info(f"YAHOO SUCCESS: Resolved {url} -> {original_source}")
                return original_source, domain, url
            else:
                LOG.info(f"SPAM REJECTED: Yahoo resolution found spam domain {domain}")
                return None, None, None
        
        # UPDATED: Keep Yahoo Finance URLs when resolution fails (they're easy to scrape)
        yahoo_domain = normalize_domain(urlparse(url).netloc.lower())
        LOG.info(f"YAHOO RESOLUTION FAILED: Keeping Yahoo URL for direct scraping: {url}")
        return url, yahoo_domain, None
    
    def _handle_direct_url(self, url):
        """Handle direct URL"""
        domain = normalize_domain(urlparse(url).netloc.lower())
        if self._is_spam_domain(domain):
            return None, None, None
        return url, domain, None
    
    def _is_spam_domain(self, domain):
        """Check if domain is spam"""
        if not domain:
            return True
        return any(spam in domain for spam in SPAM_DOMAINS)
    
    def _is_spam_source(self, source):
        """Check if source name is spam"""
        if not source:
            return True
        source_lower = source.lower()
        return any(spam in source_lower for spam in ["marketbeat", "newser", "khodrobank"])
    
    def _get_from_database(self, domain):
        """Get formal name from database"""
        try:
            with db() as conn, conn.cursor() as cur:
                cur.execute("SELECT formal_name FROM domain_names WHERE domain = %s", (domain,))
                result = cur.fetchone()
                return result["formal_name"] if result else None
        except:
            return None
    
    def _store_in_database(self, domain, formal_name, ai_generated):
        """Store formal name in database"""
        try:
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO domain_names (domain, formal_name, ai_generated)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (domain) DO UPDATE
                    SET formal_name = EXCLUDED.formal_name, updated_at = NOW()
                """, (domain, formal_name, ai_generated))
        except Exception as e:
            LOG.warning(f"Failed to store domain mapping {domain}: {e}")
    
    def _get_from_ai(self, domain):
        """Get formal name from AI with improved prompt"""
        try:
            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }

            prompt = f'''Extract the formal publication name for "{domain}".

Requirements:
- Use proper capitalization and spacing
- Return company/publication name only (no domain extension)
- Format as it would appear in a citation

Examples:
- "reuters.com" â†’ "Reuters"
- "prnewswire.co.uk" â†’ "PR Newswire"
- "theglobeandmail.com" â†’ "The Globe and Mail"
- "businessmodelanalyst.com" â†’ "Business Model Analyst"
- "arabamericannews.com" â†’ "Arab American News"

Domain: {domain}
Response:'''

            data = {
                "model": OPENAI_MODEL,
                "input": prompt,
                "max_output_tokens": 50,
                "reasoning": {"effort": "low"},
                "text": {"verbosity": "low"},
                "truncation": "auto"
            }
            
            response = get_openai_session().post(OPENAI_API_URL, headers=headers, json=data, timeout=(10, 180))
            if response.status_code == 200:
                result = response.json()
                
                # Log usage details
                u = result.get("usage", {}) or {}
                LOG.debug("Formal name lookup usage â€” input:%s output:%s (cap:%s) status:%s",
                          u.get("input_tokens"), u.get("output_tokens"),
                          result.get("max_output_tokens"),
                          result.get("status"))
                
                name = extract_text_from_responses(result).strip()
                return name if 2 < len(name) < 100 else None
        except Exception as e:
            LOG.debug(f"AI formal name lookup failed for {domain}: {e}")
        return None

# Create global instance
domain_resolver = DomainResolver()

class FeedManager:
    @staticmethod
    def create_feeds_for_ticker_enhanced(ticker: str, metadata: Dict) -> List[Dict]:
        """Enhanced feed creation with proper international ticker support"""
        feeds = []
        company_name = metadata.get("company_name", ticker)
        
        # CRITICAL: Clear any global state that might affect this ticker's processing
        import gc
        gc.collect()

        # CRITICAL DEBUG: Log exactly what data this ticker is receiving
        LOG.info(f"FEED GENERATION DEBUG for {ticker}:")
        LOG.info(f"  Company name: {metadata.get('company_name', 'MISSING')}")
        LOG.info(f"  Competitors: {metadata.get('competitors', [])}")
        LOG.info(f"  Metadata keys: {list(metadata.keys())}")

        LOG.info(f"CREATING FEEDS for {ticker} ({company_name}):")
        
        # Check existing feed counts by unique competitor
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT tf.category, COUNT(*) as count
                FROM feeds f
                JOIN ticker_feeds tf ON f.id = tf.feed_id
                WHERE tf.ticker = %s AND f.active = TRUE AND tf.active = TRUE
                GROUP BY tf.category
            """, (ticker,))
            
            existing_data = {row["category"]: row for row in cur.fetchall()}
            
            # Extract counts
            existing_company_count = existing_data.get('company', {}).get('count', 0)
            existing_industry_count = existing_data.get('industry', {}).get('count', 0)
            
            # Count unique competitors separately
            cur.execute("""
                SELECT COUNT(DISTINCT f.feed_ticker) as unique_competitors
                FROM feeds f
                JOIN ticker_feeds tf ON f.id = tf.feed_id
                WHERE tf.ticker = %s AND tf.category = 'competitor' AND f.active = TRUE AND tf.active = TRUE
                AND f.feed_ticker IS NOT NULL
            """, (ticker,))
            result = cur.fetchone()
            existing_competitor_entities = result["unique_competitors"] if result else 0
            
            LOG.info(f"  EXISTING FEEDS: Company={existing_company_count}, Industry={existing_industry_count}, Competitors={existing_competitor_entities} unique entities")
        
        # Company feeds - always ensure we have the core 2
        if existing_company_count < 2:
            # CRITICAL DEBUG: Log exactly what company data is being used
            LOG.info(f"[{ticker}] CREATING COMPANY FEEDS: company_name='{company_name}', ticker='{ticker}'")

            # CRITICAL DEBUG: Log exactly what variables are being used for company feeds
            LOG.info(f"[COMPANY_FEED_DEBUG] {ticker} creating company feeds with:")
            LOG.info(f"[COMPANY_FEED_DEBUG]   company_name='{company_name}'")
            LOG.info(f"[COMPANY_FEED_DEBUG]   ticker='{ticker}'")

            company_feeds = [
                {
                    "url": f"https://news.google.com/rss/search?q=\"{requests.utils.quote(company_name)}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
                    "name": f"Google News: {company_name}",
                    "category": "company",
                    "search_keyword": company_name
                },
                {
                    "url": f"https://finance.yahoo.com/rss/headline?s={ticker}",
                    "name": f"Yahoo Finance: {ticker}",
                    "category": "company",
                    "search_keyword": ticker
                }
            ]
            feeds.extend(company_feeds)
            LOG.info(f"  COMPANY FEEDS: Adding {len(company_feeds)} (existing: {existing_company_count})")
        
        # Industry feeds - MAX 3 TOTAL
        if existing_industry_count < 3:
            available_slots = 3 - existing_industry_count
            industry_keywords = metadata.get("industry_keywords", [])[:available_slots]
            
            for keyword in industry_keywords:
                feed = {
                    "url": f"https://news.google.com/rss/search?q=\"{requests.utils.quote(keyword)}\"+when:7d&hl=en-US&gl=US&ceid=US:en",
                    "name": f"Industry: {keyword}",
                    "category": "industry",
                    "search_keyword": keyword
                }
                feeds.append(feed)
        
        # Enhanced competitor feeds with proper international ticker validation
        if existing_competitor_entities < 3:
            available_competitor_slots = 3 - existing_competitor_entities
            competitors = metadata.get("competitors", [])[:available_competitor_slots]

            # CRITICAL DEBUG: Log competitor data for this specific ticker
            LOG.info(f"  COMPETITOR DEBUG for {ticker}: Raw data: {competitors}")
            
            # Get existing competitor tickers to avoid duplicates
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    SELECT DISTINCT f.feed_ticker
                    FROM feeds f
                    JOIN ticker_feeds tf ON f.id = tf.feed_id
                    WHERE tf.ticker = %s AND tf.category = 'competitor' AND f.active = TRUE AND tf.active = TRUE
                    AND f.feed_ticker IS NOT NULL
                """, (ticker,))
                existing_competitor_tickers = {row["feed_ticker"] for row in cur.fetchall()}
            
            for i, comp in enumerate(competitors):
                comp_name = None
                comp_ticker = None

                # CRITICAL DEBUG: Log what ticker we're processing for
                LOG.info(f"[COMPETITOR_LOOP_DEBUG] Processing competitor {i} for ticker '{ticker}'")

                if isinstance(comp, dict):
                    comp_name = comp.get('name', '')
                    comp_ticker = comp.get('ticker')
                    LOG.info(f"[COMPETITOR_LOOP_DEBUG] Dict competitor - Name: '{comp_name}', Ticker: '{comp_ticker}' for main ticker '{ticker}'")
                elif isinstance(comp, str):
                    LOG.info(f"DEBUG: String competitor: {comp}")
                    # ENHANCED: Better parsing for international tickers
                    patterns = [
                        r'^(.+?)\s*\(([A-Z]{1,8}(?:\.[A-Z]{1,4})?(?:-[A-Z])?)\)$',  # Standard with international
                        r'^(.+?)\s*\(([A-Z-]{1,8})\)$',  # Broader pattern
                    ]
                    
                    matched = False
                    for pattern in patterns:
                        match = re.search(pattern, comp)
                        if match:
                            comp_name = match.group(1).strip()
                            comp_ticker = match.group(2)
                            matched = True
                            break
                    
                    if not matched:
                        LOG.info(f"DEBUG: Skipping competitor - no ticker found in string: {comp}")
                        continue
                
                # Validate we have both name and ticker
                if not comp_name or not comp_ticker:
                    LOG.info(f"DEBUG: Skipping competitor - missing name or ticker: name='{comp_name}', ticker='{comp_ticker}'")
                    continue
                
                # Normalize competitor ticker
                comp_ticker = normalize_ticker_format(comp_ticker)
                
                # Skip if same as main ticker
                if comp_ticker.upper() == ticker.upper():
                    LOG.info(f"DEBUG: Skipping competitor - same as main ticker: {comp_ticker}")
                    continue
                
                # Skip if already exists
                if comp_ticker in existing_competitor_tickers:
                    LOG.info(f"DEBUG: Skipping competitor - already exists: {comp_ticker}")
                    continue
                
                # NOTE: Validation removed (Nov 30, 2025) - international competitor tickers allowed for feed creation
                # Yahoo Finance handles unknown tickers gracefully (returns empty feed)

                # CRITICAL DEBUG: Log what feeds we're about to create
                LOG.info(f"[{ticker}] CREATING COMPETITOR FEEDS: comp_name='{comp_name}', comp_ticker='{comp_ticker}'")

                # Create feeds for this competitor
                comp_feeds = [
                    {
                        "url": f"https://news.google.com/rss/search?q=\"{requests.utils.quote(comp_name)}\"+stock+when:7d&hl=en-US&gl=US&ceid=US:en",
                        "name": f"Competitor: {comp_name}",
                        "category": "competitor",
                        "search_keyword": comp_name,
                        "competitor_ticker": comp_ticker
                    },
                    {
                        "url": f"https://finance.yahoo.com/rss/headline?s={comp_ticker}",
                        "name": f"Yahoo Competitor: {comp_name} ({comp_ticker})",
                        "category": "competitor", 
                        "search_keyword": comp_name,
                        "competitor_ticker": comp_ticker
                    }
                ]
                
                feeds.extend(comp_feeds)
                LOG.info(f"    COMPETITOR: {comp_name} ({comp_ticker}) - 2 feeds (counts as 1 entity)")
        
        LOG.info(f"TOTAL FEEDS TO CREATE: {len(feeds)}")
        return feeds
    

# Global instance
feed_manager = FeedManager()

class TickerManager:
    @staticmethod
    def get_or_create_metadata(ticker: str) -> Dict:
        """Get ticker metadata from database. Returns database data for existing tickers.

        NOTE (Nov 2025): AI enhancement during daily processing has been DISABLED.
        - CSV (ticker_reference.csv) is the source of truth
        - AI enhancement functions still exist but are not called automatically
        - For future AI enhancement, use explicit admin endpoints
        """
        # Check database first - this is now the ONLY path for existing tickers
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker, company_name,
                       industry_keyword_1, industry_keyword_2, industry_keyword_3,
                       competitor_1_name, competitor_1_ticker,
                       competitor_2_name, competitor_2_ticker,
                       competitor_3_name, competitor_3_ticker,
                       ai_generated
                FROM ticker_reference WHERE ticker = %s AND active = TRUE
            """, (ticker,))
            config = cur.fetchone()

            if config:
                # Reconstruct industry_keywords array from individual columns
                industry_keywords = []
                for i in range(1, 4):
                    kw = config.get(f"industry_keyword_{i}")
                    if kw:
                        industry_keywords.append(kw)

                # Reconstruct competitors array from individual columns
                competitors = []
                for i in range(1, 4):
                    comp_name = config.get(f"competitor_{i}_name")
                    comp_ticker = config.get(f"competitor_{i}_ticker")
                    if comp_name:
                        competitors.append({
                            "name": comp_name,
                            "ticker": comp_ticker if comp_ticker else None
                        })

                LOG.info(f"[TickerManager] Found database data for {ticker}: {config.get('company_name')}")
                return {
                    "company_name": config.get("company_name", ticker),
                    "industry_keywords": industry_keywords,
                    "competitors": competitors
                }

        # No database record - return fallback (NO AI generation)
        # This should rarely happen since all tickers should be in CSV
        LOG.warning(f"[TickerManager] No database record for {ticker}, returning fallback")
        return {
            "company_name": ticker,
            "industry_keywords": [],
            "competitors": []
        }
    
    @staticmethod
    def store_metadata(ticker: str, metadata: Dict):
        """Store enhanced ticker metadata in ticker_reference table only"""

        # CRITICAL GUARD: Never store fallback data (company_name == ticker)
        # This prevents overwriting good CSV data with AI fallback data
        if metadata and metadata.get("company_name") == ticker:
            LOG.warning(f"âš ï¸ Refusing to store fallback metadata for {ticker} (company_name == ticker). This prevents database corruption.")
            return

        # Handle None or invalid metadata
        if not metadata or not isinstance(metadata, dict):
            LOG.warning(f"Invalid or missing metadata for {ticker}, creating fallback")
            metadata = {
                "company_name": ticker,
                "industry_keywords": [],
                "competitors": [],
                "sector": "",
                "industry": "",
                "sub_industry": ""
            }
        
        # Ensure required fields exist with defaults
        metadata.setdefault("company_name", ticker)
        metadata.setdefault("industry_keywords", [])
        metadata.setdefault("competitors", [])
        metadata.setdefault("sector", "")
        metadata.setdefault("industry", "")
        metadata.setdefault("sub_industry", "")
        
        # Convert arrays to separate fields for ticker_reference table
        industry_keywords = metadata.get("industry_keywords", [])
        competitors = metadata.get("competitors", [])
        
        ticker_data = {
            'ticker': ticker,
            'country': 'US',  # Default - can be enhanced later
            'company_name': metadata.get("company_name", ticker),
            'sector': metadata.get("sector", ""),
            'industry': metadata.get("industry", ""),
            'sub_industry': metadata.get("sub_industry", ""),
            'geographic_markets': metadata.get("geographic_markets", ""),
            'subsidiaries': metadata.get("subsidiaries", ""),
            'ai_generated': True,
            'data_source': 'ai_enhanced'
        }
        
        # Map 3 industry keywords to separate fields
        for i, keyword in enumerate(industry_keywords[:3], 1):
            if keyword and keyword.strip():
                ticker_data[f'industry_keyword_{i}'] = keyword.strip()
        
        # Map 3 competitors to separate name/ticker fields
        for i, comp in enumerate(competitors[:3], 1):
            if isinstance(comp, dict):
                name = comp.get('name', '').strip()
                ticker_field = comp.get('ticker', '').strip()
                
                if name:
                    ticker_data[f'competitor_{i}_name'] = name
                    if ticker_field:
                        # Validate and normalize competitor ticker
                        normalized_ticker = normalize_ticker_format(ticker_field)
                        if validate_ticker_format(normalized_ticker):
                            ticker_data[f'competitor_{i}_ticker'] = normalized_ticker
                        else:
                            LOG.warning(f"Invalid competitor ticker format for {name}: {ticker_field}")
            elif isinstance(comp, str) and comp.strip():
                # Handle string format as fallback
                ticker_data[f'competitor_{i}_name'] = comp.strip()
        
        # Store in ticker_reference table ONLY
        try:
            success = store_ticker_reference(ticker_data)
            if success:
                LOG.info(f"Successfully stored metadata for {ticker} in ticker_reference")
            else:
                LOG.error(f"Failed to store ticker_reference for {ticker}")
        except Exception as e:
            LOG.error(f"Database error storing ticker_reference for {ticker}: {e}")


# Global instances
ticker_manager = TickerManager()
feed_manager = FeedManager()

# ===== TICKER METADATA GENERATION WITH CLAUDE PRIMARY, OPENAI FALLBACK =====

def generate_claude_ticker_metadata(ticker: str, company_name: str = None, sector: str = "", industry: str = "") -> Optional[Dict]:
    """Generate ticker metadata using Claude API with optimized fundamental driver keywords"""
    if not ANTHROPIC_API_KEY:
        LOG.warning("Missing ANTHROPIC_API_KEY; skipping Claude metadata generation")
        return None

    if company_name is None:
        company_name = ticker

    # Build context info for user prompt
    context_info = f"Company: {company_name} ({ticker})"
    if sector:
        context_info += f", Sector: {sector}"
    if industry:
        context_info += f", Industry: {industry}"

    # Comprehensive system prompt for Claude (CACHED - optimized for prompt caching)
    system_prompt = """You are a financial analyst creating metadata for a hedge fund's stock monitoring system. Generate precise, actionable metadata that will be used for news article filtering and triage.

CRITICAL REQUIREMENTS:
- All competitors and value chain companies must be currently publicly traded with valid ticker symbols
- Fundamental driver keywords must capture QUANTIFIABLE market forces that move the stock (NOT industry labels)
- Industry keywords MUST be in Title Case (e.g., "Loan Growth", "EV Adoption", "Copper Prices")
- Benchmarks must be sector-specific, not generic market indices
- All information must be factually accurate
- The company name MUST be the official legal name (e.g., "Prologis Inc" not "PLD")
- If any field is unknown, output an empty array for lists and omit optional fields

TICKER FORMAT REQUIREMENTS:
- US companies: Use simple ticker (AAPL, MSFT)
- Canadian companies: Use .TO suffix (RY.TO, TD.TO, BMO.TO)
- UK companies: Use .L suffix (BP.L, VOD.L)
- Australian companies: Use .AX suffix (BHP.AX, CBA.AX)
- Other international: Use appropriate Yahoo Finance suffix
- Special classes: Use dash format (BRK-A, BRK-B, TECK-A.TO)

FUNDAMENTAL DRIVER KEYWORDS (exactly 3):
â­ **CRITICAL SHIFT:** Generate keywords that track EXTERNAL market forces that drive stock performance, NOT industry category labels.

**What Are Fundamental Drivers?**
Quantifiable metrics that:
1. Directly impact revenue, costs, or margins (10%+ move affects earnings)
2. Are reported with numbers in news (prices, volumes, percentages)
3. Change frequently (weekly/monthly volatility)
4. Are EXTERNAL to company (market-level, not company-specific)
5. Traders actively monitor (Bloomberg KPIs, analyst models)

**Scope Rules:**
âŒ NO company-specific: "Apple iPhone sales", "Tesla deliveries", "Exxon production"
âœ… YES category-specific: "smartphone sales", "electric vehicle sales", "oil production"
âœ… YES niche-specific if dominates: "CPAP device sales" (ResMed), "GLP-1 drug demand" (Novo)
âŒ NO industry labels: "Technology Sector", "Copper Mining", "Banking Industry"
âœ… YES market drivers: "cloud infrastructure spending", "copper price", "interest rates"

**Format:**
- Use 2-4 words maximum
- Lowercase for commodities/rates: "copper price", "jet fuel prices", "bitcoin price"
- Title Case for categories: "Enterprise Software Spending", "Auto Sales"
- Phrase as journalist writes headlines: "bitcoin price" not "BTC/USD"

**Framework by Business Type:**

COMMODITY PRODUCERS (mining, oil, agriculture):
â€¢ Copper miners (FCX): ["copper price", "China construction", "copper supply"]
â€¢ Oil producers (XOM): ["oil price", "refining margins", "natural gas price"]
â€¢ Gold miners (NEM): ["gold price", "mining costs", "central bank policy"]

CONSUMER (retail, restaurants, apparel):
â€¢ Apparel (NKE): ["athletic footwear sales", "China consumer spending", "cotton prices"]
â€¢ Restaurants (MCD): ["restaurant traffic", "food commodity prices", "labor costs"]
â€¢ E-commerce (AMZN): ["e-commerce sales", "consumer spending", "shipping costs"]

TRANSPORTATION (airlines, rails, trucking):
â€¢ Airlines (DAL): ["airline passenger demand", "jet fuel prices", "airline capacity"]
â€¢ Rail (UNP): ["rail freight volumes", "diesel fuel prices", "intermodal shipping"]

TECHNOLOGY (hardware, software, semiconductors):
â€¢ Cloud (MSFT): ["cloud infrastructure spending", "enterprise IT budgets", "AI infrastructure demand"]
â€¢ Semiconductors (NVDA): ["GPU demand", "AI chip demand", "data center spending"]
â€¢ Internet (META): ["digital advertising spending", "social media ad rates", "e-commerce advertising"]

FINANCIALS (banks, asset managers, insurance):
â€¢ Banks (JPM): ["interest rates", "loan demand", "credit quality"]
â€¢ Asset managers (BLK): ["asset management flows", "equity market performance", "ETF flows"]
â€¢ Insurance (PGR): ["insurance premiums", "catastrophic weather events", "investment yields"]

UTILITIES (electric, gas, renewables):
â€¢ Regulated (SO): ["electricity demand", "natural gas prices", "utility rate cases"]
â€¢ Independent power (VST): ["power prices", "natural gas prices", "electricity demand"]

HEALTHCARE (pharma, biotech, devices):
â€¢ Pharma (PFE): ["drug pricing policy", "prescription volumes", "patent expiration"]
â€¢ Devices (MDT): ["medical device sales", "cardiology procedures", "hospital capital spending"]

REAL ESTATE (REITs):
â€¢ Industrial (PLD): ["industrial real estate demand", "warehouse vacancy rates", "e-commerce logistics"]
â€¢ Data centers (EQIX): ["data center demand", "cloud infrastructure spending", "AI data center requirements"]

INDUSTRIALS (aerospace, defense, automation):
â€¢ Aerospace (BA): ["aircraft orders", "defense spending", "commercial aviation demand"]
â€¢ Construction (CAT): ["construction activity", "equipment rental rates", "infrastructure spending"]

**Special Cases:**

DIVERSIFIED COMPANIES - Allocate by profit contribution:
â€¢ Amazon (AWS 60% profit): ["cloud infrastructure spending", "e-commerce sales", "digital advertising spending"]
â€¢ Alphabet (Ads 80%): ["digital advertising spending", "search advertising rates", "cloud infrastructure growth"]

COMMODITY PROCESSORS - Focus on SPREADS not absolute prices:
â€¢ Oil refiners (VLO): ["refining margins", "gasoline demand", "crude oil prices"]

CYCLICALS - Lead with demand, then financing/inputs:
â€¢ Homebuilders (DHI): ["housing starts", "mortgage rates", "lumber prices"]
â€¢ Autos (GM): ["auto sales", "EV adoption", "semiconductor supply"]

REGULATORY-DRIVEN - If >30% value from policy:
â€¢ Pharma pricing risk: ["drug pricing policy", "prescription volumes", "biosimilar competition"]

NICHE LEADERS - If >70% market share, allow category-specific:
â€¢ ResMed: ["sleep apnea treatment", "CPAP reimbursement", "respiratory device sales"]

**Validation (Score 0-10):**
1. Headline Frequency: Daily (10), weekly (7-9), monthly (4-6), rare (0-3)
2. Quantifiable: Always has figures (10), usually (7-9), sometimes (4-6), rarely (0-3)
3. Stock Impact: 10% move = >10% EPS (10), 5-10% EPS (7-9), 2-5% EPS (4-6), <2% EPS (0-3)
4. Trader Monitoring: Core KPI (10), important (7-9), secondary (4-6), ignored (0-3)

Examples:
âœ… "copper price" (FCX): 40/40 excellent
âŒ "mining technology": 12/40 reject (use "copper supply")
âœ… "cloud infrastructure spending" (MSFT): 37/40 excellent

**Common Mistakes:**
âŒ "Copper Mining" â†’ âœ… "copper price"
âŒ "Apple iPhone sales" â†’ âœ… "smartphone sales"
âŒ "Supply Chain" â†’ âœ… "semiconductor supply"
âŒ "Battery Technology" â†’ âœ… "electric vehicle sales"

BUSINESS STRUCTURE GUIDANCE:

For MOST companies (single core business):
- Standard approach: 3 keywords for primary business
- Example: Netflix â†’ ["streaming subscriber growth", "content spending", "streaming competition"]

For DIVERSIFIED companies (2-3 major business lines):
- Keywords should cover top 2-3 revenue segments (prioritize profit contributors)
- Horizontal competitors can be a MIX across segments (no single competitor may compete in all areas)
- Example: Amazon â†’ Keywords: ["cloud infrastructure spending", "e-commerce sales", "digital advertising spending"]
  Horizontal Competitors: Walmart (retail), Microsoft (cloud), Shopify (platform)

For CONGLOMERATES (Berkshire, 3M, Honeywell):
- Focus on 3 largest operating segments OR conglomerate-level themes
- Competitors: Other diversified industrials/conglomerates
- Example: Berkshire â†’ ["Property Casualty Insurance", "Railroad Operations", "Diversified Holdings"]
  Competitors: 3M, Honeywell, Danaher

For REGIONAL/GEOGRAPHIC companies (utilities, Canadian banks):
- Competitors MUST be in same primary market/regulatory environment
- Example: RY.TO â†’ Competitors: TD.TO, BMO.TO, BNS.TO (NOT JPM, BAC)
- Example: Southern Company â†’ Competitors: Duke Energy, Dominion, Entergy (all Southeast regulated utilities)

HORIZONTAL COMPETITORS (0-3):
Select direct business competitors competing for the same customers in the same markets.

**Selection Criteria:**
- Must compete in SAME CUSTOMER SEGMENT and price point (not just same industry)
- GEOGRAPHIC PRIORITY: For regional companies, prioritize competitors in same primary market
- SCALE MATCHING: Select competitors of comparable size to ensure news about competitor materially affects target company
  - For mega-caps ($50B+): Prefer competitors >$5B market cap
  - For large-caps ($10-50B): Prefer competitors >$2B market cap  
  - For mid-caps ($2-10B): Prefer competitors >$500M market cap
  - For small-caps (<$2B): More flexibility on scale (limited peer set)
- INFORMATION VALUE: Prioritize competitors whose news provides actionable signals
  - Prefer competitors that institutional analysts actively cover alongside target company (cross-read for sector insights)
  - Prefer sector bellwethers whose results/guidance signal broader market trends
  - Prefer competitors with higher disclosure frequency or newsflow (quarterly reports, production updates, operational KPIs)
  - If choosing between similar competitors, select the one with more predictive value for target company's performance
- Prioritize in order: (1) Direct peer (similar scale/model), (2) Adjacent competitor, (3) Bellwether (larger sector leader)
- Return 0-3 based on availability - quality over quantity
- Strongly prefer publicly traded companies with valid tickers
- Only include private if industry-dominant with significant newsflow (â‰¥5 major articles/month)
- Exclude: Subsidiaries, companies that do not currently trade independently on major stock exchanges, companies in active bankruptcy proceedings

**Bad match examples:**
âŒ AvalonBay (luxury apartments) for ELME (value-add apartments) - different customer segment
âŒ IBM (enterprise conglomerate) for D-Wave (quantum startup) - different scale/focus
âŒ JPMorgan (universal bank) for Robinhood (retail trading) - different customer demographic

**Format:**
[
  {"name": "Company Name", "ticker": "TICKER"},
  {"name": "Company Name", "ticker": "TICKER"},
  {"name": "Company Name", "ticker": "TICKER"}
]

VALUE CHAIN (0-2 upstream, 0-2 downstream):
Select suppliers (upstream) and/or customers (downstream) that provide material intelligence signals.

**UPSTREAM (0-2 suppliers/input providers):**
Include if:
- Represents >10% of COGS, OR
- CRITICAL ENABLER: Sole-source/oligopoly (â‰¤3 global suppliers), would halt production if lost, specialized technology/patents (e.g., TSMC for Intel, ASML for semiconductor manufacturers, rare earth suppliers for EVs)

Rank by materiality (highest % of COGS first):
- Slot 1: Most material supplier
- Slot 2: Second most material supplier

**DOWNSTREAM (0-2 buyers/customers):**
Include if:
- Represents >5% of revenue, OR
- Provides demand signal proxy for market trends

Rank by materiality (highest % of revenue first):
- Slot 1: Most material customer
- Slot 2: Second most material customer

**General Rules:**
- Strongly prefer publicly traded companies (only private if â‰¥5 major articles/month or industry-dominant with pricing power)
- Geographic flexibility (can be national/international, unlike horizontal competitors)
- Exclude: Subsidiaries, companies acquired in last 2 years, companies already listed in horizontal competitors
- For conglomerates: Prioritize by absolute materiality across all segments (don't force coverage of every segment)
- Return empty arrays if no material value chain exists (direct-to-consumer brands, fragmented supply/customer base)

**Format:**
{
  "upstream": [
    {"name": "Supplier Name", "ticker": "TICKER"},
    {"name": "Supplier Name", "ticker": "TICKER"}
  ],
  "downstream": [
    {"name": "Customer Name", "ticker": "TICKER"},
    {"name": "Customer Name", "ticker": "TICKER"}
  ]
}

**Examples:**

Tesla (manufacturer with supplier dependency):
{
  "horizontal_competitors": [
    {"name": "BYD Company", "ticker": "BYDDY"},
    {"name": "Ford Motor", "ticker": "F"},
    {"name": "General Motors", "ticker": "GM"}
  ],
  "value_chain": {
    "upstream": [
      {"name": "Panasonic", "ticker": "PCRFY"},
      {"name": "CATL", "ticker": "300750.SZ"}
    ],
    "downstream": []
  },
  "industry_keywords": ["electric vehicle sales", "battery costs", "EV regulations"]
}

Intel (B2B component with major customers):
{
  "horizontal_competitors": [
    {"name": "AMD", "ticker": "AMD"},
    {"name": "NVIDIA", "ticker": "NVDA"},
    {"name": "Qualcomm", "ticker": "QCOM"}
  ],
  "value_chain": {
    "upstream": [
      {"name": "TSMC", "ticker": "TSM"}
    ],
    "downstream": [
      {"name": "Microsoft", "ticker": "MSFT"},
      {"name": "Apple", "ticker": "AAPL"}
    ]
  },
  "industry_keywords": ["x86 CPU demand", "semiconductor manufacturing capacity", "data center chip sales"]
}

Denison Mines (commodity producer):
{
  "horizontal_competitors": [
    {"name": "Cameco", "ticker": "CCJ"},
    {"name": "NexGen Energy", "ticker": "NXE"},
    {"name": "Energy Fuels", "ticker": "UUUU"}
  ],
  "value_chain": {
    "upstream": [],
    "downstream": [
      {"name": "Centrus Energy", "ticker": "LEU"},
      {"name": "Constellation Energy", "ticker": "CEG"}
    ]
  },
  "industry_keywords": ["uranium price", "nuclear power demand", "uranium supply"]
}

Netflix (direct-to-consumer, no value chain):
{
  "horizontal_competitors": [
    {"name": "Disney", "ticker": "DIS"},
    {"name": "Warner Bros Discovery", "ticker": "WBD"},
    {"name": "Paramount Global", "ticker": "PARA"}
  ],
  "value_chain": {
    "upstream": [],
    "downstream": []
  },
  "industry_keywords": ["streaming subscriber growth", "content spending", "streaming competition"]
}

GEOGRAPHIC MARKETS (string format):
- List the primary countries/regions where {company_name} has significant operations, revenue, or customers
- Format: "Region1 (major), Region2 (major), Region3 (minor)"
- Use parenthetical notes to indicate scale: (major), (major), (minor)
- Be specific for major markets, broader for minor markets
- Examples:
  â†’ EchoStar: "United States (major), Europe (minor), Latin America (minor)"
  â†’ Royal Bank: "Canada (major), United States (major), Caribbean (minor)"
  â†’ Apple: "United States (major), China (major), Europe (major)"
  â†’ Regional utility: "United States - Southeast (major)"
- If truly global with balanced presence: "Global operations"
- If unknown or insufficient information: ""

SUBSIDIARIES (string format):
- List up to 3 major operating subsidiaries or business units
- Format: "Subsidiary Name 1, Subsidiary Name 2, Subsidiary Name 3"
- Use COMMON/BRAND names, not legal entities (e.g., "Hughes Network Systems" not "Hughes Network Systems LLC")
- Include only material subsidiaries (significant revenue/operations)
- Exclude: Recently acquired companies, minor divisions, brands (brands go in aliases_brands_assets)
- Examples:
  â†’ EchoStar: "Hughes Network Systems, Viasat Inc., EchoStar Mobile"
  â†’ Berkshire Hathaway: "GEICO, BNSF Railway, Berkshire Hathaway Energy"
  â†’ Alphabet: "Google, YouTube, Waymo"
  â†’ JPMorgan: "Chase Bank, J.P. Morgan Wealth Management, J.P. Morgan Securities"
- If no significant subsidiaries or holding company structure unclear: ""

Return ONLY valid JSON in this exact format:
{{
    "ticker": "{ticker}",
    "company_name": "{company_name}",
    "sector": "{sector if sector else 'GICS Sector'}",
    "industry": "{industry if industry else 'GICS Industry'}",
    "sub_industry": "GICS Sub-Industry",
    "industry_keywords": ["keyword1", "keyword2", "keyword3"],
    "horizontal_competitors": [
        {{"name": "Company Name", "ticker": "TICKER"}},
        {{"name": "Company Name", "ticker": "TICKER.TO"}},
        {{"name": "Company Name", "ticker": "TICKER"}}
    ],
    "value_chain": {{
        "upstream": [
            {{"name": "Supplier Name", "ticker": "TICKER"}},
            {{"name": "Supplier Name", "ticker": "TICKER"}}
        ],
        "downstream": [
            {{"name": "Customer Name", "ticker": "TICKER"}},
            {{"name": "Customer Name", "ticker": "TICKER"}}
        ]
    }},
    "sector_profile": {{
        "core_inputs": ["input1", "input2", "input3"],
        "core_channels": ["channel1", "channel2", "channel3"],
        "core_geos": ["geo1", "geo2", "geo3"],
        "benchmarks": ["benchmark1", "benchmark2", "benchmark3"]
    }},
    "aliases_brands_assets": {{
        "aliases": ["alias1", "alias2", "alias3"],
        "brands": ["brand1", "brand2", "brand3"],
        "assets": ["asset1", "asset2", "asset3"]
    }},
    "geographic_markets": "Region1 (major), Region2 (major), Region3 (minor)",
    "subsidiaries": "Subsidiary Name 1, Subsidiary Name 2, Subsidiary Name 3"
}}"""

    # User prompt with company-specific context
    user_prompt = f"""Generate metadata for this company:

{context_info}

Return ONLY valid JSON with no additional commentary."""

    try:
        headers = {
            "x-api-key": ANTHROPIC_API_KEY,
            "anthropic-version": "2023-06-01",  # Prompt caching support
            "content-type": "application/json"
        }

        data = {
            "model": ANTHROPIC_MODEL,
            "max_tokens": 4096,
            "temperature": 0.0,  # Zero temperature for deterministic metadata generation
            "system": [
                {
                    "type": "text",
                    "text": system_prompt,
                    "cache_control": {"type": "ephemeral"}  # Cache the system prompt
                }
            ],
            "messages": [{"role": "user", "content": user_prompt}]
        }

        response = requests.post(ANTHROPIC_API_URL, headers=headers, json=data, timeout=(10, 180))

        if response.status_code != 200:
            LOG.error(f"Claude metadata API error {response.status_code}: {response.text[:200]}")
            return None

        result = response.json()
        text = result.get("content", [{}])[0].get("text", "")

        if not text:
            LOG.warning(f"Claude metadata response empty for {ticker}")
            return None

        # Parse JSON
        metadata = parse_json_with_fallback(text, ticker)
        if not metadata:
            LOG.warning(f"Claude metadata JSON parsing failed for {ticker}")
            return None

        # Process the results (same logic as OpenAI)
        def _list3(x):
            if isinstance(x, (list, tuple)):
                items = [item for item in list(x)[:3] if item]
                return items
            return []

        def _process_competitors(competitors_data):
            processed = []
            if not isinstance(competitors_data, list):
                return processed

            for comp in competitors_data[:3]:
                if isinstance(comp, dict):
                    name = comp.get('name', '').strip()
                    ticker_field = comp.get('ticker', '').strip()

                    if name:
                        processed_comp = {"name": name}
                        if ticker_field:
                            normalized_ticker = normalize_ticker_format(ticker_field)
                            if validate_ticker_format(normalized_ticker):
                                processed_comp["ticker"] = normalized_ticker
                            else:
                                LOG.warning(f"Claude provided invalid competitor ticker: {ticker_field}")
                        processed.append(processed_comp)
                elif isinstance(comp, str):
                    name = comp.strip()
                    if name:
                        processed.append({"name": name})

            return processed

        metadata.setdefault("ticker", ticker)
        metadata["name"] = company_name
        metadata["company_name"] = company_name

        metadata.setdefault("sector", sector)
        metadata.setdefault("industry", industry)
        metadata.setdefault("sub_industry", "")

        metadata["industry_keywords"] = _list3(metadata.get("industry_keywords", []))
        metadata["competitors"] = _process_competitors(metadata.get("competitors", []))

        sector_profile = metadata.setdefault("sector_profile", {})
        aliases_brands = metadata.setdefault("aliases_brands_assets", {})

        sector_profile["core_inputs"] = _list3(sector_profile.get("core_inputs", []))
        sector_profile["core_channels"] = _list3(sector_profile.get("core_channels", []))
        sector_profile["core_geos"] = _list3(sector_profile.get("core_geos", []))
        sector_profile["benchmarks"] = _list3(sector_profile.get("benchmarks", []))

        aliases_brands["aliases"] = _list3(aliases_brands.get("aliases", []))
        aliases_brands["brands"] = _list3(aliases_brands.get("brands", []))
        aliases_brands["assets"] = _list3(aliases_brands.get("assets", []))

        LOG.info(f"Claude metadata generated for {ticker}: {company_name}")
        return metadata

    except Exception as e:
        LOG.error(f"Claude metadata generation failed for {ticker}: {e}")
        return None


def generate_openai_ticker_metadata(ticker: str, company_name: str = None, sector: str = "", industry: str = "") -> Optional[Dict]:
    """Generate ticker metadata using OpenAI API (fallback)"""
    if not OPENAI_API_KEY:
        LOG.warning("Missing OPENAI_API_KEY; skipping metadata generation")
        return None

    if company_name is None:
        company_name = ticker

    # System prompt
    system_prompt = """You are a financial analyst creating metadata for a hedge fund's stock monitoring system. Generate precise, actionable metadata that will be used for news article filtering and triage.

CRITICAL REQUIREMENTS:
- All competitors and value chain companies must be currently publicly traded with valid ticker symbols
- Industry keywords must be SPECIFIC enough to avoid false positives in news filtering, but not so narrow that they miss material news.
- Benchmarks must be sector-specific, not generic market indices
- Industry keywords MUST be in Title Case (e.g., "Loan Growth", "EV Adoption", "Copper Prices")
- All information must be factually accurate
- The company name MUST be the official legal name (e.g., "Prologis Inc" not "PLD")
- If any field is unknown, output an empty array for lists and omit optional fields. Never refuse; always return a valid JSON object.

TICKER FORMAT REQUIREMENTS:
- US companies: Use simple ticker (AAPL, MSFT)
- Canadian companies: Use .TO suffix (RY.TO, TD.TO, BMO.TO)
- UK companies: Use .L suffix (BP.L, VOD.L)
- Australian companies: Use .AX suffix (BHP.AX, CBA.AX)
- Other international: Use appropriate Yahoo Finance suffix
- Special classes: Use dash format (BRK-A, BRK-B, TECK-A.TO)

FUNDAMENTAL DRIVER KEYWORDS (exactly 3):
â­ **CRITICAL SHIFT:** Generate keywords that track EXTERNAL market forces that drive stock performance, NOT industry category labels.

**What Are Fundamental Drivers?**
Quantifiable metrics that:
1. Directly impact revenue, costs, or margins (10%+ move affects earnings)
2. Are reported with numbers in news (prices, volumes, percentages)
3. Change frequently (weekly/monthly volatility)
4. Are EXTERNAL to company (market-level, not company-specific)
5. Traders actively monitor (Bloomberg KPIs, analyst models)

**Scope Rules:**
âŒ NO company-specific: "Apple iPhone sales", "Tesla deliveries", "Exxon production"
âœ… YES category-specific: "smartphone sales", "electric vehicle sales", "oil production"
âœ… YES niche-specific if dominates: "CPAP device sales" (ResMed), "GLP-1 drug demand" (Novo)
âŒ NO industry labels: "Technology Sector", "Copper Mining", "Banking Industry"
âœ… YES market drivers: "cloud infrastructure spending", "copper price", "interest rates"

**Format:**
- Use 2-4 words maximum
- Lowercase for commodities/rates: "copper price", "jet fuel prices", "bitcoin price"
- Title Case for categories: "Enterprise Software Spending", "Auto Sales"
- Phrase as journalist writes headlines: "bitcoin price" not "BTC/USD"

**Framework by Business Type:**

COMMODITY PRODUCERS (mining, oil, agriculture):
â€¢ Copper miners (FCX): ["copper price", "China construction", "copper supply"]
â€¢ Oil producers (XOM): ["oil price", "refining margins", "natural gas price"]
â€¢ Gold miners (NEM): ["gold price", "mining costs", "central bank policy"]

CONSUMER (retail, restaurants, apparel):
â€¢ Apparel (NKE): ["athletic footwear sales", "China consumer spending", "cotton prices"]
â€¢ Restaurants (MCD): ["restaurant traffic", "food commodity prices", "labor costs"]
â€¢ E-commerce (AMZN): ["e-commerce sales", "consumer spending", "shipping costs"]

TRANSPORTATION (airlines, rails, trucking):
â€¢ Airlines (DAL): ["airline passenger demand", "jet fuel prices", "airline capacity"]
â€¢ Rail (UNP): ["rail freight volumes", "diesel fuel prices", "intermodal shipping"]

TECHNOLOGY (hardware, software, semiconductors):
â€¢ Cloud (MSFT): ["cloud infrastructure spending", "enterprise IT budgets", "AI infrastructure demand"]
â€¢ Semiconductors (NVDA): ["GPU demand", "AI chip demand", "data center spending"]
â€¢ Internet (META): ["digital advertising spending", "social media ad rates", "e-commerce advertising"]

FINANCIALS (banks, asset managers, insurance):
â€¢ Banks (JPM): ["interest rates", "loan demand", "credit quality"]
â€¢ Asset managers (BLK): ["asset management flows", "equity market performance", "ETF flows"]
â€¢ Insurance (PGR): ["insurance premiums", "catastrophic weather events", "investment yields"]

UTILITIES (electric, gas, renewables):
â€¢ Regulated (SO): ["electricity demand", "natural gas prices", "utility rate cases"]
â€¢ Independent power (VST): ["power prices", "natural gas prices", "electricity demand"]

HEALTHCARE (pharma, biotech, devices):
â€¢ Pharma (PFE): ["drug pricing policy", "prescription volumes", "patent expiration"]
â€¢ Devices (MDT): ["medical device sales", "cardiology procedures", "hospital capital spending"]

REAL ESTATE (REITs):
â€¢ Industrial (PLD): ["industrial real estate demand", "warehouse vacancy rates", "e-commerce logistics"]
â€¢ Data centers (EQIX): ["data center demand", "cloud infrastructure spending", "AI data center requirements"]

INDUSTRIALS (aerospace, defense, automation):
â€¢ Aerospace (BA): ["aircraft orders", "defense spending", "commercial aviation demand"]
â€¢ Construction (CAT): ["construction activity", "equipment rental rates", "infrastructure spending"]

**Special Cases:**

DIVERSIFIED COMPANIES - Allocate by profit contribution:
â€¢ Amazon (AWS 60% profit): ["cloud infrastructure spending", "e-commerce sales", "digital advertising spending"]
â€¢ Alphabet (Ads 80%): ["digital advertising spending", "search advertising rates", "cloud infrastructure growth"]

COMMODITY PROCESSORS - Focus on SPREADS not absolute prices:
â€¢ Oil refiners (VLO): ["refining margins", "gasoline demand", "crude oil prices"]

CYCLICALS - Lead with demand, then financing/inputs:
â€¢ Homebuilders (DHI): ["housing starts", "mortgage rates", "lumber prices"]
â€¢ Autos (GM): ["auto sales", "EV adoption", "semiconductor supply"]

REGULATORY-DRIVEN - If >30% value from policy:
â€¢ Pharma pricing risk: ["drug pricing policy", "prescription volumes", "biosimilar competition"]

NICHE LEADERS - If >70% market share, allow category-specific:
â€¢ ResMed: ["sleep apnea treatment", "CPAP reimbursement", "respiratory device sales"]

**Validation (Score 0-10):**
1. Headline Frequency: Daily (10), weekly (7-9), monthly (4-6), rare (0-3)
2. Quantifiable: Always has figures (10), usually (7-9), sometimes (4-6), rarely (0-3)
3. Stock Impact: 10% move = >10% EPS (10), 5-10% EPS (7-9), 2-5% EPS (4-6), <2% EPS (0-3)
4. Trader Monitoring: Core KPI (10), important (7-9), secondary (4-6), ignored (0-3)

Examples:
âœ… "copper price" (FCX): 40/40 excellent
âŒ "mining technology": 12/40 reject (use "copper supply")
âœ… "cloud infrastructure spending" (MSFT): 37/40 excellent

**Common Mistakes:**
âŒ "Copper Mining" â†’ âœ… "copper price"
âŒ "Apple iPhone sales" â†’ âœ… "smartphone sales"
âŒ "Supply Chain" â†’ âœ… "semiconductor supply"
âŒ "Battery Technology" â†’ âœ… "electric vehicle sales"

BUSINESS STRUCTURE GUIDANCE:

For MOST companies (single core business):
- Standard approach: 3 keywords for primary business
- Example: Netflix â†’ ["streaming subscriber growth", "content spending", "streaming competition"]

For DIVERSIFIED companies (2-3 major business lines):
- Keywords should cover top 2-3 revenue segments (prioritize profit contributors)
- Horizontal competitors can be a MIX across segments (no single competitor may compete in all areas)
- Example: Amazon â†’ Keywords: ["cloud infrastructure spending", "e-commerce sales", "digital advertising spending"]
  Horizontal Competitors: Walmart (retail), Microsoft (cloud), Shopify (platform)

For CONGLOMERATES (Berkshire, 3M, Honeywell):
- Focus on 3 largest operating segments OR conglomerate-level themes
- Competitors: Other diversified industrials/conglomerates
- Example: Berkshire â†’ ["Property Casualty Insurance", "Railroad Operations", "Diversified Holdings"]
  Competitors: 3M, Honeywell, Danaher

For REGIONAL/GEOGRAPHIC companies (utilities, Canadian banks):
- Competitors MUST be in same primary market/regulatory environment
- Example: RY.TO â†’ Competitors: TD.TO, BMO.TO, BNS.TO (NOT JPM, BAC)
- Example: Southern Company â†’ Competitors: Duke Energy, Dominion, Entergy (all Southeast regulated utilities)

HORIZONTAL COMPETITORS (0-3):
Select direct business competitors competing for the same customers in the same markets.

**Selection Criteria:**
- Must compete in SAME CUSTOMER SEGMENT and price point (not just same industry)
- GEOGRAPHIC PRIORITY: For regional companies, prioritize competitors in same primary market
- Prioritize in order: (1) Direct peer (similar scale/model), (2) Adjacent competitor, (3) Bellwether (larger sector leader)
- Return 0-3 based on availability - quality over quantity
- Strongly prefer publicly traded companies with valid tickers
- Only include private if industry-dominant with significant newsflow (â‰¥5 major articles/month)
- Exclude: Subsidiaries, companies acquired in last 2 years, companies already listed in value chain

**Bad match examples:**
âŒ AvalonBay (luxury apartments) for ELME (value-add apartments) - different customer segment
âŒ IBM (enterprise conglomerate) for D-Wave (quantum startup) - different scale/focus
âŒ JPMorgan (universal bank) for Robinhood (retail trading) - different customer demographic

**Format:**
[
  {"name": "Company Name", "ticker": "TICKER"},
  {"name": "Company Name", "ticker": "TICKER"},
  {"name": "Company Name", "ticker": "TICKER"}
]

VALUE CHAIN (0-2 upstream, 0-2 downstream):
Select suppliers (upstream) and/or customers (downstream) that provide material intelligence signals.

**UPSTREAM (0-2 suppliers/input providers):**
Include if:
- Represents >10% of COGS, OR
- CRITICAL ENABLER: Sole-source/oligopoly (â‰¤3 global suppliers), would halt production if lost, specialized technology/patents (e.g., TSMC for Intel, ASML for semiconductor manufacturers, rare earth suppliers for EVs)

Rank by materiality (highest % of COGS first):
- Slot 1: Most material supplier
- Slot 2: Second most material supplier

**DOWNSTREAM (0-2 buyers/customers):**
Include if:
- Represents >5% of revenue, OR
- Provides demand signal proxy for market trends

Rank by materiality (highest % of revenue first):
- Slot 1: Most material customer
- Slot 2: Second most material customer

**General Rules:**
- Strongly prefer publicly traded companies (only private if â‰¥5 major articles/month or industry-dominant with pricing power)
- Geographic flexibility (can be national/international, unlike horizontal competitors)
- Exclude: Subsidiaries, companies acquired in last 2 years, companies already listed in horizontal competitors
- For conglomerates: Prioritize by absolute materiality across all segments (don't force coverage of every segment)
- Return empty arrays if no material value chain exists (direct-to-consumer brands, fragmented supply/customer base)

**Format:**
{
  "upstream": [
    {"name": "Supplier Name", "ticker": "TICKER"},
    {"name": "Supplier Name", "ticker": "TICKER"}
  ],
  "downstream": [
    {"name": "Customer Name", "ticker": "TICKER"},
    {"name": "Customer Name", "ticker": "TICKER"}
  ]
}

**Examples:**

Tesla (manufacturer with supplier dependency):
{
  "horizontal_competitors": [
    {"name": "BYD Company", "ticker": "BYDDY"},
    {"name": "Ford Motor", "ticker": "F"},
    {"name": "General Motors", "ticker": "GM"}
  ],
  "value_chain": {
    "upstream": [
      {"name": "Panasonic", "ticker": "PCRFY"},
      {"name": "CATL", "ticker": "300750.SZ"}
    ],
    "downstream": []
  },
  "industry_keywords": ["electric vehicle sales", "battery costs", "EV regulations"]
}

Intel (B2B component with major customers):
{
  "horizontal_competitors": [
    {"name": "AMD", "ticker": "AMD"},
    {"name": "NVIDIA", "ticker": "NVDA"},
    {"name": "Qualcomm", "ticker": "QCOM"}
  ],
  "value_chain": {
    "upstream": [
      {"name": "TSMC", "ticker": "TSM"}
    ],
    "downstream": [
      {"name": "Microsoft", "ticker": "MSFT"},
      {"name": "Apple", "ticker": "AAPL"}
    ]
  },
  "industry_keywords": ["x86 CPU demand", "semiconductor manufacturing capacity", "data center chip sales"]
}

Denison Mines (commodity producer):
{
  "horizontal_competitors": [
    {"name": "Cameco", "ticker": "CCJ"},
    {"name": "NexGen Energy", "ticker": "NXE"},
    {"name": "Energy Fuels", "ticker": "UUUU"}
  ],
  "value_chain": {
    "upstream": [],
    "downstream": [
      {"name": "Centrus Energy", "ticker": "LEU"},
      {"name": "Constellation Energy", "ticker": "CEG"}
    ]
  },
  "industry_keywords": ["uranium price", "nuclear power demand", "uranium supply"]
}

Netflix (direct-to-consumer, no value chain):
{
  "horizontal_competitors": [
    {"name": "Disney", "ticker": "DIS"},
    {"name": "Warner Bros Discovery", "ticker": "WBD"},
    {"name": "Paramount Global", "ticker": "PARA"}
  ],
  "value_chain": {
    "upstream": [],
    "downstream": []
  },
  "industry_keywords": ["streaming subscriber growth", "content spending", "streaming competition"]
}

GEOGRAPHIC MARKETS (string format):
- List the primary countries/regions where the company has significant operations, revenue, or customers
- Format: "Region1 (major), Region2 (major), Region3 (minor)"
- Use parenthetical notes to indicate scale: (major), (major), (minor)
- Be specific for major markets, broader for minor markets
- Examples:
  â†’ EchoStar: "United States (major), Europe (minor), Latin America (minor)"
  â†’ Royal Bank: "Canada (major), United States (major), Caribbean (minor)"
  â†’ Apple: "United States (major), China (major), Europe (major)"
  â†’ Regional utility: "United States - Southeast (major)"
- If truly global with balanced presence: "Global operations"
- If unknown or insufficient information: ""

SUBSIDIARIES (string format):
- List up to 3 major operating subsidiaries or business units
- Format: "Subsidiary Name 1, Subsidiary Name 2, Subsidiary Name 3"
- Use COMMON/BRAND names, not legal entities (e.g., "Hughes Network Systems" not "Hughes Network Systems LLC")
- Include only material subsidiaries (significant revenue/operations)
- Exclude: Recently acquired companies, minor divisions, brands (brands go in aliases_brands_assets)
- Examples:
  â†’ EchoStar: "Hughes Network Systems, Viasat Inc., EchoStar Mobile"
  â†’ Berkshire Hathaway: "GEICO, BNSF Railway, Berkshire Hathaway Energy"
  â†’ Alphabet: "Google, YouTube, Waymo"
  â†’ JPMorgan: "Chase Bank, J.P. Morgan Wealth Management, J.P. Morgan Securities"
- If no significant subsidiaries or holding company structure unclear: ""

Generate response in valid JSON format with all required fields. Be concise and precise."""

    context_info = f"Company: {company_name} ({ticker})"
    if sector:
        context_info += f", Sector: {sector}"
    if industry:
        context_info += f", Industry: {industry}"

    user_prompt = f"""Generate metadata for hedge fund news monitoring. Focus on precision to avoid irrelevant news articles.

{context_info}

Since we have basic company information, focus on generating specific industry keywords and direct competitors with accurate Yahoo Finance tickers.

CRITICAL: The "company_name" field should be: {company_name}

Required JSON format:
{{
    "ticker": "{ticker}",
    "company_name": "{company_name}",
    "sector": "{sector if sector else 'GICS Sector'}",
    "industry": "{industry if industry else 'GICS Industry'}",
    "sub_industry": "GICS Sub-Industry",
    "industry_keywords": ["keyword1", "keyword2", "keyword3"],
    "competitors": [
        {{"name": "Company Name", "ticker": "TICKER"}},
        {{"name": "Company Name", "ticker": "TICKER.TO"}},
        {{"name": "Company Name", "ticker": "TICKER"}}
    ],
    "sector_profile": {{
        "core_inputs": ["input1", "input2", "input3"],
        "core_channels": ["channel1", "channel2", "channel3"],
        "core_geos": ["geo1", "geo2", "geo3"],
        "benchmarks": ["benchmark1", "benchmark2", "benchmark3"]
    }},
    "aliases_brands_assets": {{
        "aliases": ["alias1", "alias2", "alias3"],
        "brands": ["brand1", "brand2", "brand3"],
        "assets": ["asset1", "asset2", "asset3"]
    }},
    "geographic_markets": "Region1 (major), Region2 (major), Region3 (minor)",
    "subsidiaries": "Subsidiary Name 1, Subsidiary Name 2, Subsidiary Name 3"
}}"""

    try:
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }

        data = {
            "model": OPENAI_MODEL,
            "input": f"{system_prompt}\n\n{user_prompt}",
            "max_output_tokens": 5000,
            "reasoning": {"effort": "medium"},
            "text": {
                "format": {"type": "json_object"},
                "verbosity": "low"
            },
            "truncation": "auto"
        }

        response = get_openai_session().post(OPENAI_API_URL, headers=headers, json=data, timeout=(10, 180))

        if response.status_code != 200:
            LOG.error(f"OpenAI metadata API error {response.status_code}: {response.text[:200]}")
            return None

        result = response.json()
        text = extract_text_from_responses(result)

        # Log usage
        u = result.get("usage", {}) or {}
        LOG.info("OpenAI metadata usage â€“ input:%s output:%s (cap:%s) status:%s",
                 u.get("input_tokens"), u.get("output_tokens"),
                 result.get("max_output_tokens"),
                 result.get("status"))

        if not text:
            LOG.warning(f"OpenAI metadata response empty for {ticker}")
            return None

        # Parse JSON
        metadata = parse_json_with_fallback(text, ticker)
        if not metadata:
            LOG.warning(f"OpenAI metadata JSON parsing failed for {ticker}")
            return None

        # Process results (same logic)
        def _list3(x):
            if isinstance(x, (list, tuple)):
                items = [item for item in list(x)[:3] if item]
                return items
            return []

        def _process_competitors(competitors_data):
            processed = []
            if not isinstance(competitors_data, list):
                return processed

            for comp in competitors_data[:3]:
                if isinstance(comp, dict):
                    name = comp.get('name', '').strip()
                    ticker_field = comp.get('ticker', '').strip()

                    if name:
                        processed_comp = {"name": name}
                        if ticker_field:
                            normalized_ticker = normalize_ticker_format(ticker_field)
                            if validate_ticker_format(normalized_ticker):
                                processed_comp["ticker"] = normalized_ticker
                            else:
                                LOG.warning(f"OpenAI provided invalid competitor ticker: {ticker_field}")
                        processed.append(processed_comp)
                elif isinstance(comp, str):
                    name = comp.strip()
                    if name:
                        processed.append({"name": name})

            return processed

        metadata.setdefault("ticker", ticker)
        metadata["name"] = company_name
        metadata["company_name"] = company_name

        metadata.setdefault("sector", sector)
        metadata.setdefault("industry", industry)
        metadata.setdefault("sub_industry", "")

        metadata["industry_keywords"] = _list3(metadata.get("industry_keywords", []))
        metadata["competitors"] = _process_competitors(metadata.get("competitors", []))

        sector_profile = metadata.setdefault("sector_profile", {})
        aliases_brands = metadata.setdefault("aliases_brands_assets", {})

        sector_profile["core_inputs"] = _list3(sector_profile.get("core_inputs", []))
        sector_profile["core_channels"] = _list3(sector_profile.get("core_channels", []))
        sector_profile["core_geos"] = _list3(sector_profile.get("core_geos", []))
        sector_profile["benchmarks"] = _list3(sector_profile.get("benchmarks", []))

        aliases_brands["aliases"] = _list3(aliases_brands.get("aliases", []))
        aliases_brands["brands"] = _list3(aliases_brands.get("brands", []))
        aliases_brands["assets"] = _list3(aliases_brands.get("assets", []))

        LOG.info(f"OpenAI metadata generated for {ticker}: {company_name}")
        return metadata

    except Exception as e:
        LOG.error(f"OpenAI metadata generation failed for {ticker}: {e}")
        return None


def generate_ticker_metadata_with_fallback(ticker: str, company_name: str = None, sector: str = "", industry: str = "") -> Optional[Dict]:
    """Main entry point: Try Claude first, fallback to OpenAI. Returns metadata dict"""
    metadata = None

    # Try Claude first (if enabled and API key available)
    if USE_CLAUDE_FOR_METADATA and ANTHROPIC_API_KEY:
        try:
            metadata = generate_claude_ticker_metadata(ticker, company_name, sector, industry)
            if metadata:
                LOG.info(f"âœ… Claude metadata generation succeeded for {ticker}")
                return metadata
            else:
                LOG.warning(f"Claude returned no metadata for {ticker}, falling back to OpenAI")
        except Exception as e:
            LOG.warning(f"Claude metadata generation failed for {ticker}, falling back to OpenAI: {e}")

    # Fallback to OpenAI
    if OPENAI_API_KEY:
        try:
            metadata = generate_openai_ticker_metadata(ticker, company_name, sector, industry)
            if metadata:
                LOG.info(f"âœ… OpenAI metadata generation succeeded for {ticker}")
                return metadata
        except Exception as e:
            LOG.error(f"OpenAI metadata generation also failed for {ticker}: {e}")

    return None


def parse_metadata_to_flat_fields(metadata: Dict) -> Dict:
    """
    Parse new metadata structure (horizontal_competitors + value_chain) into 14 flat database fields.

    Handles both old format ("competitors") and new format ("horizontal_competitors" + "value_chain").

    Returns dict with:
        competitor_1_name, competitor_1_ticker, ..., competitor_3_ticker (6 fields)
        upstream_1_name, upstream_1_ticker, upstream_2_name, upstream_2_ticker (4 fields)
        downstream_1_name, downstream_1_ticker, downstream_2_name, downstream_2_ticker (4 fields)
    """
    result = {
        # Initialize all 14 fields to None
        'competitor_1_name': None, 'competitor_1_ticker': None,
        'competitor_2_name': None, 'competitor_2_ticker': None,
        'competitor_3_name': None, 'competitor_3_ticker': None,
        'upstream_1_name': None, 'upstream_1_ticker': None,
        'upstream_2_name': None, 'upstream_2_ticker': None,
        'downstream_1_name': None, 'downstream_1_ticker': None,
        'downstream_2_name': None, 'downstream_2_ticker': None,
    }

    # Parse horizontal_competitors (0-3)
    horizontal_competitors = metadata.get("horizontal_competitors", [])
    # Fallback for old format
    if not horizontal_competitors:
        horizontal_competitors = metadata.get("competitors", [])

    for i, comp in enumerate(horizontal_competitors[:3], 1):
        if isinstance(comp, dict):
            result[f'competitor_{i}_name'] = comp.get('name')
            result[f'competitor_{i}_ticker'] = comp.get('ticker')

    # Parse value_chain (0-2 upstream, 0-2 downstream)
    value_chain = metadata.get("value_chain", {})
    if isinstance(value_chain, dict):
        # Upstream (0-2)
        upstream = value_chain.get("upstream", [])
        for i, comp in enumerate(upstream[:2], 1):
            if isinstance(comp, dict):
                result[f'upstream_{i}_name'] = comp.get('name')
                result[f'upstream_{i}_ticker'] = comp.get('ticker')

        # Downstream (0-2)
        downstream = value_chain.get("downstream", [])
        for i, comp in enumerate(downstream[:2], 1):
            if isinstance(comp, dict):
                result[f'downstream_{i}_name'] = comp.get('name')
                result[f'downstream_{i}_ticker'] = comp.get('ticker')

    return result


def generate_enhanced_ticker_metadata_with_ai(ticker: str, company_name: str = None, sector: str = "", industry: str = "") -> Optional[Dict]:
    """
    Enhanced AI generation with company context from ticker reference table
    Now uses Claude primary with OpenAI fallback
    """
    return generate_ticker_metadata_with_fallback(ticker, company_name, sector, industry)

def get_or_create_enhanced_ticker_metadata(ticker: str) -> Dict:
    """Get ticker metadata from reference table. Returns database data for existing tickers.

    NOTE (Nov 2025): AI enhancement during daily processing has been DISABLED.
    - CSV (ticker_reference.csv) is the source of truth
    - AI enhancement functions still exist but are not called automatically
    - For future AI enhancement, use explicit admin endpoints
    """

    # Normalize ticker format for consistent lookup
    isolated_ticker = str(ticker).strip()
    normalized_ticker = normalize_ticker_format(isolated_ticker)

    LOG.info(f"[METADATA] Looking up ticker: '{isolated_ticker}' -> normalized: '{normalized_ticker}'")

    # Step 1: Use get_ticker_config for consistent data access
    config = get_ticker_config(normalized_ticker)

    if config:
        LOG.info(f"[METADATA] Found ticker_reference data for {ticker}: {config['company_name']}")

        # Build metadata from database - NO AI enhancement
        metadata = {
            "ticker": normalized_ticker,
            "company_name": config["company_name"],
            "name": config["company_name"],
            "sector": config.get("sector", ""),
            "industry": config.get("industry", ""),
            "sub_industry": config.get("sub_industry", ""),
            "industry_keywords": config.get("industry_keywords", []),
            "competitors": config.get("competitors", []),
            "value_chain": config.get("value_chain", {"upstream": [], "downstream": []})
        }

        # Copy additional fields from config if present
        if config.get("geographic_markets"):
            metadata["geographic_markets"] = config["geographic_markets"]
        if config.get("subsidiaries"):
            metadata["subsidiaries"] = config["subsidiaries"]

        return metadata
    
    # === No config row, fallback to AI generation + INSERT ===
    LOG.info("DEBUG: Entering fallback AI generation path")

    if OPENAI_API_KEY:
        # CRITICAL: Try to fetch company name from multiple sources BEFORE calling AI without a hint
        LOG.info(f"âš ï¸ No reference data found for {ticker}, trying external sources for company name...")

        # Try yfinance first (fast, free, but gets throttled easily)
        company_name_from_source = fetch_company_name_from_yfinance(normalized_ticker)
        source_used = "yfinance"

        # If yfinance fails, try Polygon.io (slower, rate limited, but more reliable)
        if not company_name_from_source:
            LOG.info(f"âš ï¸ yfinance failed for {ticker}, trying Polygon.io fallback...")
            company_name_from_source = fetch_company_name_from_polygon(normalized_ticker)
            source_used = "Polygon.io"

        if company_name_from_source:
            LOG.info(f"âœ… {source_used} provided company name: {company_name_from_source}")
            # Call AI with company name hint from external source
            ai_metadata = generate_enhanced_ticker_metadata_with_ai(
                normalized_ticker,
                company_name=company_name_from_source
            )
        else:
            LOG.warning(f"âš ï¸ Both yfinance and Polygon.io failed for {ticker}, calling AI without hint")
            # Last resort: Call AI without company name hint
            ai_metadata = generate_enhanced_ticker_metadata_with_ai(normalized_ticker)

        # Validate AI response - warn if company_name == ticker, but ALLOW it
        if ai_metadata and ai_metadata.get('company_name') == normalized_ticker:
            LOG.warning(f"âš ï¸ AI returned ticker symbol as company name for {ticker}")
            LOG.warning(f"   This means: 1) CSV not loaded, 2) yfinance failed, 3) Polygon.io failed, 4) AI failed")
            LOG.warning(f"   Continuing with company_name='{normalized_ticker}' to avoid crash")

        # Store the AI-generated data back to reference table for future use
        if ai_metadata:
            reference_data = {
                'ticker': normalized_ticker,
                'country': 'US',
                'company_name': ai_metadata.get('company_name', normalized_ticker),  # Fallback to ticker if missing
                'sector': ai_metadata.get('sector', ''),
                'industry': ai_metadata.get('industry', ''),
                'industry_keyword_1': ai_metadata.get('industry_keywords', [None])[0] if ai_metadata.get('industry_keywords') else None,
                'industry_keyword_2': ai_metadata.get('industry_keywords', [None, None])[1] if len(ai_metadata.get('industry_keywords', [])) > 1 else None,
                'industry_keyword_3': ai_metadata.get('industry_keywords', [None, None, None])[2] if len(ai_metadata.get('industry_keywords', [])) > 2 else None,
                'geographic_markets': ai_metadata.get('geographic_markets', ''),
                'subsidiaries': ai_metadata.get('subsidiaries', ''),
                'ai_generated': True,
                'data_source': 'ai_generated'
            }

            # Convert competitors and value chain to separate fields (14 fields total)
            flat_fields = parse_metadata_to_flat_fields(ai_metadata)
            reference_data.update(flat_fields)

            store_ticker_reference(reference_data)

        return ai_metadata or {"ticker": normalized_ticker, "company_name": normalized_ticker, "industry_keywords": [], "competitors": []}

    # Step 3: Final fallback - NO AI configured (use ticker as company name)
    LOG.warning(f"âš ï¸ No data found for {ticker} and no AI configured")
    LOG.warning(f"   Using ticker symbol as company name to avoid crash")
    return {"ticker": normalized_ticker, "company_name": normalized_ticker, "industry_keywords": [], "competitors": []}

def get_ticker_reference(ticker: str) -> Optional[Dict]:
    """Get ticker reference data from database"""
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT ticker, country, company_name, industry, sector,
                   exchange, active,
                   industry_keyword_1, industry_keyword_2, industry_keyword_3,
                   competitor_1_name, competitor_1_ticker,
                   competitor_2_name, competitor_2_ticker,
                   competitor_3_name, competitor_3_ticker,
                   upstream_1_name, upstream_1_ticker,
                   upstream_2_name, upstream_2_ticker,
                   downstream_1_name, downstream_1_ticker,
                   downstream_2_name, downstream_2_ticker,
                   ai_generated
            FROM ticker_reference
            WHERE ticker = %s AND active = TRUE
        """, (ticker,))
        return cur.fetchone()

def update_ticker_reference_ai_data(ticker: str, metadata: Dict):
    """UPSERT reference table with AI-generated enhancements (INSERT if new, UPDATE if exists)"""
    try:
        # Convert array format back to separate fields for database storage
        keywords = metadata.get("industry_keywords", [])
        keyword_1 = keywords[0] if len(keywords) > 0 else None
        keyword_2 = keywords[1] if len(keywords) > 1 else None
        keyword_3 = keywords[2] if len(keywords) > 2 else None

        # Convert competitors and value chain to separate fields (14 fields total)
        comp_data = parse_metadata_to_flat_fields(metadata)

        # Extract geographic_markets and subsidiaries
        geographic_markets = metadata.get('geographic_markets', '')
        subsidiaries = metadata.get('subsidiaries', '')

        LOG.info(f"DEBUG: UPSERT {ticker} with keywords={[keyword_1, keyword_2, keyword_3]} and competitors={comp_data}")

        with db() as conn, conn.cursor() as cur:
            # UPSERT: INSERT new ticker or UPDATE existing one
            cur.execute("""
                INSERT INTO ticker_reference (
                    ticker, company_name, sector, industry, sub_industry,
                    industry_keyword_1, industry_keyword_2, industry_keyword_3,
                    competitor_1_name, competitor_1_ticker,
                    competitor_2_name, competitor_2_ticker,
                    competitor_3_name, competitor_3_ticker,
                    upstream_1_name, upstream_1_ticker,
                    upstream_2_name, upstream_2_ticker,
                    downstream_1_name, downstream_1_ticker,
                    downstream_2_name, downstream_2_ticker,
                    geographic_markets, subsidiaries,
                    ai_generated, ai_enhanced_at, created_at, updated_at
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, TRUE, NOW(), NOW(), NOW())
                ON CONFLICT (ticker) DO UPDATE SET
                    industry_keyword_1 = EXCLUDED.industry_keyword_1,
                    industry_keyword_2 = EXCLUDED.industry_keyword_2,
                    industry_keyword_3 = EXCLUDED.industry_keyword_3,
                    competitor_1_name = EXCLUDED.competitor_1_name,
                    competitor_1_ticker = EXCLUDED.competitor_1_ticker,
                    competitor_2_name = EXCLUDED.competitor_2_name,
                    competitor_2_ticker = EXCLUDED.competitor_2_ticker,
                    competitor_3_name = EXCLUDED.competitor_3_name,
                    competitor_3_ticker = EXCLUDED.competitor_3_ticker,
                    upstream_1_name = EXCLUDED.upstream_1_name,
                    upstream_1_ticker = EXCLUDED.upstream_1_ticker,
                    upstream_2_name = EXCLUDED.upstream_2_name,
                    upstream_2_ticker = EXCLUDED.upstream_2_ticker,
                    downstream_1_name = EXCLUDED.downstream_1_name,
                    downstream_1_ticker = EXCLUDED.downstream_1_ticker,
                    downstream_2_name = EXCLUDED.downstream_2_name,
                    downstream_2_ticker = EXCLUDED.downstream_2_ticker,
                    geographic_markets = EXCLUDED.geographic_markets,
                    subsidiaries = EXCLUDED.subsidiaries,
                    ai_generated = TRUE,
                    ai_enhanced_at = NOW(),
                    updated_at = NOW()
            """, (
                normalize_ticker_format(ticker),
                metadata.get('company_name', ticker),
                metadata.get('sector') if metadata.get('sector') not in ['Unknown', ''] else None,
                metadata.get('industry') if metadata.get('industry') not in ['Unknown', ''] else None,
                metadata.get('sub_industry', ''),
                keyword_1, keyword_2, keyword_3,
                comp_data['competitor_1_name'],
                comp_data['competitor_1_ticker'],
                comp_data['competitor_2_name'],
                comp_data['competitor_2_ticker'],
                comp_data['competitor_3_name'],
                comp_data['competitor_3_ticker'],
                comp_data['upstream_1_name'],
                comp_data['upstream_1_ticker'],
                comp_data['upstream_2_name'],
                comp_data['upstream_2_ticker'],
                comp_data['downstream_1_name'],
                comp_data['downstream_1_ticker'],
                comp_data['downstream_2_name'],
                comp_data['downstream_2_ticker'],
                geographic_markets,
                subsidiaries
            ))

            LOG.info(f"âœ… UPSERT successful for {ticker} reference table with AI enhancements")

    except Exception as e:
        LOG.error(f"Failed to UPSERT ticker reference AI data for {ticker}: {e}")

def update_ticker_reference_financial_data(ticker: str, financial_data: Dict):
    """UPSERT reference table with financial data from yfinance (INSERT if new, UPDATE if exists)"""
    try:
        with db() as conn, conn.cursor() as cur:
            # UPSERT: INSERT new ticker or UPDATE existing one with financial data
            cur.execute("""
                INSERT INTO ticker_reference (
                    ticker, company_name,
                    financial_last_price, financial_price_change_pct, financial_yesterday_return_pct,
                    financial_ytd_return_pct, financial_market_cap, financial_enterprise_value,
                    financial_volume, financial_avg_volume, financial_analyst_target,
                    financial_analyst_range_low, financial_analyst_range_high, financial_analyst_count,
                    financial_analyst_recommendation, financial_snapshot_date,
                    created_at, updated_at
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                ON CONFLICT (ticker) DO UPDATE SET
                    financial_last_price = EXCLUDED.financial_last_price,
                    financial_price_change_pct = EXCLUDED.financial_price_change_pct,
                    financial_yesterday_return_pct = EXCLUDED.financial_yesterday_return_pct,
                    financial_ytd_return_pct = EXCLUDED.financial_ytd_return_pct,
                    financial_market_cap = EXCLUDED.financial_market_cap,
                    financial_enterprise_value = EXCLUDED.financial_enterprise_value,
                    financial_volume = EXCLUDED.financial_volume,
                    financial_avg_volume = EXCLUDED.financial_avg_volume,
                    financial_analyst_target = EXCLUDED.financial_analyst_target,
                    financial_analyst_range_low = EXCLUDED.financial_analyst_range_low,
                    financial_analyst_range_high = EXCLUDED.financial_analyst_range_high,
                    financial_analyst_count = EXCLUDED.financial_analyst_count,
                    financial_analyst_recommendation = EXCLUDED.financial_analyst_recommendation,
                    financial_snapshot_date = EXCLUDED.financial_snapshot_date,
                    updated_at = NOW()
            """, (
                normalize_ticker_format(ticker),
                ticker,  # Use ticker as fallback company_name if creating new row
                financial_data.get('financial_last_price'),
                financial_data.get('financial_price_change_pct'),
                financial_data.get('financial_yesterday_return_pct'),
                financial_data.get('financial_ytd_return_pct'),
                financial_data.get('financial_market_cap'),
                financial_data.get('financial_enterprise_value'),
                financial_data.get('financial_volume'),
                financial_data.get('financial_avg_volume'),
                financial_data.get('financial_analyst_target'),
                financial_data.get('financial_analyst_range_low'),
                financial_data.get('financial_analyst_range_high'),
                financial_data.get('financial_analyst_count'),
                financial_data.get('financial_analyst_recommendation'),
                financial_data.get('financial_snapshot_date')
            ))

            LOG.info(f"âœ… UPSERT successful for {ticker} with financial data (snapshot: {financial_data.get('financial_snapshot_date')})")

    except Exception as e:
        LOG.error(f"Failed to UPSERT ticker reference financial data for {ticker}: {e}")

def validate_metadata(metadata):
    """
    Validate metadata quality and return warnings
    """
    warnings = []
    
    # Forbidden generic keywords
    forbidden_keywords = [
        'Technology', 'Healthcare', 'Energy', 'Oil', 'Services', 
        'Software', 'Hardware', 'Consumer', 'Financial', 'Industrial'
    ]
    
    for keyword in metadata.get('industry_keywords', []):
        if keyword in forbidden_keywords:
            warnings.append(f"Generic keyword detected: '{keyword}'")
    
    # Check competitor ticker format
    ticker_pattern = r'^.+\([A-Z0-9]{1,8}(?:\.[A-Z]{1,4})?(?:-[A-Z])?\)$'
    for competitor in metadata.get('competitors', []):
        if not re.match(ticker_pattern, competitor):
            warnings.append(f"Invalid competitor format: '{competitor}'")
    
    # Check for generic benchmarks
    generic_benchmarks = ['S&P 500', 'NASDAQ', 'Dow Jones', 'Russell 2000']
    benchmarks = metadata.get('sector_profile', {}).get('benchmarks', [])
    
    generic_count = sum(1 for b in benchmarks if any(g in b for g in generic_benchmarks))
    if generic_count == len(benchmarks) and len(benchmarks) > 0:
        warnings.append("All benchmarks are generic market indices")
    
    # Check for generic core inputs
    generic_inputs = ['raw materials', 'capital', 'labor', 'technology', 'supply chain']
    core_inputs = metadata.get('sector_profile', {}).get('core_inputs', [])
    
    for inp in core_inputs:
        if inp.lower() in generic_inputs:
            warnings.append(f"Generic core input: '{inp}'")
    
    return warnings

def resolve_google_news_url(url: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """Wrapper for backward compatibility"""
    return domain_resolver.resolve_url_and_domain(url)

def get_or_create_formal_domain_name(domain: str) -> str:
    """Wrapper for backward compatibility"""
    return domain_resolver.get_formal_name(domain)


def replace_domains_with_formal_names(text: str) -> str:
    """
    Replace domain URLs with formal publication names (case-insensitive).

    Examples:
        "per carboncredits.com (Nov 3)" â†’ "per Carbon Credits (Nov 3)"
        "reported by reuters.com" â†’ "reported by Reuters"
        "finance.yahoo.com article" â†’ "Yahoo Finance article"
        "per newstrail.com" â†’ "per newstrail" (fallback: strip TLD if no formal name)

    Args:
        text: Text containing domain URLs

    Returns:
        Text with domains replaced by formal publication names
    """
    import re

    if not text or not isinstance(text, str):
        return text

    # Match domain.tld patterns (including subdomains like finance.yahoo.com)
    # Common TLDs: .com, .org, .net, .io, .edu, .gov, .in, .ca, .au, .co.uk
    pattern = r'\b([a-z0-9]+(?:[-\.][a-z0-9]+)*\.(?:com|org|net|io|edu|gov|in|ca|au|co\.uk))\b'

    # TLD pattern for fallback stripping
    tld_pattern = r'\.(com|org|net|io|edu|gov|in|ca|au|co\.uk)$'

    def replace_match(match):
        domain_url = match.group(1).lower()  # Normalize to lowercase
        formal_name = get_or_create_formal_domain_name(domain_url)

        # Fallback: If lookup still returned a domain-like string, strip the TLD
        # This handles unmapped domains like "newstrail.com" â†’ "newstrail"
        if re.search(tld_pattern, formal_name, re.IGNORECASE):
            formal_name = re.sub(tld_pattern, '', formal_name, flags=re.IGNORECASE)

        return formal_name

    # Case-insensitive replacement
    cleaned_text = re.sub(pattern, replace_match, text, flags=re.IGNORECASE)

    return cleaned_text


def clean_executive_summary_domains(json_output):
    """
    Recursively replace domain URLs with formal publication names in all text fields.

    Applies to Phase 1 JSON (before Phase 2 context is added).

    Args:
        json_output: Phase 1 JSON structure (dict, list, or primitive)

    Returns:
        Cleaned JSON structure with domains replaced by formal names
    """
    if isinstance(json_output, dict):
        cleaned = {}
        for key, value in json_output.items():
            if isinstance(value, str):
                # Replace domains in string fields
                cleaned[key] = replace_domains_with_formal_names(value)
            elif isinstance(value, (dict, list)):
                # Recurse into nested structures
                cleaned[key] = clean_executive_summary_domains(value)
            else:
                # Leave non-string primitives unchanged
                cleaned[key] = value
        return cleaned

    elif isinstance(json_output, list):
        # Clean each item in list
        return [clean_executive_summary_domains(item) for item in json_output]

    else:
        # Primitive value (int, bool, None, etc.)
        return json_output


def cleanup_domain_data():
    """One-time script to clean up existing domain data"""
    
    # Mapping of publication names to actual domains
    cleanup_mappings = {
        'yahoo finance': 'finance.yahoo.com',
        'reuters': 'reuters.com',
        'bloomberg': 'bloomberg.com',
        'cnbc': 'cnbc.com',
        'forbes': 'forbes.com',
        'business insider': 'businessinsider.com',
        'the motley fool': 'fool.com',
        'seeking alpha': 'seekingalpha.com',
        'marketwatch': 'marketwatch.com',
        'nasdaq': 'nasdaq.com',
        'thestreet': 'thestreet.com',
        'benzinga': 'benzinga.com',
        'tipranks': 'tipranks.com',
        'morningstar': 'morningstar.com',
        'investor\'s business daily': 'investors.com',
        'zacks investment research': 'zacks.com',
        'stocktwits': 'stocktwits.com',
        'globenewswire': 'globenewswire.com',
        'business wire': 'businesswire.com',
        'pr newswire': 'prnewswire.com',
        'financialcontent': 'financialcontent.com',
        'insider monkey': 'insidermonkey.com',
        'gurufocus': 'gurufocus.com',
        'american banker': 'americanbanker.com',
        'thinkadvisor': 'thinkadvisor.com',
        'investmentnews': 'investmentnews.com',
        'the globe and mail': 'theglobeandmail.com',
        'financial news london': 'fnlondon.com',
        'steel market update': 'steelmarketupdate.com',
        'times of india': 'timesofindia.indiatimes.com',
        'business standard': 'business-standard.com',
        'fortune india': 'fortuneindia.com',
        'the new indian express': 'newindianexpress.com',
        'quiver quantitative': 'quiverquant.com',
        'stock titan': 'stocktitan.net',
        'modern healthcare': 'modernhealthcare.com'
    }
    
    with db() as conn, conn.cursor() as cur:
        total_updated = 0
        
        for old_domain, new_domain in cleanup_mappings.items():
            cur.execute("""
                UPDATE articles
                SET domain = %s
                WHERE domain = %s
            """, (new_domain, old_domain))
            
            updated_count = cur.rowcount
            total_updated += updated_count
            
            if updated_count > 0:
                LOG.info(f"Updated {updated_count} records: '{old_domain}' -> '{new_domain}'")
        
        # Handle Yahoo regional consolidation
        cur.execute("""
            UPDATE articles
            SET domain = 'finance.yahoo.com'
            WHERE domain IN ('ca.finance.yahoo.com', 'uk.finance.yahoo.com', 'sg.finance.yahoo.com')
        """)
        yahoo_updated = cur.rowcount
        total_updated += yahoo_updated
        
        if yahoo_updated > 0:
            LOG.info(f"Consolidated {yahoo_updated} Yahoo regional domains to finance.yahoo.com")
        
        # Handle duplicate benzinga entries
        cur.execute("""
            UPDATE articles
            SET domain = 'benzinga.com'
            WHERE domain = 'benzinga'
        """)
        benzinga_updated = cur.rowcount
        total_updated += benzinga_updated
        
        if benzinga_updated > 0:
            LOG.info(f"Consolidated {benzinga_updated} benzinga entries to benzinga.com")
        
        LOG.info(f"Total domain cleanup: {total_updated} records updated")
        
        return total_updated

# ------------------------------------------------------------------------------
# Feed Processing
# ------------------------------------------------------------------------------
def parse_datetime(candidate) -> Optional[datetime]:
    """Parse various datetime formats"""
    if not candidate:
        return None
    if isinstance(candidate, datetime):
        return candidate if candidate.tzinfo else candidate.replace(tzinfo=timezone.utc)
    
    if hasattr(candidate, "tm_year"):
        try:
            return datetime.fromtimestamp(time.mktime(candidate), tz=timezone.utc)
        except:
            pass
    
    try:
        return datetime.fromisoformat(str(candidate))
    except:
        return None

def format_timestamp_est(dt: datetime) -> str:
    """Format datetime to EST without time emoji"""
    if not dt:
        return "N/A"

    # Ensure we have a timezone-aware datetime
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)

    # Convert to Eastern Time
    eastern = pytz.timezone('US/Eastern')
    est_time = dt.astimezone(eastern)

    # Format without emoji
    time_part = est_time.strftime("%I:%M%p").lower().lstrip('0')
    date_part = est_time.strftime("%b %d")

    return f"{date_part}, {time_part} EST"

def format_date_short(dt: datetime) -> str:
    """Format date as (Oct 1) for compact display in AI summaries"""
    if not dt:
        return "(N/A)"

    # Ensure we have a timezone-aware datetime
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)

    # Convert to Eastern Time
    eastern = pytz.timezone('US/Eastern')
    est_time = dt.astimezone(eastern)

    # Format as (Oct 1) or (Sep 29)
    return f"({est_time.strftime('%b %-d')})"

def is_within_24_hours(dt: datetime) -> bool:
    """Check if article is from last 24 hours for ðŸ†• badge"""
    if not dt:
        return False

    # Ensure we have a timezone-aware datetime
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)

    now = datetime.now(timezone.utc)
    time_diff = now - dt

    return time_diff.total_seconds() < (24 * 60 * 60)

def is_market_open() -> bool:
    """
    Check if US stock market is currently open.

    Market hours: Monday-Friday, 9:30am - 4:00pm Eastern Time
    Returns True only during regular trading hours.
    Does NOT account for holidays or early closes.

    Returns:
        bool: True if market is open, False otherwise
    """
    eastern = pytz.timezone('US/Eastern')
    now_et = datetime.now(timezone.utc).astimezone(eastern)

    # Check if weekend (Saturday=5, Sunday=6)
    if now_et.weekday() >= 5:
        return False

    # Check if during market hours (9:30am - 4:00pm)
    market_open = now_et.replace(hour=9, minute=30, second=0, microsecond=0)
    market_close = now_et.replace(hour=16, minute=0, second=0, microsecond=0)

    return market_open <= now_et < market_close

def insert_new_badges(ai_summary: str, articles: List[Dict]) -> str:
    """
    Post-process AI-generated summary to insert ðŸ†• badges for <24h articles.
    Matches dates in the summary against article published_at timestamps.
    """
    if not ai_summary or not articles:
        return ai_summary

    # Build a set of dates that are within 24 hours
    recent_dates = set()
    for article in articles:
        published_at = article.get("published_at")
        if published_at and is_within_24_hours(published_at):
            date_str = format_date_short(published_at)
            recent_dates.add(date_str)

    if not recent_dates:
        return ai_summary

    # Process line by line, inserting ðŸ†• where bullets have recent dates
    lines = ai_summary.split('\n')
    processed_lines = []

    for line in lines:
        # Check if this is a bullet point
        stripped = line.lstrip()
        if stripped.startswith('â€¢') or stripped.startswith('-'):
            # Check if any recent date appears in this line
            for recent_date in recent_dates:
                if recent_date in line:
                    # Only add ðŸ†• if not already present
                    if 'ðŸ†•' not in line:
                        # Insert ðŸ†• after the bullet symbol
                        line = line.replace('â€¢ ', 'â€¢ ðŸ†• ', 1)
                        line = line.replace('- ', '- ðŸ†• ', 1)
                    break

        processed_lines.append(line)

    return '\n'.join(processed_lines)

def is_description_valuable(title: str, description: str) -> bool:
    """Check if description adds value beyond the title"""
    if not description or len(description.strip()) < 10:
        return False
    
    # Clean both title and description for comparison
    clean_title = re.sub(r'[^\w\s]', ' ', title.lower()).strip()
    clean_desc = re.sub(r'[^\w\s]', ' ', description.lower()).strip()
    
    # Remove HTML tags from description
    clean_desc = re.sub(r'<[^>]+>', '', clean_desc)
    
    # Check for URL patterns (common in bad descriptions)
    url_patterns = [
        r'https?://',
        r'www\.',
        r'\.com',
        r'news\.google\.com',
        r'href=',
        r'CBM[A-Za-z0-9]{20,}',  # Google News encoded URLs
    ]
    
    for pattern in url_patterns:
        if re.search(pattern, description, re.IGNORECASE):
            return False
    
    # Check if description is just a truncated version of title
    title_words = set(clean_title.split())
    desc_words = set(clean_desc.split())
    
    # If description is mostly just title words, skip it
    if len(title_words) > 0:
        overlap = len(title_words.intersection(desc_words)) / len(title_words)
        if overlap > 0.8:  # 80% overlap suggests redundancy
            return False
    
    # Check if description starts with title (common redundancy)
    if clean_desc.startswith(clean_title[:min(len(clean_title), 50)]):
        return False
    
    # Check for single character descriptions
    if len(clean_desc.strip()) == 1:
        return False
    
    # Check for descriptions that are just fragments
    if len(clean_desc) < 30 and not clean_desc.endswith('.'):
        # Short fragments without proper ending are likely truncated/useless
        return False
    
    return True


async def generate_ai_final_summaries(articles_by_ticker: Dict[str, Dict[str, List[Dict]]]) -> Dict[str, Dict[str, str]]:
    """Generate executive summaries using Claude (primary) with OpenAI fallback"""
    if not ANTHROPIC_API_KEY and not OPENAI_API_KEY:
        LOG.warning("âš ï¸ EXECUTIVE SUMMARY: No API keys configured - skipping")
        return {}

    LOG.info(f"ðŸŽ¯ EXECUTIVE SUMMARY: Starting generation for {len(articles_by_ticker)} tickers")
    summaries = {}

    for ticker, categories in articles_by_ticker.items():
        config = get_ticker_config(ticker)
        if not config:
            LOG.warning(f"[{ticker}] No config found - skipping")
            continue

        company_name = config.get("name", ticker)

        # PHASE 1: Generate executive summary from articles only (NO filings)
        from modules.executive_summary_phase1 import (
            generate_executive_summary_phase1,
            validate_phase1_json
        )

        phase1_result = generate_executive_summary_phase1(
            ticker=ticker,
            categories=categories,
            config=config,
            anthropic_api_key=ANTHROPIC_API_KEY,
            gemini_api_key=GEMINI_API_KEY
        )

        ai_analysis_summary = None
        model_used = "none"
        json_output = None
        prompt_tokens = 0
        completion_tokens = 0
        generation_time_ms = 0

        if phase1_result:
            json_output = phase1_result["json_output"]
            model_used = phase1_result["model_used"]
            prompt_tokens = phase1_result.get("prompt_tokens", 0)
            completion_tokens = phase1_result.get("completion_tokens", 0)
            generation_time_ms = phase1_result.get("generation_time_ms", 0)

            # Validate JSON structure
            is_valid, error_msg = validate_phase1_json(json_output)
            if not is_valid:
                LOG.error(f"[{ticker}] Phase 1 JSON validation failed: {error_msg}")
                ai_analysis_summary = None
                json_output = None
            else:
                # Store JSON as string for database
                json_string = json.dumps(json_output, indent=2)
                ai_analysis_summary = json_string

                # Track Phase 1 cost based on which model was used
                phase1_usage = {
                    "input_tokens": prompt_tokens,
                    "output_tokens": completion_tokens
                }

                if "gemini" in model_used.lower():
                    # Gemini primary succeeded (uses Pro for Phase 1)
                    calculate_gemini_api_cost(phase1_usage, "executive_summary_phase1", model="pro", model_name=model_used)
                elif "claude" in model_used.lower():
                    # Claude fallback succeeded
                    calculate_claude_api_cost(phase1_usage, "executive_summary_phase1", model_name=model_used)
                else:
                    # Unknown or missing model (shouldn't happen, but log warning)
                    LOG.warning(f"[{ticker}] Phase 1 cost tracking: Unknown model '{model_used}', skipping cost tracking")

                LOG.info(f"âœ… EXECUTIVE SUMMARY (Phase 1 - {model_used}) [{ticker}]: Generated valid JSON ({len(json_string)} chars, {prompt_tokens} prompt tokens, {completion_tokens} completion tokens)")

                # Clean domain URLs in Phase 1 content (before Phase 2 adds context)
                LOG.info(f"[{ticker}] Cleaning domain URLs to formal publication names in Phase 1 content")
                json_output = clean_executive_summary_domains(json_output)
                # Update stored JSON string with cleaned version
                ai_analysis_summary = json.dumps(json_output, indent=2)

                # ========================================================================
                # PHASE 1.5: Known Information Filter
                # ========================================================================
                # When enabled: Filters out claims already known from SEC filings
                # before passing to Phase 2. Reduces redundancy in final output.
                # When disabled: Skips entirely (original Phase 1 behavior)
                # Toggle: system_config.phase_1_5_enabled
                # ========================================================================
                phase1_5_model_used = None  # Track for ai_models dict
                phase1_5_prompt_tokens = 0
                phase1_5_completion_tokens = 0
                phase1_5_generation_time_ms = 0

                try:
                    from modules.known_info_filter import filter_known_information, generate_known_info_filter_email, apply_filter_to_phase1, has_phase1_bullets

                    phase1_5_enabled = is_phase_1_5_enabled()

                    if phase1_5_enabled and has_phase1_bullets(json_output):
                        LOG.info(f"[{ticker}] Phase 1.5: Running known info filter (PRODUCTION MODE)")

                        filter_result = filter_known_information(
                            ticker=ticker,
                            phase1_json=json_output,
                            db_func=db,
                            gemini_api_key=GEMINI_API_KEY,
                            anthropic_api_key=ANTHROPIC_API_KEY
                        )

                        if filter_result:
                            # Track Phase 1.5 cost
                            phase1_5_model_used = filter_result.get("model_used", "")
                            phase1_5_prompt_tokens = filter_result.get("prompt_tokens", 0)
                            phase1_5_completion_tokens = filter_result.get("completion_tokens", 0)
                            phase1_5_thought_tokens = filter_result.get("thought_tokens", 0)  # Flash 3.0
                            phase1_5_cached_tokens = filter_result.get("cached_tokens", 0)    # Flash 3.0
                            phase1_5_generation_time_ms = filter_result.get("generation_time_ms", 0)

                            phase1_5_usage = {
                                "input_tokens": phase1_5_prompt_tokens,
                                "output_tokens": phase1_5_completion_tokens,
                                "thought_tokens": phase1_5_thought_tokens,
                                "cached_tokens": phase1_5_cached_tokens
                            }

                            if "gemini" in phase1_5_model_used.lower():
                                # Use flash3 pricing for Gemini 3.0 Flash Preview
                                calculate_gemini_api_cost(phase1_5_usage, "executive_summary_phase1_5", model="flash3", model_name=phase1_5_model_used)
                            elif "claude" in phase1_5_model_used.lower():
                                calculate_claude_api_cost(phase1_5_usage, "executive_summary_phase1_5", model_name=phase1_5_model_used)

                            # Send email for monitoring (always, even in production)
                            filter_email_html = generate_known_info_filter_email(ticker, filter_result)
                            send_email(
                                subject=f"Phase 1.5 Known Info Filter: {ticker}",
                                html_body=filter_email_html,
                                to=ADMIN_EMAIL
                            )

                            # Apply filter to Phase 1 JSON
                            json_output = apply_filter_to_phase1(json_output, filter_result)
                            ai_analysis_summary = json.dumps(json_output, indent=2)

                            # Log summary stats
                            summary = filter_result.get("summary", {})
                            LOG.info(f"[{ticker}] âœ… Phase 1.5 Applied: {summary.get('kept', 0)} kept, {summary.get('removed', 0)} removed ({phase1_5_model_used}, {phase1_5_prompt_tokens} prompt, {phase1_5_completion_tokens} completion)")
                        else:
                            LOG.warning(f"[{ticker}] Phase 1.5: Filter returned no results, using original Phase 1 JSON")
                    elif phase1_5_enabled:
                        # Phase 1.5 enabled but no bullets to filter (quiet day)
                        LOG.info(f"[{ticker}] Phase 1.5: Skipped (no bullets from Phase 1 to filter)")
                    else:
                        LOG.info(f"[{ticker}] Phase 1.5: Skipped (disabled in system_config)")

                except Exception as e:
                    # Phase 1.5 failure should NEVER block the main pipeline
                    LOG.warning(f"[{ticker}] Phase 1.5: Failed (non-blocking, using original Phase 1 JSON): {e}")

        # PHASE 2: Enrich Phase 1 with filing context (10-K, 10-Q, Transcript)
        final_json = json_output  # Default to Phase 1 JSON
        generation_phase = 'phase1'  # Track which phase completed
        phase2_prompt_tokens = 0
        phase2_completion_tokens = 0
        phase2_generation_time_ms = 0
        phase2_result = None  # Initialize to None (may be set if Phase 2 runs)

        if ai_analysis_summary and json_output:
            from modules.executive_summary_phase2 import (
                _fetch_available_filings,
                generate_executive_summary_phase2,
                validate_phase2_json,
                strip_escape_hatch_context,
                merge_phase1_phase2
            )

            # Fetch available filings (1-3)
            filings = _fetch_available_filings(ticker, db)

            if filings:
                filing_types = list(filings.keys())
                LOG.info(f"[{ticker}] Running Phase 2 with {len(filings)} filing source(s): {filing_types}")

                phase2_result = generate_executive_summary_phase2(
                    ticker=ticker,
                    phase1_json=json_output,
                    filings=filings,
                    config=config,
                    anthropic_api_key=ANTHROPIC_API_KEY,
                    db_func=db,
                    gemini_api_key=GEMINI_API_KEY  # Gemini primary, Claude fallback
                )

                if phase2_result:
                    # Validate enrichments structure (with partial acceptance)
                    is_valid, error_msg, valid_enrichments = validate_phase2_json(
                        phase2_result.get("enrichments", {}),
                        phase1_json=json_output,
                        ticker=ticker
                    )

                    if is_valid:
                        # Replace enrichments with filtered valid ones
                        phase2_result["enrichments"] = valid_enrichments

                        # Log validation results
                        LOG.info(f"[{ticker}] Phase 2 validation: {error_msg}")

                        # Strip escape hatch text for cleaner display
                        phase2_result = strip_escape_hatch_context(phase2_result)

                        # Merge Phase 2 enrichments into Phase 1 JSON
                        final_json = merge_phase1_phase2(json_output, phase2_result)
                        generation_phase = 'phase2'
                        phase2_prompt_tokens = phase2_result.get("prompt_tokens", 0)
                        phase2_completion_tokens = phase2_result.get("completion_tokens", 0)
                        phase2_generation_time_ms = phase2_result.get("generation_time_ms", 0)

                        # Track Phase 2 cost based on which model was used
                        phase2_usage = {
                            "input_tokens": phase2_prompt_tokens,
                            "output_tokens": phase2_completion_tokens
                        }
                        phase2_model = phase2_result.get("ai_model", "")

                        if "gemini" in phase2_model.lower():
                            # Gemini primary succeeded (uses Pro for Phase 2)
                            calculate_gemini_api_cost(phase2_usage, "executive_summary_phase2", model="pro", model_name=phase2_model)
                        elif "claude" in phase2_model.lower():
                            # Claude fallback succeeded
                            calculate_claude_api_cost(phase2_usage, "executive_summary_phase2", model_name=phase2_model)
                        else:
                            # Unknown or missing model (shouldn't happen, but log warning)
                            LOG.warning(f"[{ticker}] Phase 2 cost tracking: Unknown model '{phase2_model}', skipping cost tracking")

                        enrichment_count = len(valid_enrichments)
                        LOG.info(f"âœ… EXECUTIVE SUMMARY (Phase 2 - {model_used}) [{ticker}]: Enriched {enrichment_count} bullets ({phase2_prompt_tokens} prompt tokens, {phase2_completion_tokens} completion tokens, {phase2_generation_time_ms}ms)")
                    else:
                        LOG.error(f"[{ticker}] Phase 2 validation failed completely: {error_msg}")
                        LOG.warning(f"[{ticker}] Using Phase 1 output only (Phase 2 validation failed)")
                else:
                    LOG.warning(f"[{ticker}] Phase 2 generation failed, using Phase 1 only")
            else:
                LOG.info(f"[{ticker}] No filings available, skipping Phase 2")

            # Update ai_analysis_summary with final JSON (Phase 1 or Phase 2)
            ai_analysis_summary = json.dumps(final_json, indent=2)

        if ai_analysis_summary:
            # Save to database with model tracking and JSON
            # CRITICAL: Save ALL flagged article IDs (not just those with ai_summary)
            # This ensures regenerate shows same articles as original Email #3
            article_ids = []
            for category in ["company", "industry", "competitor", "value_chain"]:
                for article in categories.get(category, []):
                    if article.get("id"):
                        article_ids.append(article.get("id"))

            company_count = len([a for a in categories.get("company", []) if a.get("ai_summary")])
            industry_count = len([a for a in categories.get("industry", []) if a.get("ai_summary")])
            competitor_count = len([a for a in categories.get("competitor", []) if a.get("ai_summary")])
            value_chain_count = len([a for a in categories.get("value_chain", []) if a.get("ai_summary")])

            # Build AI models tracking dict
            ai_models = {
                "phase1": phase1_result.get("model_used") if phase1_result else None,  # Phase 1 still uses "model_used"
                "phase1_5": phase1_5_model_used,  # Phase 1.5 Known Info Filter (None if disabled/skipped)
                "phase2": phase2_result.get("ai_model") if phase2_result else None,  # Phase 2 uses "ai_model"
                "phase3": None,  # Phase 3 - updated via update_executive_summary_json()
                "phase4": None   # Phase 4 - updated via update_executive_summary_json()
            }

            # Save final merged JSON (Phase 1 or Phase 2)
            save_executive_summary(
                ticker,
                ai_analysis_summary,  # JSON string (Phase 1 or Phase 2)
                model_used.lower(),
                article_ids,
                company_count,
                industry_count,
                competitor_count,
                value_chain_count,
                summary_json=final_json,  # Structured JSON (Phase 1 or Phase 2)
                prompt_tokens=prompt_tokens + phase2_prompt_tokens,  # Combined tokens
                completion_tokens=completion_tokens + phase2_completion_tokens,  # Combined tokens
                generation_time_ms=generation_time_ms + phase2_generation_time_ms,  # Combined time
                ai_models=ai_models  # Track AI models used per phase
            )

            # Update generation_phase in database
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    UPDATE executive_summaries
                    SET generation_phase = %s
                    WHERE ticker = %s AND summary_date = CURRENT_DATE
                """, (generation_phase, ticker))
                conn.commit()
        else:
            LOG.warning(f"âš ï¸ EXECUTIVE SUMMARY [{ticker}]: Phase 1 generation failed")

        summaries[ticker] = {
            "ai_analysis_summary": ai_analysis_summary or "",
            "company_name": company_name,
            "industry_articles_analyzed": len([a for a in categories.get("industry", []) if a.get("ai_summary")]),
            "model_used": model_used  # Track which AI model generated the summary
        }

    LOG.info(f"ðŸŽ¯ EXECUTIVE SUMMARY: Completed - generated summaries for {len(summaries)} tickers")
    return summaries


async def generate_executive_summary_all_phases(
    ticker: str,
    articles_by_ticker: Dict[str, Dict[str, List[Dict]]],
    config: Dict,
    report_type: str = 'daily'
) -> Dict:
    """
    Generate complete executive summary (Phase 1 + Phase 2 + Phase 3).

    This is the SINGLE entry point for executive summary generation.
    Extracts AI generation from the email building functions for clean separation of concerns.

    Args:
        ticker: Stock ticker symbol
        articles_by_ticker: Articles organized by ticker and category
        config: Ticker configuration from get_ticker_config()
        report_type: 'daily' or 'weekly'

    Returns:
        Dict with keys:
        - phase3_json: Final merged JSON (Phase 1+2+3)
        - articles_by_ticker: Original articles (for Email #2)
        - success: bool
        - error: str (if failed)
    """
    LOG.info(f"[{ticker}] ðŸŽ¯ Starting executive summary generation (all phases)...")

    try:
        # CRITICAL FIX: Split value_chain into upstream/downstream BEFORE executive summary generation
        # The executive summary module expects separate upstream/downstream categories
        categories = articles_by_ticker.get(ticker, {})
        value_chain_articles = categories.get("value_chain", [])
        if value_chain_articles:
            upstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'upstream']
            downstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'downstream']
            categories["upstream"] = upstream_articles
            categories["downstream"] = downstream_articles
            LOG.debug(f"[{ticker}] Split value_chain: upstream={len(upstream_articles)}, downstream={len(downstream_articles)}")

        # ========================================================================
        # PHASE 1+2: Generate executive summary from articles (with filing enrichment)
        # ========================================================================
        # This calls generate_ai_final_summaries() which runs Phase 1 and Phase 2
        # and saves the merged result to the database
        # ========================================================================

        LOG.info(f"[{ticker}] ðŸ“ Running Phase 1+2 (articles + filing enrichment)...")
        summaries = await generate_ai_final_summaries(articles_by_ticker)

        if not summaries or ticker not in summaries:
            LOG.error(f"[{ticker}] âŒ Phase 1+2 generation failed - no summary returned")
            return {
                "success": False,
                "error": "Phase 1+2 generation failed",
                "phase3_json": None,
                "articles_by_ticker": articles_by_ticker
            }

        summary_data = summaries[ticker]
        ai_analysis_summary = summary_data.get("ai_analysis_summary")

        if not ai_analysis_summary:
            LOG.error(f"[{ticker}] âŒ Phase 1+2 generation failed - empty summary")
            return {
                "success": False,
                "error": "Phase 1+2 returned empty summary",
                "phase3_json": None,
                "articles_by_ticker": articles_by_ticker
            }

        LOG.info(f"[{ticker}] âœ… Phase 1+2 complete, summary saved to database")

        # ========================================================================
        # PHASE 3: Context Integration + Length Enforcement
        # ========================================================================
        # Load Phase 2 JSON from database and run Phase 3
        # ========================================================================

        LOG.info(f"[{ticker}] ðŸŽ¨ Running Phase 3 (context integration)...")

        from modules.executive_summary_phase3 import generate_executive_summary_phase3
        from modules.executive_summary_utils import filter_bullets_for_email3, mark_filtered_bullets, merge_filtered_bullets_back

        # Load Phase 2 merged JSON from database (just saved by Phase 1+2)
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT summary_text FROM executive_summaries
                WHERE ticker = %s AND summary_date = CURRENT_DATE
                ORDER BY generated_at DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()

        if not result or not result['summary_text']:
            LOG.error(f"[{ticker}] âŒ Failed to load Phase 2 JSON from database")
            return {
                "success": False,
                "error": "Failed to load Phase 2 JSON from database",
                "phase3_json": None,
                "articles_by_ticker": articles_by_ticker
            }

        # Parse Phase 2 merged JSON
        phase2_merged_json = json.loads(result['summary_text'])

        # MARK bullets with filter_status (for Email #2 display)
        LOG.info(f"[{ticker}] ðŸ” Marking bullets with filter status...")
        marked_phase2_json = mark_filtered_bullets(phase2_merged_json)

        # FILTER bullets before Phase 3 (remove relevance='none' or low impact + indirect)
        filtered_json_for_phase3 = filter_bullets_for_email3(phase2_merged_json)

        # Count removed bullets for logging
        def count_bullets(json_dict):
            count = 0
            sections = json_dict.get('sections', {})
            for section_name in ['major_developments', 'financial_performance', 'risk_factors',
                               'wall_street_sentiment', 'competitive_industry_dynamics',
                               'upcoming_catalysts', 'key_variables']:
                count += len(sections.get(section_name, []))
            return count

        original_count = count_bullets(phase2_merged_json)
        filtered_count = count_bullets(filtered_json_for_phase3)
        removed_count = original_count - filtered_count

        if removed_count > 0:
            LOG.info(f"[{ticker}] ðŸ—‘ï¸ Filtered {removed_count} low-quality bullets ({filtered_count}/{original_count} remaining)")
        else:
            LOG.info(f"[{ticker}] âœ… No bullets filtered (all {original_count} are high quality)")

        # Get Phase 3 primary model from system config
        primary_model = get_phase3_primary_model()
        LOG.info(f"[{ticker}] Phase 3 primary model: {primary_model}")

        # Generate Phase 3 with FILTERED JSON
        phase3_merged_json, phase3_usage = generate_executive_summary_phase3(
            ticker=ticker,
            phase2_merged_json=filtered_json_for_phase3,
            anthropic_api_key=ANTHROPIC_API_KEY,
            gemini_api_key=GEMINI_API_KEY,
            primary_model=primary_model
        )

        # Track Phase 3 cost
        if phase3_usage:
            phase3_model = phase3_usage.get("model", "")
            if "claude" in phase3_model.lower():
                calculate_claude_api_cost(phase3_usage, "executive_summary_phase3", model_name=phase3_model)
            elif "gemini" in phase3_model.lower():
                calculate_gemini_api_cost(phase3_usage, "executive_summary_phase3", model="pro", model_name=phase3_model)
            else:
                LOG.warning(f"[{ticker}] Phase 3 cost tracking: Unknown model '{phase3_model}'")

        if not phase3_merged_json:
            LOG.error(f"[{ticker}] âŒ Phase 3 returned no merged JSON")
            return {
                "success": False,
                "error": "Phase 3 returned no merged JSON",
                "phase3_json": None,
                "articles_by_ticker": articles_by_ticker
            }

        # MERGE filtered bullets back into Phase 3 JSON (for Email #2 display)
        # This adds filtered_out bullets with their filter_status preserved
        LOG.info(f"[{ticker}] ðŸ”— Merging filtered bullets back for Email #2 display...")
        phase3_merged_json = merge_filtered_bullets_back(phase3_merged_json, marked_phase2_json)

        # Update database with Phase 3 content (includes all bullets with filter_status)
        phase3_model = phase3_usage.get("model", "") if phase3_usage else None
        success = update_executive_summary_json(
            ticker=ticker,
            summary_json=phase3_merged_json,
            phase='phase3',
            model=phase3_model
        )

        if not success:
            LOG.error(f"[{ticker}] âŒ Failed to save Phase 3 to database")
            return {
                "success": False,
                "error": "Failed to save Phase 3 to database",
                "phase3_json": phase3_merged_json,  # Return JSON even if save failed
                "articles_by_ticker": articles_by_ticker
            }

        LOG.info(f"[{ticker}] âœ… Phase 3 complete and saved to database")

        # =====================================================================
        # PHASE 4: Generate paragraphs from surviving bullets (MANDATORY)
        # Phase 4 is the ONLY source for paragraph sections (bottom_line, upside, downside)
        # =====================================================================
        LOG.info(f"[{ticker}] ðŸ“ Running Phase 4 (paragraph generation from surviving bullets)...")

        from modules.executive_summary_phase4 import generate_executive_summary_phase4, post_process_phase4_dates

        phase4_result, phase4_usage = generate_executive_summary_phase4(
            ticker=ticker,
            phase3_json=phase3_merged_json,
            anthropic_api_key=ANTHROPIC_API_KEY,
            gemini_api_key=GEMINI_API_KEY,
            primary_model='claude'  # Claude Sonnet 4.5 primary
        )

        # Phase 4 is mandatory - fail if it doesn't return a result
        if not phase4_result:
            LOG.error(f"[{ticker}] âŒ Phase 4 generation failed - no paragraphs generated")
            return {
                "success": False,
                "error": "Phase 4 generation failed - no paragraphs",
                "phase3_json": phase3_merged_json,
                "articles_by_ticker": articles_by_ticker
            }

        # Track Phase 4 cost
        if phase4_usage:
            phase4_model = phase4_usage.get("model", "")
            if "claude" in phase4_model.lower():
                calculate_claude_api_cost(phase4_usage, "executive_summary_phase4", model_name=phase4_model)
            elif "gemini" in phase4_model.lower():
                calculate_gemini_api_cost(phase4_usage, "executive_summary_phase4", model="pro", model_name=phase4_model)

        # Post-process Phase 4 dates (compute date_range from bullet dates, not AI)
        phase4_result = post_process_phase4_dates(
            phase4_result=phase4_result,
            phase3_json=phase3_merged_json,
            report_type=report_type
        )

        # Add Phase 4 results to JSON
        phase3_merged_json['phase4'] = phase4_result
        LOG.info(f"[{ticker}] âœ… Phase 4 complete - paragraphs generated from surviving bullets")

        # Save Phase 4 to database (overwrites Phase 3 save with complete JSON)
        phase4_model = phase4_usage.get("model", "") if phase4_usage else None
        success = update_executive_summary_json(
            ticker=ticker,
            summary_json=phase3_merged_json,
            phase='phase4',
            model=phase4_model
        )

        if not success:
            LOG.error(f"[{ticker}] âŒ Failed to save Phase 4 to database")
            return {
                "success": False,
                "error": "Failed to save Phase 4 to database",
                "phase3_json": phase3_merged_json,
                "articles_by_ticker": articles_by_ticker
            }

        LOG.info(f"[{ticker}] âœ… Phase 4 saved to database - all phases complete")

        return {
            "success": True,
            "error": None,
            "phase3_json": phase3_merged_json,
            "articles_by_ticker": articles_by_ticker
        }

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ Executive summary generation failed: {e}")
        LOG.error(f"[{ticker}] Stacktrace: {traceback.format_exc()}")
        return {
            "success": False,
            "error": str(e),
            "phase3_json": None,
            "articles_by_ticker": articles_by_ticker
        }


# ------------------------------------------------------------------------------
# 3-EMAIL SYSTEM - Article Sorting and Email Functions
# ------------------------------------------------------------------------------

def sort_articles_chronologically(articles: List[Dict]) -> List[Dict]:
    """Sort articles by published_at DESC (newest first), regardless of quality or flagged status"""
    return sorted(
        articles,
        key=lambda x: x.get('published_at') or x.get('found_at') or datetime.min.replace(tzinfo=timezone.utc),
        reverse=True
    )


def send_email(subject: str, html_body: str, to: str | None = None, bcc: str | None = None) -> bool:
    """Send email with HTML body only (no attachments). Supports BCC (hidden from recipients)."""
    if not all([SMTP_HOST, SMTP_USERNAME, SMTP_PASSWORD, EMAIL_FROM]):
        LOG.error("SMTP not fully configured")
        return False

    # STAGING SAFETY: Block emails to non-whitelisted addresses
    if STAGING_MODE:
        all_recipients = [r for r in [to or ADMIN_EMAIL, bcc] if r]
        allowed_lower = [e.lower() for e in STAGING_ALLOWED_EMAILS]
        for recipient in all_recipients:
            if recipient.lower() not in allowed_lower:
                LOG.warning(f"âš ï¸ STAGING: Blocked email to {recipient} (not in whitelist)")
                return False

    try:
        recipient = to or ADMIN_EMAIL

        # multipart/alternative for text + HTML, wrapped in mixed not needed if no attachments
        msg = MIMEMultipart('alternative')
        msg["Subject"] = subject
        msg["From"] = EMAIL_FROM
        msg["To"] = recipient
        # NOTE: BCC header is NOT added to message (hidden from recipients)

        # Plain-text fallback
        text_body = "This email contains HTML content. Please view in an HTML-capable email client."
        msg.attach(MIMEText(text_body, "plain", "utf-8"))

        # HTML body
        msg.attach(MIMEText(html_body, "html", "utf-8"))

        LOG.info(f"Connecting to SMTP server: {SMTP_HOST}:{SMTP_PORT}")

        # Build recipient list (includes BCC, but not shown in headers)
        recipients = [recipient]
        if bcc:
            recipients.append(bcc)
            LOG.info(f"BCC enabled: {bcc} (hidden from recipient)")

        # Add timeout to SMTP operations
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT, timeout=60) as server:
            if SMTP_STARTTLS:
                LOG.info("Starting TLS...")
                server.starttls()
            LOG.info("Logging in to SMTP server...")
            server.login(SMTP_USERNAME, SMTP_PASSWORD)
            LOG.info("Sending email...")
            server.sendmail(EMAIL_FROM, recipients, msg.as_string())

        LOG.info(f"Email sent successfully to {recipient}" + (f" (BCC: {bcc})" if bcc else ""))
        return True

    except smtplib.SMTPException as e:
        LOG.error(f"SMTP error sending email: {e}")
        return False
    except Exception as e:
        LOG.error(f"Email send failed: {e}")
        LOG.error(f"Error details: {traceback.format_exc()}")
        return False


def send_beta_signup_notification(name: str, email: str, tickers: list, returning_user: bool = False, previous_cancelled_at: str = None) -> bool:
    """
    Send admin notification email for new or returning beta signup.

    Args:
        name: User's name
        email: User's email
        tickers: List of ticker symbols
        returning_user: True if this is a previously cancelled user re-subscribing
        previous_cancelled_at: Formatted date string of when they previously unsubscribed

    Returns True if email sent successfully, False otherwise.
    """
    try:
        # Get company names for better readability
        companies = []
        for ticker in tickers:
            config = get_ticker_config(ticker)
            if config:
                companies.append(f"{ticker} ({config.get('company_name', 'Unknown')})")
            else:
                companies.append(ticker)

        ticker_list_html = ''.join(f'<li>{c}</li>' for c in companies)

        # Different header and subject for returning users
        if returning_user:
            emoji = "ðŸ”„"
            header = "Returning User Signed Up!"
            header_color = "#7c3aed"  # Purple for returning
            previous_unsub_html = f"""
                <p style="margin: 8px 0; color: #7c3aed; font-size: 14px;">
                    <strong>Previously unsubscribed:</strong> {previous_cancelled_at or 'Unknown date'}
                </p>
            """ if previous_cancelled_at else ""
        else:
            emoji = "ðŸŽ‰"
            header = "New Beta User Signed Up!"
            header_color = "#1e40af"  # Blue for new
            previous_unsub_html = ""

        html_body = f"""
        <div style="font-family: Arial, sans-serif; max-width: 600px;">
            <h2 style="color: {header_color};">{emoji} {header}</h2>

            <div style="background: #f3f4f6; padding: 20px; border-radius: 8px; margin: 20px 0;">
                <p style="margin: 8px 0;"><strong>Name:</strong> {name}</p>
                <p style="margin: 8px 0;"><strong>Email:</strong> {email}</p>
                {previous_unsub_html}
                <p style="margin: 8px 0;"><strong>Tracking ({len(tickers)} ticker{'s' if len(tickers) > 1 else ''}):</strong></p>
                <ul style="margin: 8px 0; padding-left: 20px;">
                    {ticker_list_html}
                </ul>
                <p style="margin: 8px 0; color: #6b7280; font-size: 14px;">
                    <strong>Signed up:</strong> {datetime.now(pytz.timezone('America/New_York')).strftime('%B %d, %Y at %I:%M %p %Z')}
                </p>
            </div>

            <p style="color: #6b7280; font-size: 14px;">
                This user will be included in tomorrow's 7 AM processing run.
            </p>
        </div>
        """

        subject = f"{emoji} {'Returning' if returning_user else 'New Beta'} User: {name} tracking {', '.join(tickers)}"

        return send_email(subject=subject, html_body=html_body, to=ADMIN_EMAIL)

    except Exception as e:
        LOG.error(f"Failed to send beta signup notification: {e}")
        return False


def send_welcome_email(user_id: int, name: str, email: str) -> bool:
    """
    Send welcome email to newly approved user.
    Called when admin approves a pending user (status: pending â†’ active).

    Args:
        user_id: User ID for fetching tickers and unsubscribe token
        name: User's full name (will extract first name)
        email: User's email address

    Returns:
        True if email sent successfully, False otherwise.
    """
    try:
        # Extract first name
        first_name = name.split()[0] if name else "there"

        # Fetch user's tickers with company names
        tickers_display = []
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker FROM user_tickers
                WHERE user_id = %s
                ORDER BY id
            """, (user_id,))
            rows = cur.fetchall()

            for row in rows:
                ticker = row['ticker']
                config = get_ticker_config(ticker)
                company_name = config.get('company_name', ticker) if config else ticker
                tickers_display.append(f"{ticker} ({company_name})")

        if not tickers_display:
            LOG.warning(f"No tickers found for user {user_id}, skipping welcome email")
            return False

        tickers_str = ", ".join(tickers_display)

        # Get or create unsubscribe token
        unsubscribe_token = get_or_create_unsubscribe_token(email)
        if unsubscribe_token:
            unsubscribe_url = f"https://weavara.io/unsubscribe?token={unsubscribe_token}"
        else:
            unsubscribe_url = "https://weavara.io/unsubscribe"
            LOG.warning(f"No unsubscribe token for {email}, using generic link")

        # Render welcome email template
        from jinja2 import Environment, FileSystemLoader
        template_env = Environment(loader=FileSystemLoader('templates'))
        template = template_env.get_template('email_welcome.html')

        html_body = template.render(
            user_name=first_name,
            tickers=tickers_str,
            unsubscribe_url=unsubscribe_url
        )

        subject = "Welcome to Weavara"

        success = send_email(subject=subject, html_body=html_body, to=email)

        if success:
            LOG.info(f"ðŸ“§ Welcome email sent to {email} (user {user_id})")
        else:
            LOG.error(f"Failed to send welcome email to {email}")

        return success

    except Exception as e:
        LOG.error(f"Error sending welcome email to {email}: {e}")
        LOG.error(traceback.format_exc())
        return False


def send_enhanced_quick_intelligence_email(articles_by_ticker: Dict[str, Dict[str, List[Dict]]], triage_results: Dict[str, Dict[str, List[Dict]]], time_window_minutes: int = 1440, mode: str = 'daily', report_type: str = 'daily') -> bool:
    """Email #1: Article Selection QA - Shows which articles were flagged by AI triage (NEW Nov 2025: report_type for subject labels)"""
    try:
        current_time_est = format_timestamp_est(datetime.now(timezone.utc))

        # Calculate period display from time window
        hours = time_window_minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        period_label = f"Last {days} days" if days > 0 else f"Last {int(hours)} hours"

        # Format ticker list with company names
        ticker_display_list = []
        for ticker in articles_by_ticker.keys():
            config = get_ticker_config(ticker)
            company_name = config.get("company_name", ticker) if config else ticker
            ticker_display_list.append(f"{company_name} ({ticker})")
        ticker_list = ', '.join(ticker_display_list)

        html = [
            "<html><head><style>",
            "body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 13px; line-height: 1.6; color: #333; }",
            "h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }",
            "h2 { color: #34495e; margin-top: 25px; border-bottom: 2px solid #ecf0f1; padding-bottom: 5px; }",
            "h3 { color: #7f8c8d; margin-top: 15px; margin-bottom: 8px; font-size: 14px; text-transform: uppercase; letter-spacing: 1px; }",
            ".article { margin: 8px 0; padding: 8px; border-left: 3px solid transparent; transition: all 0.3s; background-color: #fafafa; border-radius: 4px; }",
            ".company { border-left-color: #27ae60; }",
            ".industry { border-left-color: #f39c12; }",
            ".competitor { border-left-color: #e74c3c; }",
            ".value_chain { border-left-color: #9b59b6; }",
            ".upstream { border-left-color: #0891b2; }",
            ".downstream { border-left-color: #3b82f6; }",
            ".company-summary { background-color: #f0f8ff; padding: 15px; margin: 15px 0; border-radius: 8px; border-left: 4px solid #3498db; }",
            ".summary-title { font-weight: bold; color: #2c3e50; margin-bottom: 10px; font-size: 14px; }",
            ".summary-content { color: #34495e; line-height: 1.6; margin-bottom: 10px; white-space: pre-wrap; font-family: Arial, Helvetica, sans-serif; }",
            ".company-name-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e8f5e8; color: #2e7d32; border: 1px solid #a5d6a7; }",
            ".source-badge { display: inline-block; padding: 2px 8px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e9ecef; color: #495057; }",
            ".quality-badge { display: inline-block; padding: 2px 8px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e1f5fe; color: #0277bd; border: 1px solid #81d4fa; }",
            ".flagged-badge { display: inline-block; padding: 2px 8px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }",
            ".official-badge { display: inline-block; padding: 2px 8px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e8eaf6; color: #3f51b5; border: 1px solid #7986cb; }",
            ".ai-triage { display: inline-block; padding: 2px 8px; border-radius: 3px; font-weight: bold; font-size: 10px; margin-left: 5px; }",
            "/* Gemini Scoring - Green theme */",
            ".gemini-high { background-color: #d4f4dd; color: #0d5f1f; border: 1px solid #81c995; }",
            ".gemini-medium { background-color: #e8f5e9; color: #2e7d32; border: 1px solid #a5d6a7; }",
            ".gemini-low { background-color: #f1f8f2; color: #388e3c; border: 1px solid #c8e6c9; }",
            ".gemini-none { background-color: #f5f5f5; color: #9e9e9e; border: 1px solid #e0e0e0; }",
            ".qb-score { display: inline-block; padding: 2px 6px; border-radius: 3px; font-weight: bold; font-size: 10px; margin-left: 5px; }",
            "/* Claude Scoring - Purple theme */",
            ".claude-high { background-color: #f3e5f5; color: #6a1b9a; border: 1px solid #ce93d8; }",
            ".claude-medium { background-color: #f8e4f8; color: #8e24aa; border: 1px solid #e1bee7; }",
            ".claude-low { background-color: #fce4ec; color: #ab47bc; border: 1px solid #f8bbd0; }",
            ".claude-none { background-color: #f5f5f5; color: #9e9e9e; border: 1px solid #e0e0e0; }",
            ".competitor-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fdeaea; color: #c53030; border: 1px solid #feb2b2; }",
            ".industry-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fef5e7; color: #b7791f; border: 1px solid #f6e05e; }",
            ".value-chain-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #f3e5f5; color: #6a1b9a; border: 1px solid #ce93d8; }",
            ".upstream-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #cffafe; color: #0891b2; border: 1px solid #67e8f9; }",
            ".downstream-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #dbeafe; color: #3b82f6; border: 1px solid #93c5fd; }",
            ".summary { margin-top: 20px; padding: 15px; background-color: #ecf0f1; border-radius: 5px; }",
            ".ticker-section { margin-bottom: 40px; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }",
            ".meta { color: #95a5a6; font-size: 11px; }",
            "a { color: #2980b9; text-decoration: none; }",
            "a:hover { text-decoration: underline; }",
            "</style></head><body>",
        ]

        # Collect all industry keywords, competitors, and value chain for header
        all_industry_keywords = set()
        all_competitors = []
        all_upstream = []
        all_downstream = []
        for ticker in articles_by_ticker.keys():
            config = get_ticker_config(ticker)
            if config:
                keywords = config.get("industry_keywords", [])
                all_industry_keywords.update(keywords)
                comps = config.get("competitors", [])
                for comp in comps:
                    comp_display = f"{comp['name']} ({comp['ticker']})" if comp.get('ticker') else comp['name']
                    if comp_display not in all_competitors:
                        all_competitors.append(comp_display)

                # Collect upstream and downstream value chain
                value_chain = config.get("value_chain", {})

                # Upstream
                upstream = value_chain.get("upstream", [])
                for comp in upstream:
                    if isinstance(comp, dict) and comp.get('name'):
                        comp_display = f"{comp['name']} ({comp['ticker']})" if comp.get('ticker') else comp['name']
                        if comp_display not in all_upstream:
                            all_upstream.append(comp_display)

                # Downstream
                downstream = value_chain.get("downstream", [])
                for comp in downstream:
                    if isinstance(comp, dict) and comp.get('name'):
                        comp_display = f"{comp['name']} ({comp['ticker']})" if comp.get('ticker') else comp['name']
                        if comp_display not in all_downstream:
                            all_downstream.append(comp_display)

        total_articles = 0
        total_flagged = 0

        for ticker, categories in articles_by_ticker.items():
            ticker_count = sum(len(articles) for articles in categories.values())
            total_articles += ticker_count

            # Get company name from config for display
            config = get_ticker_config(ticker)
            full_company_name = config.get("name", ticker) if config else ticker

            # Count ONLY flagged articles (not quality domains)
            ticker_flagged = 0
            triage_data = triage_results.get(ticker, {})
            for category in ["company", "industry", "competitor", "value_chain"]:
                ticker_flagged += len(triage_data.get(category, []))

            total_flagged += ticker_flagged

        # Updated header with flagged count
        html.append(f"<h1>ðŸ” Article Selection QA: {ticker_list} - {total_flagged} flagged from {total_articles} articles</h1>")
        html.append(f"<div class='summary'>")
        html.append(f"<strong>ðŸ“… Report Period:</strong> {period_label}<br>")
        html.append(f"<strong>â° Generated:</strong> {current_time_est}<br>")
        html.append(f"<strong>ðŸ“Š Tickers Covered:</strong> {ticker_list}<br>")

        # Add industry keywords, competitors, and value chain to header
        if all_industry_keywords:
            html.append(f"<strong>ðŸ­ Industry Keywords:</strong> {', '.join(sorted(all_industry_keywords))}<br>")
        if all_competitors:
            html.append(f"<strong>âš”ï¸ Competitors:</strong> {', '.join(all_competitors)}<br>")

        # Always show upstream/downstream (even if empty) - internal email
        upstream_display = ', '.join(all_upstream) if all_upstream else "None"
        downstream_display = ', '.join(all_downstream) if all_downstream else "None"
        html.append(f"<strong>â¬†ï¸ Upstream:</strong> {upstream_display}<br>")
        html.append(f"<strong>â¬‡ï¸ Downstream:</strong> {downstream_display}<br>")

        html.append("</div>")

        for ticker, categories in articles_by_ticker.items():
            ticker_count = sum(len(articles) for articles in categories.values())

            # Get company name from config for display
            config = get_ticker_config(ticker)
            full_company_name = config.get("name", ticker) if config else ticker

            # Count ONLY flagged articles
            ticker_flagged = 0
            triage_data = triage_results.get(ticker, {})
            for category in ["company", "industry", "competitor", "value_chain"]:
                ticker_flagged += len(triage_data.get(category, []))

            html.append(f"<div class='ticker-section'>")
            html.append(f"<h2>ðŸŽ¯ Target Company: {full_company_name} ({ticker})</h2>")

            html.append(f"<p><strong>âœ… Flagged for Analysis:</strong> {ticker_flagged} articles</p>")

            # Split value_chain articles into upstream and downstream for separate display
            value_chain_articles = categories.get("value_chain", [])
            upstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'upstream']
            downstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'downstream']
            categories["upstream"] = upstream_articles
            categories["downstream"] = downstream_articles

            # Process each category independently with proper sorting
            category_icons = {
                "company": "ðŸŽ¯",
                "industry": "ðŸ­",
                "competitor": "âš”ï¸",
                "upstream": "â¬†ï¸",
                "downstream": "â¬‡ï¸"
            }

            for category in ["company", "industry", "competitor", "upstream", "downstream"]:
                articles = categories.get(category, [])
                if not articles:
                    continue

                # Map upstream/downstream to value_chain triage data
                triage_category = "value_chain" if category in ["upstream", "downstream"] else category
                category_triage = triage_data.get(triage_category, [])

                # Build flagged article database ID set (fixes index space mismatch for upstream/downstream)
                flagged_article_ids = set()
                article_scores = {}  # Map article DB ID -> {openai_score, claude_score}

                if category in ["upstream", "downstream"]:
                    # For upstream/downstream: map indices from COMBINED value_chain list
                    all_value_chain_articles = categories.get("value_chain", [])
                    for item in category_triage:
                        article_idx = item["id"]
                        if article_idx < len(all_value_chain_articles):
                            article = all_value_chain_articles[article_idx]
                            article_db_id = article.get("id")
                            if article_db_id:
                                flagged_article_ids.add(article_db_id)
                                article_scores[article_db_id] = {
                                    "gemini_score": item.get("gemini_score", 0),
                                    "claude_score": item.get("claude_score", 0)
                                }
                else:
                    # For company/industry/competitor: indices match directly
                    for item in category_triage:
                        article_idx = item["id"]
                        if article_idx < len(articles):
                            article = articles[article_idx]
                            article_db_id = article.get("id")
                            if article_db_id:
                                flagged_article_ids.add(article_db_id)
                                article_scores[article_db_id] = {
                                    "gemini_score": item.get("gemini_score", 0),
                                    "claude_score": item.get("claude_score", 0)
                                }

                # Chronological sorting - newest to oldest within category
                enhanced_articles = []
                for idx, article in enumerate(articles):
                    domain = normalize_domain(article.get("domain", ""))
                    article_db_id = article.get("id")
                    is_ai_selected = article_db_id in flagged_article_ids
                    is_quality_domain = domain in QUALITY_DOMAINS
                    is_problematic = domain in PROBLEMATIC_SCRAPE_DOMAINS

                    # Extract Gemini and Claude scores from article_scores dict
                    scores = article_scores.get(article_db_id, {})
                    gemini_score = scores.get("gemini_score", 0)
                    claude_score = scores.get("claude_score", 0)

                    enhanced_articles.append({
                        "article": article,
                        "idx": idx,
                        "is_ai_selected": is_ai_selected,
                        "is_quality_domain": is_quality_domain,
                        "is_problematic": is_problematic,
                        "gemini_score": gemini_score,
                        "claude_score": claude_score,
                        "published_at": article.get("published_at")
                    })

                # Sort chronologically (newest first)
                enhanced_articles.sort(key=lambda x: (
                    -(x["published_at"].timestamp() if x["published_at"] else 0)
                ))

                selected_count = len([a for a in enhanced_articles if a["is_ai_selected"]])

                html.append(f"<h3>{category_icons.get(category, 'ðŸ“°')} {category.title()} ({len(articles)} articles, {selected_count} flagged)</h3>")

                for enhanced_article in enhanced_articles[:50]:
                    article = enhanced_article["article"]
                    domain = article.get("domain", "unknown")
                    title = article.get("title", "No Title")

                    header_badges = []

                    # 1. First badge depends on category
                    if category == "company":
                        header_badges.append(f'<span class="company-name-badge">ðŸŽ¯ {full_company_name}</span>')
                    elif category == "competitor":
                        # Use company_name from article metadata (populated from feeds.company_name)
                        comp_name = article.get('company_name') or get_competitor_display_name(article.get('search_keyword'), article.get('feed_ticker'))
                        header_badges.append(f'<span class="competitor-badge">ðŸ¢ {comp_name}</span>')
                    elif category == "upstream":
                        # Show upstream supplier company name
                        vc_name = article.get('company_name') or article.get('search_keyword', 'Unknown')
                        header_badges.append(f'<span class="upstream-badge">ðŸ¢ {vc_name}</span>')
                    elif category == "downstream":
                        # Show downstream customer company name
                        vc_name = article.get('company_name') or article.get('search_keyword', 'Unknown')
                        header_badges.append(f'<span class="downstream-badge">ðŸ¢ {vc_name}</span>')
                    elif category == "industry" and article.get('search_keyword'):
                        header_badges.append(f'<span class="industry-badge">ðŸ­ {article["search_keyword"]}</span>')

                    # 2. Domain name second
                    header_badges.append(f'<span class="source-badge">ðŸ“° {get_or_create_formal_domain_name(domain)}</span>')

                    # 3. OFFICIAL badge for company releases (NEW - Nov 2025)
                    if article.get("is_company_release"):
                        header_badges.append('<span class="official-badge">ðŸ›ï¸ Official</span>')

                    # 4. Quality badge
                    if enhanced_article["is_quality_domain"]:
                        header_badges.append('<span class="quality-badge">â­ Quality</span>')

                    # 4. Flagged badge ONLY if dual AI selected it
                    if enhanced_article["is_ai_selected"]:
                        header_badges.append('<span class="flagged-badge">ðŸš© Flagged</span>')

                    # 5. Gemini Score - 1=High, 2=Medium, 3=Low, 0=None
                    gemini_score = enhanced_article.get("gemini_score", 0)
                    if gemini_score == 1:
                        gemini_class = "gemini-high"
                        gemini_level = "Gemini: High"
                        gemini_emoji = "ðŸ”¥"
                    elif gemini_score == 2:
                        gemini_class = "gemini-medium"
                        gemini_level = "Gemini: Medium"
                        gemini_emoji = "âš¡"
                    elif gemini_score >= 3:
                        gemini_class = "gemini-low"
                        gemini_level = "Gemini: Low"
                        gemini_emoji = "ðŸ”‹"
                    else:
                        gemini_class = "gemini-none"
                        gemini_level = "Gemini: None"
                        gemini_emoji = "â—‹"
                    header_badges.append(f'<span class="ai-triage {gemini_class}">{gemini_emoji} {gemini_level}</span>')

                    # 6. Claude Score - 1=High, 2=Medium, 3=Low, 0=None
                    claude_score = enhanced_article.get("claude_score", 0)
                    if claude_score == 1:
                        claude_class = "claude-high"
                        claude_level = "Claude: High"
                        claude_emoji = "ðŸ†"
                    elif claude_score == 2:
                        claude_class = "claude-medium"
                        claude_level = "Claude: Medium"
                        claude_emoji = "ðŸ’Ž"
                    elif claude_score >= 3:
                        claude_class = "claude-low"
                        claude_level = "Claude: Low"
                        claude_emoji = "ðŸ’¡"
                    else:
                        claude_class = "claude-none"
                        claude_level = "Claude: None"
                        claude_emoji = "â—‹"
                    header_badges.append(f'<span class="qb-score {claude_class}">{claude_emoji} {claude_level}</span>')

                    # Publication time
                    pub_time = ""
                    if article.get("published_at"):
                        pub_time = format_timestamp_est(article["published_at"])

                    html.append(f"""
                    <div class='article {category}'>
                        <div class='article-header'>
                            {' '.join(header_badges)}
                        </div>
                        <div class='article-content'>
                            <a href='{article.get("resolved_url") or article.get("url", "")}'>{title}</a>
                            <span class='meta'> | {pub_time}</span>
                        </div>
                    </div>
                    """)

            html.append("</div>")

        html.append(f"<div class='summary'>")
        html.append(f"<strong>ðŸ“Š Total Articles:</strong> {total_articles}<br>")
        html.append(f"<strong>âœ… Flagged for Analysis:</strong> {total_flagged}<br>")
        html.append(f"<strong>ðŸ“„ Next:</strong> Full content analysis and hedge fund summaries in progress...")
        html.append("</div>")
        html.append("</body></html>")

        html_content = "".join(html)
        # NEW (Nov 2025): Add report_type label to subject
        report_label = "(WEEKLY)" if report_type == 'weekly' else "(DAILY)"
        subject = f"QA Article Selection {report_label}: {ticker_list} - {total_flagged} flagged from {total_articles} articles"

        # Save Email #1 snapshot to database (for admin dashboard preview)
        # Skip saving for test runs - only save production runs
        if mode != 'test' and articles_by_ticker:
            # Email #1 is always single-ticker, so safe to use first key
            ticker = list(articles_by_ticker.keys())[0]
            try:
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        INSERT INTO email_queue (ticker, email_1_html, created_at)
                        VALUES (%s, %s, NOW())
                        ON CONFLICT (ticker) DO UPDATE
                        SET email_1_html = EXCLUDED.email_1_html,
                            updated_at = NOW()
                    """, (ticker, html_content))
                LOG.debug(f"[{ticker}] Saved Email #1 snapshot to database")
            except Exception as e:
                LOG.error(f"[{ticker}] Failed to save Email #1 snapshot: {e}")
                # Continue sending email even if snapshot save fails
        else:
            LOG.debug(f"[TEST MODE] Skipping Email #1 snapshot save")

        return send_email(subject, html_content)

    except Exception as e:
        LOG.error(f"Enhanced quick intelligence email failed: {e}")
        LOG.error(f"Email #1 traceback: {traceback.format_exc()}")
        return False


async def build_enhanced_digest_html(articles_by_ticker: Dict[str, Dict[str, List[Dict]]], period_days: int,
                              show_ai_analysis: bool = True, show_descriptions: bool = True,
                              flagged_article_ids: List[int] = None,
                              phase3_json: Dict = None) -> str:
    """Build Email #2 HTML (Content QA) from articles and Phase 3 executive summary.

    This is a PURE HTML building function - NO AI generation happens here.
    AI generation is handled separately by generate_executive_summary_all_phases().

    Args:
        articles_by_ticker: Articles organized by ticker and category
        period_days: Number of days covered in the report
        show_ai_analysis: If True, show AI analysis boxes under articles (default True)
        show_descriptions: If True, show article descriptions (default True)
        flagged_article_ids: Optional list of flagged article IDs for sorting priority
        phase3_json: Phase 3 merged JSON (REQUIRED). Must be pre-generated.

    Returns:
        HTML string for Email #2
    """

    if not phase3_json:
        raise ValueError("phase3_json is required - call generate_executive_summary_all_phases() first")

    # Split value_chain into upstream/downstream for display
    for ticker, categories in articles_by_ticker.items():
        value_chain_articles = categories.get("value_chain", [])
        if value_chain_articles:
            upstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'upstream']
            downstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'downstream']
            categories["upstream"] = upstream_articles
            categories["downstream"] = downstream_articles
            LOG.debug(f"[{ticker}] Split value_chain for display: upstream={len(upstream_articles)}, downstream={len(downstream_articles)}")

    # Format ticker list with company names
    ticker_display_list = []
    for ticker in articles_by_ticker.keys():
        config = get_ticker_config(ticker)
        company_name = config.get("company_name", ticker) if config else ticker
        ticker_display_list.append(f"{company_name} ({ticker})")
    ticker_list = ', '.join(ticker_display_list)
    current_time_est = format_timestamp_est(datetime.now(timezone.utc))

    html = [
        "<html><head><style>",
        "body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 13px; line-height: 1.6; color: #333; }",
        "h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }",
        "h2 { color: #34495e; margin-top: 25px; border-bottom: 2px solid #ecf0f1; padding-bottom: 5px; }",
        "h3 { color: #7f8c8d; margin-top: 15px; margin-bottom: 8px; font-size: 14px; text-transform: uppercase; letter-spacing: 1px; }",
        ".article { margin: 8px 0; padding: 8px; border-left: 3px solid transparent; transition: all 0.3s; background-color: #fafafa; border-radius: 4px; }",
        ".article:hover { background-color: #f0f8ff; border-left-color: #3498db; }",
        ".article-header { margin-bottom: 5px; }",
        ".article-content { }",
        ".company { border-left-color: #27ae60; }",
        ".industry { border-left-color: #f39c12; }",
        ".competitor { border-left-color: #e74c3c; }",
        ".value_chain { border-left-color: #9b59b6; }",
        ".upstream { border-left-color: #0891b2; }",
        ".downstream { border-left-color: #3b82f6; }",
        ".company-summary { background-color: #f0f8ff; padding: 15px; margin: 15px 0; border-radius: 8px; border-left: 4px solid #3498db; }",
        ".summary-title { font-weight: bold; color: #2c3e50; margin-bottom: 10px; font-size: 14px; }",
        ".summary-content { color: #34495e; line-height: 1.6; margin-bottom: 10px; white-space: pre-wrap; font-family: Arial, Helvetica, sans-serif; }",
        ".summary-section { margin: 12px 0; }",
        ".section-header { font-weight: bold; font-size: 15px; color: #2c3e50; margin-bottom: 5px; padding: 4px 0; border-bottom: 2px solid #ecf0f1; }",
        ".section-bullets { margin: 0 0 0 20px; padding: 0; list-style-type: disc; }",
        ".section-bullets li { margin: 4px 0; line-height: 1.4; color: #34495e; }",
        ".company-name-badge { display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 5px; font-weight: bold; font-size: 10px; background-color: #e8f5e8; color: #2e7d32; border: 1px solid #a5d6a7; }",
        ".source-badge { display: inline-block; padding: 2px 8px; margin-left: 0px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e9ecef; color: #495057; }",
        ".ai-model-badge { display: inline-block; padding: 2px 8px; margin-right: 8px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #f3e5f5; color: #6a1b9a; border: 1px solid #ce93d8; }",
        ".quality-badge { display: inline-block; padding: 2px 6px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e1f5fe; color: #0277bd; border: 1px solid #81d4fa; }",
        ".official-badge { display: inline-block; padding: 2px 8px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e8eaf6; color: #3f51b5; border: 1px solid #7986cb; }",
        ".analyzed-badge { display: inline-block; padding: 2px 8px; margin-left: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #e3f2fd; color: #1565c0; border: 1px solid #90caf9; }",
        ".competitor-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fdeaea; color: #c53030; border: 1px solid #feb2b2; }",
        ".industry-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #fef5e7; color: #b7791f; border: 1px solid #f6e05e; }",
        ".value-chain-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #f3e5f5; color: #6a1b9a; border: 1px solid #ce93d8; }",
        ".upstream-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #cffafe; color: #0891b2; border: 1px solid #67e8f9; }",
        ".downstream-badge { display: inline-block; padding: 2px 8px; margin-right: 5px; border-radius: 3px; font-weight: bold; font-size: 10px; background-color: #dbeafe; color: #3b82f6; border: 1px solid #93c5fd; }",
        ".description { color: #6c757d; font-size: 11px; font-style: italic; margin-top: 5px; line-height: 1.4; display: block; }",
        ".ai-summary { color: #2c5aa0; font-size: 12px; margin-top: 8px; line-height: 1.4; background-color: #f8f9ff; padding: 8px; border-radius: 4px; border-left: 3px solid #3498db; }",
        ".meta { color: #95a5a6; font-size: 11px; }",
        ".ticker-section { margin-bottom: 40px; padding: 20px; background-color: #ffffff; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }",
        ".summary { margin-top: 20px; padding: 15px; background-color: #ecf0f1; border-radius: 5px; }",
        "a { color: #2980b9; text-decoration: none; }",
        "a:hover { text-decoration: underline; }",
        "</style></head><body>",
        f"<h1>ðŸ“Š Stock Intelligence Report: {ticker_list}</h1>",
        f"<div class='summary'>",
        f"<strong>ðŸ“… Report Period:</strong> Last {period_days} days<br>",
        f"<strong>â° Generated:</strong> {current_time_est}<br>",
        f"<strong>ðŸ“Š Tickers Covered:</strong> {ticker_list}<br>"
    ]

    # Collect all industry keywords, competitors, and value chain for header (match triage email)
    all_industry_keywords = set()
    all_competitors = []
    all_upstream = []
    all_downstream = []
    for ticker in articles_by_ticker.keys():
        config = get_ticker_config(ticker)
        if config:
            keywords = config.get("industry_keywords", [])
            all_industry_keywords.update(keywords)
            comps = config.get("competitors", [])
            for comp in comps:
                comp_display = f"{comp['name']} ({comp['ticker']})" if comp.get('ticker') else comp['name']
                if comp_display not in all_competitors:
                    all_competitors.append(comp_display)

            # Collect upstream and downstream value chain
            value_chain = config.get("value_chain", {})

            # Upstream
            upstream = value_chain.get("upstream", [])
            for comp in upstream:
                if isinstance(comp, dict) and comp.get('name'):
                    comp_display = f"{comp['name']} ({comp['ticker']})" if comp.get('ticker') else comp['name']
                    if comp_display not in all_upstream:
                        all_upstream.append(comp_display)

            # Downstream
            downstream = value_chain.get("downstream", [])
            for comp in downstream:
                if isinstance(comp, dict) and comp.get('name'):
                    comp_display = f"{comp['name']} ({comp['ticker']})" if comp.get('ticker') else comp['name']
                    if comp_display not in all_downstream:
                        all_downstream.append(comp_display)

    # Add industry keywords, competitors, and value chain to header
    if all_industry_keywords:
        html.append(f"<strong>ðŸ­ Industry Keywords:</strong> {', '.join(sorted(all_industry_keywords))}<br>")
    if all_competitors:
        html.append(f"<strong>âš”ï¸ Competitors:</strong> {', '.join(all_competitors)}<br>")

    # Always show upstream/downstream (even if empty) - internal email
    upstream_display = ', '.join(all_upstream) if all_upstream else "None"
    downstream_display = ', '.join(all_downstream) if all_downstream else "None"
    html.append(f"<strong>â¬†ï¸ Upstream:</strong> {upstream_display}<br>")
    html.append(f"<strong>â¬‡ï¸ Downstream:</strong> {downstream_display}<br>")

    html.append("</div>")

    for ticker, categories in articles_by_ticker.items():
        # Get company name for display
        config = get_ticker_config(ticker)
        company_name = config.get("name", ticker) if config else ticker

        html.append(f"<div class='ticker-section'>")

        # Fetch live financial data (FMP primary â†’ yfinance fallback)
        # No Polygon fallback here since it doesn't have the metrics we need
        live_stock_data = get_stock_context(ticker)

        if live_stock_data:
            html.append("<div style='background:#f5f5f5; padding:12px; margin:16px 0; font-size:13px; border-left:3px solid #0066cc;'>")
            html.append(f"<div style='font-weight:bold; margin-bottom:6px;'>ðŸ“Š Market Context (Live)</div>")

            # Build 4-metric line: Market Cap | EV | Daily Volume (Xx Avg) | 52-Week Range
            metrics_parts = []

            # Market Cap
            mcap = format_financial_number(live_stock_data.get('financial_market_cap'))
            metrics_parts.append(f"Market Cap: {mcap}" if mcap else "Market Cap: N/A")

            # Enterprise Value
            ev = format_financial_number(live_stock_data.get('financial_enterprise_value'))
            metrics_parts.append(f"EV: {ev}" if ev else "EV: N/A")

            # Daily Volume with relative multiplier
            volume = live_stock_data.get('financial_volume')
            avg_volume = live_stock_data.get('financial_avg_volume')
            if volume and avg_volume and avg_volume > 0:
                rel_vol = volume / avg_volume
                metrics_parts.append(f"Daily Volume: {rel_vol:.1f}x Avg")
            else:
                metrics_parts.append("Daily Volume: N/A")

            # 52-Week Range
            year_high = live_stock_data.get('financial_year_high')
            year_low = live_stock_data.get('financial_year_low')
            if year_high and year_low:
                metrics_parts.append(f"52-Week: ${year_low:.2f} - ${year_high:.2f}")
            else:
                metrics_parts.append("52-Week: N/A")

            html.append(f"<div>{' | '.join(metrics_parts)}</div>")
            html.append("</div>")

        # Filing Sources box (Phase 2 Context) - shows what filings were available for enrichment
        try:
            filing_entries = []

            with db() as conn, conn.cursor() as cur:
                # Fetch latest Transcript
                cur.execute("""
                    SELECT 'Transcript' as filing_type, fiscal_quarter, fiscal_year, report_date as filing_date,
                           NULL as item_codes, NULL as exhibit_number,
                           CONCAT(fiscal_quarter, ' ', fiscal_year, ' Earnings Call') as description
                    FROM transcript_summaries
                    WHERE ticker = %s AND report_type = 'transcript'
                    ORDER BY fiscal_year DESC, fiscal_quarter DESC
                    LIMIT 1
                """, (ticker,))
                row = cur.fetchone()
                if row and row['filing_date']:
                    filing_entries.append({
                        'type': 'Transcript',
                        'description': row['description'],
                        'date': row['filing_date'],
                        'item_codes': None,
                        'exhibit_number': None
                    })

                # Fetch latest 10-Q
                cur.execute("""
                    SELECT 'SEC Filing' as filing_type, fiscal_quarter, fiscal_year, filing_date,
                           NULL as item_codes, NULL as exhibit_number,
                           CONCAT(fiscal_quarter, ' ', fiscal_year, ' 10-Q') as description
                    FROM sec_filings
                    WHERE ticker = %s AND filing_type = '10-Q'
                    ORDER BY fiscal_year DESC, fiscal_quarter DESC
                    LIMIT 1
                """, (ticker,))
                row = cur.fetchone()
                if row and row['filing_date']:
                    filing_entries.append({
                        'type': '10-Q',
                        'description': row['description'],
                        'date': row['filing_date'],
                        'item_codes': None,
                        'exhibit_number': None
                    })

                # Fetch latest 10-K
                cur.execute("""
                    SELECT 'SEC Filing' as filing_type, fiscal_year, filing_date,
                           NULL as item_codes, NULL as exhibit_number,
                           CONCAT('FY', fiscal_year, ' 10-K') as description
                    FROM sec_filings
                    WHERE ticker = %s AND filing_type = '10-K'
                    ORDER BY fiscal_year DESC
                    LIMIT 1
                """, (ticker,))
                row = cur.fetchone()
                if row and row['filing_date']:
                    filing_entries.append({
                        'type': '10-K',
                        'description': row['description'],
                        'date': row['filing_date'],
                        'item_codes': None,
                        'exhibit_number': None
                    })

                # Fetch 8-K filings (same filter as Phase 2 - no T-7 delay, after last transcript)
                # Get transcript date for time window
                cur.execute("""
                    SELECT report_date FROM transcript_summaries
                    WHERE ticker = %s AND report_type = 'transcript'
                    ORDER BY fiscal_year DESC, fiscal_quarter DESC
                    LIMIT 1
                """, (ticker,))
                transcript_row = cur.fetchone()
                transcript_date = transcript_row['report_date'] if transcript_row else None

                from datetime import date as date_type, timedelta
                today = date_type.today()
                max_lookback = today - timedelta(days=90)

                if transcript_date:
                    t_date = transcript_date.date() if hasattr(transcript_date, 'date') else transcript_date
                    start_date = max(t_date, max_lookback)
                else:
                    start_date = max_lookback

                cur.execute("""
                    SELECT filing_date, report_title, item_codes, exhibit_number
                    FROM company_releases
                    WHERE ticker = %s
                      AND source_type = '8k_exhibit'
                      AND filing_date > %s
                      AND filing_date <= %s
                      AND (
                          item_codes LIKE '%%1.01%%' OR item_codes LIKE '%%1.02%%' OR
                          item_codes LIKE '%%2.01%%' OR item_codes LIKE '%%2.02%%' OR
                          item_codes LIKE '%%2.03%%' OR item_codes LIKE '%%2.05%%' OR
                          item_codes LIKE '%%2.06%%' OR item_codes LIKE '%%4.01%%' OR
                          item_codes LIKE '%%4.02%%' OR item_codes LIKE '%%5.01%%' OR
                          item_codes LIKE '%%5.02%%' OR item_codes LIKE '%%5.07%%' OR
                          item_codes LIKE '%%7.01%%' OR item_codes LIKE '%%8.01%%' OR
                          item_codes = 'Unknown'
                      )
                      AND (exhibit_number LIKE '99%%' OR exhibit_number = 'MAIN' OR exhibit_number = '2.1')
                      AND report_title NOT ILIKE '%%Legal Opinion%%'
                      AND report_title NOT ILIKE '%%Underwriting Agreement%%'
                      AND report_title NOT ILIKE '%%Indenture%%'
                      AND report_title NOT ILIKE '%%Officers'' Certificate%%'
                      AND report_title NOT ILIKE '%%Notes Due%%'
                      AND report_title NOT ILIKE '%%Bylaws%%'
                    ORDER BY filing_date DESC, exhibit_number ASC
                """, (ticker, start_date, today))

                for row in cur.fetchall():
                    filing_entries.append({
                        'type': '8-K',
                        'description': row['report_title'],
                        'date': row['filing_date'],
                        'item_codes': row['item_codes'],
                        'exhibit_number': row['exhibit_number']
                    })

            # Sort all entries by date descending (newest first)
            filing_entries.sort(key=lambda x: x['date'] if x['date'] else date_type.min, reverse=True)

            if filing_entries:
                html.append("<div style='background:#f0f7ff; padding:12px; margin:16px 0; font-size:13px; border-left:3px solid #6366f1;'>")
                html.append("<div style='font-weight:bold; margin-bottom:6px;'>ðŸ“ Filing Sources (Phase 2 Context)</div>")
                html.append("<div>")

                for entry in filing_entries:
                    date_str = entry['date'].strftime('%b %d, %Y') if hasattr(entry['date'], 'strftime') else str(entry['date'])

                    if entry['type'] == '8-K':
                        # Full details for 8-K: name, date, items, exhibit
                        item_codes = entry['item_codes'] or 'Unknown'
                        exhibit = entry['exhibit_number'] or 'N/A'
                        # Truncate long titles
                        title = entry['description'][:60] + '...' if len(entry['description'] or '') > 60 else entry['description']
                        html.append(f"â€¢ <strong>8-K:</strong> {title} ({date_str}) [Items: {item_codes}] [Ex: {exhibit}]<br>")
                    else:
                        # Simple format for Transcript, 10-Q, 10-K
                        html.append(f"â€¢ <strong>{entry['type']}:</strong> {entry['description']} ({date_str})<br>")

                html.append("</div>")
                html.append("</div>")
        except Exception as e:
            LOG.warning(f"[{ticker}] Could not fetch filing sources for Email #2: {e}")

        html.append(f"<h2>ðŸŽ¯ Target Company: {company_name} ({ticker})</h2>")

        # Display executive summary
        # Phase 3 format with deduplication info (REQUIRED - Nov 2025)
        # phase3_json must always be provided - Email #2 is only sent after Phase 3 completes
        if not phase3_json:
            LOG.error(f"[{ticker}] phase3_json is required for Email #2 - this should never happen")
            html.append("<div class='company-summary'>")
            html.append("<div class='summary-title'>ðŸ“° Executive Summary - ERROR</div>")
            html.append("<div class='summary-content'>Phase 3 JSON not provided. Email #2 requires Phase 3 to be completed first.</div>")
            html.append("</div>")
        else:
            from modules.executive_summary_phase1 import convert_phase3_to_email2_sections

            # Fetch model info from database
            models_display = "Phase 1+2+3"
            try:
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        SELECT ai_models FROM executive_summaries
                        WHERE ticker = %s AND summary_date = CURRENT_DATE
                        ORDER BY generated_at DESC LIMIT 1
                    """, (ticker,))
                    result = cur.fetchone()
                    if result and result.get('ai_models'):
                        ai_models = result['ai_models']
                        if isinstance(ai_models, dict):
                            p1 = ai_models.get('phase1', 'N/A')
                            p1_5 = ai_models.get('phase1_5')  # May be None if disabled
                            p2 = ai_models.get('phase2', 'N/A')
                            p3 = ai_models.get('phase3', 'N/A')
                            p4 = ai_models.get('phase4', 'N/A')
                            if p1_5:
                                models_display = f"P1: {p1} | P1.5: {p1_5} | P2: {p2} | P3: {p3} | P4: {p4}"
                            else:
                                models_display = f"P1: {p1} | P1.5: OFF | P2: {p2} | P3: {p3} | P4: {p4}"
            except Exception as e:
                LOG.warning(f"[{ticker}] Could not fetch model info: {e}")

            html.append("<div class='company-summary'>")
            html.append(f"<div class='summary-title'>ðŸ“° Executive Summary (with Deduplication) - {models_display}</div>")
            html.append("<div class='summary-content'>")

            sections = convert_phase3_to_email2_sections(phase3_json)
            summary_html = build_executive_summary_html(sections, strip_emojis=False)
            html.append(summary_html)

            html.append("</div>")
            html.append("</div>")

        # NOTE: value_chain was already split into upstream/downstream at function start (line 20310)
        # This code kept for backward compatibility and debugging visibility
        value_chain_articles = categories.get("value_chain", [])

        # DEBUG: Log value_chain articles and their types
        LOG.info(f"[DEBUG] Email #2 value_chain count: {len(value_chain_articles)}")
        if value_chain_articles:
            for i, a in enumerate(value_chain_articles[:5]):  # Show first 5
                LOG.info(f"  [{i}] ID={a.get('id')} | value_chain_type='{a.get('value_chain_type')}' | title={a.get('title', '')[:50]}")

        # Re-confirm split (already done at function start, but kept for visibility)
        upstream_articles = categories.get("upstream", [])
        downstream_articles = categories.get("downstream", [])

        LOG.info(f"[DEBUG] After filtering: upstream={len(upstream_articles)}, downstream={len(downstream_articles)}")

        # Sort articles chronologically within each category (newest first)
        for category in ["company", "industry", "competitor", "upstream", "downstream"]:
            if category in categories and categories[category]:
                articles = categories[category]

                # Simple chronological sorting (newest first)
                articles = sort_articles_chronologically(articles)

                category_icons = {
                    "company": "ðŸŽ¯",
                    "industry": "ðŸ­",
                    "competitor": "âš”ï¸",
                    "upstream": "â¬†ï¸",
                    "downstream": "â¬‡ï¸"
                }

                html.append(f"<h3>{category_icons.get(category, 'ðŸ“°')} {category.replace('_', ' ').title()} News ({len(articles)} articles)</h3>")
                for article in articles[:100]:
                    # Use simplified ticker metadata cache (just company name)
                    simple_cache = {ticker: {"company_name": company_name}}
                    html.append(_format_article_html_with_ai_summary(article, category, simple_cache,
                                                                     show_ai_analysis, show_descriptions))

        html.append("</div>")

    # Add Article IDs footer for debugging and transparency
    if flagged_article_ids:
        article_ids_str = str(flagged_article_ids)  # Converts to "[1451, 1084, ...]"
        html.append(f"""
        <div style='margin-top: 30px; padding: 15px; background-color: #f0f9ff;
                    border-left: 4px solid #3b82f6; border-radius: 5px;'>
            <p style='margin: 0; font-size: 13px; font-weight: bold; color: #1e40af;'>
                ðŸ“Š All Article IDs Used in This Run ({len(flagged_article_ids)}):
            </p>
            <p style='margin: 8px 0 0 0; font-size: 12px; font-family: monospace;
                      color: #374151; word-break: break-all; line-height: 1.6;'>
                {article_ids_str}
            </p>
        </div>
    """)

    html.append("""
        <div style='margin-top: 30px; padding: 15px; background-color: #f8f9fa; border-radius: 5px; font-size: 11px; color: #6c757d;'>
            <strong>ðŸ¤– Enhanced AI Features:</strong><br>
            ðŸ“Š Investment Thesis: AI synthesis of all company deep analysis<br>
            ðŸ“° Content Analysis: Full article scraping with intelligent extraction<br>
            ðŸ’¼ Hedge Fund Summaries: AI-generated analytical summaries for scraped content<br>
            ðŸŽ¯ Enhanced Selection: AI Triage â†’ Quality Domains â†’ Exclude Problematic â†’ QB Score Backfill<br>
            âœ… "Analyzed" badge indicates articles with both scraped content and AI summary<br>
            â­ "Quality" badge indicates high-authority news sources
        </div>
        </body></html>
    """)

    html_content = "".join(html)
    return html_content


# ------------------------------------------------------------------------------
# Structured Summary Parsing for Emails
# ------------------------------------------------------------------------------

def parse_structured_summary(summary_text: str) -> list:
    """
    Parse AI-generated structured summary into sections with headers and bullets.

    Expected format:
    ðŸ”´ MAJOR DEVELOPMENTS
    â€¢ Bullet point 1
    â€¢ Bullet point 2

    ðŸ“Š FINANCIAL/OPERATIONAL PERFORMANCE
    â€¢ Bullet point 1
    """
    if not summary_text:
        return []

    sections = []
    current_section = None

    # Known section emojis to detect headers (includes new sections)
    section_emojis = ['ðŸ“Œ', 'ðŸ”´', 'ðŸ“Š', 'âš ï¸', 'ðŸ“ˆ', 'âš¡', 'ðŸ“…', 'ðŸŽ¯']

    # Special sections that contain paragraph/structured text (not just bullets)
    special_sections = ['ðŸ“Œ BOTTOM LINE', 'ðŸŽ¯ INVESTMENT IMPLICATIONS']

    for line in summary_text.split('\n'):
        line = line.strip()

        # Skip empty lines
        if not line:
            continue

        # Detect section headers (lines with section emojis, not starting with bullet)
        is_header = False
        if not line.startswith('â€¢') and not line.startswith('-'):
            # Check if line contains any section emoji
            for emoji in section_emojis:
                if emoji in line:
                    is_header = True
                    break

        if is_header:
            # Save previous section if exists
            if current_section:
                sections.append(current_section)

            # Start new section
            # Check if this is a special section (paragraph content)
            is_special = any(special in line for special in special_sections)
            current_section = {
                'header': line,
                'bullets': [],
                'is_special': is_special  # Flag for special rendering
            }

        # Detect bullets (lines starting with â€¢ or -)
        elif (line.startswith('â€¢') or line.startswith('-')) and current_section:
            # Remove leading bullet character and whitespace
            bullet_text = line.lstrip('â€¢- ').strip()
            if bullet_text:  # Only add non-empty bullets
                current_section['bullets'].append(bullet_text)

        # Special sections: capture ALL text (paragraph content)
        elif current_section and current_section.get('is_special'):
            # Capture paragraph text for Bottom Line and Investment Implications
            if line:  # Non-empty lines
                current_section['bullets'].append(line)  # Store as "bullets" but will render as paragraphs

    # Add final section
    if current_section:
        sections.append(current_section)

    return sections


def render_structured_summary_html(sections: list) -> str:
    """
    Convert parsed sections into properly formatted HTML.

    Returns HTML string with sections, headers, and bullet lists.
    Handles special sections (Bottom Line, Investment Implications) as paragraphs.
    """
    if not sections:
        return ""

    html_parts = []

    for section in sections:
        html_parts.append("<div class='summary-section'>")

        # Render section header with emoji
        html_parts.append(f"<div class='section-header'>{section['header']}</div>")

        # Check if this is a special section (paragraph content)
        if section.get('is_special'):
            # Render as paragraph text (join with <br> to preserve structure)
            if section['bullets']:
                # Remove empty lines to avoid gaps
                bullets_filtered = [line for line in section['bullets'] if line.strip()]
                paragraph_text = '<br>'.join(bullets_filtered)

                html_parts.append(f"<div style='margin: 8px 0 0 0; line-height: 1.5;'>{paragraph_text}</div>")
        else:
            # Render as bullet list (standard sections)
            if section['bullets']:
                html_parts.append("<ul class='section-bullets'>")
                for bullet in section['bullets']:
                    html_parts.append(f"<li>{bullet}</li>")
                html_parts.append("</ul>")

        html_parts.append("</div>")

    return ''.join(html_parts)


async def fetch_digest_articles(hours: int = 24, tickers: List[str] = None,
                                flagged_article_ids: List[int] = None) -> Dict:
    """Fetch categorized articles for digest processing.

    This is a PURE data fetching function - no AI generation, no email sending.
    Returns articles organized by ticker and category for use in:
    - generate_executive_summary_all_phases() for AI generation
    - build_enhanced_digest_html() for Email #2 building

    Args:
        hours: Time window for articles
        tickers: Specific tickers to fetch, or None for all
        flagged_article_ids: If provided, only fetch articles with these IDs (from triage)

    Returns:
        Dict with:
        - articles_by_ticker: Dict[ticker] = Dict[category] = List[articles]
        - articles: Total article count
        - tickers: List of tickers found
        - by_category: Category counts
        - content_scraping_stats: Scraping statistics
    """
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)

    days = int(hours / 24) if hours >= 24 else 0
    period_label = f"{days} days" if days > 0 else f"{hours:.0f} hours"

    LOG.info(f"=== FETCHING DIGEST ARTICLES ===")
    LOG.info(f"Time window: {period_label} (cutoff: {cutoff})")
    LOG.info(f"Target tickers: {tickers or 'ALL'}")
    if flagged_article_ids is not None and len(flagged_article_ids) > 0:
        LOG.info(f"Flagged article filter: ENABLED ({len(flagged_article_ids)} IDs)")
    else:
        LOG.info(f"â„¹ï¸ No flagged articles provided (quiet news day)")

    with db() as conn, conn.cursor() as cur:
        # Enhanced query to get articles from new schema - MATCHES triage query
        # Add optional filter for flagged articles (from triage selection)
        if tickers:
            if flagged_article_ids:
                cur.execute("""
                    SELECT DISTINCT ON (a.url_hash, ta.ticker)
                        a.id, a.url, a.resolved_url, a.title, a.description,
                        ta.ticker, a.domain, a.published_at,
                        ta.found_at, tf.category,
                        f.search_keyword, ta.ai_summary, ta.ai_model,
                        a.scraped_content, a.content_scraped_at, a.scraping_failed, a.scraping_error,
                        a.quality_score,
                        f.feed_ticker,
                        tf.value_chain_type,
                        ta.relevance_score, ta.relevance_reason, ta.is_rejected
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    JOIN feeds f ON ta.feed_id = f.id
                    WHERE ta.ticker = ANY(%s)
                        AND a.id = ANY(%s)
                    ORDER BY a.url_hash, ta.ticker,
                        tf.value_chain_type NULLS LAST,
                        COALESCE(a.published_at, ta.found_at) DESC, ta.found_at DESC
                """, (tickers, flagged_article_ids))
            else:
                # No flagged IDs - return empty results (quiet news day)
                LOG.info(f"â„¹ï¸ [{tickers}] No flagged articles")
                # Return empty query structure (no articles)
                cur.execute("SELECT NULL LIMIT 0")
        else:
            if flagged_article_ids:
                cur.execute("""
                    SELECT DISTINCT ON (a.url_hash, ta.ticker)
                        a.id, a.url, a.resolved_url, a.title, a.description,
                        ta.ticker, a.domain, a.published_at,
                        ta.found_at, tf.category,
                        f.search_keyword, ta.ai_summary, ta.ai_model,
                        a.scraped_content, a.content_scraped_at, a.scraping_failed, a.scraping_error,
                        a.quality_score,
                        f.feed_ticker,
                        tf.value_chain_type,
                        ta.relevance_score, ta.relevance_reason, ta.is_rejected
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    JOIN feeds f ON ta.feed_id = f.id
                    WHERE a.id = ANY(%s)
                        AND (ta.is_rejected = FALSE OR ta.is_rejected IS NULL)
                    ORDER BY a.url_hash, ta.ticker,
                        tf.value_chain_type NULLS LAST,
                        COALESCE(a.published_at, ta.found_at) DESC, ta.found_at DESC
                """, (flagged_article_ids,))
            else:
                # No flagged IDs - return empty results (quiet news day)
                LOG.info(f"â„¹ï¸ [ALL TICKERS] No flagged articles")
                # Return empty query structure (no articles)
                cur.execute("SELECT NULL LIMIT 0")

        # Group articles by ticker
        articles_by_ticker = {}
        for row in cur.fetchall():
            target_ticker = row["ticker"]

            # Use stored category from ticker_articles table
            category = row["category"]

            if target_ticker not in articles_by_ticker:
                articles_by_ticker[target_ticker] = {}
            if category not in articles_by_ticker[target_ticker]:
                articles_by_ticker[target_ticker][category] = []

            # Convert row to dict and add to results
            article_dict = dict(row)
            articles_by_ticker[target_ticker][category].append(article_dict)

            # Debug logging for AI summary presence
            if article_dict.get('ai_summary'):
                LOG.debug(f"DIGEST QUERY: Found AI summary for {target_ticker} - {len(article_dict['ai_summary'])} chars")
            else:
                LOG.debug(f"DIGEST QUERY: No AI summary for {target_ticker} article: {article_dict.get('title', 'No title')[:50]}")

        # FIX #2: Ensure ticker in dict even with 0 articles + add diagnostic logging
        if tickers:
            for ticker in tickers:
                if ticker not in articles_by_ticker:
                    # Query returned 0 rows for this ticker
                    articles_by_ticker[ticker] = {}
                    LOG.info(f"[{ticker}] 0 articles with AI summaries (flagged: {len(flagged_article_ids or [])})")
                else:
                    # Count articles with ai_summary across all categories
                    articles_with_summaries = sum(
                        1 for arts in articles_by_ticker[ticker].values()
                        for a in arts if a.get('ai_summary')
                    )
                    total_flagged = len(flagged_article_ids) if flagged_article_ids else 0
                    LOG.info(f"[{ticker}] {articles_with_summaries} articles with AI summaries (flagged: {total_flagged})")

        # NEW (Nov 2025): Inject company releases (8-K and FMP press releases) into articles
        if tickers:
            LOG.info(f"Fetching company releases for {len(tickers)} tickers (lookback: {hours}h)...")
            company_releases = fetch_company_releases_for_digest(tickers, hours)

            if company_releases:
                LOG.info(f"Found {len(company_releases)} company releases within lookback window")

                added_count = 0
                skipped_count = 0

                for release_row in company_releases:
                    release_ticker = release_row["ticker"]

                    # Map release to article format (validates and returns None if invalid)
                    article_dict = map_company_release_to_article_dict(release_row)

                    if not article_dict:
                        # Validation failed - mapping function already logged error
                        skipped_count += 1
                        continue

                    # Initialize ticker structure if needed
                    if release_ticker not in articles_by_ticker:
                        articles_by_ticker[release_ticker] = {}
                    if "company" not in articles_by_ticker[release_ticker]:
                        articles_by_ticker[release_ticker]["company"] = []

                    # Append validated release
                    articles_by_ticker[release_ticker]["company"].append(article_dict)
                    added_count += 1

                    LOG.info(f"[{release_ticker}] âœ… Added company release: {article_dict['title']} ({release_row['filing_date']})")

                LOG.info(f"Company releases integration: {added_count} added, {skipped_count} skipped")
            else:
                LOG.info(f"No company releases found within {hours}h lookback window")

        # Mark articles as sent (only new ones)
        total_to_mark = 0
        if tickers:
            cur.execute("""
                SELECT COUNT(DISTINCT (a.url_hash, ta.ticker)) as count
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                WHERE ta.found_at >= %s
                AND ta.ticker = ANY(%s)
                AND NOT ta.sent_in_digest
            """, (cutoff, tickers))
        else:
            cur.execute("""
                SELECT COUNT(DISTINCT (a.url_hash, ta.ticker)) as count
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                WHERE ta.found_at >= %s
                AND NOT ta.sent_in_digest
            """, (cutoff,))

        result = cur.fetchone()
        total_to_mark = result["count"] if result else 0

        if total_to_mark > 0:
            if tickers:
                cur.execute("""
                    UPDATE ticker_articles
                    SET sent_in_digest = TRUE
                    WHERE found_at >= %s
                    AND ticker = ANY(%s)
                    AND NOT sent_in_digest
                """, (cutoff, tickers))
            else:
                cur.execute("""
                    UPDATE ticker_articles
                    SET sent_in_digest = TRUE
                    WHERE found_at >= %s
                    AND NOT sent_in_digest
                """, (cutoff,))

            LOG.info(f"Marked {total_to_mark} articles as sent in digest")
        else:
            LOG.info("No new articles to mark as sent (smart reuse mode)")

    total_articles = sum(
        sum(len(arts) for arts in categories.values())
        for categories in articles_by_ticker.values()
    )

    # Info logging for quiet days
    if total_articles == 0:
        LOG.info(f"â„¹ï¸ 0 articles found (quiet news day)")

    # Collect stats for return value
    category_counts = {"company": 0, "industry": 0, "competitor": 0, "value_chain": 0, "upstream": 0, "downstream": 0}
    content_stats = {"scraped": 0, "failed": 0, "skipped": 0, "ai_summaries": 0}
    for ticker_cats in articles_by_ticker.values():
        for cat, arts in ticker_cats.items():
            category_counts[cat] = category_counts.get(cat, 0) + len(arts)
            for art in arts:
                if art.get('scraped_content'):
                    content_stats['scraped'] += 1
                elif art.get('scraping_failed'):
                    content_stats['failed'] += 1
                else:
                    content_stats['skipped'] += 1
                if art.get('ai_summary'):
                    content_stats['ai_summaries'] += 1

    LOG.info(f"DIGEST STATS: {content_stats['ai_summaries']} articles with AI summaries out of {total_articles} total")

    return {
        "articles": total_articles,
        "tickers": list(articles_by_ticker.keys()),
        "by_category": category_counts,
        "content_scraping_stats": content_stats,
        "articles_by_ticker": articles_by_ticker
    }


# Backward-compatible alias (deprecated - use fetch_digest_articles instead)
async def fetch_digest_articles_with_enhanced_content(hours: int = 24, tickers: List[str] = None,
                                               show_ai_analysis: bool = True,
                                               show_descriptions: bool = True,
                                               flagged_article_ids: List[int] = None,
                                               mode: str = 'daily',
                                               report_type: str = 'daily',
                                               send_email_enabled: bool = True) -> Dict:
    """DEPRECATED: Use fetch_digest_articles() instead.

    This wrapper exists for backward compatibility only.
    All parameters except hours, tickers, and flagged_article_ids are ignored.
    """
    LOG.warning("fetch_digest_articles_with_enhanced_content is DEPRECATED - use fetch_digest_articles() instead")
    return await fetch_digest_articles(hours=hours, tickers=tickers, flagged_article_ids=flagged_article_ids)


def parse_research_summary_sections(summary_text: str) -> Dict[str, List[str]]:
    """
    Parse research summary text (transcript or press release) into sections by emoji headers.
    Handles special Q&A format (Q:/A: paragraphs) and top-level Upside/Downside/Variables sections.
    Returns dict: {section_name: [line1, line2, ...]}
    """
    sections = {
        "bottom_line": [],
        "financial_results": [],
        "major_developments": [],
        "operational_metrics": [],
        "guidance": [],
        "strategic_initiatives": [],
        "management_sentiment": [],
        "risk_factors": [],
        "industry_competitive": [],
        "capital_allocation": [],
        "qa_highlights": [],
        "upside_scenario": [],
        "downside_scenario": [],
        "key_variables": []
    }

    if not summary_text:
        return sections

    # Split by emoji headers (updated Oct 2025 - upside/downside/variables are top-level)
    section_markers = [
        ("ðŸ“Œ BOTTOM LINE", "bottom_line"),
        ("ðŸ’° FINANCIAL RESULTS", "financial_results"),
        ("ðŸ¢ MAJOR DEVELOPMENTS", "major_developments"),
        ("ðŸ“Š OPERATIONAL METRICS", "operational_metrics"),
        ("ðŸ“ˆ GUIDANCE", "guidance"),
        ("ðŸŽ¯ STRATEGIC INITIATIVES", "strategic_initiatives"),
        ("ðŸ’¼ MANAGEMENT SENTIMENT", "management_sentiment"),
        ("âš ï¸ RISK FACTORS", "risk_factors"),
        ("ðŸ­ INDUSTRY", "industry_competitive"),  # Matches "INDUSTRY & COMPETITIVE"
        ("ðŸ’¡ CAPITAL ALLOCATION", "capital_allocation"),
        ("ðŸ’¬ Q&A HIGHLIGHTS", "qa_highlights"),
        ("ðŸ“ˆ UPSIDE SCENARIO", "upside_scenario"),
        ("ðŸ“‰ DOWNSIDE SCENARIO", "downside_scenario"),
        ("ðŸ” KEY VARIABLES TO MONITOR", "key_variables")
    ]

    current_section = None
    section_marker_prefixes = tuple(marker for marker, _ in section_markers)

    for line in summary_text.split('\n'):
        line_stripped = line.strip()

        # Skip horizontal rule separators (Claude sometimes adds these)
        if line_stripped == '---':
            continue

        # Check if line is a section header
        is_header = False
        for marker, section_key in section_markers:
            if line_stripped.startswith(marker):
                current_section = section_key
                is_header = True
                break

        if not is_header and current_section:
            # Line is content, not a header

            # Special handling for sections that capture ALL text (paragraphs)
            if current_section in ['bottom_line', 'qa_highlights', 'upside_scenario', 'downside_scenario']:
                # Skip lines that start with section markers
                if not line_stripped.startswith(section_marker_prefixes):
                    # Skip empty lines at start, but keep them once content exists
                    if line_stripped or sections[current_section]:
                        sections[current_section].append(line_stripped)

            # Standard handling for bullet sections
            else:
                if line_stripped.startswith('â€¢') or line_stripped.startswith('-'):
                    # Extract bullet text
                    bullet_text = line_stripped.lstrip('â€¢- ').strip()
                    if bullet_text:
                        sections[current_section].append(bullet_text)
                elif line.startswith('  ') and sections[current_section]:
                    # Indented continuation line (e.g., "  Context: ...")
                    continuation = line_stripped
                    if continuation:
                        # Append to the last bullet with a line break
                        sections[current_section][-1] += '\n' + continuation

    return sections


def is_paywall_article(domain: str) -> bool:
    """Check if domain is a known paywall using PAYWALL_DOMAINS constant"""
    if not domain:
        return False
    normalized = normalize_domain(domain)
    return normalized in PAYWALL_DOMAINS


def build_executive_summary_html(sections: Dict[str, List[str]], strip_emojis: bool = False) -> str:
    """
    Convert executive summary sections dict into HTML string.
    Used by Jinja2 template.

    Args:
        sections: Parsed sections dict
        strip_emojis: If True, remove emojis from section headers (for Email #3)
    """
    def strip_emoji(text: str) -> str:
        """Remove emoji characters from text"""
        import re
        # Remove emoji pattern (Unicode emoji ranges)
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001F900-\U0001F9FF"  # supplemental symbols
            u"\U00002600-\U000026FF"  # misc symbols
            "]+", flags=re.UNICODE)
        return emoji_pattern.sub('', text).strip()

    def convert_markdown_to_html(text: str, single_break: bool = False) -> str:
        """
        Convert markdown formatting to HTML.
        For Phase 3 editorial format: **bold** â†’ <strong>bold</strong>
        For Phase 1/2: Strip markdown that AI adds despite instructions
        Handles newlines: \n\n â†’ <br><br> (or <br> if single_break=True), \n â†’ <br>

        Args:
            text: Text to convert
            single_break: If True, convert \n\n to <br> instead of <br><br> (for bullets)
        """
        import re
        # Convert markdown bold to HTML strong (**text** â†’ <strong>text</strong>)
        text = re.sub(r'\*\*([^*]+?)\*\*', r'<strong>\1</strong>', text)
        text = re.sub(r'__([^_]+?)__', r'<strong>\1</strong>', text)
        # Strip markdown italic (not used in our outputs, remove if present)
        text = re.sub(r'\*([^*]+?)\*', r'\1', text)
        text = re.sub(r'_([^_]+?)_', r'\1', text)
        # Convert newlines to HTML line breaks (Phase 3 editorial format)
        if single_break:
            # For bullets: use single <br> for all breaks (tighter spacing)
            text = text.replace('\n\n', '<br>')
            text = text.replace('\n', '<br>')
        else:
            # For paragraphs: preserve paragraph breaks
            text = text.replace('\n\n', '<br><br>')  # Paragraph breaks
            text = text.replace('\n', '<br>')        # Single line breaks
        return text

    def parse_markdown_list_section(content: str) -> str:
        """
        Convert markdown lists to HTML lists within Bottom Line/Upside/Downside sections.

        Handles Phase 3 editorial format where sections contain:
        - Thesis paragraph
        - "Key Developments:" or "Primary Drivers:" header
        - Markdown list items (- **Label**: Details)
        - Optional closing paragraph

        Input:
            [Thesis]

            Key Developments:
            - **Theme 1**: Details
            - **Theme 2**: Details

            [Optional closing]

        Output (HTML):
            [Thesis]<br><br>
            <strong>Key Developments:</strong>
            <ul>
                <li><strong>Theme 1</strong>: Details</li>
                <li><strong>Theme 2</strong>: Details</li>
            </ul>
            [Optional closing]
        """
        import re
        lines = content.split('\n')
        result = []
        in_list = False
        list_items = []
        prev_was_list_header = False

        for i, line in enumerate(lines):
            # Check if line is a markdown list item (starts with "- ")
            if line.strip().startswith('- '):
                if not in_list:
                    in_list = True
                    list_items = []
                # Extract list item content (remove "- " prefix)
                item_content = line.strip()[2:].strip()
                # Convert markdown bold to HTML
                item_content = re.sub(r'\*\*([^*]+?)\*\*', r'<strong>\1</strong>', item_content)
                list_items.append(f'<li style="margin-bottom: 6px; font-size: 13px; line-height: 1.5; color: #374151;">{item_content}</li>')
                prev_was_list_header = False
            else:
                # Not a list item
                if in_list:
                    # Close previous list
                    result.append('<ul style="margin: 4px 0 8px 0; padding-left: 20px; list-style-type: disc;">')
                    result.extend(list_items)
                    result.append('</ul>')
                    in_list = False
                    list_items = []

                # Add non-list line
                if line.strip():
                    # Check if this is a list header (Key Developments:, Primary Drivers:, etc.)
                    is_list_header = any(header in line for header in ['Key Developments:', 'Primary Drivers:', 'Primary Risks:'])

                    # Check if this is a date reference (Month DD-DD) or (Month DD)
                    is_date_ref = bool(re.match(r'^\([A-Z][a-z]{2,8}\s+\d{1,2}[-,]\s*\d{0,2}\)$', line.strip()))

                    # Convert markdown bold
                    line_html = re.sub(r'\*\*([^*]+?)\*\*', r'<strong>\1</strong>', line.strip())

                    # Add spacing logic
                    if is_list_header:
                        # List header: add <br><br> before if we have content (paragraph break from thesis)
                        if result:
                            result.append('<br><br>')
                        result.append(line_html)
                        prev_was_list_header = True
                    elif is_date_ref:
                        # Date reference: keep close to previous content (no extra spacing)
                        if result:
                            result.append('<br>')
                        result.append(line_html)
                        prev_was_list_header = False
                    else:
                        # Regular paragraph: add paragraph break if we have content
                        if result and not prev_was_list_header:
                            result.append('<br><br>')
                        result.append(line_html)
                        prev_was_list_header = False
                elif result and not prev_was_list_header:
                    # Empty line after non-header content = potential paragraph break
                    # But don't add if previous was list header (list will provide spacing)
                    pass

        # Close any remaining list
        if in_list and list_items:
            result.append('<ul style="margin: 4px 0 8px 0; padding-left: 20px; list-style-type: disc;">')
            result.extend(list_items)
            result.append('</ul>')

        return ''.join(result)

    def bold_bullet_labels(text: str, context_only: bool = False) -> str:
        """
        Bold "Context:" labels in text.

        Args:
            text: Text to process
            context_only: Unused (kept for backward compatibility)

        Transforms:
            - "Context: Details" â†’ "<strong>Context:</strong> Details"

        Also converts markdown bold syntax (**text**) to HTML (<strong>text</strong>).

        Note: Headers are already bolded via format_bullet_header() which uses **...**,
        so no additional label bolding is needed.
        """
        import re

        # DEFENSIVE: Convert markdown formatting to HTML
        # For bullets, use single line breaks (tighter spacing between header and body)
        # For paragraphs, use paragraph breaks
        single_break = not context_only
        text = convert_markdown_to_html(text, single_break=single_break)

        # Bold "Context:" labels in all formats (paragraphs and bullets)
        text = text.replace('Context:', '<strong>Context:</strong>')

        return text

    def style_sentiment_badges(html: str) -> str:
        """
        Replace sentiment words with color-coded badges in HTML.

        Targets sentiment words that appear in bullet headers after the â€¢ separator.
        Pattern: â€¢ Bullish</strong> â†’ â€¢ <span style="...">Bullish</span></strong>

        Colors (matching mockup exactly):
        - Bullish: green (#e8f5ef bg, #1e6b4a text)
        - Bearish: red (#fdf2f2 bg, #9b2c2c text)
        - Mixed: amber (#fef3c7 bg, #92400e text)
        - Neutral: gray (#eef0f2 bg, #5a6570 text)

        Badge styling: 10px font, 600 weight, uppercase, letter-spacing 0.5px, 2px 8px padding, 2px border-radius
        """
        import re

        SENTIMENT_STYLES = {
            'Bullish': 'background-color:#e8f5ef;color:#1e6b4a;',
            'Bearish': 'background-color:#fdf2f2;color:#9b2c2c;',
            'Mixed': 'background-color:#fef3c7;color:#92400e;',
            'Neutral': 'background-color:#eef0f2;color:#5a6570;',
        }

        BASE_STYLE = 'display:inline-block;font-family:Arial,Helvetica,sans-serif;font-size:10px;font-weight:600;text-transform:uppercase;letter-spacing:0.5px;padding:2px 8px;border-radius:2px;vertical-align:middle;'

        for sentiment, colors in SENTIMENT_STYLES.items():
            # Match: â€¢ Sentiment</strong> (sentiment at end of header before closing tag)
            # This ensures we only style sentiment in headers, not in body content
            pattern = f'â€¢ {sentiment}</strong>'
            styled_badge = f'</span><span style="{colors}{BASE_STYLE}">{sentiment}</span></strong>'
            html = html.replace(pattern, styled_badge)

        return html

    def build_section(title: str, content: List, use_bullets: bool = True, bold_labels: bool = False, context_only: bool = False, section_type: str = None) -> str:
        """Build section with consistent styling matching mockup design.

        Args:
            title: Section title
            content: List of bullet points or paragraphs (can be List[Dict] with 'formatted' field or List[str])
            use_bullets: If True, format as bullet list
            bold_labels: If True, bold topic labels in format "Topic: Details"
            context_only: If True with bold_labels, ONLY bold "Context:" (for paragraphs)
            section_type: Special section type ('bottom_line', 'upside', 'downside', 'key_variables')
        """
        if not content:
            return ""

        # Strip emojis from title if requested
        display_title = strip_emoji(title) if strip_emojis else title

        # Determine section type from title if not explicitly provided
        title_lower = display_title.lower()
        if section_type is None:
            if "bottom line" in title_lower:
                section_type = 'bottom_line'
            elif "upside" in title_lower:
                section_type = 'upside'
            elif "downside" in title_lower:
                section_type = 'downside'
            elif "key variables" in title_lower:
                section_type = 'key_variables'

        # ============================================================
        # SPECIAL HANDLING: Key Variables (table rows with borders)
        # ============================================================
        if section_type == 'key_variables':
            rows_html = ""
            for i, item in enumerate(content):
                # Extract formatted string if item is dict (new format)
                if isinstance(item, dict) and 'formatted' in item:
                    text = item['formatted']
                else:
                    text = item

                # Key Variables format: **Label**\nDescription
                # Convert to: <span style="...">Label</span> â€” Description
                import re

                # Check for **Label** pattern at start
                label_match = re.match(r'\*\*(.+?)\*\*\s*\n?(.*)', text, re.DOTALL)
                if label_match:
                    label = label_match.group(1)
                    description = label_match.group(2).strip()
                    # Style label with explicit font-weight and color (matching mockup)
                    label_html = f'<span style="font-weight: 600; color: #1a1a1a;">{label}</span>'
                    processed_item = f'{label_html} â€” {description}'
                else:
                    # Fallback: apply markdown conversion
                    processed_item = convert_markdown_to_html(text, single_break=True)

                # Determine if this is the last item (no bottom border, no bottom margin/padding)
                is_last = (i == len(content) - 1)
                border_style = "" if is_last else "border-bottom: 1px solid #ebe8e3;"
                spacing_style = "margin-bottom: 16px;" if is_last else "margin-bottom: 14px; padding-bottom: 14px;"

                rows_html += f'''
                    <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="{spacing_style} {border_style}">
                        <tr>
                            <td>
                                <p style="margin: 0; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 1.6; color: #3d3d3d;">{processed_item}</p>
                            </td>
                        </tr>
                    </table>
                '''

            return f'''
                <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-top: 20px; margin-bottom: 0;">
                    <tr>
                        <td>
                            <!-- Section Header -->
                            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 16px;">
                                <tr>
                                    <td style="font-family: Arial, Helvetica, sans-serif; font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; font-weight: 600; color: #8b2c24;">{display_title}</td>
                                </tr>
                            </table>
                            {rows_html}
                        </td>
                    </tr>
                </table>
            '''

        # ============================================================
        # SPECIAL HANDLING: Upside/Downside Scenarios (header INSIDE box)
        # ============================================================
        if section_type in ('upside', 'downside'):
            # Process content
            raw_content = '\n'.join(content) if isinstance(content, list) else content
            has_markdown_list = '\n- ' in raw_content or raw_content.startswith('- ')

            if has_markdown_list:
                text = parse_markdown_list_section(raw_content)
            else:
                if bold_labels:
                    content_filtered = [bold_bullet_labels(line, context_only=context_only) for line in content if line.strip()]
                else:
                    content_filtered = [convert_markdown_to_html(line) for line in content if line.strip()]
                paragraphs = []
                for item in content_filtered:
                    item_paragraphs = item.split('\n\n')
                    paragraphs.extend([p.strip() for p in item_paragraphs if p.strip()])
                text = "<br><br>".join(paragraphs)

            # Colors for upside vs downside
            if section_type == 'upside':
                bg_color = "#f0fdf4"
                border_color = "#27ae60"
                header_color = "#1e6b4a"
            else:  # downside
                bg_color = "#fef2f2"
                border_color = "#c0392b"
                header_color = "#9b2c2c"

            # Return just the box (no outer margin - will be wrapped in container at call site)
            # mobile-scenario-padding class reduces padding from 20px to 12px on mobile
            return f'''
                <tr>
                    <td class="mobile-scenario-padding" style="padding: 20px; background-color: {bg_color}; border-left: 3px solid {border_color};">
                        <p style="margin: 0 0 10px 0; font-family: Arial, Helvetica, sans-serif; font-size: 10px; text-transform: uppercase; letter-spacing: 1.5px; font-weight: 700; color: {header_color};">{display_title}</p>
                        <p style="margin: 0; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 1.65; color: #3d3d3d;">{text}</p>
                    </td>
                </tr>
            '''

        # ============================================================
        # SPECIAL HANDLING: Bottom Line (heavy border separator)
        # ============================================================
        if section_type == 'bottom_line':
            raw_content = '\n'.join(content) if isinstance(content, list) else content
            has_markdown_list = '\n- ' in raw_content or raw_content.startswith('- ')

            if has_markdown_list:
                text = parse_markdown_list_section(raw_content)
            else:
                if bold_labels:
                    content_filtered = [bold_bullet_labels(line, context_only=context_only) for line in content if line.strip()]
                else:
                    content_filtered = [convert_markdown_to_html(line) for line in content if line.strip()]
                paragraphs = []
                for item in content_filtered:
                    item_paragraphs = item.split('\n\n')
                    paragraphs.extend([p.strip() for p in item_paragraphs if p.strip()])
                text = "<br><br>".join(paragraphs)

            return f'''
                <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 0; padding-bottom: 18px; border-bottom: 2px solid #1a1a1a;">
                    <tr>
                        <td>
                            <p style="margin: 0 0 12px 0; font-family: Arial, Helvetica, sans-serif; font-size: 11px; text-transform: uppercase; letter-spacing: 2px; color: #8b2c24; font-weight: 600;">Bottom Line</p>
                            <p style="margin: 0; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 1.7; color: #3d3d3d;">{text}</p>
                        </td>
                    </tr>
                </table>
            '''

        # ============================================================
        # STANDARD BULLET SECTIONS (Major Developments, Financial, etc.)
        # ============================================================
        if use_bullets:
            items_html = ""
            for item in content:
                # Extract formatted string if item is dict (new format)
                if isinstance(item, dict) and 'formatted' in item:
                    text = item['formatted']
                else:
                    text = item

                # Input format: **[Entity] Topic â€¢ Sentiment**\nBody text (Date)
                # Split into header and body at first newline
                if '\n' in text:
                    header_raw, body_raw = text.split('\n', 1)
                else:
                    header_raw = text
                    body_raw = ""

                # Process header: Convert **text** to styled spans (not <strong>)
                # Extract content between ** markers
                import re
                header_match = re.match(r'\*\*(.+?)\*\*', header_raw)
                if header_match:
                    header_content = header_match.group(1)
                    # Check if header contains sentiment (has â€¢ separator)
                    if ' â€¢ ' in header_content:
                        topic_part, sentiment_part = header_content.rsplit(' â€¢ ', 1)
                        # Style topic label: 14px, 600 weight, #1a1a1a, margin-right 6px
                        topic_html = f'<span style="font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-weight: 600; color: #1a1a1a; vertical-align: middle; margin-right: 6px;">{topic_part}</span>'
                        # Style sentiment badge
                        sentiment_styles = {
                            'Bullish': 'background-color:#e8f5ef;color:#1e6b4a;',
                            'Bearish': 'background-color:#fdf2f2;color:#9b2c2c;',
                            'Mixed': 'background-color:#fef3c7;color:#92400e;',
                            'Neutral': 'background-color:#eef0f2;color:#5a6570;',
                        }
                        badge_base = 'display:inline-block;font-family:Arial,Helvetica,sans-serif;font-size:10px;font-weight:600;text-transform:uppercase;letter-spacing:0.5px;padding:2px 8px;border-radius:2px;vertical-align:middle;'
                        # Extract sentiment word (may have reason in parentheses)
                        sentiment_word = sentiment_part.split('(')[0].strip()
                        sentiment_color = sentiment_styles.get(sentiment_word, sentiment_styles['Neutral'])
                        sentiment_html = f'<span style="{sentiment_color}{badge_base}">{sentiment_word}</span>'
                        header_html = f'{topic_html}{sentiment_html}'
                    else:
                        # No sentiment - just topic label
                        header_html = f'<span style="font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-weight: 600; color: #1a1a1a; vertical-align: middle;">{header_content}</span>'
                else:
                    # No ** markers - use as-is with styling
                    header_html = f'<span style="font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-weight: 600; color: #1a1a1a; vertical-align: middle;">{header_raw}</span>'

                # Process body: Convert markdown and clean up
                body_html = convert_markdown_to_html(body_raw, single_break=True) if body_raw else ""

                # Build bullet item with TWO separate paragraphs (matching mockup)
                items_html += f'''
                    <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 16px;">
                        <tr>
                            <td>
                                <p style="margin: 0 0 8px 0;">{header_html}</p>
                                <p style="margin: 0; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 1.7; color: #3d3d3d;">{body_html}</p>
                            </td>
                        </tr>
                    </table>
                '''

            return f'''
                <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-top: 20px; margin-bottom: 0;">
                    <tr>
                        <td>
                            <!-- Section Header -->
                            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 16px;">
                                <tr>
                                    <td style="font-family: Arial, Helvetica, sans-serif; font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; font-weight: 600; color: #8b2c24;">{display_title}</td>
                                </tr>
                            </table>
                            {items_html}
                        </td>
                    </tr>
                </table>
            '''

        # ============================================================
        # STANDARD PARAGRAPH SECTIONS (fallback)
        # ============================================================
        raw_content = '\n'.join(content) if isinstance(content, list) else content
        has_markdown_list = '\n- ' in raw_content or raw_content.startswith('- ')

        if has_markdown_list:
            text = parse_markdown_list_section(raw_content)
        else:
            if bold_labels:
                content_filtered = [bold_bullet_labels(line, context_only=context_only) for line in content if line.strip()]
            else:
                content_filtered = [convert_markdown_to_html(line) for line in content if line.strip()]
            paragraphs = []
            for item in content_filtered:
                item_paragraphs = item.split('\n\n')
                paragraphs.extend([p.strip() for p in item_paragraphs if p.strip()])
            text = "<br><br>".join(paragraphs)

        return f'''
            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-top: 20px; margin-bottom: 16px;">
                <tr>
                    <td>
                        <!-- Section Header -->
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 16px;">
                            <tr>
                                <td style="font-family: Arial, Helvetica, sans-serif; font-size: 11px; text-transform: uppercase; letter-spacing: 1.5px; font-weight: 600; color: #8b2c24;">{display_title}</td>
                            </tr>
                        </table>
                        <p style="margin: 0; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 1.7; color: #3d3d3d;">{text}</p>
                    </td>
                </tr>
            </table>
        '''

    # Check for quiet day (FLAGGED ARTICLE COUNT = 0)
    bottom_line_content = sections.get("bottom_line", [])
    is_quiet_day = any("QUIET DAY - NO MATERIAL DEVELOPMENTS" in line for line in bottom_line_content)

    if is_quiet_day:
        # Quiet day: render ONLY bottom line section (3 lines)
        return build_section("ðŸ“Œ Bottom Line" if not strip_emojis else "Bottom Line",
                           bottom_line_content, use_bullets=False, bold_labels=True, context_only=True)

    # Material news day: build sections
    html = ""

    # Bottom Line first (paragraph format, context_only=True to avoid bolding colons in prose)
    html += build_section("ðŸ“Œ Bottom Line" if not strip_emojis else "Bottom Line",
                         sections.get("bottom_line", []), use_bullets=False, bold_labels=True, context_only=True)

    # 32px spacer after Bottom Line black dash (matches metrics strip to Bottom Line gap)
    html += '<table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td height="32" style="font-size: 0; line-height: 0;">&nbsp;</td></tr></table>'

    # Standard sections (bullet format)
    html += build_section("ðŸ”´ Major Developments" if not strip_emojis else "Major Developments",
                         sections.get("major_developments", []), use_bullets=True, bold_labels=True)
    html += build_section("ðŸ“Š Financial/Operational Performance" if not strip_emojis else "Financial/Operational Performance",
                         sections.get("financial_operational", []), use_bullets=True, bold_labels=True)
    html += build_section("âš ï¸ Risk Factors" if not strip_emojis else "Risk Factors",
                         sections.get("risk_factors", []), use_bullets=True, bold_labels=True)
    html += build_section("ðŸ“ˆ Wall Street Sentiment" if not strip_emojis else "Wall Street Sentiment",
                         sections.get("wall_street", []), use_bullets=True, bold_labels=True)
    html += build_section("âš¡ Competitive/Industry Dynamics" if not strip_emojis else "Competitive/Industry Dynamics",
                         sections.get("competitive_industry", []), use_bullets=True, bold_labels=True)
    html += build_section("ðŸ“… Upcoming Catalysts" if not strip_emojis else "Upcoming Catalysts",
                         sections.get("upcoming_catalysts", []), use_bullets=True, bold_labels=True)

    # Three new top-level sections (Oct 2025 - upgraded from sub-sections)
    # Upside/Downside are PARAGRAPHS (context_only=True to avoid bolding colons in prose)
    # Key Variables are BULLETS (full bolding for "Label: Detail" patterns)

    # Wrap Upside/Downside in single container with 16px spacer between them (matching reference)
    upside_row = build_section("ðŸ“ˆ Upside Scenario" if not strip_emojis else "Upside Scenario",
                         sections.get("upside_scenario", []), use_bullets=False, bold_labels=True, context_only=True)
    downside_row = build_section("ðŸ“‰ Downside Scenario" if not strip_emojis else "Downside Scenario",
                         sections.get("downside_scenario", []), use_bullets=False, bold_labels=True, context_only=True)

    # Only add container if at least one scenario has content
    if upside_row or downside_row:
        html += f'''
            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-top: 20px; margin-bottom: 16px;">
                {upside_row}
                <tr>
                    <td height="16" style="font-size: 0; line-height: 0;">&nbsp;</td>
                </tr>
                {downside_row}
            </table>
        '''

    html += build_section("ðŸ” Key Variables to Monitor" if not strip_emojis else "Key Variables to Monitor",
                         sections.get("key_variables", []), use_bullets=True, bold_labels=True)

    return html


def build_research_summary_html(sections: Dict[str, List[str]], content_type: str) -> str:
    """
    Build HTML for research summary (earnings transcript or press release).
    Strips emojis from section headers.
    Handles special Q&A format for transcripts and Investment Implications sub-sections.

    Args:
        sections: Parsed sections from parse_research_summary_sections()
        content_type: 'transcript' or 'press_release'

    Returns:
        HTML string with all sections rendered
    """
    # Import helper functions (defined within scope for encapsulation)
    def strip_emoji(text: str) -> str:
        """Remove emoji characters from text"""
        import re
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001F900-\U0001F9FF"  # supplemental symbols
            u"\U00002600-\U000026FF"  # misc symbols
            "]+", flags=re.UNICODE)
        return emoji_pattern.sub('', text).strip()

    def strip_markdown_formatting(text: str) -> str:
        """Strip markdown formatting (bold, italic) that AI sometimes adds"""
        import re
        text = re.sub(r'\*\*([^*]+?)\*\*', r'\1', text)
        text = re.sub(r'__([^_]+?)__', r'\1', text)
        text = re.sub(r'\*([^*]+?)\*', r'\1', text)
        text = re.sub(r'_([^_]+?)_', r'\1', text)
        return text

    def bold_bullet_labels(text: str, context_only: bool = False) -> str:
        """
        Bold "Context:" labels in text.

        Args:
            text: Text to process
            context_only: Unused (kept for backward compatibility)

        Transforms:
            - "Context: Details" â†’ "<strong>Context:</strong> Details"

        Also strips markdown bold syntax (**text**).

        Note: Headers are already bolded via format_bullet_header() which uses **...**,
        so no additional label bolding is needed.
        """
        import re
        text = strip_markdown_formatting(text)

        # Bold "Context:" labels in all formats (paragraphs and bullets)
        text = text.replace('Context:', '<strong>Context:</strong>')

        return text

    def build_section(title: str, content: List[str], use_bullets: bool = True, bold_labels: bool = False, context_only: bool = False) -> str:
        """Build section with consistent styling (always strips emojis for research reports)

        Args:
            title: Section title
            content: List of bullet points or paragraphs
            use_bullets: If True, format as bullet list
            bold_labels: If True, bold topic labels in format "Topic: Details"
            context_only: If True with bold_labels, ONLY bold "Context:" (for paragraphs)
        """
        if not content:
            return ""

        # Always strip emojis from title
        display_title = strip_emoji(title)

        if use_bullets:
            # Bullet list format
            bullet_html = ""
            for item in content:
                # Apply label bolding if requested (bullets never use context_only)
                processed_item = bold_bullet_labels(item, context_only=False) if bold_labels else item
                bullet_html += f'<li style="margin-bottom: 8px; font-size: 13px; line-height: 1.5; color: #374151;">{processed_item}</li>'

            return f'''
                <div style="margin-bottom: 20px;">
                    <h2 style="margin: 0 0 8px 0; font-size: 14px; font-weight: 700; color: #1e40af; text-transform: uppercase; letter-spacing: 0.5px;">{display_title}</h2>
                    <ul style="margin: 0; padding-left: 20px; list-style-type: disc;">
                        {bullet_html}
                    </ul>
                </div>
            '''
        else:
            # Paragraph format (used by Bottom Line, Upside, Downside scenarios)
            # Apply bold labels if requested, otherwise convert markdown to HTML
            if bold_labels:
                content_filtered = [bold_bullet_labels(line, context_only=context_only) for line in content if line.strip()]
            else:
                content_filtered = [convert_markdown_to_html(line) for line in content if line.strip()]

            # Preserve paragraph breaks from Phase 3 markdown (\n\n â†’ <br><br>)
            # Split each item on double newlines (paragraph breaks) first
            paragraphs = []
            for item in content_filtered:
                # Each item might have multiple paragraphs (separated by \n\n)
                item_paragraphs = item.split('\n\n')
                paragraphs.extend([p.strip() for p in item_paragraphs if p.strip()])

            # Join paragraphs with <br><br> for visual spacing
            text = "<br><br>".join(paragraphs)

            return f'''
                <div style="margin-bottom: 20px;">
                    <h2 style="margin: 0 0 8px 0; font-size: 14px; font-weight: 700; color: #1e40af; text-transform: uppercase; letter-spacing: 0.5px;">{display_title}</h2>
                    <div style="margin: 0; font-size: 13px; line-height: 1.6; color: #374151;">{text}</div>
                </div>
            '''

    def build_qa_section(qa_content: List[str]) -> str:
        """Build Q&A section with Q:/A: paragraph format"""
        if not qa_content:
            return ""

        html = '<div style="margin-bottom: 20px;">'
        html += '<h2 style="margin: 0 0 8px 0; font-size: 14px; font-weight: 700; color: #1e40af; text-transform: uppercase; letter-spacing: 0.5px;">Q&A Highlights</h2>'

        for line in qa_content:
            line_stripped = line.strip()
            if line_stripped.startswith("Q:"):
                # Bold Q, no bottom margin (A will follow immediately)
                html += f'<p style="margin: 0; font-size: 13px; line-height: 1.6; color: #374151;"><strong>{line_stripped}</strong></p>'
            elif line_stripped.startswith("A:"):
                # Regular A, with bottom margin (creates space before next Q)
                html += f'<p style="margin: 0 0 16px 0; font-size: 13px; line-height: 1.6; color: #374151;">{line_stripped}</p>'
            elif not line_stripped:
                # Blank lines are ignored (spacing controlled by A margin)
                pass

        html += '</div>'
        return html

    # Build HTML in order
    html = ""

    # 1. Bottom Line (always, paragraph format, context_only=True to avoid bolding colons in prose)
    if "bottom_line" in sections:
        html += build_section("ðŸ“Œ Bottom Line", sections["bottom_line"], use_bullets=False, bold_labels=True, context_only=True)

    # 2-10. Conditional sections (bullet format with bold labels)
    if "financial_results" in sections:
        html += build_section("ðŸ’° Financial Results", sections["financial_results"], use_bullets=True, bold_labels=True)

    if "major_developments" in sections:
        html += build_section("ðŸ¢ Major Developments", sections["major_developments"], use_bullets=True, bold_labels=True)

    if "operational_metrics" in sections:
        html += build_section("ðŸ“Š Operational Metrics", sections["operational_metrics"], use_bullets=True, bold_labels=True)

    if "guidance" in sections:
        html += build_section("ðŸ“ˆ Guidance", sections["guidance"], use_bullets=True, bold_labels=True)

    if "strategic_initiatives" in sections:
        html += build_section("ðŸŽ¯ Strategic Initiatives", sections["strategic_initiatives"], use_bullets=True, bold_labels=True)

    if "management_sentiment" in sections:
        html += build_section("ðŸ’¼ Management Sentiment & Tone", sections["management_sentiment"], use_bullets=True, bold_labels=True)

    if "risk_factors" in sections:
        html += build_section("âš ï¸ Risk Factors & Headwinds", sections["risk_factors"], use_bullets=True, bold_labels=True)

    if "industry_competitive" in sections:
        html += build_section("ðŸ­ Industry & Competitive Landscape", sections["industry_competitive"], use_bullets=True, bold_labels=True)

    if "capital_allocation" in sections:
        html += build_section("ðŸ’¡ Capital Allocation & Balance Sheet", sections["capital_allocation"], use_bullets=True, bold_labels=True)

    # 11. Q&A Highlights (transcripts only, special format)
    if content_type == 'transcript' and "qa_highlights" in sections:
        html += build_qa_section(sections["qa_highlights"])

    # 12-14. Top-level Upside/Downside/Variables sections (Oct 2025 - promoted from sub-sections)
    # Upside/Downside are PARAGRAPHS (context_only=True to avoid bolding colons in prose)
    # Key Variables are BULLETS (full bolding for "Label: Detail" patterns)
    html += build_section("ðŸ“ˆ Upside Scenario", sections.get("upside_scenario", []), use_bullets=False, bold_labels=True, context_only=True)
    html += build_section("ðŸ“‰ Downside Scenario", sections.get("downside_scenario", []), use_bullets=False, bold_labels=True, context_only=True)
    html += build_section("ðŸ” Key Variables to Monitor", sections.get("key_variables", []), use_bullets=True, bold_labels=True)

    return html


def build_articles_html(articles_by_category: Dict[str, List[Dict]]) -> str:
    """
    Convert articles by category into HTML string for email template.
    Styling matches mockup design (Nov 2025).
    """
    # Badge base style matching mockup
    BADGE_BASE_STYLE = "display: inline-block; padding: 2px 6px; font-size: 9px; font-weight: 600; letter-spacing: 0.5px; border-radius: 2px; margin-right: 8px;"

    # Badge colors by category (matching mockup)
    BADGE_COLORS = {
        'company': 'background-color: #1a1a1a; color: #faf9f7;',      # Black
        'industry': 'background-color: #6366f1; color: #ffffff;',     # Indigo
        'competitor': 'background-color: #dc2626; color: #ffffff;',   # Red (was maroon)
        'upstream': 'background-color: #d97706; color: #ffffff;',     # Orange/Amber
        'downstream': 'background-color: #059669; color: #ffffff;',   # Green
    }

    def format_article_date(dt) -> str:
        """Format date as 'Nov 27' without parentheses for Source Articles section."""
        if not dt:
            return "Recent"
        # Ensure we have a timezone-aware datetime
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        # Convert to Eastern Time
        eastern = pytz.timezone('US/Eastern')
        est_time = dt.astimezone(eastern)
        # Format as "Nov 27" (no parentheses)
        return est_time.strftime('%b %-d')

    def build_category_section(title: str, articles: List[Dict], category: str) -> str:
        if not articles:
            return ""

        article_links = ""
        for i, article in enumerate(articles):
            # Check if article is paywalled
            is_paywalled = is_paywall_article(article.get('domain', ''))
            paywall_badge = ' <span style="font-size: 10px; color: #ef4444; font-weight: 600; margin-left: 4px;">PAYWALL</span>' if is_paywalled else ''

            # Build category-specific inline tag
            tag_html = ""
            badge_color = BADGE_COLORS.get(category, BADGE_COLORS['company'])
            domain = article.get('domain', '')

            if category == "industry":
                # Indigo tag with keyword
                keyword = article.get('search_keyword', '')
                if keyword:
                    tag_html = f'<span style="{badge_color}{BADGE_BASE_STYLE}">{keyword.title()}</span>'

            elif category == "competitor":
                # Red tag with ticker or company name
                comp_ticker = article.get('feed_ticker')
                comp_name = strip_legal_suffixes(article.get('search_keyword', 'Unknown'))
                partner_tag = comp_ticker if comp_ticker else comp_name
                tag_html = f'<span style="{badge_color}{BADGE_BASE_STYLE}">{partner_tag}</span>'

            elif category == "company":
                # Black tag with ticker
                ticker = article.get('ticker', 'N/A')
                tag_html = f'<span style="{badge_color}{BADGE_BASE_STYLE}">{ticker}</span>'

            domain_name = get_or_create_formal_domain_name(domain) if domain else "Unknown Source"
            date_str = format_article_date(article.get('published_at'))

            # Last article has no border (cleaner look matching mockup)
            is_last = (i == len(articles) - 1)
            border_style = "" if is_last else "border-bottom: 1px solid #e0ddd8;"

            article_links += f'''
                <tr>
                    <td style="padding: 10px 0; {border_style}">
                        <a href="{article.get('resolved_url', '#')}" style="font-family: Arial, Helvetica, sans-serif; font-size: 13px; color: #1a1a1a; text-decoration: none; font-weight: 500; line-height: 1.4;">{tag_html}{article.get('title', 'Untitled')}{paywall_badge}</a>
                        <p style="margin: 4px 0 0 0; font-family: Arial, Helvetica, sans-serif; font-size: 11px; color: #6b6b6b;">{domain_name} Â· {date_str}</p>
                    </td>
                </tr>
            '''

        return f'''
            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 20px;">
                <tr>
                    <td>
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 10px;">
                            <tr>
                                <td style="font-family: Arial, Helvetica, sans-serif; font-size: 10px; text-transform: uppercase; letter-spacing: 1px; color: #6b6b6b; font-weight: 600; padding-right: 8px; white-space: nowrap;">{title} ({len(articles)})</td>
                                <td width="100%" style="border-bottom: 1px solid #e0ddd8;"></td>
                            </tr>
                        </table>
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%">
                            {article_links}
                        </table>
                    </td>
                </tr>
            </table>
        '''

    html = ""
    html += build_category_section("COMPANY", articles_by_category.get('company', []), "company")
    html += build_category_section("INDUSTRY", articles_by_category.get('industry', []), "industry")
    html += build_category_section("COMPETITORS", articles_by_category.get('competitor', []), "competitor")

    # UPSTREAM section (separate top-level section)
    value_chain_articles = articles_by_category.get('value_chain', [])
    upstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'upstream']

    if upstream_articles:
        upstream_links = ""
        for i, article in enumerate(upstream_articles):
            is_paywalled = is_paywall_article(article.get('domain', ''))
            paywall_badge = ' <span style="font-size: 10px; color: #ef4444; font-weight: 600; margin-left: 4px;">PAYWALL</span>' if is_paywalled else ''

            domain = article.get('domain', '')

            # Orange/Amber tag with ticker or company name
            partner_ticker = article.get('feed_ticker')
            partner_name = strip_legal_suffixes(article.get('search_keyword', 'Unknown'))
            partner_tag = partner_ticker if partner_ticker else partner_name
            tag_html = f'<span style="{BADGE_COLORS["upstream"]}{BADGE_BASE_STYLE}">{partner_tag}</span>'

            domain_name = get_or_create_formal_domain_name(domain) if domain else "Unknown Source"
            date_str = format_article_date(article.get('published_at'))

            # Last article has no border
            is_last = (i == len(upstream_articles) - 1)
            border_style = "" if is_last else "border-bottom: 1px solid #e0ddd8;"

            upstream_links += f'''
                <tr>
                    <td style="padding: 10px 0; {border_style}">
                        <a href="{article.get('resolved_url', '#')}" style="font-family: Arial, Helvetica, sans-serif; font-size: 13px; color: #1a1a1a; text-decoration: none; font-weight: 500; line-height: 1.4;">{tag_html}{article.get('title', 'Untitled')}{paywall_badge}</a>
                        <p style="margin: 4px 0 0 0; font-family: Arial, Helvetica, sans-serif; font-size: 11px; color: #6b6b6b;">{domain_name} Â· {date_str}</p>
                    </td>
                </tr>
            '''

        html += f'''
            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 20px;">
                <tr>
                    <td>
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 10px;">
                            <tr>
                                <td style="font-family: Arial, Helvetica, sans-serif; font-size: 10px; text-transform: uppercase; letter-spacing: 1px; color: #6b6b6b; font-weight: 600; padding-right: 8px; white-space: nowrap;">Upstream ({len(upstream_articles)})</td>
                                <td width="100%" style="border-bottom: 1px solid #e0ddd8;"></td>
                            </tr>
                        </table>
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%">
                            {upstream_links}
                        </table>
                    </td>
                </tr>
            </table>
        '''

    # DOWNSTREAM section (separate top-level section)
    downstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'downstream']

    if downstream_articles:
        downstream_links = ""
        for i, article in enumerate(downstream_articles):
            is_paywalled = is_paywall_article(article.get('domain', ''))
            paywall_badge = ' <span style="font-size: 10px; color: #ef4444; font-weight: 600; margin-left: 4px;">PAYWALL</span>' if is_paywalled else ''

            domain = article.get('domain', '')

            # Green tag with ticker or company name
            partner_ticker = article.get('feed_ticker')
            partner_name = strip_legal_suffixes(article.get('search_keyword', 'Unknown'))
            partner_tag = partner_ticker if partner_ticker else partner_name
            tag_html = f'<span style="{BADGE_COLORS["downstream"]}{BADGE_BASE_STYLE}">{partner_tag}</span>'

            domain_name = get_or_create_formal_domain_name(domain) if domain else "Unknown Source"
            date_str = format_article_date(article.get('published_at'))

            # Last article has no border
            is_last = (i == len(downstream_articles) - 1)
            border_style = "" if is_last else "border-bottom: 1px solid #e0ddd8;"

            downstream_links += f'''
                <tr>
                    <td style="padding: 10px 0; {border_style}">
                        <a href="{article.get('resolved_url', '#')}" style="font-family: Arial, Helvetica, sans-serif; font-size: 13px; color: #1a1a1a; text-decoration: none; font-weight: 500; line-height: 1.4;">{tag_html}{article.get('title', 'Untitled')}{paywall_badge}</a>
                        <p style="margin: 4px 0 0 0; font-family: Arial, Helvetica, sans-serif; font-size: 11px; color: #6b6b6b;">{domain_name} Â· {date_str}</p>
                    </td>
                </tr>
            '''

        html += f'''
            <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 20px;">
                <tr>
                    <td>
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%" style="margin-bottom: 10px;">
                            <tr>
                                <td style="font-family: Arial, Helvetica, sans-serif; font-size: 10px; text-transform: uppercase; letter-spacing: 1px; color: #6b6b6b; font-weight: 600; padding-right: 8px; white-space: nowrap;">Downstream ({len(downstream_articles)})</td>
                                <td width="100%" style="border-bottom: 1px solid #e0ddd8;"></td>
                            </tr>
                        </table>
                        <table role="presentation" cellpadding="0" cellspacing="0" border="0" width="100%">
                            {downstream_links}
                        </table>
                    </td>
                </tr>
            </table>
        '''

    return html



# ------------------------------------------------------------------------------
# Email #3: Stock Intelligence (Phase 3)
# ------------------------------------------------------------------------------
def generate_email_html_core(
    ticker: str,
    hours: int = 24,
    recipient_email: str = None,
    report_type: str = 'daily'
) -> Dict[str, any]:
    """
    Email #3 generation - Stock Intelligence Report with Phase 3 context integration.

    Generates Stock Intelligence Report HTML with Phase 3 formatted content.

    Args:
        ticker: Stock ticker symbol
        hours: Lookback window in hours (default: 24)
        recipient_email:
            - If provided: Generate real unsubscribe token (for test/immediate send)
            - If None: Use placeholder {{UNSUBSCRIBE_TOKEN}} (for production multi-recipient)
        report_type: 'daily' or 'weekly' - determines section filtering and branding (NEW Nov 2025)

    Returns:
        {
            "html": Full HTML email string,
            "subject": Email subject line,
            "company_name": Company name,
            "article_count": Number of articles analyzed
        }
    """
    LOG.info(f"Generating Email #3 (Stock Intelligence) for {ticker} (recipient: {recipient_email or 'placeholder'})")

    # Fetch ticker config
    config = get_ticker_config(ticker)
    if not config:
        LOG.error(f"No config found for {ticker}")
        return None

    company_name = config.get("company_name", ticker)
    sector = config.get("sector")
    sector_display = f" â€¢ {sector}" if sector and sector.strip() else ""

    # Fetch stock price using unified helper (live yfinance/Polygon â†’ database fallback)
    stock_data = get_filing_stock_data(ticker)
    stock_price = stock_data.get('stock_price') or "$0.00"
    price_change_pct = stock_data.get('price_change_pct')
    price_change_color = stock_data.get('price_change_color') or "#1e6b4a"
    ytd_return_pct = stock_data.get('ytd_return_pct')
    ytd_return_color = stock_data.get('ytd_return_color') or "#1e6b4a"
    market_status = stock_data.get('market_status')
    return_label = stock_data.get('return_label')

    # Fetch executive summary from database (Phase 3 merged JSON)
    executive_summary_text = ""
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT summary_text FROM executive_summaries
            WHERE ticker = %s AND summary_date = CURRENT_DATE
            ORDER BY generated_at DESC LIMIT 1
        """, (ticker,))
        result = cur.fetchone()
        if result:
            executive_summary_text = result['summary_text']
        else:
            LOG.warning(f"No executive summary found for {ticker} on CURRENT_DATE")
            return None

    # Parse Phase 3 JSON (same converter as Email #3)
    from modules.executive_summary_phase1 import convert_phase1_to_sections_dict, get_used_article_indices
    from modules.executive_summary_phase2 import apply_deduplication
    try:
        json_output = json.loads(executive_summary_text)

        # Apply deduplication - removes duplicate bullets and merges source_articles for primaries
        json_output = apply_deduplication(json_output)

        sections = convert_phase1_to_sections_dict(json_output)
        # Get indices of articles used in bullets that survived filtering
        # Pass report_type to exclude hidden sections (daily hides upcoming_catalysts, key_variables)
        used_article_indices = get_used_article_indices(json_output, report_type=report_type)
        LOG.info(f"[{ticker}] Email #3: {len(used_article_indices)} article indices used in final report (after deduplication, report_type={report_type})")
    except json.JSONDecodeError as e:
        LOG.error(f"[{ticker}] Failed to parse Phase 3 JSON in Email #3: {e}")
        sections = {}  # Empty sections
        used_article_indices = None  # No filtering (backward compat - show all)

    # NEW (Nov 2025): Filter sections based on report_type
    if report_type == 'daily':
        # Daily reports hide 4 sections (AI still generates them, just don't display)
        sections.pop('upside_scenario', None)
        sections.pop('downside_scenario', None)
        sections.pop('key_variables', None)
        sections.pop('upcoming_catalysts', None)
        LOG.info(f"[{ticker}] Daily report: Filtered to {len(sections)} sections (hid upside/downside/variables/catalysts)")
    else:
        LOG.info(f"[{ticker}] Weekly report: Showing all {len(sections)} sections")

    # Extract preheader text from bottom_line (full content for email preview)
    # Bottom line is already curated to be 1-2 sentences - use it all
    # Previously truncated at first ". " which broke on abbreviations like "U.S."
    preheader_text = ""
    if sections.get("bottom_line") and len(sections["bottom_line"]) > 0:
        raw_text = sections["bottom_line"][0]
        # Strip HTML tags (e.g., <em>context</em>) for plain text preheader
        preheader_text = re.sub(r'<[^>]+>', '', raw_text)

    # Fetch flagged articles (already sorted by SQL) - SAME AS EMAIL #3
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
    articles_by_category = {"company": [], "industry": [], "competitor": [], "value_chain": []}

    # Note: Email #3 uses same flagged articles as Email #3 (from Phase 1/2)
    # We fetch from executive_summaries metadata using CURRENT_DATE
    flagged_article_ids = []
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT article_ids FROM executive_summaries
            WHERE ticker = %s AND summary_date = CURRENT_DATE
            ORDER BY generated_at DESC LIMIT 1
        """, (ticker,))
        result = cur.fetchone()
        if result and result['article_ids']:
            # Parse JSON string to Python list for SQL query
            flagged_article_ids = json.loads(result['article_ids'])

    # Split article IDs by type: positive = real articles, negative = company releases
    real_article_ids = [aid for aid in flagged_article_ids if aid > 0]
    company_release_ids = [abs(aid) for aid in flagged_article_ids if aid < 0]

    articles = []

    with db() as conn, conn.cursor() as cur:
        # Query real articles from articles table
        if real_article_ids:
            cur.execute("""
                SELECT a.id, a.title, a.resolved_url, a.domain, a.published_at,
                       tf.category, ta.ticker, f.search_keyword, f.feed_ticker, tf.value_chain_type,
                       ta.relevance_score, ta.relevance_reason, ta.is_rejected
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                JOIN feeds f ON ta.feed_id = f.id
                WHERE ta.ticker = %s
                AND a.id = ANY(%s)
                AND (ta.is_rejected = FALSE OR ta.is_rejected IS NULL)
                AND ta.ai_summary IS NOT NULL
                AND (ta.ai_model IS NULL OR ta.ai_model != 'spam')
                ORDER BY a.published_at DESC NULLS LAST
            """, (ticker, real_article_ids))
            articles = list(cur.fetchall())
            LOG.info(f"[{ticker}] Email #3: Fetched {len(articles)} real articles")

        # Filter REAL ARTICLES to only those used in the final report (based on source_articles tracking)
        # IMPORTANT: Filter BEFORE adding company releases, as releases are not in Phase 1 timeline
        # The articles list is ordered by published_at DESC, matching Phase 1 timeline ordering
        # Index 0 = first article, index 1 = second article, etc.
        if used_article_indices is not None and articles:
            total_real_articles = len(articles)
            # Keep only articles whose timeline index is in the used set
            # Note: empty set is valid - means no articles were used (e.g., "no material developments")
            articles = [
                article for idx, article in enumerate(articles)
                if idx in used_article_indices
            ]
            filtered_count = total_real_articles - len(articles)
            if filtered_count > 0:
                LOG.info(f"[{ticker}] Email #3: Filtered {filtered_count} unused articles (showing {len(articles)} of {total_real_articles})")
            elif total_real_articles > 0 and len(articles) == 0:
                LOG.info(f"[{ticker}] Email #3: No articles used in final report (filtered all {total_real_articles})")
        elif articles:
            # No source_articles tracking (backward compatibility) - show all articles
            LOG.info(f"[{ticker}] Email #3: No source_articles tracking, showing all {len(articles)} articles")

        # Query company releases and map to article format
        # Company releases are always included (not filtered by source_articles)
        # They're not in the Phase 1 article timeline, so no index-based filtering applies
        # LEFT JOIN with sec_8k_filings to get exhibit URLs for 8-K releases
        if company_release_ids:
            cur.execute("""
                SELECT cr.id, cr.ticker, cr.company_name, cr.filing_date, cr.report_title,
                       cr.summary_markdown, cr.release_type, cr.source_type,
                       s8k.sec_html_url,
                       cr.exhibit_number
                FROM company_releases cr
                LEFT JOIN sec_8k_filings s8k ON cr.source_id = s8k.id
                WHERE cr.id = ANY(%s)
                ORDER BY cr.filing_date DESC
            """, (company_release_ids,))

            releases_fetched = 0
            for release_row in cur.fetchall():
                article_dict = map_company_release_to_article_dict(dict(release_row))
                if article_dict:
                    # Convert to match articles table structure (add missing fields)
                    article_dict.update({
                        'search_keyword': None,
                        'competitor_ticker': None,
                        'value_chain_type': None,
                        'relevance_score': None,
                        'relevance_reason': None,
                        'is_rejected': False
                    })
                    articles.append(article_dict)
                    releases_fetched += 1

            LOG.info(f"[{ticker}] Email #3: Fetched {releases_fetched} company releases (always included)")

        # Group all articles (real + company releases) by category
        for article in articles:
            category = article.get('category', 'company')  # Default to company if missing
            if category in articles_by_category:
                articles_by_category[category].append(article)

    # Calculate metrics
    analyzed_count = sum(len(arts) for arts in articles_by_category.values())
    paywall_count = sum(
        1 for articles in articles_by_category.values()
        for a in articles
        if is_paywall_article(a.get('domain', ''))
    )

    # Current date and market status
    eastern = pytz.timezone('US/Eastern')
    now_eastern = datetime.now(timezone.utc).astimezone(eastern)

    # Calculate date strings for subject and header
    if report_type == 'weekly':
        # Weekly: 7-day lookback ending yesterday (t-1)
        # Report runs at 2am, so lookback is previous 7 days before today
        end_date = now_eastern - timedelta(days=1)  # Yesterday
        start_date = end_date - timedelta(days=6)   # 7-day window

        # Subject: Full month name, date range (en-dash for proper typography)
        subject_date = f"{start_date.strftime('%B %d')}â€“{end_date.strftime('%d, %Y')}"
        # Header: Abbreviated month, title case, date range (en-dash for proper typography)
        current_date = f"{start_date.strftime('%b %d')}â€“{end_date.strftime('%d, %Y')}"
    else:  # daily
        # Subject: Full month name
        subject_date = now_eastern.strftime("%B %d, %Y")
        # Header: Abbreviated month, title case
        current_date = now_eastern.strftime("%b %d, %Y")

    # market_status and return_label already set from get_filing_stock_data() above

    # Build HTML sections - SAME AS EMAIL #3
    summary_html = build_executive_summary_html(sections, strip_emojis=True)
    articles_html = build_articles_html(articles_by_category)

    # Analysis message
    lookback_days = hours // 24 if hours >= 24 else 1
    analysis_message = f"Analysis based on {analyzed_count} articles from the past {lookback_days} {'days' if lookback_days > 1 else 'day'}."
    if paywall_count > 0:
        analysis_message += f" {paywall_count} {'article' if paywall_count == 1 else 'articles'} behind paywalls (titles shown)."

    # NEW (Nov 2025): Dynamic branding based on report_type
    if report_type == 'weekly':
        header_title = "WEAVARA WEEKLY WRAP-UP"
        footer_brand = "Weavara"
        footer_subtitle = "Intelligence Delivered Daily"
    else:  # daily
        header_title = "WEAVARA DAILY MORNING BRIEF"
        footer_brand = "Weavara"
        footer_subtitle = "Intelligence Delivered Daily"

    # Unsubscribe URL
    if recipient_email:
        unsubscribe_token = get_or_create_unsubscribe_token(recipient_email)
        if unsubscribe_token:
            unsubscribe_url = f"https://weavara.io/unsubscribe?token={unsubscribe_token}"
        else:
            unsubscribe_url = "https://weavara.io/unsubscribe"
            LOG.warning(f"No unsubscribe token for {recipient_email}, using generic link")
    else:
        unsubscribe_url = "{{UNSUBSCRIBE_TOKEN}}"

    # Build full HTML using Jinja2 template (Nov 2025 redesign)
    from jinja2 import Environment, FileSystemLoader
    template_env = Environment(loader=FileSystemLoader('templates'))
    user_report_template = template_env.get_template('email_user_report.html')

    html = user_report_template.render(
        # Email preheader (for Gmail/Outlook preview text)
        preheader_text=preheader_text,

        # Core identifiers
        ticker=ticker,
        company_name=company_name,
        sector=sector,  # Template handles "Â· Sector" formatting

        # Header branding
        header_title=header_title,
        current_date=current_date,
        market_status=market_status,

        # Price card (header)
        stock_price=stock_price,
        price_change_pct=price_change_pct,
        price_change_color=price_change_color,
        return_label=return_label,
        ytd_return_pct=ytd_return_pct,
        ytd_return_color=ytd_return_color,

        # 4-metric strip (Nov 2025)
        market_cap=stock_data.get('market_cap'),
        enterprise_value=stock_data.get('enterprise_value'),
        volume_ratio=stock_data.get('volume_ratio'),
        year_range=stock_data.get('year_range'),

        # Content (pre-rendered HTML)
        summary_html=summary_html,
        articles_html=articles_html,
        analysis_message=analysis_message,

        # Footer
        footer_brand=footer_brand,
        footer_subtitle=footer_subtitle,
        contact_email='support@weavara.io',
        unsubscribe_url=unsubscribe_url
    )

    # ========== Official user-facing email subject ==========
    if report_type == 'weekly':
        subject = f"{ticker} Weekly Wrap-up - {subject_date}"
    else:  # daily
        subject = f"{ticker} Daily Morning Brief - {subject_date}"

    return {
        "html": html,
        "subject": subject,
        "company_name": company_name,
        "article_count": analyzed_count
    }


def send_user_intelligence_report(
    hours: int = 24,
    tickers: List[str] = None,
    recipient_email: str = None,
    bcc: str = None,
    summary_date: date = None,
    report_type: str = None
) -> Dict:
    """
    Email #3 wrapper - SAME AS send_user_intelligence_report but uses Phase 3 integrated version.

    Args:
        hours: Lookback window
        tickers: List with one ticker
        recipient_email: Email recipient
        bcc: Optional BCC recipient (for admin monitoring during A/B testing)
        summary_date: Optional date for summary (defaults to today)

    Returns: {"status": "sent" | "failed", "articles_analyzed": X, ...}
    """
    LOG.info("=== EMAIL #4: STOCK INTELLIGENCE ===")

    # Single ticker only
    if not tickers or len(tickers) == 0:
        return {"status": "error", "message": "No ticker specified"}

    ticker = tickers[0]
    LOG.info(f"Generating stock intelligence report for {ticker} â†’ {recipient_email or ADMIN_EMAIL}")

    # Determine report_type if not provided (use day-of-week detection)
    if not report_type:
        report_type, _ = get_report_type_and_lookback()
        LOG.info(f"Test mode: Using day-of-week detection for report_type: {report_type}")
    else:
        LOG.info(f"Test mode: Using explicit report_type: {report_type}")

    # Call core function
    email_data = generate_email_html_core(
        ticker=ticker,
        hours=hours,
        recipient_email=recipient_email or ADMIN_EMAIL,
        report_type=report_type
    )

    if not email_data:
        return {"status": "error", "message": "Failed to generate email HTML"}

    # Send email immediately (with optional BCC)
    success = send_email(
        email_data['subject'],
        email_data['html'],
        to=recipient_email or ADMIN_EMAIL,
        bcc=bcc
    )

    LOG.info(f"ðŸ“§ Email #3 (Stock Intelligence): {'âœ… SENT' if success else 'âŒ FAILED'} to {recipient_email or ADMIN_EMAIL}")

    return {
        "status": "sent" if success else "failed",
        "articles_analyzed": email_data['article_count'],
        "ticker": ticker,
        "recipient": recipient_email or ADMIN_EMAIL,
        "email_type": "stock_intelligence"
    }

# ------------------------------------------------------------------------------
# Auth Middleware
# ------------------------------------------------------------------------------
def require_admin(request: Request):
    """Verify admin token from headers OR query params"""
    # Check headers first (more secure)
    token = request.headers.get("x-admin-token") or \
            request.headers.get("authorization", "").replace("Bearer ", "")

    # Fallback to query param if not in headers
    if not token:
        token = request.query_params.get("token")

    if token != ADMIN_TOKEN:
        raise HTTPException(status_code=401, detail="Unauthorized")

# ------------------------------------------------------------------------------
# JOB QUEUE MODELS & INFRASTRUCTURE
# ------------------------------------------------------------------------------

class JobSubmitRequest(BaseModel):
    tickers: List[str]
    minutes: int = 1440
    batch_size: int = 3
    triage_batch_size: int = 3
    mode: str = 'test'  # 'test' or 'daily' - defaults to test for safety
    report_type: str = None  # 'daily' or 'weekly' - if None, uses day-of-week detection

# ------------------------------------------------------------------------------
# JOB QUEUE SYSTEM - PostgreSQL-Based Background Processing
# ------------------------------------------------------------------------------

# Circuit Breaker for System-Wide Failure Detection
class CircuitBreaker:
    """Detect and halt processing on systematic failures"""
    def __init__(self, failure_threshold=3, reset_timeout=300):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.last_failure_time = None
        self.state = 'closed'  # closed = working, open = failing
        self.lock = threading.Lock()

    def record_failure(self, error_type: str, error_msg: str):
        with self.lock:
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                self.state = 'open'
                LOG.critical(f"ðŸš¨ CIRCUIT BREAKER OPEN: {self.failure_count} consecutive failures")
                LOG.critical(f"   Last error: {error_type}: {error_msg}")
                # TODO: Send alert email in production

    def record_success(self):
        with self.lock:
            # Reset if we've been in open state long enough
            if self.state == 'open' and self.last_failure_time:
                if time.time() - self.last_failure_time > self.reset_timeout:
                    LOG.info("âœ… Circuit breaker CLOSED: Resuming after timeout")
                    self.state = 'closed'
                    self.failure_count = 0
            elif self.state == 'closed':
                # Gradual recovery - reduce count on success
                self.failure_count = max(0, self.failure_count - 1)

    def is_open(self):
        with self.lock:
            return self.state == 'open'

    def reset(self):
        with self.lock:
            self.state = 'closed'
            self.failure_count = 0
            self.last_failure_time = None
            LOG.info("ðŸ”„ Circuit breaker manually reset")

# Global circuit breaker instance
job_circuit_breaker = CircuitBreaker(failure_threshold=3, reset_timeout=300)

# Job Queue Worker State
_job_worker_running = False
_job_worker_thread = None
_worker_heartbeat_monitor_thread = None
_worker_restart_count = 0
_last_worker_activity = None

# Forward declarations - these reference functions defined later in the file
# We use globals() to avoid circular imports

async def process_ingest_phase(job_id: str, ticker: str, minutes: int, batch_size: int, triage_batch_size: int, mode: str = 'daily', report_type: str = 'daily'):
    """Wrapper for ingest logic with error handling and progress tracking"""
    try:
        # Call the actual cron_ingest function which is defined later
        cron_ingest_func = globals().get('cron_ingest')
        if not cron_ingest_func:
            raise RuntimeError("cron_ingest function not yet defined")

        # Create mock request
        class MockRequest:
            def __init__(self):
                self.headers = {"x-admin-token": ADMIN_TOKEN}

        LOG.info(f"[JOB {job_id}] Calling cron_ingest for {ticker}...")

        result = await cron_ingest_func(
            MockRequest(),
            minutes=minutes,
            tickers=[ticker],
            batch_size=batch_size,
            triage_batch_size=triage_batch_size,
            mode=mode,
            report_type=report_type
        )

        LOG.info(f"[JOB {job_id}] cron_ingest completed for {ticker}")

        # Extract and store flagged articles in job config
        if result and isinstance(result, dict):
            phase2 = result.get('phase_2_triage', {})
            flagged_articles = phase2.get('flagged_articles', [])

            if flagged_articles:
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        UPDATE ticker_processing_jobs
                        SET config = jsonb_set(COALESCE(config, '{}'), '{flagged_articles}', %s::jsonb)
                        WHERE job_id = %s
                    """, (json.dumps(flagged_articles), job_id))
                LOG.info(f"[JOB {job_id}] Stored {len(flagged_articles)} flagged article IDs in job config")
            else:
                LOG.warning(f"[JOB {job_id}] No flagged articles found in triage results")

        return result

    except Exception as e:
        LOG.error(f"[JOB {job_id}] INGEST FAILED for {ticker}: {e}")
        LOG.error(f"[JOB {job_id}] Stacktrace: {traceback.format_exc()}")
        raise

async def resolve_google_news_url_with_scrapfly(url: str, ticker: str) -> tuple[Optional[str], Optional[str]]:
    """
    Use ScrapFly to resolve Google News redirects by fetching the final URL

    Uses ASP (Anti-Scraping Protection) + JS rendering to handle Google's anti-bot measures.
    ScrapFly follows redirects and returns the final URL after executing any JavaScript.

    Cost: ~$0.005-0.010 per request (higher due to ASP + JS rendering)
    Success rate: ~95% (ScrapFly designed specifically for Google scraping)

    Reference: https://scrapfly.io/blog/how-to-scrape-google-search/

    Returns:
        (resolved_url, error_message): URL if successful, error message if failed
    """
    try:
        if not SCRAPFLY_API_KEY:
            return None, "No ScrapFly API key configured"

        # Build params with anti-bot bypass for Google News
        # ASP (Anti-Scraping Protection) + JS rendering specifically recommended for Google
        params = {
            "key": SCRAPFLY_API_KEY,
            "url": url,
            "country": "us",
            "asp": "true",        # Anti-bot bypass (handles Google's anti-scraping)
            "render_js": "true",  # JavaScript execution (required for redirects)
        }

        session = get_http_session()
        async with session.get("https://api.scrapfly.io/scrape", params=params, timeout=aiohttp.ClientTimeout(total=15)) as response:
                if response.status == 200:
                    result = await response.json()
                    final_url = result.get("result", {}).get("url")

                    if final_url and "news.google.com" not in final_url:
                        return final_url, None

                    return None, "Still Google News URL after resolution"

                # Non-200 status
                error_text = await response.text()
                return None, f"HTTP {response.status}: {error_text[:100]}"

    except asyncio.TimeoutError:
        return None, "Timeout (15s exceeded)"
    except Exception as e:
        return None, f"{type(e).__name__}: {str(e)[:100]}"

async def resolve_flagged_google_news_urls(ticker: str, flagged_article_ids: List[int]) -> List[int]:
    """
    Resolve Google News URLs for flagged articles only (after triage, before digest)

    Resolution methods (NO title extraction - domain already extracted during ingestion):
    1. Advanced API resolution (Google internal API) - Free, fast when works
    2. Direct HTTP redirect (follow redirects) - Free, works for simple redirects
    3. ScrapFly resolution (paid) - ~$0.005-0.010/URL, 95% success rate (ASP + JS rendering)
    4. If all fail: Keep Google News URL with existing domain from DB

    Then check if resolved to Yahoo Finance:
    - If Yahoo: Extract original source (Google â†’ Yahoo â†’ Original chain)
    - If not Yahoo: Use resolved URL directly

    Benefits:
    - Only resolves 20-30 URLs (flagged articles) vs 150 URLs (all articles)
    - Spread out over time (no burst of concurrent requests)
    - Natural request pattern (like a human clicking links)
    - Domain from title already available for Email #1 and deduplication

    Returns:
    - Updated flagged_article_ids list with duplicates removed
    """
    LOG.info(f"[{ticker}] ðŸ”— Phase 1.5: Resolving Google News URLs for flagged articles...")

    # Get unresolved Google News articles
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT id, url, title, domain
            FROM articles
            WHERE id = ANY(%s)
            AND url LIKE '%%news.google.com%%'
            AND resolved_url IS NULL
        """, (flagged_article_ids,))

        unresolved = cur.fetchall()

    if not unresolved:
        LOG.info(f"[{ticker}] âœ… No Google News URLs need resolution (all already resolved or no Google News articles)")
        return flagged_article_ids

    total = len(unresolved)
    LOG.info(f"[{ticker}] ðŸ“‹ Found {total} unresolved Google News URLs")
    LOG.info(f"[{ticker}] ðŸ”„ Starting resolution process (Advanced API + Direct HTTP only)...")

    resolved_count = 0
    failed_count = 0
    yahoo_chain_count = 0  # Track Google â†’ Yahoo â†’ Final chains
    spam_blocked_count = 0  # Track spam articles blocked post-resolution
    spam_blocked_ids = []  # Track spam article IDs to remove from flagged list

    for idx, article in enumerate(unresolved, 1):
        article_id = article['id']
        url = article['url']
        title = article['title']
        existing_domain = article['domain']

        resolved_url = None
        resolution_method = None
        scrapfly_error = None

        try:
            # METHOD 1: Try Advanced API resolution first (silent)
            resolved_url = domain_resolver._resolve_google_news_url_advanced(url)
            if resolved_url:
                resolution_method = "Tier 1 (Advanced API)"

            # METHOD 2: Try direct HTTP redirect (silent)
            if not resolved_url:
                try:
                    response = requests.get(url, timeout=10, allow_redirects=True, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    })
                    final_url = response.url
                    if final_url != url and "news.google.com" not in final_url:
                        resolved_url = final_url
                        resolution_method = "Tier 2 (Direct HTTP)"
                except:
                    pass

            # METHOD 3: Try ScrapFly resolution (capture error if fails)
            if not resolved_url:
                resolved_url, scrapfly_error = await resolve_google_news_url_with_scrapfly(url, ticker)
                if resolved_url:
                    resolution_method = "Tier 3 (ScrapFly)"

            # If all methods failed, log and skip
            if not resolved_url:
                failed_count += 1
                # Smart categorization: ERROR for our infrastructure issues, WARNING for external failures
                if scrapfly_error:
                    if "429" in str(scrapfly_error):
                        # ScrapFly rate limited - SERIOUS (breaks workflow)
                        LOG.error(f"[{ticker}] âŒ [{idx}/{total}] SCRAPFLY RATE LIMITED â†’ {scrapfly_error}")
                        LOG.error(f"[{ticker}]    âš ï¸ This will cause broken google.news links and duplicates!")
                    elif "422" in str(scrapfly_error):
                        # ScrapFly config error - OUR mistake
                        LOG.error(f"[{ticker}] âŒ [{idx}/{total}] SCRAPFLY CONFIG ERROR â†’ {scrapfly_error}")
                    elif "Timeout" in str(scrapfly_error):
                        # Timeout - might indicate rate limiting or service issues
                        LOG.warning(f"[{ticker}] âš ï¸ [{idx}/{total}] ScrapFly timeout â†’ {scrapfly_error}")
                    else:
                        # Other failures (network, etc.)
                        LOG.warning(f"[{ticker}] âš ï¸ [{idx}/{total}] ScrapFly failed â†’ {scrapfly_error}")
                continue

            # Check if resolved to Yahoo Finance â†’ Extract original source
            is_yahoo_finance = any(yahoo_domain in resolved_url for yahoo_domain in [
                "finance.yahoo.com", "ca.finance.yahoo.com", "uk.finance.yahoo.com"
            ])

            if is_yahoo_finance:
                yahoo_original = extract_yahoo_finance_source_optimized(resolved_url)
                # Check if extraction actually succeeded (returned a DIFFERENT URL)
                if yahoo_original and yahoo_original != resolved_url:
                    final_resolved_url = yahoo_original
                    final_domain = normalize_domain(urlparse(yahoo_original).netloc.lower())
                    yahoo_chain_count += 1
                else:
                    # Extraction failed, keep Yahoo URL
                    final_resolved_url = resolved_url
                    final_domain = normalize_domain(urlparse(resolved_url).netloc.lower())
                    if yahoo_original == resolved_url:
                        LOG.warning(f"[{ticker}] âš ï¸ Yahoo extraction failed (no providerContentUrl): {resolved_url[:80]}")
            else:
                final_resolved_url = resolved_url
                final_domain = normalize_domain(urlparse(resolved_url).netloc.lower())

            # SPAM CHECK: Block spam domains after full resolution chain
            if final_domain in SPAM_DOMAINS:
                LOG.info(f"[{ticker}] ðŸš« SPAM BLOCKED (post-resolution): {final_domain} - {title[:60]}")
                spam_blocked_count += 1
                spam_blocked_ids.append(article_id)

                # Delete spam article from database
                with db() as conn2, conn2.cursor() as cur2:
                    cur2.execute("DELETE FROM ticker_articles WHERE article_id = %s", (article_id,))
                    cur2.execute("DELETE FROM articles WHERE id = %s", (article_id,))
                    LOG.info(f"[{ticker}] ðŸ—‘ï¸ Deleted spam article ID {article_id} from database")

                # Skip database UPDATE and move to next article
                continue

            # Update database (no source_url)
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    UPDATE articles
                    SET resolved_url = %s, domain = %s
                    WHERE id = %s
                """, (final_resolved_url, final_domain, article_id))

            # Store domain mapping for future use (if domain changed from fallback)
            # This prevents future ScrapFly calls for the same publication
            if existing_domain != final_domain and existing_domain.endswith('.com'):
                # Extract publication name from title
                clean_title, source_name = extract_source_from_title_smart(title)

                if source_name:
                    try:
                        domain_resolver._store_in_database(final_domain, source_name, ai_generated=False)
                        LOG.info(f"[{ticker}] ðŸ’¾ Stored mapping: '{source_name}' â†’ '{final_domain}' (learned from resolution)")
                    except Exception as e:
                        LOG.warning(f"[{ticker}] Failed to store domain mapping: {e}")

            resolved_count += 1
            # Show full resolved URL (not just domain)
            LOG.info(f"[{ticker}] âœ… [{idx}/{total}] {resolution_method} â†’ {final_resolved_url}")

        except Exception as e:
            failed_count += 1
            LOG.error(f"[{ticker}] âŒ [{idx}/{total}] Resolution failed: {str(e)[:100]}")

    # Summary
    LOG.info(f"[{ticker}] {'='*60}")
    LOG.info(f"[{ticker}] ðŸ“Š Resolution Summary:")
    LOG.info(f"[{ticker}]    Total processed: {total}")
    LOG.info(f"[{ticker}]    âœ… Succeeded: {resolved_count} ({resolved_count/total*100:.1f}%)")
    LOG.info(f"[{ticker}]    âŒ Failed: {failed_count} ({failed_count/total*100:.1f}%)")
    LOG.info(f"[{ticker}]    ðŸš« Spam blocked: {spam_blocked_count}")
    LOG.info(f"[{ticker}]    ðŸ”— Googleâ†’Yahooâ†’Final chains: {yahoo_chain_count}")
    LOG.info(f"[{ticker}] {'='*60}")

    # ============================================================================
    # POST-RESOLUTION DEDUPLICATION
    # ============================================================================
    # After resolution, check if multiple articles resolved to the SAME final URL
    # This catches: Google News â†’ Source A, Yahoo Finance â†’ Source A (same resolved_url)
    # Keep the best article, remove duplicates from flagged list
    # NOTE: System uses ID lists (not database column) to track flagged articles
    # ============================================================================
    LOG.info(f"[{ticker}] ðŸ” Checking for duplicate resolved URLs...")

    # Return early if no flagged articles
    if not flagged_article_ids:
        LOG.info(f"[{ticker}] âœ… No flagged articles to deduplicate")
        return flagged_article_ids or []  # Return empty list if None

    with db() as conn, conn.cursor() as cur:
        # Find flagged articles with duplicate resolved_url (use ID list, not database column)
        cur.execute("""
            SELECT resolved_url, array_agg(a.id ORDER BY a.published_at DESC) as article_ids,
                   array_agg(a.domain ORDER BY a.published_at DESC) as domains,
                   array_agg(a.title ORDER BY a.published_at DESC) as titles
            FROM articles a
            WHERE a.id = ANY(%s)
            AND a.resolved_url IS NOT NULL
            GROUP BY resolved_url
            HAVING COUNT(*) > 1
        """, (flagged_article_ids,))

        duplicates = cur.fetchall()

        if duplicates:
            removed_count = 0
            removed_ids = []

            for dup in duplicates:
                resolved_url = dup['resolved_url']
                article_ids = dup['article_ids']
                domains = dup['domains']
                titles = dup['titles']

                # Keep first article (newest by published_at), remove the rest from ID list
                keep_id = article_ids[0]
                remove_ids = article_ids[1:]

                LOG.info(f"[{ticker}] ðŸ”„ Duplicate resolved URL: {resolved_url[:80]}...")
                LOG.info(f"[{ticker}]    âœ… Keeping: ID {keep_id} ({domains[0]}) - {titles[0][:60]}...")

                for idx, remove_id in enumerate(remove_ids, 1):
                    LOG.info(f"[{ticker}]    âŒ Removing: ID {remove_id} ({domains[idx]}) - {titles[idx][:60]}...")
                    removed_ids.append(remove_id)
                    removed_count += 1

            # Remove duplicates from Python list (not database - no flagged column exists)
            flagged_article_ids = [aid for aid in flagged_article_ids if aid not in removed_ids]

            LOG.info(f"[{ticker}] âœ… Removed {removed_count} duplicate articles from flagged list")
        else:
            LOG.info(f"[{ticker}] âœ… No duplicate resolved URLs found")

    return flagged_article_ids

async def process_scrape_phase(job_id: str, ticker: str, flagged_article_ids: List[int] = None) -> Dict:
    """Scrape content for flagged articles.

    This is a PURE scraping function - no AI generation, no email sending.
    Runs AFTER Phase 1.5 URL resolution, ensuring all URLs are resolved before scraping.

    Args:
        job_id: Job ID for logging
        ticker: Stock ticker symbol
        flagged_article_ids: List of article IDs to scrape

    Returns:
        Dict with scraping stats (scraped, reused, failed counts)
    """
    try:
        # ============================================================================
        # PHASE 4: CONTENT SCRAPING (MOVED FROM INGEST PHASE - Oct 2025)
        # ============================================================================
        # Now runs AFTER Phase 1.5 Google URL resolution, ensuring all URLs are resolved
        # before scraping attempts
        # ============================================================================

        if flagged_article_ids:
            LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Phase 4: Scraping {len(flagged_article_ids)} flagged articles...")

            # Get flagged articles that need scraping
            # NOTE: Articles with resolved_url = NULL are included (happens when resolution failed)
            # The scraper will fall back to the original URL, which may fail but won't crash
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    SELECT a.id, a.url, a.url_hash, a.resolved_url, a.title, a.description,
                           a.domain, a.published_at, ta.feed_id,
                           a.scraped_content, a.content_scraped_at, ta.ai_summary,
                           tf.category, tf.value_chain_type, f.feed_ticker, f.search_keyword
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    JOIN ticker_feeds tf ON (ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker)
                    JOIN feeds f ON tf.feed_id = f.id
                    WHERE a.id = ANY(%s)
                    AND ta.ticker = %s
                    AND a.scraping_failed = FALSE
                    AND (
                        a.scraped_content IS NULL
                        OR
                        (a.scraped_content IS NOT NULL AND ta.ai_summary IS NULL)
                    )
                    ORDER BY a.published_at DESC NULLS LAST
                """, (flagged_article_ids, ticker))

                articles_to_scrape = cur.fetchall()

            if articles_to_scrape:
                # Count articles by processing type
                needs_scraping = sum(1 for a in articles_to_scrape if a['scraped_content'] is None)
                needs_ai_only = len(articles_to_scrape) - needs_scraping

                LOG.info(f"[{ticker}] ðŸ” Found {len(articles_to_scrape)} articles to process: {needs_scraping} need scraping, {needs_ai_only} need AI summary only")

                # Get ticker metadata for AI analysis
                config = get_ticker_config(ticker)
                metadata = {
                    "industry_keywords": config.get("industry_keywords", []) if config else [],
                    "competitors": config.get("competitors", []) if config else [],
                    "company_name": config.get("company_name", ticker) if config else ticker
                }

                # Process articles in batches using existing batch scraping logic
                BATCH_SIZE = 5
                total_scraped = 0
                total_reused = 0
                total_failed = 0

                for i in range(0, len(articles_to_scrape), BATCH_SIZE):
                    batch = articles_to_scrape[i:i + BATCH_SIZE]
                    batch_num = (i // BATCH_SIZE) + 1
                    total_batches = (len(articles_to_scrape) + BATCH_SIZE - 1) // BATCH_SIZE

                    LOG.info(f"[{ticker}] ðŸ“¦ Processing scraping batch {batch_num}/{total_batches} ({len(batch)} articles)")

                    # Convert to format expected by process_article_batch_async
                    batch_articles = [dict(row) for row in batch]
                    batch_categories = [row['category'] for row in batch]

                    try:
                        # Use existing batch scraping function
                        batch_results = await process_article_batch_async(
                            batch_articles,
                            batch_categories,
                            metadata,
                            ticker
                        )

                        # Count successes and failures
                        batch_scraped = 0
                        batch_reused = 0
                        batch_failed = 0
                        for result in batch_results:
                            if result["success"] and result.get("scraped_content"):
                                if result.get("reused"):
                                    batch_reused += 1
                                    total_reused += 1
                                else:
                                    batch_scraped += 1
                                    total_scraped += 1
                            else:
                                batch_failed += 1
                                total_failed += 1

                        LOG.info(f"[{ticker}] âœ… Batch {batch_num}/{total_batches} complete: {batch_scraped} scraped, {batch_reused} reused, {batch_failed} failed")

                    except Exception as e:
                        LOG.error(f"[{ticker}] âŒ Batch {batch_num} scraping error: {e}")
                        total_failed += len(batch)

                total_successful = total_scraped + total_reused
                LOG.info(f"[{ticker}] ðŸ“Š Scraping complete: {total_successful} successful ({total_scraped} scraped + {total_reused} reused), {total_failed} failed")

                return {
                    "scraped": total_scraped,
                    "reused": total_reused,
                    "failed": total_failed,
                    "total": len(articles_to_scrape)
                }
            else:
                LOG.info(f"[{ticker}] âœ… All flagged articles already processed")
                return {"scraped": 0, "reused": 0, "failed": 0, "total": 0}
        else:
            LOG.info(f"[{ticker}] âš ï¸ No flagged articles to scrape")
            return {"scraped": 0, "reused": 0, "failed": 0, "total": 0}

    except Exception as e:
        LOG.error(f"[JOB {job_id}] SCRAPE PHASE FAILED for {ticker}: {e}")
        LOG.error(f"[JOB {job_id}] Stacktrace: {traceback.format_exc()}")
        raise


# Backward-compatible alias (deprecated - use process_scrape_phase instead)
async def process_digest_phase(job_id: str, ticker: str, minutes: int, flagged_article_ids: List[int] = None, mode: str = 'daily', report_type: str = 'daily'):
    """DEPRECATED: Use process_scrape_phase() instead.

    This wrapper exists for backward compatibility only.
    The minutes, mode, and report_type parameters are ignored.
    """
    LOG.warning("process_digest_phase is DEPRECATED - use process_scrape_phase() instead")
    return await process_scrape_phase(job_id=job_id, ticker=ticker, flagged_article_ids=flagged_article_ids)

# NOTE (Nov 2025): process_commit_phase() REMOVED
# - Per-ticker GitHub commits are no longer needed
# - CSV is source of truth, database doesn't change during processing
# - Function was only called from process_ticker_job() which now skips commits

def get_worker_id():
    """Get unique worker ID (Render instance or hostname)"""
    return os.getenv('RENDER_INSTANCE_ID') or os.getenv('HOSTNAME') or 'worker-local'

def json_serialize_default(obj):
    """Custom JSON serializer for objects not serializable by default json module.

    Handles:
    - datetime/date objects â†’ ISO 8601 format strings
    - Decimal objects â†’ float (PostgreSQL numeric columns)

    Used by update_job_status() to safely serialize job results that may contain
    datetime objects from database queries (e.g., articles_by_ticker with published_at)
    or Decimal values from numeric columns.
    """
    from decimal import Decimal
    if isinstance(obj, (datetime, date)):
        return obj.isoformat()
    if isinstance(obj, Decimal):
        return float(obj)
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")

def update_job_status(job_id: str, status: str = None, phase: str = None, progress: int = None,
                     error_message: str = None, error_stacktrace: str = None, result: dict = None,
                     memory_mb: float = None, duration_seconds: float = None):
    """Update job status in database"""
    updates = ["last_updated = NOW()"]
    params = []

    if status:
        updates.append("status = %s")
        params.append(status)

        if status == 'processing':
            updates.append("started_at = NOW()")
        elif status in ('completed', 'failed', 'timeout', 'cancelled'):
            updates.append("completed_at = NOW()")

    if phase:
        updates.append("phase = %s")
        params.append(phase)

    if progress is not None:
        updates.append("progress = %s")
        params.append(progress)

    if error_message:
        updates.append("error_message = %s")
        params.append(error_message)

    if error_stacktrace:
        updates.append("error_stacktrace = %s")
        params.append(error_stacktrace)

    if result:
        updates.append("result = %s")
        params.append(json.dumps(result, default=json_serialize_default))

    if memory_mb is not None:
        updates.append("memory_mb = %s")
        params.append(memory_mb)

    if duration_seconds is not None:
        updates.append("duration_seconds = %s")
        params.append(duration_seconds)

    params.append(job_id)

    with db() as conn, conn.cursor() as cur:
        cur.execute(f"""
            UPDATE ticker_processing_jobs
            SET {', '.join(updates)}
            WHERE job_id = %s
        """, params)

def get_next_queued_job():
    """Get next queued job with atomic claim (prevents race conditions)"""
    with db() as conn, conn.cursor() as cur:
        try:
            cur.execute("""
                UPDATE ticker_processing_jobs
                SET status = 'processing',
                    started_at = NOW(),
                    timeout_at = NOW() + INTERVAL '45 minutes',
                    worker_id = %s,
                    last_updated = NOW()
                WHERE job_id = (
                    SELECT job_id FROM ticker_processing_jobs
                    WHERE status = 'queued'
                    ORDER BY created_at
                    LIMIT 1
                    FOR UPDATE SKIP LOCKED
                )
                RETURNING *
            """, (get_worker_id(),))

            job = cur.fetchone()
            if job:
                LOG.info(f"[{job['ticker']}] ðŸ“‹ Claimed job {job['job_id']} for ticker {job['ticker']}")
            return dict(job) if job else None

        except Exception as e:
            LOG.error(f"Error claiming job: {e}")
            return None

# ============================================================================
# Background Heartbeat Thread (Prevents Premature Job Reclaim)
# ============================================================================

# Global dict to track active heartbeats: {job_id: threading.Event}
_active_heartbeats = {}
_heartbeat_lock = threading.Lock()

def start_heartbeat_thread(job_id: str):
    """
    Start background heartbeat thread for a job.

    Updates last_updated every 60 seconds to prevent job reclaim during long phases.
    Prevents the job from being marked as "stale" when ingest/digest phases take >3 minutes.

    Thread automatically stops when stop_heartbeat_thread() is called.
    """
    stop_event = threading.Event()

    def heartbeat_loop():
        """Background thread that updates job heartbeat every 60 seconds"""
        job_id_str = str(job_id)  # Convert to string for logging
        while not stop_event.is_set():
            try:
                # Update last_updated to keep job alive
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        UPDATE ticker_processing_jobs
                        SET last_updated = NOW()
                        WHERE job_id = %s AND status = 'processing'
                    """, (job_id_str,))

                    # Check if update succeeded
                    if cur.rowcount > 0:
                        LOG.debug(f"ðŸ’“ [JOB {job_id_str[:8]}] Heartbeat updated")
                    else:
                        # Job no longer processing (completed or failed)
                        LOG.debug(f"ðŸ’“ [JOB {job_id_str[:8]}] Heartbeat stopped (job no longer processing)")
                        break

            except Exception as e:
                LOG.error(f"ðŸ’“ [JOB {job_id_str[:8]}] Heartbeat update failed: {e}")
                # Continue trying - don't let heartbeat thread crash

            # Wait 60 seconds (or until stop event is set)
            stop_event.wait(60)

        LOG.debug(f"ðŸ’“ [JOB {job_id_str[:8]}] Heartbeat thread exiting")

    # Store stop event in global dict
    with _heartbeat_lock:
        _active_heartbeats[job_id] = stop_event

    # Start background thread
    thread = threading.Thread(
        target=heartbeat_loop,
        daemon=True,
        name=f"Heartbeat-{str(job_id)[:8]}"
    )
    thread.start()
    LOG.info(f"ðŸ’“ [JOB {str(job_id)[:8]}] Heartbeat thread started (updates every 60s)")

def stop_heartbeat_thread(job_id: str):
    """
    Stop background heartbeat thread for a job.

    Should be called when job completes, fails, or is cancelled.
    """
    job_id_str = str(job_id)  # Convert to string for logging and dict lookup
    with _heartbeat_lock:
        stop_event = _active_heartbeats.pop(job_id, None)

    if stop_event:
        stop_event.set()  # Signal thread to stop
        LOG.info(f"ðŸ’“ [JOB {job_id_str[:8]}] Heartbeat thread stopped")
    else:
        LOG.warning(f"ðŸ’“ [JOB {job_id_str[:8]}] No heartbeat thread found to stop")

# ============================================================================
# Filing Stock Data Helper
# ============================================================================

def get_filing_stock_data(ticker: str) -> dict:
    """
    Get real-time stock data for filing emails (10-K, 10-Q, Transcripts, Press Releases, User Reports).

    Priority order:
    1. FMP (Financial Modeling Prep) - Fast, reliable, has pre-calculated YTD
    2. yfinance - Full data but unreliable (timeouts, rate limits)
    3. Polygon.io - Emergency fallback, price + daily return only (no YTD)
    4. Return all None if everything fails (email will hide price card)

    NOTE: Database fallback removed (Nov 2025) - stale data is worse than no data.

    Returns:
        dict with template variables:
        - Header price card: stock_price, price_change_pct, price_change_color,
          ytd_return_pct, ytd_return_color, market_status, return_label
        - 4-metric strip (Nov 2025): market_cap, enterprise_value, volume_ratio, year_range
        All values are None if data fetch completely fails.
    """
    LOG.info(f"[{ticker}] Fetching stock data for email (FMP â†’ yfinance â†’ Polygon)")

    # Get live data using 3-tier fallback (FMP â†’ yfinance â†’ Polygon)
    live_data = get_stock_context(ticker)

    # If all sources failed, return None for everything (email will hide price card)
    if not live_data or not live_data.get('financial_last_price'):
        LOG.warning(f"[{ticker}] âš ï¸ All price sources failed (FMP, yfinance, Polygon) - email will hide price card")
        return {
            'stock_price': None,
            'price_change_pct': None,
            'price_change_color': None,
            'ytd_return_pct': None,
            'ytd_return_color': None,
            'market_status': None,
            'return_label': None,
            # 4-metric strip (Nov 2025)
            'market_cap': None,
            'enterprise_value': None,
            'volume_ratio': None,
            'year_range': None
        }

    # Successfully got data - format for templates
    market_is_open = is_market_open()
    daily_return = live_data.get('financial_price_change_pct')
    ytd_return = live_data.get('financial_ytd_return_pct')

    # Extract raw values for 4-metric strip (Nov 2025)
    market_cap_raw = live_data.get('financial_market_cap')
    enterprise_value_raw = live_data.get('financial_enterprise_value')
    volume_raw = live_data.get('financial_volume')
    avg_volume_raw = live_data.get('financial_avg_volume')
    year_high_raw = live_data.get('financial_year_high')
    year_low_raw = live_data.get('financial_year_low')

    # Format 4-metric strip values
    # Market Cap: "$4.32T"
    market_cap_formatted = format_financial_number(market_cap_raw) if market_cap_raw else None

    # Enterprise Value: "$4.29T"
    enterprise_value_formatted = format_financial_number(enterprise_value_raw) if enterprise_value_raw else None

    # Volume Ratio: "1.4x Avg"
    volume_ratio_formatted = None
    if volume_raw and avg_volume_raw and avg_volume_raw > 0:
        ratio = volume_raw / avg_volume_raw
        volume_ratio_formatted = f"{ratio:.1f}x Avg"

    # 52-Week Range: "$86.24 â€“ $195.87"
    year_range_formatted = None
    if year_low_raw and year_high_raw:
        year_range_formatted = f"${year_low_raw:.2f} â€“ ${year_high_raw:.2f}"

    return {
        # Header price card (existing)
        'stock_price': f"${live_data['financial_last_price']:.2f}",
        'price_change_pct': f"{'+' if daily_return >= 0 else ''}{daily_return:.2f}%" if daily_return is not None else None,
        'price_change_color': "#1e6b4a" if daily_return is not None and daily_return >= 0 else "#9b2c2c",
        'ytd_return_pct': f"{'+' if ytd_return >= 0 else ''}{ytd_return:.2f}%" if ytd_return is not None else None,
        'ytd_return_color': "#1e6b4a" if ytd_return is not None and ytd_return >= 0 else "#9b2c2c",
        'market_status': "Intraday" if market_is_open else "Last Close",
        'return_label': "TODAY" if market_is_open else "1D",
        # 4-metric strip (Nov 2025)
        'market_cap': market_cap_formatted,
        'enterprise_value': enterprise_value_formatted,
        'volume_ratio': volume_ratio_formatted,
        'year_range': year_range_formatted
    }


# ============================================================================
# Job Processing
# ============================================================================

async def process_company_profile_phase(job: dict):
    """Process company profile generation (5-15 min, Gemini 2.5)"""
    job_id = job['job_id']
    ticker_raw = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    # DEFENSE IN DEPTH: Normalize ticker even if job config should already have it normalized
    ticker = normalize_ticker_format(ticker_raw)
    if ticker != ticker_raw:
        LOG.warning(f"[JOB {job_id}] Normalized ticker from '{ticker_raw}' to '{ticker}'")

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        # Progress: 10% - Extracting 10-K text
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Extracting 10-K text...")

        # Check if using FMP mode (SEC.gov HTML) or file upload mode
        if 'sec_html_url' in config and config['sec_html_url']:
            # FMP MODE: Fetch HTML from SEC.gov
            LOG.info(f"[{ticker}] Using FMP mode - fetching from SEC.gov")
            content = fetch_sec_html_text(config['sec_html_url'])
        else:
            # FILE UPLOAD MODE: Extract from uploaded PDF/TXT
            LOG.info(f"[{ticker}] Using file upload mode")
            file_path = config.get('file_path')
            if not file_path:
                raise ValueError("Neither sec_html_url nor file_path provided in config")

            file_ext = config.get('file_ext', 'pdf')

            if file_ext == 'pdf':
                content = extract_pdf_text(file_path)
            else:  # txt
                content = extract_text_file(file_path)

        if not content or len(content) < 1000:
            raise ValueError(f"Extracted text too short ({len(content)} chars)")

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Extracted {len(content)} characters")

        # Progress: 30% - Generating profile with Gemini (this takes 5-10 min)
        update_job_status(job_id, progress=30)
        LOG.info(f"[{ticker}] ðŸ¤– [JOB {job_id}] Generating profile with Gemini 2.5 Flash (5-10 min)...")

        ticker_config = get_ticker_config(ticker)

        # Use unified function with comprehensive GEMINI_10K_PROMPT (254 lines, 16 sections)
        result = generate_sec_filing_profile_with_gemini(
            ticker=ticker,
            content=content,
            config=ticker_config,
            filing_type='10-K',
            fiscal_year=config['fiscal_year'],
            filing_date=config['filing_date'],
            gemini_api_key=GEMINI_API_KEY
        )

        if not result:
            raise ValueError("Gemini profile generation failed")

        profile_markdown = result['profile_markdown']
        metadata = result['metadata']

        # Progress: 80% - Saving to database
        update_job_status(job_id, progress=80)
        LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Saving profile to database...")

        with db() as conn:
            # Determine source file name based on mode
            if 'sec_html_url' in config and config['sec_html_url']:
                # FMP Mode - use SEC.gov as source
                source_file = f"SEC.gov 10-K (FY{config['fiscal_year']})"
            else:
                # File Upload Mode - use uploaded file name
                source_file = config.get('file_name', f"10-K FY{config['fiscal_year']}")

            save_config = {
                'company_name': ticker_config['company_name'],
                'industry': ticker_config.get('industry'),
                'fiscal_year': config['fiscal_year'],
                'filing_date': config['filing_date'],
                'period_end_date': config.get('period_end_date'),
                'source_file': source_file
            }

            save_company_profile_to_database(
                ticker, profile_markdown, save_config, metadata, conn
            )

        # Progress: 95% - Sending email
        if config.get('send_email', True):
            update_job_status(job_id, progress=95)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending email notification...")

            try:
                # Get real-time stock data (yfinance â†’ Polygon â†’ database â†’ None)
                stock_data = get_filing_stock_data(ticker)

                email_data = generate_company_profile_email(
                    ticker=ticker,
                    company_name=ticker_config['company_name'],
                    industry=ticker_config.get('industry', 'N/A'),
                    fiscal_year=config['fiscal_year'],
                    filing_date=config['filing_date'],
                    profile_markdown=profile_markdown,
                    stock_price=stock_data['stock_price'],
                    price_change_pct=stock_data['price_change_pct'],
                    price_change_color=stock_data['price_change_color'],
                    ytd_return_pct=stock_data['ytd_return_pct'],
                    ytd_return_color=stock_data['ytd_return_color'],
                    market_status=stock_data['market_status'],
                    return_label=stock_data['return_label'],
                    filing_type="10-K"
                )

                send_email(
                    subject=email_data['subject'],
                    html_body=email_data['html'],
                    to=ADMIN_EMAIL  # âœ… FIXED: Was hardcoded 'stockdigest.research@gmail.com'
                )
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email sent to {ADMIN_EMAIL}")

            except Exception as email_error:
                LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send email: {email_error}")
                # Continue - profile was saved successfully, email failure shouldn't mark job as failed

        # Clean up temp file (only in file upload mode)
        if 'file_path' in locals() and file_path and os.path.exists(file_path):
            os.remove(file_path)
            LOG.info(f"[{ticker}] ðŸ—‘ï¸ [JOB {job_id}] Cleaned up temp file: {file_path}")

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Company profile generation complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Company profile generation failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],  # Limit size
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_quality_review_phase(job: dict):
    """Process comprehensive quality review (2-4 min, 4 Gemini calls)

    Runs all 4 quality review phases:
    - Phase 1: Article verification against executive summary
    - Phase 2: Filing context verification (10-K/10-Q/Transcript)
    - Phase 3: Context relevance verification
    - Phase 4: Metadata & structure validation

    Sends email report with pass/fail status.
    """
    job_id = job['job_id']
    ticker = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    try:
        # Start heartbeat thread (CRITICAL: Quality review takes 2-4 minutes)
        start_heartbeat_thread(job_id)

        # Progress: 10% - Fetch executive summary from database
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ” [JOB {job_id}] Fetching executive summary for quality review...")

        # Determine target date
        review_date_str = config.get('review_date')
        if review_date_str:
            if isinstance(review_date_str, str):
                target_date = datetime.strptime(review_date_str, '%Y-%m-%d').date()
            else:
                target_date = review_date_str
            LOG.info(f"[{ticker}] Using explicit date parameter: {target_date}")
        else:
            target_date = datetime.now(timezone.utc).date()
            LOG.info(f"[{ticker}] Using current date (UTC): {target_date}")

        # Fetch executive summary
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT summary_json, article_ids
                FROM executive_summaries
                WHERE ticker = %s AND summary_date = %s
                ORDER BY generated_at DESC LIMIT 1
            """, (ticker, target_date))

            summary_row = cur.fetchone()

        if not summary_row:
            raise ValueError(f"No executive summary found for {ticker} on {target_date}")

        # Parse executive summary JSON
        if summary_row['summary_json']:
            executive_summary = summary_row['summary_json']
        else:
            # Legacy: parse from summary_text if summary_json not available
            executive_summary = json.loads(summary_row.get('summary_text', '{}'))

        if not executive_summary:
            raise ValueError("Executive summary JSON is empty")

        # Get article IDs
        article_ids_json = summary_row.get('article_ids')
        if isinstance(article_ids_json, str):
            article_ids = json.loads(article_ids_json)
        elif isinstance(article_ids_json, list):
            article_ids = article_ids_json
        else:
            raise ValueError("No article IDs found in executive summary")

        LOG.info(f"[{ticker}] Found {len(article_ids)} article IDs for review")

        # Split article IDs by type: positive = real articles, negative = company releases
        real_article_ids = [aid for aid in article_ids if aid > 0]
        company_release_ids = [abs(aid) for aid in article_ids if aid < 0]

        articles = []

        # Fetch article objects (domain + summary) for Phase 1 attribution validation
        with db() as conn, conn.cursor() as cur:
            # Fetch real articles
            if real_article_ids:
                cur.execute("""
                    SELECT a.domain, a.title, ta.ai_summary, ta.article_id
                    FROM ticker_articles ta
                    JOIN articles a ON ta.article_id = a.id
                    WHERE ta.ticker = %s
                    AND ta.article_id = ANY(%s)
                    AND ta.ai_summary IS NOT NULL
                    ORDER BY ta.article_id
                """, (ticker, real_article_ids))

                articles.extend([
                    {
                        "domain": row['domain'],
                        "title": row['title'],
                        "ai_summary": row['ai_summary'],
                        "article_id": row['article_id']
                    }
                    for row in cur.fetchall() if row['ai_summary']
                ])
                LOG.info(f"[{ticker}] Quality review: Fetched {len(articles)} real article summaries")

            # Fetch company releases
            if company_release_ids:
                cur.execute("""
                    SELECT id, report_title, summary_markdown
                    FROM company_releases
                    WHERE id = ANY(%s)
                    ORDER BY id
                """, (company_release_ids,))

                releases_fetched = 0
                for row in cur.fetchall():
                    if row['summary_markdown']:
                        articles.append({
                            "domain": "Company Release",
                            "title": row['report_title'],
                            "ai_summary": row['summary_markdown'],
                            "article_id": -row['id']  # Preserve negative ID for consistency
                        })
                        releases_fetched += 1

                LOG.info(f"[{ticker}] Quality review: Fetched {releases_fetched} company release summaries")

        if not articles:
            raise ValueError("No articles found for verification")

        LOG.info(f"[{ticker}] Found {len(articles)} articles for Phase 1 verification")

        # ============================================================
        # PHASE 1: Article Verification (30-90 seconds)
        # ============================================================
        update_job_status(job_id, progress=25)
        LOG.info(f"[{ticker}] ðŸ“° [JOB {job_id}] Phase 1: Article verification (30-90s)...")

        from modules.quality_review import review_executive_summary_quality

        phase1_result = review_executive_summary_quality(
            ticker=ticker,
            phase1_json=executive_summary,
            articles=articles,
            gemini_api_key=GEMINI_API_KEY
        )

        if not phase1_result:
            raise ValueError("Phase 1 quality review failed")

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 1 complete")

        # ============================================================
        # PHASE 2: Filing Context Verification (30-90 seconds)
        # ============================================================
        update_job_status(job_id, progress=45)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Phase 2: Filing context verification (30-90s)...")

        from modules.executive_summary_phase2 import _fetch_available_filings
        from modules.quality_review_phase2 import review_phase2_context_quality

        # Fetch available filings (10-K, 10-Q, Transcript)
        filings = _fetch_available_filings(ticker, db)

        # Fetch ticker metadata for validation
        ticker_metadata = get_ticker_config(ticker)

        phase2_result = None
        if filings:
            LOG.info(f"[{ticker}] Found {len(filings)} filing(s) for Phase 2: {list(filings.keys())}")

            phase2_result = review_phase2_context_quality(
                ticker=ticker,
                executive_summary=executive_summary,
                filings=filings,
                gemini_api_key=GEMINI_API_KEY,
                ticker_metadata=ticker_metadata
            )

            if phase2_result:
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 2 complete")
            else:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Phase 2 failed, continuing without it")
        else:
            LOG.info(f"[{ticker}] â„¹ï¸ [JOB {job_id}] No filings available - skipping Phase 2")

        # ============================================================
        # PHASE 3: Context Relevance Verification (20-40 seconds)
        # ============================================================
        update_job_status(job_id, progress=65)
        LOG.info(f"[{ticker}] ðŸ”— [JOB {job_id}] Phase 3: Context relevance verification (20-40s)...")

        from modules.quality_review_phase3 import review_context_relevance

        phase3_result = review_context_relevance(
            ticker=ticker,
            executive_summary=executive_summary,
            gemini_api_key=GEMINI_API_KEY
        )

        if phase3_result:
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 3 complete")
        else:
            LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Phase 3 failed, continuing without it")

        # ============================================================
        # PHASE 4: Metadata & Structure Validation (20-40 seconds)
        # ============================================================
        update_job_status(job_id, progress=85)
        LOG.info(f"[{ticker}] ðŸ—ï¸ [JOB {job_id}] Phase 4: Metadata & structure validation (20-40s)...")

        from modules.quality_review_phase4 import review_phase4_metadata_and_structure

        phase4_result = review_phase4_metadata_and_structure(
            ticker=ticker,
            executive_summary=executive_summary,
            gemini_api_key=GEMINI_API_KEY
        )

        if phase4_result:
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 4 complete")
        else:
            LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Phase 4 failed, continuing without it")

        # ============================================================
        # EMAIL GENERATION & SENDING (5-10 seconds)
        # ============================================================
        if config.get('send_email', True):
            update_job_status(job_id, progress=95)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Generating and sending email report...")

            try:
                from modules.quality_review import generate_bullet_centric_review_email_html

                report_html = generate_bullet_centric_review_email_html(
                    phase1_result=phase1_result,
                    phase2_result=phase2_result,
                    phase3_result=phase3_result,
                    phase4_result=phase4_result
                )

                # Determine subject line (pass/fail based on critical errors)
                p1_summary = phase1_result["summary"]
                p1_critical = p1_summary["errors_by_severity"].get("CRITICAL", 0)

                if phase2_result:
                    p2_summary = phase2_result["summary"]
                    p2_critical = p2_summary["errors_by_severity"].get("CRITICAL", 0)
                    total_critical = p1_critical + p2_critical

                    if total_critical > 0:
                        subject = f"QA Quality Review: {ticker} - FAIL ({p1_critical} Phase 1 + {p2_critical} Phase 2 critical errors)"
                    else:
                        subject = f"QA Quality Review: {ticker} - PASS"
                else:
                    if p1_critical > 0:
                        subject = f"QA Quality Review: {ticker} - FAIL ({p1_critical} critical errors, Phase 2 skipped)"
                    else:
                        subject = f"QA Quality Review: {ticker} - PASS (Phase 2 skipped)"

                # Send email
                send_email(subject, report_html)

                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email sent successfully")

            except Exception as email_error:
                LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send email: {email_error}")
                # Continue - review completed successfully, email failure shouldn't mark job as failed

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Quality review complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Quality review failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_regenerate_email_phase(job: dict):
    """Process email regeneration for a ticker (2-4 min, multiple AI phases)

    Regenerates Email #3 using the SAME articles from the original run:
    - Phase 1: Generate executive summary from articles
    - Phase 1.5: Known info filter (if enabled)
    - Phase 2: Filing context enrichment
    - Phase 3: Context integration
    - Email #2 + Email #3 generation

    This is a worker-based version of /api/regenerate-email for parallel execution.
    """
    job_id = job['job_id']
    ticker = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    try:
        # Start heartbeat thread (CRITICAL: Regeneration takes 2-4 minutes)
        start_heartbeat_thread(job_id)

        # Progress: 5% - Starting
        update_job_status(job_id, progress=5)
        LOG.info(f"[{ticker}] â™»ï¸ [JOB {job_id}] Starting email regeneration...")

        # Step 1: Fetch ticker config
        ticker_config = get_ticker_config(ticker)
        if not ticker_config:
            raise ValueError(f"No config found for {ticker}")

        # Step 2: Fetch original article IDs from executive_summaries table
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] [JOB {job_id}] Fetching original article IDs...")

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT article_ids
                FROM executive_summaries
                WHERE ticker = %s AND summary_date = CURRENT_DATE
                ORDER BY generated_at DESC LIMIT 1
            """, (ticker,))
            summary_row = cur.fetchone()

        # Step 2.5: Fetch report_type from email_queue
        report_type = 'daily'  # Default
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT report_type
                FROM email_queue
                WHERE ticker = %s
                ORDER BY created_at DESC LIMIT 1
            """, (ticker,))
            queue_row = cur.fetchone()
            if queue_row and queue_row.get('report_type'):
                report_type = queue_row['report_type']
                LOG.info(f"[{ticker}] Using report_type='{report_type}' from email_queue")

        if not summary_row or not summary_row['article_ids']:
            raise ValueError(f"No original article IDs found for {ticker} on CURRENT_DATE. Run full processing first.")

        # Parse JSON to get list of article IDs
        original_article_ids = json.loads(summary_row['article_ids'])
        LOG.info(f"[{ticker}] [JOB {job_id}] Using {len(original_article_ids)} article IDs from original run")

        # Step 3: Split article IDs by type and fetch separately
        real_article_ids = [aid for aid in original_article_ids if aid > 0]
        company_release_ids = [abs(aid) for aid in original_article_ids if aid < 0]

        articles = []

        update_job_status(job_id, progress=15)

        with db() as conn, conn.cursor() as cur:
            # Fetch real articles from articles table
            if real_article_ids:
                # Query matches fetch_digest_articles() for consistency:
                # - Includes ta.ticker for company name badge in Email #2
                # - Includes ta.ai_model for AI model badge in Email #2
                # - Includes tf.value_chain_type for upstream/downstream categorization
                # - No is_rejected filter (original stored article_ids, trust them)
                cur.execute("""
                    SELECT a.id, a.title, a.url, a.resolved_url, a.domain, a.published_at,
                           ta.ticker,
                           tf.category, f.search_keyword, f.feed_ticker,
                           tf.value_chain_type,
                           ta.relevance_score, ta.relevance_reason, ta.is_rejected,
                           ta.ai_summary, ta.ai_model,
                           a.quality_score
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    JOIN feeds f ON ta.feed_id = f.id
                    WHERE ta.ticker = %s
                    AND a.id = ANY(%s)
                    ORDER BY a.published_at DESC
                """, (ticker, real_article_ids))
                articles = list(cur.fetchall())
                LOG.info(f"[{ticker}] [JOB {job_id}] Fetched {len(articles)} real articles")

            # Fetch company releases and map to article format
            if company_release_ids:
                cur.execute("""
                    SELECT cr.id, cr.ticker, cr.company_name, cr.filing_date, cr.report_title,
                           cr.summary_markdown, cr.release_type, cr.source_type,
                           s8k.sec_html_url,
                           cr.exhibit_number
                    FROM company_releases cr
                    LEFT JOIN sec_8k_filings s8k ON cr.source_id = s8k.id
                    WHERE cr.id = ANY(%s)
                    ORDER BY cr.filing_date DESC
                """, (company_release_ids,))

                releases_fetched = 0
                for release_row in cur.fetchall():
                    article_dict = map_company_release_to_article_dict(dict(release_row))
                    if article_dict:
                        article_dict.update({
                            'url': '#',
                            'resolved_url': None,
                            'search_keyword': None,
                            'competitor_ticker': None,
                            'relevance_score': None,
                            'relevance_reason': None,
                            'ai_summary': article_dict['ai_summary']
                        })
                        articles.append(article_dict)
                        releases_fetched += 1

                LOG.info(f"[{ticker}] [JOB {job_id}] Fetched {releases_fetched} company releases")

        if not articles:
            raise ValueError("No articles found with IDs from original run. Data may have been deleted.")

        if len(articles) != len(original_article_ids):
            LOG.warning(f"[{ticker}] [JOB {job_id}] Expected {len(original_article_ids)} articles, found {len(articles)}")

        LOG.info(f"[{ticker}] [JOB {job_id}] Found {len(articles)} total items from original run")

        # Step 4: Group articles by category for executive summary generation
        categories = {"company": [], "industry": [], "competitor": [], "value_chain": []}
        flagged_article_ids = []

        for article in articles:
            category = article.get('category', 'company')
            if category in categories:
                categories[category].append(dict(article))
            article_id = article.get('id')
            if article_id:
                flagged_article_ids.append(article_id)

        # CRITICAL FIX: Split value_chain into upstream/downstream BEFORE Phase 1
        # The executive summary module expects separate upstream/downstream categories
        # Without this split, upstream/downstream articles are excluded from Phase 1 timeline,
        # causing index mismatch between Phase 1 and Email #3 (articles appear/disappear incorrectly)
        value_chain_articles = categories.get("value_chain", [])
        if value_chain_articles:
            upstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'upstream']
            downstream_articles = [a for a in value_chain_articles if a.get('value_chain_type') == 'downstream']
            categories["upstream"] = upstream_articles
            categories["downstream"] = downstream_articles
            LOG.info(f"[{ticker}] [JOB {job_id}] Split value_chain: upstream={len(upstream_articles)}, downstream={len(downstream_articles)}")

        # ============================================================
        # PHASE 1: Executive Summary Generation (30-60s)
        # ============================================================
        update_job_status(job_id, progress=20)
        LOG.info(f"[{ticker}] ðŸ“ [JOB {job_id}] Phase 1: Generating executive summary...")

        from modules.executive_summary_phase1 import (
            generate_executive_summary_phase1,
            validate_phase1_json
        )

        phase1_result = generate_executive_summary_phase1(
            ticker=ticker,
            categories=categories,
            config=ticker_config,
            anthropic_api_key=ANTHROPIC_API_KEY,
            gemini_api_key=GEMINI_API_KEY
        )

        if not phase1_result:
            raise ValueError("Failed to generate Phase 1 executive summary")

        json_output = phase1_result["json_output"]
        model_used = phase1_result["model_used"]
        prompt_tokens = phase1_result.get("prompt_tokens", 0)
        completion_tokens = phase1_result.get("completion_tokens", 0)
        generation_time_ms = phase1_result.get("generation_time_ms", 0)

        # Validate JSON structure
        is_valid, error_msg = validate_phase1_json(json_output)
        if not is_valid:
            raise ValueError(f"Phase 1 JSON validation failed: {error_msg}")

        LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Phase 1 validation passed")

        # Clean domain URLs in Phase 1 content
        json_output = clean_executive_summary_domains(json_output)

        # Track Phase 1 cost
        phase1_usage = {
            "input_tokens": prompt_tokens,
            "output_tokens": completion_tokens
        }

        if "gemini" in model_used.lower():
            calculate_gemini_api_cost(phase1_usage, "executive_summary_phase1", model="pro", model_name=model_used)
        elif "claude" in model_used.lower():
            calculate_claude_api_cost(phase1_usage, "executive_summary_phase1", model_name=model_used)

        # ============================================================
        # PHASE 1.5: Known Information Filter (10-20s)
        # ============================================================
        update_job_status(job_id, progress=35)
        phase1_5_model_used = None

        try:
            from modules.known_info_filter import filter_known_information, generate_known_info_filter_email, apply_filter_to_phase1, has_phase1_bullets

            phase1_5_enabled = is_phase_1_5_enabled()

            if phase1_5_enabled and has_phase1_bullets(json_output):
                LOG.info(f"[{ticker}] [JOB {job_id}] Phase 1.5: Running known info filter...")

                filter_result = filter_known_information(
                    ticker=ticker,
                    phase1_json=json_output,
                    db_func=db,
                    gemini_api_key=GEMINI_API_KEY,
                    anthropic_api_key=ANTHROPIC_API_KEY
                )

                if filter_result:
                    phase1_5_model_used = filter_result.get("model_used", "")
                    phase1_5_prompt_tokens = filter_result.get("prompt_tokens", 0)
                    phase1_5_completion_tokens = filter_result.get("completion_tokens", 0)
                    phase1_5_thought_tokens = filter_result.get("thought_tokens", 0)  # Flash 3.0
                    phase1_5_cached_tokens = filter_result.get("cached_tokens", 0)    # Flash 3.0

                    phase1_5_usage = {
                        "input_tokens": phase1_5_prompt_tokens,
                        "output_tokens": phase1_5_completion_tokens,
                        "thought_tokens": phase1_5_thought_tokens,
                        "cached_tokens": phase1_5_cached_tokens
                    }

                    if "gemini" in phase1_5_model_used.lower():
                        # Use flash3 pricing for Gemini 3.0 Flash Preview
                        calculate_gemini_api_cost(phase1_5_usage, "executive_summary_phase1_5", model="flash3", model_name=phase1_5_model_used)
                    elif "claude" in phase1_5_model_used.lower():
                        calculate_claude_api_cost(phase1_5_usage, "executive_summary_phase1_5", model_name=phase1_5_model_used)

                    # Send email for monitoring
                    filter_email_html = generate_known_info_filter_email(ticker, filter_result)
                    send_email(
                        subject=f"Phase 1.5 Known Info Filter (Regen Worker): {ticker}",
                        html_body=filter_email_html,
                        to=ADMIN_EMAIL
                    )

                    # Apply filter to Phase 1 JSON
                    json_output = apply_filter_to_phase1(json_output, filter_result)

                    summary = filter_result.get("summary", {})
                    LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 1.5 Applied: {summary.get('kept', 0)} kept, {summary.get('removed', 0)} removed")
                else:
                    LOG.warning(f"[{ticker}] [JOB {job_id}] Phase 1.5: Filter returned no results")
            elif phase1_5_enabled:
                # Phase 1.5 enabled but no bullets to filter (quiet day)
                LOG.info(f"[{ticker}] [JOB {job_id}] Phase 1.5: Skipped (no bullets from Phase 1 to filter)")
            else:
                LOG.info(f"[{ticker}] [JOB {job_id}] Phase 1.5: Skipped (disabled)")

        except Exception as e:
            LOG.warning(f"[{ticker}] [JOB {job_id}] Phase 1.5: Failed (non-blocking): {e}")

        # ============================================================
        # PHASE 2: Filing Context Enrichment (20-40s)
        # ============================================================
        update_job_status(job_id, progress=50)
        final_json = json_output
        generation_phase = 'phase1'
        phase2_result = None  # Initialize before conditional block

        from modules.executive_summary_phase2 import (
            generate_executive_summary_phase2,
            validate_phase2_json,
            strip_escape_hatch_context,
            merge_phase1_phase2,
            _fetch_available_filings
        )

        filings = _fetch_available_filings(ticker, db)

        if filings:
            LOG.info(f"[{ticker}] [JOB {job_id}] Phase 2: Found filings: {list(filings.keys())}")

            phase2_result = generate_executive_summary_phase2(
                ticker=ticker,
                phase1_json=json_output,
                filings=filings,
                config=ticker_config,
                anthropic_api_key=ANTHROPIC_API_KEY,
                db_func=db,
                gemini_api_key=GEMINI_API_KEY
            )

            if phase2_result:
                is_valid_p2, error_msg_p2, valid_enrichments_p2 = validate_phase2_json(
                    phase2_result.get("enrichments", {}),
                    phase1_json=json_output,
                    ticker=ticker
                )

                if is_valid_p2:
                    phase2_result["enrichments"] = valid_enrichments_p2
                    LOG.info(f"[{ticker}] [JOB {job_id}] Phase 2 validation: {error_msg_p2}")

                    phase2_result = strip_escape_hatch_context(phase2_result)
                    final_json = merge_phase1_phase2(json_output, phase2_result)
                    generation_phase = 'phase2'

                    phase2_prompt_tokens = phase2_result.get("prompt_tokens", 0)
                    phase2_completion_tokens = phase2_result.get("completion_tokens", 0)

                    phase2_usage = {
                        "input_tokens": phase2_prompt_tokens,
                        "output_tokens": phase2_completion_tokens
                    }
                    phase2_model = phase2_result.get("ai_model", "")

                    if "gemini" in phase2_model.lower():
                        calculate_gemini_api_cost(phase2_usage, "executive_summary_phase2", model="pro", model_name=phase2_model)
                    elif "claude" in phase2_model.lower():
                        calculate_claude_api_cost(phase2_usage, "executive_summary_phase2", model_name=phase2_model)

                    enrichment_count = len(valid_enrichments_p2)
                    LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Phase 2: {enrichment_count} bullets enriched")
                else:
                    LOG.error(f"[{ticker}] [JOB {job_id}] Phase 2 validation failed: {error_msg_p2}")
            else:
                LOG.warning(f"[{ticker}] [JOB {job_id}] Phase 2 generation failed")
        else:
            LOG.info(f"[{ticker}] [JOB {job_id}] No filings available, skipping Phase 2")

        # Store final JSON as string
        summary_text = json.dumps(final_json, indent=2)

        # Build ai_models dict for tracking
        ai_models = {
            "phase1": model_used,
            "phase1_5": phase1_5_model_used,
            "phase2": phase2_result.get("ai_model") if phase2_result else None,
            "phase3": None,  # Updated via update_executive_summary_json()
            "phase4": None   # Updated via update_executive_summary_json()
        }

        # Update executive summary WITHOUT overwriting article_ids
        company_count = len(categories['company'])
        industry_count = len(categories['industry'])
        competitor_count = len(categories['competitor'])

        update_job_status(job_id, progress=60)

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE executive_summaries
                SET summary_text = %s,
                    summary_json = %s,
                    ai_provider = %s,
                    ai_models = %s,
                    company_articles_count = %s,
                    industry_articles_count = %s,
                    competitor_articles_count = %s,
                    generation_phase = %s,
                    prompt_tokens = %s,
                    completion_tokens = %s,
                    generation_time_ms = %s,
                    generated_at = NOW()
                WHERE ticker = %s AND summary_date = CURRENT_DATE
            """, (
                summary_text,
                json.dumps(final_json) if final_json else None,
                model_used.lower(),
                json.dumps(ai_models),
                company_count,
                industry_count,
                competitor_count,
                generation_phase,
                prompt_tokens,
                completion_tokens,
                generation_time_ms,
                ticker
            ))
            conn.commit()

        LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Executive summary updated (article_ids preserved)")

        # Use fixed 7-day window for regeneration
        hours = 168
        admin_email = os.getenv("ADMIN_EMAIL")

        # ============================================================
        # PHASE 3: Context Integration (15-30s)
        # ============================================================
        update_job_status(job_id, progress=70)
        LOG.info(f"[{ticker}] ðŸŽ¨ [JOB {job_id}] Phase 3: Context integration...")

        try:
            from modules.executive_summary_phase3 import generate_executive_summary_phase3
            from modules.executive_summary_utils import filter_bullets_for_email3, mark_filtered_bullets, merge_filtered_bullets_back

            # Fetch Phase 2 merged JSON from database
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    SELECT summary_text FROM executive_summaries
                    WHERE ticker = %s AND summary_date = CURRENT_DATE
                    ORDER BY generated_at DESC LIMIT 1
                """, (ticker,))
                result = cur.fetchone()

            if result and result['summary_text']:
                phase2_merged_json = json.loads(result['summary_text'])

                # Mark bullets with filter_status (for Email #2 display)
                marked_phase2_json = mark_filtered_bullets(phase2_merged_json)

                # Filter bullets before Phase 3
                filtered_json_for_phase3 = filter_bullets_for_email3(phase2_merged_json)

                # Get Phase 3 primary model from system config
                primary_model = get_phase3_primary_model()

                # Generate Phase 3
                phase3_merged_json, phase3_usage = generate_executive_summary_phase3(
                    ticker=ticker,
                    phase2_merged_json=filtered_json_for_phase3,
                    anthropic_api_key=ANTHROPIC_API_KEY,
                    gemini_api_key=GEMINI_API_KEY,
                    primary_model=primary_model
                )

                # Track Phase 3 cost
                if phase3_usage:
                    phase3_model = phase3_usage.get("model", "")
                    if "claude" in phase3_model.lower():
                        calculate_claude_api_cost(phase3_usage, "executive_summary_phase3", model_name=phase3_model)
                    elif "gemini" in phase3_model.lower():
                        calculate_gemini_api_cost(phase3_usage, "executive_summary_phase3", model="pro", model_name=phase3_model)

                update_job_status(job_id, progress=80)

                if phase3_merged_json:
                    # Merge filtered bullets back for Email #2 display
                    phase3_merged_json = merge_filtered_bullets_back(phase3_merged_json, marked_phase2_json)

                    # Update database with Phase 3 content
                    phase3_model = phase3_usage.get("model", "") if phase3_usage else None
                    success = update_executive_summary_json(
                        ticker=ticker,
                        summary_json=phase3_merged_json,
                        phase='phase3',
                        model=phase3_model
                    )

                    if success:
                        LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Phase 3 content saved")

                        # ============================================================
                        # PHASE 4: Paragraph Generation (MANDATORY)
                        # ============================================================
                        LOG.info(f"[{ticker}] ðŸ“ [JOB {job_id}] Running Phase 4...")

                        from modules.executive_summary_phase4 import generate_executive_summary_phase4, post_process_phase4_dates

                        phase4_result, phase4_usage = generate_executive_summary_phase4(
                            ticker=ticker,
                            phase3_json=phase3_merged_json,
                            anthropic_api_key=ANTHROPIC_API_KEY,
                            gemini_api_key=GEMINI_API_KEY,
                            primary_model='claude'
                        )

                        # Track Phase 4 cost
                        if phase4_usage:
                            phase4_model = phase4_usage.get("model", "")
                            if "claude" in phase4_model.lower():
                                calculate_claude_api_cost(phase4_usage, "executive_summary_phase4", model_name=phase4_model)
                            elif "gemini" in phase4_model.lower():
                                calculate_gemini_api_cost(phase4_usage, "executive_summary_phase4", model="pro", model_name=phase4_model)

                        # Phase 4 is mandatory - fail if it doesn't return a result
                        if not phase4_result:
                            LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Phase 4 generation failed - no paragraphs generated")
                            raise RuntimeError(f"Phase 4 generation failed for {ticker} - no paragraphs")

                        # Post-process Phase 4 dates (compute date_range from bullet dates, not AI)
                        phase4_result = post_process_phase4_dates(
                            phase4_result=phase4_result,
                            phase3_json=phase3_merged_json,
                            report_type=report_type
                        )

                        # Add Phase 4 results to JSON
                        phase3_merged_json['phase4'] = phase4_result
                        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 4 complete")

                        # Save Phase 4 to database (overwrites Phase 3 save with complete JSON)
                        phase4_model = phase4_usage.get("model", "") if phase4_usage else None
                        phase4_save_success = update_executive_summary_json(
                            ticker=ticker,
                            summary_json=phase3_merged_json,
                            phase='phase4',
                            model=phase4_model
                        )
                        if phase4_save_success:
                            LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Phase 4 content saved to database")

                        # ============================================================
                        # EMAIL #2: Content QA (5-10s)
                        # ============================================================
                        update_job_status(job_id, progress=85)
                        try:
                            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Generating Email #2...")

                            articles_by_ticker = {ticker: categories}

                            days = int(hours / 24) if hours >= 24 else 1
                            email2_html = await build_enhanced_digest_html(
                                articles_by_ticker=articles_by_ticker,
                                period_days=days,
                                show_ai_analysis=True,
                                show_descriptions=True,
                                flagged_article_ids=flagged_article_ids,
                                phase3_json=phase3_merged_json
                            )

                            company_name = ticker_config.get('company_name', ticker) if ticker_config else ticker
                            report_label = "(WEEKLY)" if report_type == 'weekly' else "(DAILY)"
                            total_articles = len(flagged_article_ids)
                            email2_subject = f"QA Content Review (Regen Worker) {report_label}: {company_name} ({ticker}) - {total_articles} articles"

                            email2_success = send_email(email2_subject, email2_html)
                            if email2_success:
                                LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Email #2 sent")

                        except Exception as e:
                            LOG.error(f"âŒ [{ticker}] [JOB {job_id}] Email #2 failed: {e}")

                        # ============================================================
                        # EMAIL #3: User Report (5-10s)
                        # ============================================================
                        update_job_status(job_id, progress=92)
                        LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Generating Email #3...")

                        email3_data = generate_email_html_core(
                            ticker=ticker,
                            hours=hours,
                            recipient_email=None,
                            report_type=report_type
                        )

                        if email3_data:
                            # Update email_queue with Email #3
                            with db() as conn, conn.cursor() as cur:
                                cur.execute("""
                                    UPDATE email_queue
                                    SET email_html = %s,
                                        email_subject = %s,
                                        article_count = %s,
                                        status = 'ready',
                                        updated_at = NOW(),
                                        error_message = NULL,
                                        sent_at = NULL
                                    WHERE ticker = %s
                                """, (
                                    email3_data['html'],
                                    email3_data['subject'],
                                    email3_data['article_count'],
                                    ticker
                                ))
                                rows_updated = cur.rowcount
                                conn.commit()

                            if rows_updated > 0:
                                LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Email #3 updated in queue")

                                # Send Email #3 preview to admin
                                if admin_email:
                                    send_email(email3_data['subject'], email3_data['html'], to=admin_email)
                                    LOG.info(f"âœ… [{ticker}] [JOB {job_id}] Email #3 preview sent")
                            else:
                                LOG.warning(f"âš ï¸ [{ticker}] [JOB {job_id}] Ticker not found in queue")
                        else:
                            LOG.error(f"âŒ [{ticker}] [JOB {job_id}] Failed to generate Email #3 HTML")
                    else:
                        LOG.error(f"âŒ [{ticker}] [JOB {job_id}] Failed to update database with Phase 3")
                else:
                    LOG.warning(f"âš ï¸ [{ticker}] [JOB {job_id}] Phase 3 returned no merged JSON")
            else:
                LOG.warning(f"âš ï¸ [{ticker}] [JOB {job_id}] No Phase 2 JSON found for Phase 3")

        except Exception as e:
            LOG.error(f"âŒ [{ticker}] [JOB {job_id}] Phase 3/Email generation failed: {e}", exc_info=True)

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email regeneration complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Email regeneration failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_10q_profile_phase(job: dict):
    """Process 10-Q quarterly profile generation (5-15 min, Gemini 2.5)"""
    job_id = job['job_id']
    ticker_raw = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    # DEFENSE IN DEPTH: Normalize ticker even if job config should already have it normalized
    ticker = normalize_ticker_format(ticker_raw)
    if ticker != ticker_raw:
        LOG.warning(f"[JOB {job_id}] Normalized ticker from '{ticker_raw}' to '{ticker}'")

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        fiscal_year = config.get('fiscal_year')
        fiscal_quarter = config.get('fiscal_quarter')  # e.g., "Q3"
        filing_date = config.get('filing_date')
        period_end_date = config.get('period_end_date')  # Actual quarter end date
        sec_html_url = config.get('sec_html_url')

        # Progress: 10% - Extracting 10-Q text
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Extracting 10-Q text for {fiscal_quarter} {fiscal_year}...")

        # Fetch HTML from SEC.gov
        if not sec_html_url:
            raise ValueError("sec_html_url is required for 10-Q generation")

        content = fetch_sec_html_text(sec_html_url)

        if not content or len(content) < 1000:
            raise ValueError(f"Extracted text too short ({len(content)} chars)")

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Extracted {len(content)} characters")

        # Progress: 30% - Generating 10-Q profile with Gemini
        update_job_status(job_id, progress=30)
        LOG.info(f"[{ticker}] ðŸ¤– [JOB {job_id}] Generating 10-Q profile with Gemini 2.5 Flash (5-10 min)...")

        ticker_config = get_ticker_config(ticker)

        # Use the unified function with filing_type='10-Q'
        result = generate_sec_filing_profile_with_gemini(
            ticker=ticker,
            content=content,
            config=ticker_config,
            filing_type='10-Q',
            fiscal_year=fiscal_year,
            fiscal_quarter=fiscal_quarter,
            filing_date=filing_date,
            gemini_api_key=GEMINI_API_KEY
        )

        if not result:
            raise ValueError("Gemini 10-Q profile generation failed")

        profile_markdown = result['profile_markdown']
        metadata = result['metadata']

        # Progress: 80% - Saving to database
        update_job_status(job_id, progress=80)
        LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Saving 10-Q profile to database...")

        with db() as conn, conn.cursor() as cur:
            # First, delete any existing 10-Q for this ticker/year/quarter
            cur.execute("""
                DELETE FROM sec_filings
                WHERE ticker = %s
                  AND filing_type = %s
                  AND fiscal_year = %s
                  AND fiscal_quarter = %s
            """, (
                ticker,
                '10-Q',
                fiscal_year,
                fiscal_quarter
            ))

            # Then insert the new 10-Q profile
            cur.execute("""
                INSERT INTO sec_filings (
                    ticker, filing_type, fiscal_year, fiscal_quarter,
                    company_name, industry, filing_date, period_end_date,
                    profile_markdown, source_type, sec_html_url,
                    ai_provider, ai_model,
                    generation_time_seconds, token_count_input, token_count_output,
                    status
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                ticker,
                '10-Q',
                fiscal_year,
                fiscal_quarter,
                ticker_config.get('company_name'),
                ticker_config.get('industry'),
                filing_date,
                period_end_date,
                profile_markdown,
                'fmp_sec',  # Always SEC.gov for 10-Q
                sec_html_url,
                'gemini',
                metadata.get('model'),
                metadata.get('generation_time_seconds'),
                metadata.get('token_count_input'),
                metadata.get('token_count_output'),
                'active'
            ))
            conn.commit()

        # Progress: 95% - Sending email
        if config.get('send_email', True):
            update_job_status(job_id, progress=95)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending email notification...")

            try:
                # Get real-time stock data (yfinance â†’ Polygon â†’ database â†’ None)
                stock_data = get_filing_stock_data(ticker)

                # Generate email (reuse company profile email function)
                email_data = generate_company_profile_email(
                    ticker=ticker,
                    company_name=ticker_config['company_name'],
                    industry=ticker_config.get('industry', 'N/A'),
                    fiscal_year=fiscal_year,
                    filing_date=filing_date,
                    profile_markdown=profile_markdown,
                    stock_price=stock_data['stock_price'],
                    price_change_pct=stock_data['price_change_pct'],
                    price_change_color=stock_data['price_change_color'],
                    ytd_return_pct=stock_data['ytd_return_pct'],
                    ytd_return_color=stock_data['ytd_return_color'],
                    market_status=stock_data['market_status'],
                    return_label=stock_data['return_label'],
                    filing_type="10-Q",
                    fiscal_quarter=fiscal_quarter
                )

                send_email(
                    subject=email_data['subject'],
                    html_body=email_data['html'],
                    to=ADMIN_EMAIL  # âœ… FIXED: Was hardcoded 'stockdigest.research@gmail.com'
                )
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email sent to {ADMIN_EMAIL}")

            except Exception as email_error:
                LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send email: {email_error}")
                # Continue - profile was saved successfully, email failure shouldn't mark job as failed

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] 10-Q profile generation complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] 10-Q profile generation failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_transcript_phase(job: dict):
    """Process earnings transcript generation (30-60s, Claude)"""
    job_id = job['job_id']
    ticker_raw = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    # DEFENSE IN DEPTH: Normalize ticker even if job config should already have it normalized
    ticker = normalize_ticker_format(ticker_raw)
    if ticker != ticker_raw:
        LOG.warning(f"[JOB {job_id}] Normalized ticker from '{ticker_raw}' to '{ticker}'")

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        quarter = config.get('quarter')  # Integer: 2
        year = config.get('year')  # Integer: 2026

        # Progress: 10% - Fetching transcript from FMP
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Fetching transcript Q{quarter} FY{year} from FMP...")

        # Fetch transcript content from FMP
        fmp_url = f"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}?quarter={quarter}&year={year}"
        response = requests.get(f"{fmp_url}&apikey={FMP_API_KEY}", timeout=30)

        if response.status_code != 200:
            raise ValueError(f"FMP API returned status {response.status_code}")

        data = response.json()
        if not data or len(data) == 0:
            raise ValueError(f"No transcript data returned from FMP")

        content = data[0].get('content', '')
        if not content:
            raise ValueError(f"Transcript content is empty")

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Fetched {len(content)} characters")

        # Progress: 30% - Generating summary with AI (Gemini Pro â†’ Claude Sonnet fallback)
        update_job_status(job_id, progress=30)
        LOG.info(f"[{ticker}] ðŸ¤– [JOB {job_id}] Generating summary with AI v2 (JSON output, 30-60s)...")

        ticker_config = get_ticker_config(ticker)

        # Use v2 JSON generation (Gemini-first, Claude-fallback)
        from modules.transcript_summaries import generate_transcript_json_with_fallback
        result = generate_transcript_json_with_fallback(
            ticker=ticker,
            content=content,
            config=ticker_config,
            content_type='transcript',
            anthropic_api_key=ANTHROPIC_API_KEY,
            gemini_api_key=GEMINI_API_KEY
        )

        if not result or not result.get('json_output'):
            raise ValueError("Both Gemini and Claude transcript v2 summarization failed")

        json_output = result['json_output']
        model_used = result['model_used']  # e.g., "gemini-2.5-pro" or "claude-sonnet-4-5-20250929"
        generation_time_ms = result.get('generation_time_ms', 0)
        prompt_tokens = result.get('prompt_tokens', 0)
        completion_tokens = result.get('completion_tokens', 0)

        # Extract provider name from model string
        ai_provider = "gemini" if "gemini" in model_used.lower() else "claude"

        LOG.info(
            f"[{ticker}] âœ… [JOB {job_id}] Generated {len(json_output.get('sections', {}))} sections "
            f"with {model_used} ({generation_time_ms}ms, {prompt_tokens}â†’{completion_tokens} tokens)"
        )

        # Progress: 80% - Saving to database
        update_job_status(job_id, progress=80)
        LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Saving summary v2 (JSON) to database...")

        # Ensure quarter has Q prefix (defensive check - don't double-prefix)
        quarter_str = str(quarter)
        quarter_formatted = quarter_str if quarter_str.startswith('Q') else f"Q{quarter_str}"

        # Generate markdown for research page display (summary_text column)
        from modules.transcript_summaries import convert_json_to_markdown
        import json
        summary_text_markdown = convert_json_to_markdown(json_output, 'transcript')

        # Save both JSON (for email v2) and markdown (for Phase 2) to database
        with db() as conn, conn.cursor() as cur:
            # Use Json adapter for psycopg3 JSONB support
            try:
                from psycopg.types.json import Json
                json_param = Json(json_output)
            except ImportError:
                # psycopg2 fallback - pass dict directly
                json_param = json.dumps(json_output)

            cur.execute("""
                INSERT INTO transcript_summaries (
                    ticker, company_name, report_type, fiscal_quarter, fiscal_year,
                    report_date, summary_text, summary_json, prompt_version,
                    source_url, ai_provider, ai_model
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (ticker, report_type, fiscal_quarter, fiscal_year)
                DO UPDATE SET
                    summary_text = EXCLUDED.summary_text,
                    summary_json = EXCLUDED.summary_json,
                    prompt_version = EXCLUDED.prompt_version,
                    ai_provider = EXCLUDED.ai_provider,
                    ai_model = EXCLUDED.ai_model,
                    generated_at = NOW()
            """, (
                ticker,
                ticker_config.get('company_name'),
                'transcript',
                quarter_formatted,
                year,
                data[0].get('date'),
                summary_text_markdown,  # Clean markdown for Phase 2
                json_param,             # Structured JSON for email v2
                'v2',                   # Prompt version
                fmp_url,
                ai_provider,  # "gemini" or "claude"
                model_used    # Full model name
            ))
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Saved transcript with JSON + markdown")
            conn.commit()

        # Progress: 95% - Sending email
        if config.get('send_email', True):
            update_job_status(job_id, progress=95)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending email notification...")

            try:
                # Import v2 email generation function
                from modules.transcript_summaries import generate_transcript_email_v2

                # Get real-time stock data (yfinance â†’ Polygon â†’ database â†’ None)
                stock_data = get_filing_stock_data(ticker)

                # Generate email v2 from JSON
                email_data = generate_transcript_email_v2(
                    ticker=ticker,
                    json_output=json_output,
                    config=ticker_config,
                    content_type='transcript',
                    quarter=quarter,
                    year=year,
                    report_date=data[0].get('date'),
                    stock_data=stock_data
                )

                send_email(subject=email_data['subject'], html_body=email_data['html'], to=ADMIN_EMAIL)
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email v2 sent to {ADMIN_EMAIL}")

            except Exception as email_error:
                LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send email: {email_error}")
                # Continue - transcript was saved successfully, email failure shouldn't mark job as failed

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Transcript generation complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Transcript generation failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_transcript_generation_phase(job: dict):
    """Process transcript generation (30-60s, Gemini/Claude)"""
    job_id = job['job_id']
    ticker_raw = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    # DEFENSE IN DEPTH: Normalize ticker even if job config should already have it normalized
    ticker = normalize_ticker_format(ticker_raw)
    if ticker != ticker_raw:
        LOG.warning(f"[JOB {job_id}] Normalized ticker from '{ticker_raw}' to '{ticker}'")

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        quarter = config.get('quarter')  # Integer (3) for transcripts
        year = config.get('year')  # Integer (2024) for transcripts

        # Progress: 10% - Fetching transcript from FMP
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Fetching transcript Q{quarter} {year} from FMP...")

        # Fetch transcript content from FMP
        data = fetch_fmp_transcript(ticker, quarter, year, FMP_API_KEY)
        if not data:
            raise ValueError(f"No transcript found for {ticker} Q{quarter} {year}")

        content = data['content']
        report_date = data.get('date', f"{year}-{quarter*3:02d}-01")  # Approximate date

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Fetched {len(content)} characters")

        # Progress: 30% - Generating summary with AI v2
        update_job_status(job_id, progress=30)
        LOG.info(f"[{ticker}] ðŸ¤– [JOB {job_id}] Generating summary with AI v2...")

        ticker_config = get_ticker_config(ticker)
        company_name = ticker_config.get('company_name', ticker)

        # Use v2 JSON generation (Gemini-first, Claude-fallback)
        from modules.transcript_summaries import generate_transcript_json_with_fallback, convert_json_to_markdown
        result = generate_transcript_json_with_fallback(
            ticker=ticker,
            content=content,
            config=ticker_config,
            content_type='transcript',
            anthropic_api_key=ANTHROPIC_API_KEY,
            gemini_api_key=GEMINI_API_KEY
        )

        if not result or not result.get('json_output'):
            raise ValueError("Both Gemini and Claude transcript summarization failed")

        json_output = result['json_output']
        model_used = result['model_used']
        generation_time_ms = result.get('generation_time_ms', 0)
        prompt_tokens = result.get('prompt_tokens', 0)
        completion_tokens = result.get('completion_tokens', 0)

        # Extract provider name from model string
        ai_provider = "gemini" if "gemini" in model_used.lower() else "claude"

        LOG.info(
            f"[{ticker}] âœ… [JOB {job_id}] Generated {len(json_output.get('sections', {}))} sections "
            f"with {model_used} ({generation_time_ms}ms, {prompt_tokens}â†’{completion_tokens} tokens)"
        )

        # Generate markdown for database storage
        summary_text_markdown = convert_json_to_markdown(json_output, 'transcript')

        # Format quarter with Q prefix
        quarter_str = str(quarter)
        quarter_formatted = quarter_str if quarter_str.startswith('Q') else f"Q{quarter_str}"

        # Progress: 80% - Saving to database
        update_job_status(job_id, progress=80)
        LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Saving summary to database...")

        with db() as conn, conn.cursor() as cur:
            # Check if summary_json column exists (backward compatibility)
            cur.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = 'transcript_summaries'
                AND column_name IN ('summary_json', 'prompt_version')
            """)
            rows = cur.fetchall()
            existing_columns = {row['column_name'] for row in rows}
            has_json_columns = 'summary_json' in existing_columns and 'prompt_version' in existing_columns

            fmp_url = f"https://financialmodelingprep.com/api/v3/earning_call_transcript/{ticker}?quarter={quarter}&year={year}"

            if has_json_columns:
                try:
                    from psycopg.types.json import Json
                    json_param = Json(json_output)
                except ImportError:
                    json_param = json.dumps(json_output)

                cur.execute("""
                    INSERT INTO transcript_summaries (
                        ticker, company_name, report_type, fiscal_quarter, fiscal_year,
                        report_date, summary_text, summary_json, prompt_version,
                        source_url, ai_provider, ai_model
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (ticker, report_type, fiscal_quarter, fiscal_year)
                    DO UPDATE SET
                        summary_text = EXCLUDED.summary_text,
                        summary_json = EXCLUDED.summary_json,
                        prompt_version = EXCLUDED.prompt_version,
                        ai_provider = EXCLUDED.ai_provider,
                        ai_model = EXCLUDED.ai_model,
                        generated_at = NOW()
                """, (
                    ticker, company_name, 'transcript',
                    quarter_formatted, year, report_date,
                    summary_text_markdown, json_param, 'v2',
                    fmp_url, ai_provider, model_used
                ))
            else:
                cur.execute("""
                    INSERT INTO transcript_summaries (
                        ticker, company_name, report_type, fiscal_quarter, fiscal_year,
                        report_date, summary_text, source_url, ai_provider, ai_model
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (ticker, report_type, fiscal_quarter, fiscal_year)
                    DO UPDATE SET
                        summary_text = EXCLUDED.summary_text,
                        ai_provider = EXCLUDED.ai_provider,
                        ai_model = EXCLUDED.ai_model,
                        generated_at = NOW()
                """, (
                    ticker, company_name, 'transcript',
                    quarter_formatted, year, report_date,
                    summary_text_markdown, fmp_url, ai_provider, model_used
                ))
            conn.commit()

        # Progress: 95% - Sending email
        if config.get('send_email', True):
            update_job_status(job_id, progress=95)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending email notification...")

            try:
                from modules.transcript_summaries import generate_transcript_email_v2

                # Get real-time stock data
                stock_data = get_filing_stock_data(ticker)

                # Generate email v2 from JSON
                email_data = generate_transcript_email_v2(
                    ticker=ticker,
                    json_output=json_output,
                    config=ticker_config,
                    content_type='transcript',
                    quarter=quarter,
                    year=year,
                    report_date=report_date,
                    stock_data=stock_data
                )

                send_email(subject=email_data['subject'], html_body=email_data['html'], to=ADMIN_EMAIL)
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email sent to {ADMIN_EMAIL}")

            except Exception as email_error:
                LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send email: {email_error}")

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Transcript generation complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Transcript generation failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_press_release_phase(job: dict):
    """Process press release generation (30-60s, Claude)"""
    job_id = job['job_id']
    ticker_raw = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    # DEFENSE IN DEPTH: Normalize ticker even if job config should already have it normalized
    ticker = normalize_ticker_format(ticker_raw)
    if ticker != ticker_raw:
        LOG.warning(f"[JOB {job_id}] Normalized ticker from '{ticker_raw}' to '{ticker}'")

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        pr_date = config.get('pr_date')  # String: '2024-10-25 14:00:00' (full datetime from FMP)
        pr_title = config.get('pr_title')  # String: Full press release title

        # Progress: 10% - Fetching press release from FMP
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Fetching press release {pr_date} from FMP...")

        # Fetch press release content from FMP
        from modules.transcript_summaries import fetch_fmp_press_release_by_date_and_title

        data = fetch_fmp_press_release_by_date_and_title(ticker, pr_date, pr_title, FMP_API_KEY)
        if not data:
            raise ValueError(f"No press release found for {ticker} on {pr_date}")

        content = data.get('text', '')
        if not content:
            raise ValueError(f"Press release content is empty")

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Fetched {len(content)} characters")

        # Get ticker config
        ticker_config = get_ticker_config(ticker)

        # FMP doesn't have separate source table (unlike 8-K which has sec_8k_filings)
        source_id = None

        # Progress: 30% - Generating company release summary with 8-K filing prompt
        update_job_status(job_id, progress=30)
        LOG.info(f"[{ticker}] ðŸ”„ [JOB {job_id}] Generating company release summary using 8-K filing prompt...")

        try:
            # Use new unified prompt for ALL FMP press releases
            parsed_result = generate_earnings_release_with_gemini(
                ticker=ticker,
                company_name=ticker_config.get('company_name', ticker),
                content=content,  # Raw FMP content
                document_title=pr_title,
                item_codes=None,
                gemini_api_key=GEMINI_API_KEY
            )

            if parsed_result and parsed_result.get('parsed_summary'):
                # Extract metadata and JSON output
                result_metadata = parsed_result.get('metadata', {})

                # Override report_title with FMP's actual title (FMP provides better titles)
                report_title = pr_title

                # FMP: Don't track fiscal period (abbreviated content = unreliable extraction)
                # Only 8-K releases (full official documents) get fiscal period tracking
                fiscal_quarter_db = None
                fiscal_year_db = None

                json_output_parsed = parsed_result.get('json_data', {})
                markdown_summary = parsed_result.get('parsed_summary', '')  # Extract markdown

                # Progress: 90% - Generate email and save to database (ALWAYS, regardless of send_email flag)
                update_job_status(job_id, progress=90)
                LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Generating company release email and saving to database...")

                try:
                    # Get stock data for email header
                    stock_data = get_filing_stock_data(ticker)

                    # Format press release date nicely
                    try:
                        from datetime import datetime
                        date_obj = datetime.strptime(pr_date[:10], '%Y-%m-%d')  # Extract date part only
                        formatted_pr_date = date_obj.strftime('%b %d, %Y')  # "Nov 19, 2024"
                    except:
                        formatted_pr_date = pr_date[:10] if len(pr_date) >= 10 else pr_date

                    # Generate email using new company release system
                    email_result = generate_company_release_email(
                        ticker=ticker,
                        company_name=ticker_config.get('company_name', ticker),
                        release_type='fmp_press_release',
                        filing_date=formatted_pr_date,
                        json_output=json_output_parsed,
                        stock_price=stock_data.get('stock_price'),
                        price_change_pct=stock_data.get('price_change_pct'),
                        price_change_color=stock_data.get('price_change_color', '#1e6b4a'),
                        ytd_return_pct=stock_data.get('ytd_return_pct'),
                        ytd_return_color=stock_data.get('ytd_return_color', '#1e6b4a'),
                        market_status=stock_data.get('market_status', 'Last Close'),
                        return_label=stock_data.get('return_label', '1D')
                    )
                    email_html = email_result.get('html', '')
                    email_subject = email_result.get('subject', f"ðŸ“„ {ticker} Company Release")

                    # ALWAYS save to company_releases table (even when send_email=False)
                    with db() as conn, conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO company_releases (
                                ticker, company_name, release_type, filing_date, report_title,
                                source_id, source_type, summary_json, summary_html, summary_markdown,
                                ai_provider, ai_model, processing_duration_seconds,
                                token_count_input, token_count_output, job_id,
                                fiscal_year, fiscal_quarter, generated_at
                            )
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                            ON CONFLICT (ticker, filing_date, report_title)
                            DO UPDATE SET
                                summary_json = EXCLUDED.summary_json,
                                summary_html = EXCLUDED.summary_html,
                                summary_markdown = EXCLUDED.summary_markdown,
                                fiscal_year = EXCLUDED.fiscal_year,
                                fiscal_quarter = EXCLUDED.fiscal_quarter,
                                generated_at = NOW()
                        """, (
                            ticker,
                            ticker_config.get('company_name', ticker),
                            'fmp_press_release',
                            pr_date[:10],  # Date only
                            report_title,
                            source_id,
                            'fmp_press_release',
                            json.dumps(json_output_parsed),
                            email_html,
                            markdown_summary,  # Add markdown
                            'gemini',
                            result_metadata.get('model', 'gemini-2.5-flash'),
                            result_metadata.get('generation_time_seconds', 0),
                            result_metadata.get('token_count_input', 0),
                            result_metadata.get('token_count_output', 0),
                            job_id,
                            fiscal_year_db,
                            fiscal_quarter_db
                        ))
                        conn.commit()

                    LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Press release saved to company_releases table")

                    # Progress: 95% - Send email (CONDITIONAL based on send_email flag)
                    if config.get('send_email', True):
                        update_job_status(job_id, progress=95)
                        LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending company release email...")

                        send_email(
                            subject=email_subject,
                            html_body=email_html,
                            to=ADMIN_EMAIL
                        )
                        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Company release email sent")
                    else:
                        LOG.info(f"[{ticker}] â­ï¸ [JOB {job_id}] Email sending disabled - press release saved silently")

                except Exception as email_error:
                    LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to generate/save company release: {email_error}")
                    LOG.error(f"Stacktrace: {traceback.format_exc()}")
                    raise  # Re-raise to mark job as failed
            else:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Summary generation returned empty result")

        except Exception as parsed_error:
            LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to generate company release: {parsed_error}")
            LOG.error(f"Stacktrace: {traceback.format_exc()}")
            # Continue - original PR was saved successfully

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Press release generation complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Press release generation failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_8k_summary_phase(job: dict):
    """
    Process 8-K exhibits - extract ALL HTML exhibits and send separate email per exhibit.

    Extracts ANY exhibit type (1.1, 4.1, 10.1, 99.1, etc.), not just earnings releases.
    No AI processing - just shows raw SEC content exactly as published.
    Fast (<1 min per exhibit).
    """
    job_id = job['job_id']
    ticker_raw = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    # DEFENSE IN DEPTH: Normalize ticker even if job config should already have it normalized
    ticker = normalize_ticker_format(ticker_raw)
    if ticker != ticker_raw:
        LOG.warning(f"[JOB {job_id}] Normalized ticker from '{ticker_raw}' to '{ticker}'")

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        cik = config.get('cik')
        accession_number = config.get('accession_number')
        filing_date = config.get('filing_date')
        documents_url = config.get('documents_url')  # NEW: Use documents page URL
        item_codes = config.get('item_codes')
        ticker_config = get_ticker_config(ticker)

        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Fetching all HTML exhibits from documents page...")

        # Step 1: Get ALL HTML exhibits (any number: 1.1, 4.1, 10.1, 99.1, etc.)
        # If no exhibits found, fall back to main 8-K body
        try:
            exhibits = get_all_8k_exhibits(documents_url)
        except ValueError as e:
            if "No HTML exhibits found" in str(e):
                LOG.info(f"[{ticker}] âš ï¸ [JOB {job_id}] No exhibits found, extracting main 8-K body...")
                # Fall back to main 8-K body content
                main_8k_url = get_main_8k_url(documents_url)
                if main_8k_url:
                    exhibits = [{
                        'exhibit_number': 'MAIN',
                        'description': 'Main 8-K Body',
                        'url': main_8k_url,
                        'size': 0
                    }]
                    LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Using main 8-K body as fallback")
                else:
                    raise ValueError("No HTML exhibits and no main 8-K body found")
            else:
                raise

        if not exhibits:
            raise ValueError("No HTML exhibits found in this 8-K")

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Found {len(exhibits)} exhibit(s) to process")

        # Step 2: Process each exhibit sequentially
        for idx, exhibit in enumerate(exhibits, 1):
            exhibit_num = exhibit['exhibit_number']
            exhibit_desc = exhibit['description']
            exhibit_url = exhibit['url']

            LOG.info(f"[{ticker}] ðŸ“¥ [JOB {job_id}] Processing Exhibit {exhibit_num}: {exhibit_desc}")

            # Filter: Skip zero-value exhibits (auditor letters, consents, certifications, XBRL)
            if not should_process_exhibit(exhibit_num):
                LOG.info(f"[{ticker}] â­ï¸  [JOB {job_id}] Skipping Exhibit {exhibit_num} (zero info value)")
                continue

            # Extract HTML content for this exhibit
            raw_content = extract_8k_html_content(exhibit_url)

            if not raw_content or len(raw_content) < 500:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Exhibit {exhibit_num} too short ({len(raw_content)} chars), skipping")
                continue

            char_count = len(raw_content)
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Extracted Exhibit {exhibit_num}: {char_count:,} chars")

            # Classify exhibit type for future filtering
            exhibit_type = classify_exhibit_type(exhibit_num, exhibit_desc, char_count, item_codes)
            LOG.info(f"[{ticker}] ðŸ·ï¸  [JOB {job_id}] Exhibit {exhibit_num} classified as: {exhibit_type}")

            # Send separate email for this exhibit
            if config.get('send_email', True):
                LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending email for Exhibit {exhibit_num}...")

                try:
                    from jinja2 import Environment, FileSystemLoader
                    from datetime import datetime

                    # Load template
                    template_env = Environment(loader=FileSystemLoader('templates'))
                    raw_email_template = template_env.get_template('email_8k_raw_content.html')

                    # Format date nicely (Oct 30, 2025)
                    try:
                        date_obj = datetime.strptime(filing_date, '%Y-%m-%d')
                        formatted_date = date_obj.strftime('%b %d, %Y')  # "Oct 30, 2025"
                    except:
                        formatted_date = filing_date

                    # Map exhibit type to friendly name
                    type_display = {
                        'earnings_release': 'Earnings Release',
                        'investor_presentation': 'Investor Presentation',
                        'press_release': 'Press Release',
                        'other': 'SEC Filing'
                    }.get(exhibit_type, 'SEC Filing')

                    # Render email (pass HTML directly, template uses |safe)
                    email_html = raw_email_template.render(
                        ticker=ticker,
                        company_name=ticker_config.get('company_name', ticker),
                        filing_date=formatted_date,
                        item_codes=item_codes,
                        exhibit_number=exhibit_num,
                        exhibit_description=exhibit_desc,
                        exhibit_url=exhibit_url,
                        char_count=f"{char_count:,}",
                        raw_content_html=raw_content
                    )

                    # Raw 8-K subject line (pre-AI processing)
                    subject = f"{ticker} 8-K Raw - Exhibit {exhibit_num} - {formatted_date}"

                    send_email(subject=subject, html_body=email_html, to=ADMIN_EMAIL)
                    LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email sent for Exhibit {exhibit_num}")

                except Exception as email_error:
                    LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send email for Exhibit {exhibit_num}: {email_error}")
                    # Continue to save to database

            # Save to database with exhibit-level granularity
            LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Saving Exhibit {exhibit_num} to database...")

            with db() as conn, conn.cursor() as cur:
                # Build exhibit list string for summary_text (format: "Exhibit 99.1: Description; Exhibit 99.2: Description")
                summary_text = f"Exhibit {exhibit_num}: {exhibit_desc}"

                cur.execute("""
                    INSERT INTO sec_8k_filings (
                        ticker, company_name, cik, accession_number,
                        filing_date, filing_title, item_codes, sec_html_url,
                        exhibit_number, exhibit_description, exhibit_type,
                        raw_content, char_count, summary_text, ai_provider, job_id, generated_at
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                    ON CONFLICT (ticker, accession_number, exhibit_number)
                    DO UPDATE SET
                        raw_content = EXCLUDED.raw_content,
                        char_count = EXCLUDED.char_count,
                        exhibit_type = EXCLUDED.exhibit_type,
                        ai_provider = EXCLUDED.ai_provider,
                        generated_at = NOW()
                    RETURNING id
                """, (
                    ticker,
                    ticker_config.get('company_name', ticker),
                    cik,
                    accession_number,
                    filing_date,
                    '8-K Filing',  # Generic placeholder (Gemini will extract actual title)
                    item_codes,
                    exhibit_url,
                    exhibit_num,
                    exhibit_desc,
                    exhibit_type,
                    raw_content,
                    char_count,
                    summary_text,
                    'none',  # ai_provider (no AI processing for raw exhibits)
                    job_id
                ))
                source_id_row = cur.fetchone()
                # Handle both tuple and dict cursor types
                if source_id_row:
                    source_id = source_id_row['id'] if isinstance(source_id_row, dict) else source_id_row[0]
                else:
                    source_id = None
                conn.commit()

            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Exhibit {exhibit_num} saved to database")

            # Generate parsed PR summary with Gemini
            # Route earnings releases to comprehensive analysis, others to basic PR prompt
            if source_id:
                LOG.info(f"[{ticker}] ðŸ”„ [JOB {job_id}] Generating parsed PR for Exhibit {exhibit_num}...")

                try:
                    # ALL exhibits use the new 8-K filing prompt (no filtering by type)
                    LOG.info(f"[{ticker}] ðŸ“Š [JOB {job_id}] Generating summary for Exhibit {exhibit_num} using 8-K filing prompt")
                    parsed_result = generate_earnings_release_with_gemini(
                        ticker=ticker,
                        company_name=ticker_config.get('company_name', ticker),
                        content=raw_content,
                        document_title=f"8-K Exhibit {exhibit_num}",
                        item_codes=item_codes,
                        gemini_api_key=GEMINI_API_KEY
                    )

                    if parsed_result and parsed_result.get('parsed_summary'):
                        # Extract metadata from result
                        result_metadata = parsed_result.get('metadata', {})
                        report_title = result_metadata.get('report_title', f"Exhibit {exhibit_num}")

                        # Extract fiscal period from Gemini
                        fiscal_quarter = result_metadata.get('fiscal_quarter', '').strip()
                        fiscal_year_raw = result_metadata.get('fiscal_year', '')

                        # Convert fiscal_year to INTEGER (Gemini might return string or number)
                        fiscal_year = None
                        if fiscal_year_raw:
                            try:
                                fiscal_year = int(fiscal_year_raw)
                            except (ValueError, TypeError):
                                LOG.warning(f"[{ticker}] Could not convert fiscal_year to int: {fiscal_year_raw}")

                        # Convert empty strings to NULL for database
                        fiscal_quarter_db = fiscal_quarter if fiscal_quarter else None
                        fiscal_year_db = fiscal_year

                        json_output = parsed_result.get('json_data', {})
                        markdown_summary = parsed_result.get('parsed_summary', '')  # Extract markdown

                        # Generate email and save to database for ALL exhibits (ALWAYS, regardless of send_email flag)
                        try:
                            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Generating company release email and saving to database for Exhibit {exhibit_num}...")

                            # Get stock data for email header
                            stock_data = get_filing_stock_data(ticker)

                            # Format filing date nicely
                            try:
                                from datetime import datetime
                                date_obj = datetime.strptime(filing_date, '%Y-%m-%d')
                                formatted_filing_date = date_obj.strftime('%b %d, %Y')  # "Nov 19, 2024"
                            except:
                                formatted_filing_date = filing_date

                            # Generate email using new company release system
                            email_result = generate_company_release_email(
                                ticker=ticker,
                                company_name=ticker_config.get('company_name', ticker),
                                release_type='8k',
                                filing_date=formatted_filing_date,
                                json_output=json_output,
                                exhibit_number=exhibit_num,
                                stock_price=stock_data.get('stock_price'),
                                price_change_pct=stock_data.get('price_change_pct'),
                                price_change_color=stock_data.get('price_change_color', '#1e6b4a'),
                                ytd_return_pct=stock_data.get('ytd_return_pct'),
                                ytd_return_color=stock_data.get('ytd_return_color', '#1e6b4a'),
                                market_status=stock_data.get('market_status', 'Last Close'),
                                return_label=stock_data.get('return_label', '1D')
                            )
                            email_html = email_result.get('html', '')
                            email_subject = email_result.get('subject', f"ðŸ“„ {ticker} Company Release")

                            # ALWAYS save to company_releases table (even when send_email=False)
                            with db() as conn, conn.cursor() as cur:
                                cur.execute("""
                                    INSERT INTO company_releases (
                                        ticker, company_name, release_type, filing_date, report_title,
                                        source_id, source_type, summary_json, summary_html, summary_markdown,
                                        ai_provider, ai_model, processing_duration_seconds,
                                        token_count_input, token_count_output, job_id,
                                        fiscal_year, fiscal_quarter, exhibit_number, item_codes,
                                        generated_at
                                    )
                                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                                    ON CONFLICT (ticker, filing_date, report_title)
                                    DO UPDATE SET
                                        summary_json = EXCLUDED.summary_json,
                                        summary_html = EXCLUDED.summary_html,
                                        summary_markdown = EXCLUDED.summary_markdown,
                                        fiscal_year = EXCLUDED.fiscal_year,
                                        fiscal_quarter = EXCLUDED.fiscal_quarter,
                                        exhibit_number = EXCLUDED.exhibit_number,
                                        item_codes = EXCLUDED.item_codes,
                                        generated_at = NOW()
                                """, (
                                    ticker,
                                    ticker_config.get('company_name', ticker),
                                    '8k',
                                    filing_date,
                                    report_title,
                                    source_id,
                                    '8k_exhibit',
                                    json.dumps(json_output),
                                    email_html,
                                    markdown_summary,  # Add markdown
                                    'gemini',
                                    result_metadata.get('model', 'gemini-2.5-flash'),
                                    result_metadata.get('generation_time_seconds', 0),
                                    result_metadata.get('token_count_input', 0),
                                    result_metadata.get('token_count_output', 0),
                                    job_id,
                                    fiscal_year_db,
                                    fiscal_quarter_db,
                                    exhibit_num,
                                    item_codes
                                ))
                                conn.commit()

                            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] 8-K exhibit saved to company_releases table for Exhibit {exhibit_num}")

                            # Send email (CONDITIONAL based on send_email flag)
                            if config.get('send_email', True):
                                LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending company release email for Exhibit {exhibit_num}...")
                                send_email(
                                    subject=email_subject,
                                    html_body=email_html,
                                    to=ADMIN_EMAIL
                                )
                                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Company release email sent for Exhibit {exhibit_num}")
                            else:
                                LOG.info(f"[{ticker}] â­ï¸ [JOB {job_id}] Email sending disabled - 8-K exhibit saved silently for Exhibit {exhibit_num}")

                        except Exception as email_error:
                            LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to generate/save company release for Exhibit {exhibit_num}: {email_error}")
                            LOG.error(f"Stacktrace: {traceback.format_exc()}")
                            raise  # Re-raise to mark job as failed
                    else:
                        LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Summary generation returned empty for Exhibit {exhibit_num}")

                except Exception as parsed_error:
                    LOG.error(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to generate parsed PR for Exhibit {exhibit_num}: {parsed_error}")
                    # Continue - original 8-K was saved successfully
            else:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Could not get source_id for Exhibit {exhibit_num}")

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] 8-K processing complete ({len(exhibits)} exhibit(s))")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] 8-K processing failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_presentation_phase(job: dict):
    """Process investor presentation analysis (5-15 min, Gemini 2.5 Pro with multimodal vision)"""
    job_id = job['job_id']
    ticker = job['ticker']
    config = job['config'] if isinstance(job['config'], dict) else {}

    try:
        # Start heartbeat thread
        start_heartbeat_thread(job_id)

        presentation_date = config.get('presentation_date')
        presentation_type = config.get('presentation_type')
        presentation_title = config.get('presentation_title')
        file_path = config.get('file_path')

        # Progress: 10% - Uploading PDF to Gemini File API
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Uploading PDF to Gemini File API for multimodal analysis...")

        if not file_path or not os.path.exists(file_path):
            raise ValueError(f"PDF file not found: {file_path}")

        # Configure Gemini API
        genai.configure(api_key=GEMINI_API_KEY)

        # Read PDF file as bytes (direct multimodal input, no File API needed)
        LOG.info(f"[{ticker}] Reading {file_path} as bytes for multimodal analysis...")
        import base64
        with open(file_path, 'rb') as f:
            pdf_bytes = f.read()

        # Create inline data part using base64 encoding (Gemini API format)
        # This works for PDFs under 20MB without using the File API
        pdf_part = {
            "inline_data": {
                "mime_type": "application/pdf",
                "data": base64.b64encode(pdf_bytes).decode('utf-8')
            }
        }

        LOG.info(f"[{ticker}] âœ… PDF loaded ({len(pdf_bytes)} bytes), ready for analysis")

        # Progress: 30% - Analyzing presentation with Gemini 2.5 Pro Multimodal
        update_job_status(job_id, progress=30)
        LOG.info(f"[{ticker}] ðŸ¤– [JOB {job_id}] Analyzing presentation with Gemini 2.5 Pro (multimodal vision) (5-15 min)...")

        ticker_config = get_ticker_config(ticker)

        # Generate analysis using Gemini multimodal with GEMINI_INVESTOR_DECK_PROMPT
        from modules.company_profiles import GEMINI_INVESTOR_DECK_PROMPT

        # Build simplified prompt for multimodal (PDF contains all content)
        prompt_text = f"""You are analyzing an Investor Presentation/Deck for {ticker_config['company_name']} ({ticker}).

Presentation Date: {presentation_date}
Deck Type: {presentation_type}

I have provided you with the complete PDF presentation. Analyze it thoroughly and provide a comprehensive analysis following the structure in the prompt.

{GEMINI_INVESTOR_DECK_PROMPT.split('---')[0]}

Generate the complete page-by-page deck analysis now.
"""

        # Use Gemini 2.5 Flash (multimodal vision)
        model = genai.GenerativeModel('gemini-2.5-flash')

        start_time = time.time()

        # Send PDF bytes + prompt to model
        response = model.generate_content(
            [pdf_part, prompt_text],
            generation_config=genai.types.GenerationConfig(
                temperature=0.3,
                max_output_tokens=16000
            )
        )

        generation_time = time.time() - start_time

        if not response or not response.text:
            raise ValueError("Gemini presentation analysis failed")

        profile_markdown = response.text

        # Extract token counts from response metadata
        token_count_input = getattr(response.usage_metadata, 'prompt_token_count', None) if hasattr(response, 'usage_metadata') else None
        token_count_output = getattr(response.usage_metadata, 'candidates_token_count', None) if hasattr(response, 'usage_metadata') else None

        LOG.info(f"[{ticker}] âœ… Generated {len(profile_markdown)} characters (input: {token_count_input} tokens, output: {token_count_output} tokens)")

        # Note: No file cleanup needed since we're passing PDF bytes directly (not using File API)

        metadata = {
            'model': 'gemini-2.5-flash',
            'generation_time_seconds': round(generation_time, 2),
            'token_count_input': token_count_input,
            'token_count_output': token_count_output
        }

        # Progress: 80% - Saving to database
        update_job_status(job_id, progress=80)
        LOG.info(f"[{ticker}] ðŸ’¾ [JOB {job_id}] Saving presentation analysis to database...")

        with db() as conn, conn.cursor() as cur:
            # Get page count from PDF
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                page_count = len(reader.pages)

            file_size_bytes = os.path.getsize(file_path)

            # First, delete any existing presentation for this ticker/date/type
            cur.execute("""
                DELETE FROM sec_filings
                WHERE ticker = %s
                  AND filing_type = %s
                  AND presentation_date = %s
                  AND presentation_type = %s
            """, (
                ticker,
                'PRESENTATION',
                presentation_date,
                presentation_type
            ))

            # Then insert the new presentation analysis
            cur.execute("""
                INSERT INTO sec_filings (
                    ticker, filing_type,
                    company_name, industry,
                    presentation_date, presentation_type, presentation_title,
                    profile_markdown,
                    source_type, source_file,
                    page_count, file_size_bytes,
                    ai_provider, ai_model,
                    generation_time_seconds, token_count_input, token_count_output,
                    status
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                ticker,
                'PRESENTATION',
                ticker_config.get('company_name'),
                ticker_config.get('industry'),
                presentation_date,
                presentation_type,
                presentation_title,
                profile_markdown,
                'gemini_multimodal',
                config.get('file_name'),
                page_count,
                file_size_bytes,
                'gemini',
                metadata['model'],
                metadata['generation_time_seconds'],
                metadata['token_count_input'],
                metadata['token_count_output'],
                'active'
            ))
            conn.commit()

        # Progress: 95% - Sending email
        if config.get('send_email', True):
            update_job_status(job_id, progress=95)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Sending email notification...")

            # Get real-time stock data using unified helper (yfinance â†’ Polygon â†’ database â†’ None)
            stock_data = get_filing_stock_data(ticker)

            # Generate email (reuse company profile email function)
            email_data = generate_company_profile_email(
                ticker=ticker,
                company_name=ticker_config['company_name'],
                industry=ticker_config.get('industry', 'N/A'),
                fiscal_year=None,  # Not applicable for presentations
                filing_date=presentation_date,
                profile_markdown=profile_markdown,
                stock_price=stock_data['stock_price'],
                price_change_pct=stock_data['price_change_pct'],
                price_change_color=stock_data['price_change_color'],
                ytd_return_pct=stock_data['ytd_return_pct'],
                ytd_return_color=stock_data['ytd_return_color'],
                market_status=stock_data['market_status'],
                return_label=stock_data['return_label'],
                filing_type='PRESENTATION'
            )

            # Subject already set correctly by generate_company_profile_email()
            # (format: "{ticker} {presentation_title}")

            send_email(
                subject=email_data['subject'],
                html_body=email_data['html'],
                to=ADMIN_EMAIL  # âœ… Uses ADMIN_EMAIL env var
            )

        # Clean up temp PDF file
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            LOG.info(f"[{ticker}] ðŸ—‘ï¸ [JOB {job_id}] Cleaned up temp file: {file_path}")

        # Mark complete
        update_job_status(job_id, status='completed', progress=100)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Presentation analysis complete")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Presentation analysis failed: {str(e)}")
        LOG.error(f"Stacktrace: {traceback.format_exc()}")

        # Stop heartbeat
        stop_heartbeat_thread(job_id)

        # Mark failed
        update_job_status(
            job_id,
            status='failed',
            error_message=str(e)[:1000],
            error_stacktrace=traceback.format_exc()[:5000]
        )


async def process_ticker_job(job: dict):
    """Process a single ticker job (ingest + digest + commit) OR company profile generation"""
    job_id = job['job_id']
    ticker = job['ticker']
    phase = job.get('phase', 'ingest_start')

    # psycopg returns JSONB as dict, not string
    config = job['config'] if isinstance(job['config'], dict) else {}

    # ROUTE: If this is a company profile generation job, handle separately
    if phase == 'profile_generation':
        await process_company_profile_phase(job)
        return

    # ROUTE: If this is a 10-Q generation job, handle separately
    if phase == '10q_generation':
        await process_10q_profile_phase(job)
        return

    # ROUTE: If this is a transcript generation job, handle separately
    if phase == 'transcript_generation':
        await process_transcript_generation_phase(job)
        return

    # ROUTE: If this is a press release generation job, handle separately
    if phase == 'press_release_generation':
        await process_press_release_phase(job)
        return

    # ROUTE: If this is an 8-K summary generation job, handle separately
    if phase == '8k_summary_generation':
        await process_8k_summary_phase(job)
        return

    # ROUTE: If this is an investor presentation job, handle separately
    if phase == 'presentation_generation':
        await process_presentation_phase(job)
        return

    # ROUTE: If this is a quality review job, handle separately
    if phase == 'quality_review_generation':
        await process_quality_review_phase(job)
        return

    # ROUTE: If this is an email regeneration job, handle separately
    if phase == 'regenerate_email_generation':
        await process_regenerate_email_phase(job)
        return

    # Standard ticker processing (news articles)
    minutes = config.get('minutes', 1440)
    batch_size = config.get('batch_size', 3)
    triage_batch_size = config.get('triage_batch_size', 3)
    report_type = config.get('report_type', 'daily')

    start_time = time.time()
    memory_start = memory_monitor.get_current_mb() if hasattr(memory_monitor, 'get_current_mb') else 0

    # Reset cost tracker for this ticker
    reset_cost_tracker()

    LOG.info(f"[{ticker}] ðŸš€ [JOB {job_id}] Starting processing for {ticker}")
    LOG.info(f"   Config: minutes={minutes}, batch={batch_size}, triage_batch={triage_batch_size}, report_type={report_type}")

    try:
        # START HEARTBEAT THREAD - Updates last_updated every 60s to prevent premature reclaim
        start_heartbeat_thread(job_id)

        # NOTE: TICKER_PROCESSING_LOCK removed from cron_ingest/cron_digest for parallel processing
        # We don't acquire it here to avoid deadlock (lock is not reentrant)

        # Check if job was cancelled before starting
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT status FROM ticker_processing_jobs WHERE job_id = %s", (job_id,))
            current_status = cur.fetchone()
            if current_status and current_status['status'] == 'cancelled':
                LOG.warning(f"[{ticker}] ðŸš« [JOB {job_id}] Job cancelled before starting, exiting")
                return

        # ========================================================================
        # FIX #1: STORE RECIPIENTS AT JOB START (Dec 2025)
        # Creates email_queue entry immediately so we have recipients even if job fails
        # This ensures rerun button always has the recipient list available
        # ========================================================================
        mode = config.get('mode', 'test')
        recipients = config.get('recipients', [])

        if mode == 'daily' and recipients:
            ticker_config_early = get_ticker_config(ticker)
            company_name_early = ticker_config_early.get('company_name', ticker) if ticker_config_early else ticker

            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO email_queue (
                        ticker, company_name, recipients, status, report_type,
                        summary_date, heartbeat, created_at, is_production
                    )
                    VALUES (%s, %s, %s, 'processing', %s, CURRENT_DATE, NOW(), NOW(), TRUE)
                    ON CONFLICT (ticker) DO UPDATE
                    SET company_name = EXCLUDED.company_name,
                        recipients = EXCLUDED.recipients,
                        status = 'processing',
                        report_type = EXCLUDED.report_type,
                        summary_date = EXCLUDED.summary_date,
                        heartbeat = NOW(),
                        updated_at = NOW(),
                        error_message = NULL
                """, (ticker, company_name_early, recipients, report_type))
                conn.commit()

            LOG.info(f"[{ticker}] ðŸ“‹ [JOB {job_id}] Email queue entry created with {len(recipients)} recipients (status=processing)")

        # PHASE 0: SAFETY CHECK - Ensure feeds exist (failsafe)
        # This is a defensive check in case /jobs/submit initialization somehow failed
        LOG.info(f"[{ticker}] ðŸ” [JOB {job_id}] Phase 0: Checking feeds exist...")

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) as count FROM ticker_feeds
                WHERE ticker = %s AND active = TRUE
            """, (ticker,))
            feed_count = cur.fetchone()['count']

            if feed_count == 0:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No feeds found! Creating now (failsafe)...")

                # Get metadata from database (no AI enhancement)
                metadata = get_or_create_enhanced_ticker_metadata(ticker)
                LOG.info(f"[{ticker}] ðŸ“‹ [JOB {job_id}] Metadata: company={metadata.get('company_name', 'N/A')}")

                # Create feeds
                feeds_created = create_feeds_for_ticker_new_architecture(ticker, metadata)

                if feeds_created:
                    LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Created {len(feeds_created)} feeds (failsafe recovery)")
                else:
                    LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Failed to create feeds - job will likely fail")
            else:
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] {feed_count} feeds verified")

        # PHASE 1: Ingest (already implemented in /cron/ingest)
        update_job_status(job_id, progress=10)
        LOG.info(f"[{ticker}] ðŸ“¥ [JOB {job_id}] Phase 1: Ingest starting...")

        # Call ingest logic (will be defined later in file)
        # We can't import it here due to circular dependency
        # So we'll call it by name after it's defined
        ingest_result = await process_ingest_phase(
            job_id=job_id,
            ticker=ticker,
            minutes=minutes,
            batch_size=batch_size,
            triage_batch_size=triage_batch_size,
            mode=config.get('mode', 'daily'),
            report_type=report_type
        )

        update_job_status(job_id, progress=60)

        # Log detailed ingest stats
        if ingest_result:
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 1: Ingest complete")
            if isinstance(ingest_result, dict):
                phase1 = ingest_result.get('phase_1_ingest', {})
                phase2 = ingest_result.get('phase_2_triage', {})
                phase4 = ingest_result.get('phase_4_async_batch_scraping', {})

                if phase1:
                    LOG.info(f"   Articles: New({phase1.get('total_inserted', 0)}) Total({phase1.get('total_articles_in_timeframe', 0)})")

                if phase2 and phase2.get('selections_by_ticker'):
                    sel = phase2['selections_by_ticker'].get(ticker, {})
                    LOG.info(f"   Triage: Company({sel.get('company', 0)}) Industry({sel.get('industry', 0)}) Competitor({sel.get('competitor', 0)})")

                if phase4:
                    LOG.info(f"   Scraping: New({phase4.get('scraped', 0)}) Reused({phase4.get('reused_existing', 0)}) Success({phase4.get('overall_success_rate', 'N/A')})")
        else:
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 1: Ingest complete (no detailed stats)")

        # Check if cancelled after Phase 1
        # Also re-fetch config to get flagged_articles that were stored during ingest
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT status, config FROM ticker_processing_jobs WHERE job_id = %s", (job_id,))
            job_status = cur.fetchone()
            if job_status and job_status['status'] == 'cancelled':
                LOG.warning(f"[{ticker}] ðŸš« [JOB {job_id}] Job cancelled after Phase 1, exiting")
                # Update email_queue to cancelled so it doesn't show as stuck processing
                if mode == 'daily':
                    cur.execute("""
                        UPDATE email_queue SET status = 'cancelled', updated_at = NOW()
                        WHERE ticker = %s AND status = 'processing'
                    """, (ticker,))
                    conn.commit()
                return

            # Re-fetch flagged_articles that were stored during ingest phase
            updated_config = job_status['config'] if job_status and isinstance(job_status['config'], dict) else {}
            flagged_article_ids = updated_config.get('flagged_articles', [])

            if flagged_article_ids:
                LOG.info(f"[{ticker}] ðŸ“‹ [JOB {job_id}] Retrieved {len(flagged_article_ids)} flagged article IDs from ingest phase")
            else:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No flagged articles found in config after ingest")

        # PHASE 1.5: Resolve Google News URLs for flagged articles (NEW!)
        update_job_status(job_id, progress=62)
        LOG.info(f"[{ticker}] ðŸ”— [JOB {job_id}] Phase 1.5: Google News URL resolution starting...")

        if flagged_article_ids:
            # Resolve URLs and get deduplicated list back
            flagged_article_ids = await resolve_flagged_google_news_urls(ticker, flagged_article_ids)
            LOG.info(f"[{ticker}] ðŸ“‹ [JOB {job_id}] After resolution & deduplication: {len(flagged_article_ids or [])} flagged articles remain")
        else:
            LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No flagged articles to resolve")

        update_job_status(job_id, progress=64)

        # ========================================================================
        # PHASE 2: SCRAPING (content extraction for flagged articles)
        # ========================================================================
        update_job_status(job_id, progress=65)
        LOG.info(f"[{ticker}] ðŸ“„ [JOB {job_id}] Phase 2: Scraping starting...")

        scrape_result = await process_scrape_phase(
            job_id=job_id,
            ticker=ticker,
            flagged_article_ids=flagged_article_ids
        )

        update_job_status(job_id, progress=75)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 2: Scraping complete ({scrape_result.get('scraped', 0)} scraped, {scrape_result.get('reused', 0)} reused)")

        # ========================================================================
        # PHASE 3: ARTICLE FETCHING (prepare data for AI generation)
        # ========================================================================
        LOG.info(f"[{ticker}] ðŸ“¥ [JOB {job_id}] Phase 3: Fetching articles for AI generation...")

        articles_result = await fetch_digest_articles(
            hours=minutes / 60,
            tickers=[ticker],
            flagged_article_ids=flagged_article_ids
        )

        articles_by_ticker = articles_result.get('articles_by_ticker', {})
        total_articles = articles_result.get('articles', 0)

        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 3: Fetched {total_articles} articles")

        # ========================================================================
        # PHASE 4: EXECUTIVE SUMMARY GENERATION (Phase 1 + 2 + 3)
        # ========================================================================
        update_job_status(job_id, progress=80)
        LOG.info(f"[{ticker}] ðŸ§  [JOB {job_id}] Phase 4: Generating executive summary (all phases)...")

        ticker_config = get_ticker_config(ticker)
        summary_result = await generate_executive_summary_all_phases(
            ticker=ticker,
            articles_by_ticker=articles_by_ticker,
            config=ticker_config or {},
            report_type=report_type
        )

        update_job_status(job_id, progress=92)

        if not summary_result.get('success'):
            LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Executive summary generation failed: {summary_result.get('error')}")
            # Phase 3 failed - try to load Phase 1+2 JSON from database as fallback
            LOG.info(f"[{ticker}] ðŸ”„ [JOB {job_id}] Attempting to load Phase 1+2 JSON from database as fallback...")
            try:
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        SELECT summary_text FROM executive_summaries
                        WHERE ticker = %s AND summary_date = CURRENT_DATE
                        ORDER BY generated_at DESC LIMIT 1
                    """, (ticker,))
                    fallback_result = cur.fetchone()
                    if fallback_result and fallback_result['summary_text']:
                        phase3_json = json.loads(fallback_result['summary_text'])
                        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Loaded Phase 1+2 JSON as fallback (emails will use this)")
                    else:
                        phase3_json = None
                        LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No Phase 1+2 JSON found in database - emails will be skipped")
            except Exception as fallback_err:
                LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Failed to load fallback JSON: {fallback_err}")
                phase3_json = None
        else:
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Phase 4: Executive summary complete (all phases saved)")
            phase3_json = summary_result.get('phase3_json')

        # Check if cancelled after Phase 4
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT status FROM ticker_processing_jobs WHERE job_id = %s", (job_id,))
            current_status = cur.fetchone()
            if current_status and current_status['status'] == 'cancelled':
                LOG.warning(f"[{ticker}] ðŸš« [JOB {job_id}] Job cancelled after Phase 4, exiting")
                # Update email_queue to cancelled so it doesn't show as stuck processing
                if mode == 'daily':
                    cur.execute("""
                        UPDATE email_queue SET status = 'cancelled', updated_at = NOW()
                        WHERE ticker = %s AND status = 'processing'
                    """, (ticker,))
                    conn.commit()
                return

        # Log resource usage
        memory_after_phase4 = memory_monitor.get_current_mb() if hasattr(memory_monitor, 'get_current_mb') else 0
        if DB_POOL:
            try:
                pool_stats = DB_POOL.get_stats()
                LOG.info(f"[{ticker}] ðŸ“Š Resource Status: Memory={memory_after_phase4:.1f}MB, DB Pool={pool_stats.get('pool_size', 0)}/{pool_stats.get('pool_max', 0)} connections")
            except:
                LOG.info(f"[{ticker}] ðŸ“Š Resource Status: Memory={memory_after_phase4:.1f}MB")
        else:
            LOG.info(f"[{ticker}] ðŸ“Š Resource Status: Memory={memory_after_phase4:.1f}MB")

        # ========================================================================
        # PHASE 5: EMAIL GENERATION AND DELIVERY
        # ========================================================================
        update_job_status(job_id, progress=95)

        mode = config.get('mode', 'test')
        recipients = config.get('recipients', [])

        try:
            # ========================================================================
            # EMAIL #2: Content QA (sent to admin)
            # ========================================================================
            if phase3_json:
                LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Generating Email #2 (Content QA)...")

                try:
                    days = int(minutes / 60 / 24) if minutes >= 1440 else 1
                    email2_html = await build_enhanced_digest_html(
                        articles_by_ticker=articles_by_ticker,
                        period_days=days,
                        show_ai_analysis=True,
                        show_descriptions=True,
                        flagged_article_ids=flagged_article_ids,
                        phase3_json=phase3_json
                    )

                    # Build subject
                    company_name = ticker_config.get('company_name', ticker) if ticker_config else ticker
                    report_label = "(WEEKLY)" if report_type == 'weekly' else "(DAILY)"
                    email2_subject = f"QA Content Review {report_label}: {company_name} ({ticker}) - {total_articles} articles"
                    if mode == 'test':
                        email2_subject += " (TEST)"

                    # Save Email #2 to database (daily mode only)
                    if mode == 'daily':
                        with db() as conn, conn.cursor() as cur:
                            cur.execute("""
                                INSERT INTO email_queue (ticker, email_2_html, created_at)
                                VALUES (%s, %s, NOW())
                                ON CONFLICT (ticker) DO UPDATE
                                SET email_2_html = EXCLUDED.email_2_html,
                                    updated_at = NOW()
                            """, (ticker, email2_html))
                        LOG.debug(f"[{ticker}] Saved Email #2 to database")

                    # Send Email #2 to admin
                    email2_success = send_email(email2_subject, email2_html)
                    if email2_success:
                        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email #2 sent to admin")
                    else:
                        LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Email #2 send failed")

                except Exception as e:
                    LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Email #2 generation failed: {e}")
                    LOG.error(f"[{ticker}] Stacktrace: {traceback.format_exc()}")
            else:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No Phase 3 JSON - skipping Email #2")

            # ========================================================================
            # EMAIL #3: User Intelligence Report
            # ========================================================================
            update_job_status(job_id, progress=97)
            LOG.info(f"[{ticker}] ðŸ“§ [JOB {job_id}] Generating Email #3 (Stock Intelligence)...")

            # Track Email #3 success for failure detection (Fix #2 - Dec 2025)
            email3_success = False

            if phase3_json:
                try:
                    # Generate Email #3 HTML
                    email3_data = generate_email_html_core(
                        ticker=ticker,
                        hours=int(minutes/60),
                        recipient_email=None,  # Use {{UNSUBSCRIBE_TOKEN}} placeholder
                        report_type=report_type
                    )

                    if email3_data:
                        if mode == 'daily':
                            # DAILY MODE: Queue for 8:30 AM send
                            if recipients:
                                company_name = ticker_config.get('company_name', ticker) if ticker_config else ticker

                                with db() as conn, conn.cursor() as cur:
                                    cur.execute("""
                                        INSERT INTO email_queue (
                                            ticker, company_name, recipients, email_html, email_subject,
                                            article_count, status, is_production, report_type, summary_date,
                                            heartbeat, created_at
                                        )
                                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, CURRENT_DATE, NOW(), NOW())
                                        ON CONFLICT (ticker) DO UPDATE
                                        SET company_name = EXCLUDED.company_name,
                                            recipients = EXCLUDED.recipients,
                                            email_html = EXCLUDED.email_html,
                                            email_subject = EXCLUDED.email_subject,
                                            article_count = EXCLUDED.article_count,
                                            status = 'ready',
                                            error_message = NULL,
                                            is_production = EXCLUDED.is_production,
                                            report_type = EXCLUDED.report_type,
                                            summary_date = EXCLUDED.summary_date,
                                            heartbeat = NOW(),
                                            updated_at = NOW()
                                    """, (
                                        ticker, company_name, recipients,
                                        email3_data['html'], email3_data['subject'],
                                        email3_data['article_count'], 'ready', True, report_type
                                    ))
                                    conn.commit()

                                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email #3 queued for {len(recipients)} recipients")
                                email3_success = True  # Mark success for daily mode

                                # Send preview to admin
                                try:
                                    preview_subject = f"[PREVIEW] {email3_data['subject']}"
                                    send_email(preview_subject, email3_data['html'])
                                    LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email #3 preview sent to admin")
                                except Exception as e:
                                    LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Failed to send preview: {e}")
                            else:
                                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No recipients - skipping Email #3 queue")

                        else:
                            # TEST MODE: Send directly to admin
                            editorial_result = send_user_intelligence_report(
                                hours=int(minutes/60),
                                tickers=[ticker],
                                recipient_email=ADMIN_EMAIL,
                                summary_date=datetime.now().date(),
                                report_type=report_type
                            )

                            if editorial_result and editorial_result.get('status') == 'sent':
                                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Email #3 sent to admin (test mode)")
                                email3_success = True  # Mark success for test mode
                            else:
                                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] Email #3 send failed")
                    else:
                        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Failed to generate Email #3 HTML")

                except Exception as e:
                    LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Email #3 generation failed: {e}")
                    LOG.error(f"[{ticker}] Stacktrace: {traceback.format_exc()}")
            else:
                LOG.warning(f"[{ticker}] âš ï¸ [JOB {job_id}] No Phase 3 JSON - skipping Email #3")

        except Exception as e:
            LOG.error(f"[{ticker}] âŒ [JOB {job_id}] Email phase failed: {e}")
            LOG.error(f"[{ticker}] Stacktrace: {traceback.format_exc()}")
            # Continue to finalization even if emails fail

        # Check if cancelled after emails
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT status FROM ticker_processing_jobs WHERE job_id = %s", (job_id,))
            current_status = cur.fetchone()
            if current_status and current_status['status'] == 'cancelled':
                LOG.warning(f"[{ticker}] ðŸš« [JOB {job_id}] Job cancelled after emails, exiting")
                # Update email_queue to cancelled so it doesn't show as stuck processing
                if mode == 'daily':
                    cur.execute("""
                        UPDATE email_queue SET status = 'cancelled', updated_at = NOW()
                        WHERE ticker = %s AND status = 'processing'
                    """, (ticker,))
                    conn.commit()
                return

        # ========================================================================
        # FINALIZATION
        # ========================================================================
        update_job_status(job_id, progress=99)
        LOG.info(f"[{ticker}] âœ… [JOB {job_id}] Finalizing job...")

        # Calculate final metrics
        duration = time.time() - start_time
        memory_end = memory_monitor.get_current_mb() if hasattr(memory_monitor, 'get_current_mb') else 0
        memory_used = max(0, memory_end - memory_start)

        # Log Claude API cost summary
        ticker_config = get_ticker_config(ticker)
        company_name = ticker_config.get("company_name", "") if ticker_config else ""
        log_cost_summary(ticker, company_name)

        # Build result dict
        result = {
            "ticker": ticker,
            "ingest": ingest_result,
            "scrape": scrape_result,
            "articles": articles_result,
            "summary": {"success": summary_result.get('success'), "error": summary_result.get('error')},
            "duration_seconds": duration,
            "memory_mb": memory_used,
            "email3_success": email3_success
        }

        # ========================================================================
        # FIX #2: Mark as FAILED if Email #3 was not generated in daily mode (Dec 2025)
        # A "no material developments" email is still success - only truly failed if
        # email_html was never generated (phase3_json is None or generate failed)
        # ========================================================================
        if mode == 'daily' and recipients and not email3_success:
            error_msg = "Email #3 generation failed - no user email was created"
            LOG.error(f"[{ticker}] âŒ [JOB {job_id}] FAILED: {error_msg}")

            # Mark email_queue as failed
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    UPDATE email_queue
                    SET status = 'failed',
                        error_message = %s,
                        updated_at = NOW()
                    WHERE ticker = %s
                """, (error_msg, ticker))
                conn.commit()

            # Mark job as failed
            update_job_status(
                job_id,
                status='failed',
                phase='email_failed',
                progress=99,
                result=result,
                error_message=error_msg,
                duration_seconds=duration,
                memory_mb=memory_used
            )

            # Update batch counters (failed job)
            with db() as conn, conn.cursor() as cur:
                cur.execute("""
                    UPDATE ticker_processing_batches
                    SET failed_jobs = failed_jobs + 1
                    WHERE batch_id = %s
                """, (job['batch_id'],))

            return result

        # Mark complete (success case)
        update_job_status(
            job_id,
            status='completed',
            phase='complete',
            progress=100,
            result=result,
            duration_seconds=duration,
            memory_mb=memory_used
        )

        # Log final resource stats
        if DB_POOL:
            try:
                pool_stats = DB_POOL.get_stats()
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] COMPLETED in {duration:.1f}s (memory: {memory_used:.1f}MB, DB pool: {pool_stats.get('pool_size', 0)}/{pool_stats.get('pool_max', 0)})")
            except:
                LOG.info(f"[{ticker}] âœ… [JOB {job_id}] COMPLETED in {duration:.1f}s (memory: {memory_used:.1f}MB)")
        else:
            LOG.info(f"[{ticker}] âœ… [JOB {job_id}] COMPLETED in {duration:.1f}s (memory: {memory_used:.1f}MB)")

        # Record success with circuit breaker
        job_circuit_breaker.record_success()

        # Update batch counters
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE ticker_processing_batches
                SET completed_jobs = completed_jobs + 1
                WHERE batch_id = %s
            """, (job['batch_id'],))

        return result

    except Exception as e:
        error_msg = str(e)
        error_trace = traceback.format_exc()
        duration = time.time() - start_time

        LOG.error(f"[{ticker}] âŒ [JOB {job_id}] FAILED after {duration:.1f}s: {error_msg}")
        LOG.error(f"   Stacktrace: {error_trace}")

        # Determine if this is a system-wide failure
        is_system_failure = any(keyword in error_msg.lower() for keyword in [
            'database', 'connection', 'psycopg', 'timeout', 'memory'
        ])

        if is_system_failure:
            job_circuit_breaker.record_failure(type(e).__name__, error_msg)
        else:
            # Ticker-specific failure, not system-wide
            job_circuit_breaker.record_success()

        update_job_status(
            job_id,
            status='failed',
            error_message=error_msg[:1000],  # Limit size
            error_stacktrace=error_trace[:5000],
            duration_seconds=duration
        )

        # Update email_queue to failed (if it exists and is still processing)
        # This ensures we don't leave orphaned 'processing' entries
        try:
            mode = config.get('mode', 'test')
            if mode == 'daily':
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        UPDATE email_queue
                        SET status = 'failed',
                            error_message = %s,
                            updated_at = NOW()
                        WHERE ticker = %s AND status = 'processing'
                    """, (f"Job exception: {error_msg[:500]}", ticker))
                    conn.commit()
        except Exception as eq_err:
            LOG.warning(f"[{ticker}] Failed to update email_queue on exception: {eq_err}")

        # Update batch counters
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE ticker_processing_batches
                SET failed_jobs = failed_jobs + 1
                WHERE batch_id = %s
            """, (job['batch_id'],))

        raise

    finally:
        # STOP HEARTBEAT THREAD (always runs, even on exception)
        stop_heartbeat_thread(job_id)

        # Cleanup thread-local HTTP session and connector
        try:
            await cleanup_http_session()
            LOG.debug(f"[{ticker}] ðŸ§¹ Cleaned up HTTP session + connector for thread")
        except Exception as cleanup_error:
            LOG.warning(f"[{ticker}] Failed to cleanup HTTP session/connector: {cleanup_error}")

def job_worker_loop():
    """Background worker that polls database for jobs and processes them concurrently"""
    global _job_worker_running
    from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED

    LOG.info(f"ðŸ”§ Job worker started (worker_id: {get_worker_id()}, max_concurrent_jobs: {MAX_CONCURRENT_JOBS})")

    # STEP 1: Capture feed-relevant fields BEFORE CSV sync (for smart comparison)
    # This allows us to detect which tickers' feeds actually changed
    feed_fields_before_sync = None
    try:
        LOG.info("ðŸ“¸ Capturing feed-relevant fields before CSV sync...")
        ticker_recipients = load_active_users()
        active_tickers = list(ticker_recipients.keys())

        if active_tickers:
            feed_fields_before_sync = get_feed_relevant_fields(active_tickers)
            LOG.info(f"ðŸ“¸ Captured fields for {len(feed_fields_before_sync)} active tickers")
        else:
            LOG.info("ðŸ“¸ No active tickers - will skip comparison")
    except Exception as e:
        LOG.warning(f"âš ï¸ Failed to capture before-sync state: {e}")
        LOG.warning("âš ï¸ Will fall back to refreshing all tickers")

    # STEP 2: Sync CSV from GitHub (updates ticker_reference table)
    LOG.info("ðŸ”„ Syncing ticker reference from GitHub (job worker initialization)...")
    try:
        github_sync_result = sync_ticker_references_from_github()
        if github_sync_result["status"] == "success":
            LOG.info(f"âœ… GitHub sync successful: {github_sync_result.get('message', 'Completed')}")
        else:
            LOG.error(f"âŒ GitHub sync failed: {github_sync_result.get('message', 'Unknown error')}")
            LOG.error("âš ï¸ Worker will continue, but tickers may have incorrect data!")
    except Exception as e:
        LOG.error(f"âŒ GitHub sync crashed: {e}")
        LOG.error("âš ï¸ Worker will continue, but tickers may have incorrect data!")

    # STEP 3: Refresh feeds for active tickers AFTER CSV sync (smart comparison)
    # Only refreshes tickers whose feed-relevant fields changed (Dec 2025 optimization)
    LOG.info("ðŸ”„ Checking for feed changes (smart refresh)...")
    try:
        feed_refresh_result = refresh_feeds_for_active_tickers(before_state=feed_fields_before_sync)
        if feed_refresh_result["status"] == "success":
            skipped = feed_refresh_result.get('tickers_skipped', 0)
            skipped_msg = f", {skipped} unchanged" if skipped > 0 else ""
            LOG.info(f"âœ… Feed refresh successful: {feed_refresh_result.get('tickers_refreshed', 0)} tickers refreshed{skipped_msg}, "
                     f"{feed_refresh_result.get('total_feeds_created', 0)} feed associations")
        elif feed_refresh_result["status"] == "skipped":
            LOG.info(f"â­ï¸ Feed refresh skipped: {feed_refresh_result.get('message', 'No changes detected')}")
        else:
            LOG.warning(f"âš ï¸ Feed refresh had issues: {feed_refresh_result.get('status', 'unknown')}")
            if feed_refresh_result.get('errors'):
                for err in feed_refresh_result['errors'][:5]:  # Log first 5 errors
                    LOG.warning(f"  â†’ {err}")
    except Exception as e:
        LOG.error(f"âŒ Feed refresh crashed: {e}")
        LOG.error("âš ï¸ Worker will continue, but feeds may be stale!")

    # Create thread pool for concurrent job processing
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_JOBS, thread_name_prefix="TickerWorker") as executor:
        active_futures = {}  # Map of future -> job_id

        while _job_worker_running:
            try:
                # Check circuit breaker
                if job_circuit_breaker.is_open():
                    LOG.warning("âš ï¸ Circuit breaker is OPEN, skipping job polling")
                    time.sleep(30)
                    continue

                # Poll for new jobs if we have capacity
                while len(active_futures) < MAX_CONCURRENT_JOBS:
                    job = get_next_queued_job()

                    if job:
                        # Submit job to thread pool
                        future = executor.submit(asyncio.run, process_ticker_job(job))
                        active_futures[future] = job['job_id']
                        LOG.info(f"ðŸ“¤ [JOB {job['job_id']}] Submitted to worker pool ({len(active_futures)}/{MAX_CONCURRENT_JOBS} active)")
                    else:
                        # No more jobs available
                        break

                # Wait for at least one job to complete (or timeout)
                if active_futures:
                    done_futures, _ = wait(active_futures.keys(), timeout=5, return_when=FIRST_COMPLETED)

                    # Process completed jobs
                    for done in done_futures:
                        job_id = active_futures.pop(done)
                        try:
                            done.result()  # Raises exception if job failed

                            # Check actual job status in database (job might have caught its own exception)
                            with db() as conn, conn.cursor() as cur:
                                cur.execute("SELECT status FROM ticker_processing_jobs WHERE job_id = %s", (job_id,))
                                result = cur.fetchone()
                                final_status = result['status'] if result else 'unknown'

                            if final_status == 'completed':
                                LOG.info(f"âœ… [JOB {job_id}] Completed successfully ({len(active_futures)}/{MAX_CONCURRENT_JOBS} active)")
                            elif final_status == 'failed':
                                LOG.error(f"âŒ [JOB {job_id}] Failed (marked in database) ({len(active_futures)}/{MAX_CONCURRENT_JOBS} active)")
                            elif final_status == 'timeout':
                                LOG.error(f"â±ï¸ [JOB {job_id}] Timed out ({len(active_futures)}/{MAX_CONCURRENT_JOBS} active)")
                            else:
                                LOG.warning(f"âš ï¸ [JOB {job_id}] Finished with status: {final_status} ({len(active_futures)}/{MAX_CONCURRENT_JOBS} active)")
                        except Exception as e:
                            LOG.error(f"âŒ [JOB {job_id}] Failed with exception: {e}")
                else:
                    # No active jobs, sleep and poll again
                    time.sleep(10)

            except KeyboardInterrupt:
                LOG.info("ðŸ›‘ Job worker received interrupt signal")
                break

            except Exception as e:
                LOG.error(f"ðŸ’¥ Job worker error: {e}")
                LOG.error(traceback.format_exc())
                time.sleep(30)  # Back off on errors

    LOG.info("ðŸ”š Job worker stopped")

def start_job_worker():
    """Start the background job worker thread"""
    global _job_worker_running, _job_worker_thread

    if _job_worker_running:
        LOG.warning("Job worker already running")
        return

    _job_worker_running = True
    _job_worker_thread = threading.Thread(target=job_worker_loop, daemon=True, name="JobWorker")
    _job_worker_thread.start()

    LOG.info("âœ… Job worker thread started")

def stop_job_worker():
    """Stop the background job worker thread"""
    global _job_worker_running

    if not _job_worker_running:
        return

    _job_worker_running = False
    LOG.info("â¸ï¸ Job worker stopping...")

    if _job_worker_thread:
        _job_worker_thread.join(timeout=10)
        LOG.info("âœ… Job worker stopped")

def restart_worker_thread():
    """Called when worker appears frozen - exit process for clean restart by Render"""
    LOG.critical("ðŸ’€ Worker frozen detected - requeuing jobs and exiting for Render restart")
    LOG.critical("This ensures all threads are properly terminated and jobs can retry")

    # Requeue all processing jobs so they can be retried after restart
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE ticker_processing_jobs
                SET status = 'queued',
                    started_at = NULL,
                    worker_id = NULL,
                    progress = 0,
                    error_message = COALESCE(error_message, '') || ' | Requeued: Worker frozen detected',
                    last_updated = NOW()
                WHERE status = 'processing'
                RETURNING ticker
            """)
            requeued = [row['ticker'] for row in cur.fetchall()]
            if requeued:
                LOG.info(f"ðŸ“‹ Requeued {len(requeued)} jobs for retry after restart: {', '.join(requeued)}")
            else:
                LOG.info("ðŸ“‹ No processing jobs to requeue")
    except Exception as e:
        LOG.error(f"Failed to requeue jobs before exit: {e}")

    # Force exit - Render will restart the service with a clean slate
    os._exit(1)

def worker_heartbeat_monitor_loop():
    """Monitor worker health and restart if frozen"""
    global _last_worker_activity, _worker_restart_count

    LOG.info("ðŸ’“ Worker heartbeat monitor started (checks every 60 seconds, 5-minute threshold)")

    while True:
        try:
            time.sleep(60)  # Check every minute

            # Check if worker thread is alive
            if not _job_worker_running or not _job_worker_thread or not _job_worker_thread.is_alive():
                LOG.warning("âš ï¸ Worker thread is not running - attempting thread restart...")
                try:
                    start_job_worker()
                    _worker_restart_count += 1
                    LOG.info(f"âœ… Worker thread restarted (total restarts: {_worker_restart_count})")
                except Exception as e:
                    LOG.error(f"Failed to restart worker thread: {e} - exiting for Render restart")
                    restart_worker_thread()
                continue

            # Check for recent worker activity (includes completed jobs for better tracking)
            with db() as conn, conn.cursor() as cur:
                # Look for any recent job activity (processing, queued, or recently completed)
                cur.execute("""
                    SELECT MAX(GREATEST(
                        COALESCE(last_updated, '1970-01-01'::timestamp),
                        COALESCE(completed_at, '1970-01-01'::timestamp)
                    )) as latest_activity
                    FROM ticker_processing_jobs
                    WHERE status IN ('processing', 'queued', 'completed')
                    AND (last_updated > NOW() - INTERVAL '1 hour' OR completed_at > NOW() - INTERVAL '1 hour')
                """)
                row = cur.fetchone()
                latest_activity = row['latest_activity'] if row else None

                if latest_activity:
                    _last_worker_activity = latest_activity
                    # Make timezone-aware if it's naive (database returns naive timestamps)
                    if latest_activity.tzinfo is None:
                        latest_activity = latest_activity.replace(tzinfo=timezone.utc)
                    time_since_activity = (datetime.now(timezone.utc) - latest_activity).total_seconds() / 60

                    # If no activity for 5 minutes AND there are jobs to process, worker is frozen
                    if time_since_activity > 5:
                        # Double-check there are actually jobs waiting
                        cur.execute("""
                            SELECT COUNT(*) as count
                            FROM ticker_processing_jobs
                            WHERE status = 'queued'
                        """)
                        queued_count = cur.fetchone()['count']

                        if queued_count > 0:
                            LOG.error(f"ðŸš¨ Worker frozen! {queued_count} jobs queued, no activity for {time_since_activity:.1f} minutes")
                            restart_worker_thread()

        except Exception as e:
            LOG.error(f"Worker heartbeat monitor error: {e}")
            LOG.error(traceback.format_exc())
            time.sleep(60)  # Continue monitoring even if error

def timeout_watchdog_loop():
    """Monitor for timed-out jobs and retry up to 3 times"""
    LOG.info("â° Timeout watchdog started (with automatic retry)")

    while _job_worker_running:
        try:
            time.sleep(60)  # Check every minute

            with db() as conn, conn.cursor() as cur:
                # Find jobs that exceeded timeout and update based on retry count
                # Jobs with retry_count < 3 get requeued, others fail permanently
                cur.execute("""
                    UPDATE ticker_processing_jobs
                    SET
                        status = CASE
                            WHEN retry_count < 3 THEN 'queued'
                            ELSE 'failed'
                        END,
                        retry_count = retry_count + 1,
                        started_at = CASE WHEN retry_count < 3 THEN NULL ELSE started_at END,
                        worker_id = CASE WHEN retry_count < 3 THEN NULL ELSE worker_id END,
                        progress = CASE WHEN retry_count < 3 THEN 0 ELSE progress END,
                        error_message = CASE
                            WHEN retry_count < 3 THEN 'Timeout - retry ' || (retry_count + 1) || '/3'
                            ELSE 'Failed after 3 timeout retries'
                        END,
                        completed_at = CASE
                            WHEN retry_count >= 3 THEN NOW()
                            ELSE NULL
                        END,
                        last_updated = NOW()
                    WHERE status = 'processing'
                    AND timeout_at < NOW()
                    RETURNING job_id, ticker, worker_id, retry_count,
                              CASE WHEN retry_count <= 3 THEN 'queued' ELSE 'failed' END as new_status
                """)

                timed_out = cur.fetchall()
                for job in timed_out:
                    if job['new_status'] == 'queued':
                        # Job will be retried
                        LOG.error(f"â° [RETRY {job['retry_count']}/3] {job['ticker']} timed out - requeuing for retry (job_id: {job['job_id']})")
                    else:
                        # Job permanently failed
                        LOG.error(f"âŒ [FAILED] {job['ticker']} failed after 3 timeout retries - manual intervention required (job_id: {job['job_id']})")

                        # Update email_queue to failed (if exists and still processing)
                        cur.execute("""
                            UPDATE email_queue
                            SET status = 'failed',
                                error_message = 'Job timed out after 3 retries',
                                updated_at = NOW()
                            WHERE ticker = %s AND status = 'processing'
                        """, (job['ticker'],))

                        # Only update batch failed counter for permanent failures
                        cur.execute("""
                            UPDATE ticker_processing_batches
                            SET failed_jobs = failed_jobs + 1
                            WHERE batch_id = (
                                SELECT batch_id FROM ticker_processing_jobs WHERE job_id = %s
                            )
                        """, (job['job_id'],))

                # Also check for queue timeouts (jobs stuck in queue past their timeout)
                # This catches jobs that were never claimed (worker down, etc.)
                cur.execute("""
                    UPDATE ticker_processing_jobs
                    SET status = 'failed',
                        error_message = 'Queue timeout - job never claimed after 4 hours',
                        completed_at = NOW()
                    WHERE status = 'queued'
                    AND timeout_at < NOW()
                    RETURNING job_id, ticker
                """)

                queue_timed_out = cur.fetchall()
                for job in queue_timed_out:
                    LOG.error(f"âŒ [QUEUE TIMEOUT] {job['ticker']} never claimed after 4 hours (job_id: {job['job_id']})")

                    # Update batch failed counter
                    cur.execute("""
                        UPDATE ticker_processing_batches
                        SET failed_jobs = failed_jobs + 1
                        WHERE batch_id = (
                            SELECT batch_id FROM ticker_processing_jobs WHERE job_id = %s
                        )
                    """, (job['job_id'],))

        except Exception as e:
            LOG.error(f"Timeout watchdog error: {e}")
            time.sleep(30)

    LOG.info("â° Timeout watchdog stopped")

# Start workers on app startup
def job_queue_reclaim_loop():
    """
    Job queue reclaim thread - monitors for dead workers via stale heartbeat.
    Runs every 60 seconds and requeues jobs with stale last_updated (>3 minutes).

    CRITICAL: Prevents jobs from getting stuck forever during rolling deployments.
    When Render kills OLD worker mid-job, this thread detects stale heartbeat and requeues.
    """
    LOG.info("ðŸ”„ Job queue reclaim thread started (checks every 60s, requeues after 3min stale heartbeat)")

    while True:
        try:
            time.sleep(60)  # Check every minute

            with db() as conn, conn.cursor() as cur:
                # Find jobs with stale heartbeat (no update in 3 minutes = worker likely dead)
                # CRITICAL: Do NOT overwrite phase - preserve original so job routes correctly when restarted
                cur.execute("""
                    UPDATE ticker_processing_jobs
                    SET status = 'queued',
                        started_at = NULL,
                        worker_id = NULL,
                        progress = 0,
                        error_message = COALESCE(error_message, '') || ' | Reclaimed: Dead worker detected (heartbeat stale >3min)',
                        last_updated = NOW()
                    WHERE status = 'processing'
                    AND last_updated < NOW() - INTERVAL '3 minutes'
                    RETURNING job_id, ticker, worker_id, phase, progress AS old_progress,
                              EXTRACT(EPOCH FROM (NOW() - last_updated)) / 60 AS minutes_stale
                """)

                reclaimed = cur.fetchall()

                if reclaimed:
                    conn.commit()
                    LOG.warning(f"ðŸ”„ Job queue reclaim thread reclaimed {len(reclaimed)} jobs with stale heartbeat:")
                    for job in reclaimed:
                        LOG.info(f"   â†’ {job['ticker']} (job_id: {job['job_id']}, worker: {job['worker_id']}, "
                                f"phase: {job['phase']}, was at {job['old_progress']}%, stale for {job['minutes_stale']:.1f}min, phase preserved for retry)")

                        # Update batch counters
                        cur.execute("""
                            SELECT batch_id FROM ticker_processing_jobs WHERE job_id = %s
                        """, (job['job_id'],))
                        batch_result = cur.fetchone()

                        if batch_result:
                            # Note: Don't increment failed_jobs counter - job is being requeued for retry
                            LOG.info(f"   â†’ Job {job['job_id']} requeued in batch {batch_result['batch_id']}")

                    conn.commit()

        except Exception as e:
            LOG.error(f"âŒ Job queue reclaim thread error: {e}")
            LOG.error(traceback.format_exc())
            # Continue running despite errors

def email_queue_watchdog_loop():
    """
    Email queue watchdog - monitors for stalled jobs.
    Runs every 60 seconds and marks jobs with stale heartbeat as failed.
    """
    LOG.info("ðŸ• Email queue watchdog started (checks every 60s, kills after 3min stale heartbeat)")

    while True:
        try:
            time.sleep(60)  # Check every minute

            with db() as conn, conn.cursor() as cur:
                # Find jobs with stale heartbeat (no update in 3 minutes)
                cur.execute("""
                    UPDATE email_queue
                    SET status = 'failed',
                        error_message = 'Processing stalled (no heartbeat for 3 minutes)',
                        updated_at = NOW()
                    WHERE status = 'processing'
                    AND (heartbeat IS NULL OR heartbeat < NOW() - INTERVAL '3 minutes')
                    RETURNING ticker
                """)

                stalled = cur.fetchall()

                if stalled:
                    conn.commit()
                    LOG.warning(f"âš ï¸ Email queue watchdog killed {len(stalled)} stalled jobs:")
                    for row in stalled:
                        LOG.info(f"   â†’ {row['ticker']}")

        except Exception as e:
            LOG.error(f"âŒ Email queue watchdog error: {e}")
            # Continue running despite errors


def stuck_transaction_monitor_loop():
    """
    Stuck transaction monitor - detects and logs zombie transactions.

    Monitors pg_stat_activity for:
    - Transactions idle >3 minutes (idle in transaction)
    - Queries waiting for locks >1 minute

    CRITICAL: Detects the root cause of job freezes (zombie transactions holding locks).
    The per-connection timeouts will kill these automatically, but we log warnings.
    """
    LOG.info("ðŸ” Stuck transaction monitor started (checks every 60s)")

    while True:
        try:
            time.sleep(60)  # Check every minute

            with db() as conn, conn.cursor() as cur:
                # Find idle-in-transaction connections
                cur.execute("""
                    SELECT pid, usename,
                           EXTRACT(EPOCH FROM (NOW() - query_start))::INT as seconds_idle,
                           EXTRACT(EPOCH FROM (NOW() - state_change))::INT as seconds_in_transaction,
                           substring(query, 1, 100) as query_preview
                    FROM pg_stat_activity
                    WHERE state = 'idle in transaction'
                      AND (NOW() - state_change) > interval '3 minutes'
                      AND pid != pg_backend_pid()
                    ORDER BY state_change
                """)

                idle_transactions = cur.fetchall()

                if idle_transactions:
                    LOG.warning(f"âš ï¸ Found {len(idle_transactions)} zombie transactions (idle >3min):")
                    for txn in idle_transactions:
                        LOG.warning(
                            f"   â†’ PID {txn['pid']}: {txn['usename']} "
                            f"(idle {txn['seconds_in_transaction']}s) "
                            f"Query: {txn['query_preview']}"
                        )
                    LOG.warning(
                        f"   ðŸ’¡ These will be auto-killed by idle_in_transaction_session_timeout (300s)"
                    )

                # Find queries waiting for locks
                cur.execute("""
                    SELECT pid, usename, wait_event,
                           EXTRACT(EPOCH FROM (NOW() - query_start))::INT as seconds_waiting,
                           substring(query, 1, 100) as query_preview
                    FROM pg_stat_activity
                    WHERE state = 'active'
                      AND wait_event = 'relation'
                      AND (NOW() - query_start) > interval '1 minute'
                      AND pid != pg_backend_pid()
                    ORDER BY query_start
                """)

                blocked_queries = cur.fetchall()

                if blocked_queries:
                    LOG.warning(f"âš ï¸ Found {len(blocked_queries)} queries waiting for locks >1min:")
                    for query in blocked_queries:
                        LOG.warning(
                            f"   â†’ PID {query['pid']}: {query['usename']} "
                            f"(waiting {query['seconds_waiting']}s) "
                            f"Query: {query['query_preview']}"
                        )
                    LOG.warning(
                        f"   ðŸ’¡ These will fail at lock_timeout (30s) or statement_timeout (120s)"
                    )

        except Exception as e:
            LOG.error(f"âŒ Stuck transaction monitor error: {e}")
            # Continue running despite errors


@APP.on_event("startup")
async def startup_event():
    """Initialize job queue system on startup"""
    worker_id = get_worker_id()
    LOG.info("=" * 80)
    LOG.info(f"ðŸš€ FastAPI STARTUP EVENT - Worker: {worker_id}")
    LOG.info(f"   Python: {sys.version}")
    LOG.info(f"   Platform: {sys.platform}")
    LOG.info(f"   Environment: Render.com" if os.getenv('RENDER') else "   Environment: Local")
    LOG.info(f"   Port: {os.getenv('PORT', '10000')}")
    LOG.info(f"   Memory: {memory_monitor.get_current_mb() if hasattr(memory_monitor, 'get_current_mb') else 'N/A'} MB")

    # Initialize connection pool BEFORE starting job worker
    LOG.info("=" * 80)
    LOG.info("ðŸ”„ Initializing database connection pool...")
    try:
        init_connection_pool()
        LOG.info("=" * 80)
    except Exception as e:
        LOG.error("=" * 80)
        LOG.error(f"ðŸ’¥ STARTUP FAILED: Cannot initialize database connection pool")
        LOG.error(f"   Error: {e}")
        LOG.error(f"   Stacktrace: {traceback.format_exc()}")
        LOG.error("=" * 80)
        LOG.error("ðŸš¨ APPLICATION WILL NOT START - Database connection required for parallel processing")
        LOG.error("   Check database status and retry")
        LOG.error("=" * 80)
        raise  # Re-raise to fail FastAPI startup

    # Initialize database schema ONCE at startup (CRITICAL: Prevents lock contention during concurrent processing)
    LOG.info("=" * 80)
    LOG.info("ðŸ”„ Ensuring database schema is initialized...")
    try:
        ensure_schema()
        LOG.info("âœ… Database schema verified/created successfully")
        LOG.info("=" * 80)
    except Exception as e:
        LOG.error("=" * 80)
        LOG.error(f"ðŸ’¥ STARTUP FAILED: Cannot initialize database schema")
        LOG.error(f"   Error: {e}")
        LOG.error(f"   Stacktrace: {traceback.format_exc()}")
        LOG.error("=" * 80)
        LOG.error("ðŸš¨ APPLICATION WILL NOT START - Schema required for operation")
        LOG.error("   Check database permissions and retry")
        LOG.error("=" * 80)
        raise  # Re-raise to fail FastAPI startup

    LOG.info("ðŸ”§ Initializing job queue system...")

    # Reclaim orphaned jobs from previous worker instance (handles Render restarts)
    try:
        with db() as conn, conn.cursor() as cur:
            # First, log ALL processing jobs to understand restart impact
            cur.execute("""
                SELECT job_id, ticker, phase, progress, worker_id,
                       EXTRACT(EPOCH FROM (NOW() - started_at)) / 60 AS minutes_running
                FROM ticker_processing_jobs
                WHERE status = 'processing'
                ORDER BY started_at
            """)
            all_processing = cur.fetchall()

            if all_processing:
                LOG.warning(f"âš ï¸ STARTUP: Found {len(all_processing)} jobs in 'processing' state:")
                for job in all_processing:
                    LOG.info(f"   â†’ {job['ticker']} ({job['phase']}, {job['progress']}%, {job['minutes_running']:.1f}min, worker: {job['worker_id']})")

            # Reclaim jobs that were processing but worker died (older than 3 minutes = definitely orphaned)
            cur.execute("""
                UPDATE ticker_processing_jobs
                SET status = 'queued',
                    started_at = NULL,
                    worker_id = NULL,
                    progress = 0,
                    last_updated = NOW(),
                    error_message = COALESCE(error_message, '') || ' | Server restart detected, job reclaimed'
                WHERE status = 'processing'
                AND started_at < NOW() - INTERVAL '3 minutes'
                RETURNING job_id, ticker, phase, progress AS old_progress
            """)

            orphaned = cur.fetchall()
            if orphaned:
                LOG.warning(f"ðŸ”„ RECLAIMED {len(orphaned)} orphaned jobs (>3min old, server likely restarted):")
                for job in orphaned:
                    LOG.info(f"   â†’ {job['ticker']} was at {job['phase']} ({job['old_progress']}%), now queued for retry (phase preserved)")

            # Also check for jobs processing <3 minutes (possible crash mid-job)
            cur.execute("""
                SELECT COUNT(*) as recent_count
                FROM ticker_processing_jobs
                WHERE status = 'processing'
                AND started_at >= NOW() - INTERVAL '3 minutes'
            """)
            recent_result = cur.fetchone()
            if recent_result and recent_result['recent_count'] > 0:
                LOG.warning(f"âš ï¸ {recent_result['recent_count']} jobs started <3min ago still marked 'processing'")
                LOG.warning("   These will NOT be reclaimed yet (might still be running on old worker)")
                LOG.warning("   Job queue reclaim thread will requeue them if heartbeat becomes stale (>3min)")

            if not orphaned and not all_processing:
                LOG.info("âœ… No orphaned jobs found - clean startup")

    except Exception as e:
        LOG.error(f"âŒ Failed to reclaim orphaned jobs: {e}")
        LOG.error(f"   Stacktrace: {traceback.format_exc()}")

    # Email Queue Recovery: Mark stuck processing jobs as failed
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker, heartbeat
                FROM email_queue
                WHERE status = 'processing'
            """)
            stuck_emails = cur.fetchall()

            if stuck_emails:
                LOG.warning(f"âš ï¸ Found {len(stuck_emails)} email queue jobs in 'processing' state")

                # Mark as failed (server restarted during processing)
                cur.execute("""
                    UPDATE email_queue
                    SET status = 'failed',
                        error_message = 'Server restarted during processing',
                        updated_at = NOW()
                    WHERE status = 'processing'
                    RETURNING ticker
                """)

                failed = cur.fetchall()
                if failed:
                    LOG.warning(f"ðŸ”„ Marked {len(failed)} email queue jobs as failed:")
                    for row in failed:
                        LOG.info(f"   â†’ {row['ticker']}")

                conn.commit()
            else:
                LOG.info("âœ… No stuck email queue jobs found - clean startup")

    except Exception as e:
        LOG.error(f"âŒ Failed to recover email queue: {e}")
        LOG.error(f"   Stacktrace: {traceback.format_exc()}")

    start_job_worker()

    # Start job queue reclaim thread (monitors for dead workers via stale heartbeat)
    job_reclaim_thread = threading.Thread(target=job_queue_reclaim_loop, daemon=True, name="JobQueueReclaimThread")
    job_reclaim_thread.start()

    # Start email queue watchdog in separate thread
    email_watchdog_thread = threading.Thread(target=email_queue_watchdog_loop, daemon=True, name="EmailQueueWatchdog")
    email_watchdog_thread.start()

    # Start timeout watchdog in separate thread
    timeout_thread = threading.Thread(target=timeout_watchdog_loop, daemon=True, name="TimeoutWatchdog")
    timeout_thread.start()

    # Start stuck transaction monitor (NEW - Oct 2025: monitors for zombie transactions)
    stuck_txn_thread = threading.Thread(target=stuck_transaction_monitor_loop, daemon=True, name="StuckTransactionMonitor")
    stuck_txn_thread.start()

    # Start worker heartbeat monitor (restarts worker if frozen)
    global _worker_heartbeat_monitor_thread
    _worker_heartbeat_monitor_thread = threading.Thread(target=worker_heartbeat_monitor_loop, daemon=True, name="WorkerHeartbeatMonitor")
    _worker_heartbeat_monitor_thread.start()

    LOG.info("âœ… Job queue system initialized (5 watchdog threads running + worker heartbeat monitor)")

@APP.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    worker_id = get_worker_id()
    LOG.info("=" * 80)
    LOG.info(f"ðŸ›‘ FastAPI SHUTDOWN EVENT - Worker: {worker_id}")
    LOG.info(f"   Reason: Unknown (check Render logs)")
    LOG.info(f"   Memory: {memory_monitor.get_current_mb() if hasattr(memory_monitor, 'get_current_mb') else 'N/A'} MB")

    # Log any jobs currently processing (will be orphaned after shutdown)
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT job_id, ticker, phase, progress,
                       EXTRACT(EPOCH FROM (NOW() - started_at)) / 60 AS minutes_running
                FROM ticker_processing_jobs
                WHERE status = 'processing'
                AND worker_id = %s
            """, (worker_id,))
            active_jobs = cur.fetchall()

            if active_jobs:
                LOG.warning(f"âš ï¸ SHUTDOWN: {len(active_jobs)} jobs were still processing:")
                for job in active_jobs:
                    LOG.warning(f"   â†’ {job['ticker']} ({job['phase']}, {job['progress']}%, {job['minutes_running']:.1f}min)")
                LOG.warning("   These jobs will be reclaimed on next startup (>5min threshold)")
            else:
                LOG.info("âœ… No active jobs at shutdown")
    except Exception as e:
        LOG.error(f"Failed to check active jobs during shutdown: {e}")

    LOG.info("=" * 80)
    stop_job_worker()

    # Note: Thread-local HTTP connectors are cleaned up automatically when threads exit
    # (via cleanup_http_session() at end of each job + thread-local storage garbage collection)

    # Close connection pool
    LOG.info("ðŸ”„ Closing database connection pool...")
    close_connection_pool()

# ------------------------------------------------------------------------------
# Pydantic Request Models
# ------------------------------------------------------------------------------
class CleanFeedsRequest(BaseModel):
    tickers: Optional[List[str]] = None

class ResetDigestRequest(BaseModel):
    tickers: Optional[List[str]] = None

class ForceDigestRequest(BaseModel):
    tickers: Optional[List[str]] = None

class RegenerateMetadataRequest(BaseModel):
    ticker: str

class InitRequest(BaseModel):
    tickers: List[str]
    force_refresh: bool = False

class CLIRequest(BaseModel):
    action: str
    tickers: List[str]
    minutes: int = 1440
    triage_batch_size: int = 2

# ------------------------------------------------------------------------------
# API Routes
# ------------------------------------------------------------------------------
@APP.get("/", response_class=HTMLResponse)
async def landing_page(request: Request):
    """Serve the Weavara beta signup landing page"""
    return templates.TemplateResponse("signup.html", {"request": request})


@APP.get("/terms-of-service", response_class=HTMLResponse)
async def terms_page(request: Request):
    """Serve Terms of Service page"""
    return templates.TemplateResponse("terms_of_service.html", {"request": request})


@APP.get("/privacy-policy", response_class=HTMLResponse)
async def privacy_page(request: Request):
    """Serve Privacy Policy page"""
    return templates.TemplateResponse("privacy_policy.html", {"request": request})


@APP.get("/unsubscribe", response_class=HTMLResponse)
async def unsubscribe_page(request: Request, token: str = Query(...)):
    """
    Handle unsubscribe requests via token link.
    Idempotent: Can be called multiple times safely.

    Uses: users table (Dec 2025 schema)
    """
    LOG.info(f"Unsubscribe request with token: {token[:10]}...")

    try:
        with db() as conn, conn.cursor() as cur:
            # Validate token and get user info
            cur.execute("""
                SELECT ut.user_id, ut.user_email, ut.used_at, u.name, u.status
                FROM unsubscribe_tokens ut
                JOIN users u ON ut.user_id = u.id
                WHERE ut.token = %s
            """, (token,))
            result = cur.fetchone()

            if not result:
                LOG.warning(f"Invalid unsubscribe token: {token[:10]}...")
                return templates.TemplateResponse(
                    "unsubscribe.html",
                    {"request": request, "status": "invalid"},
                    status_code=404
                )

            user_id = result['user_id']
            email = result['user_email']
            name = result['name']
            already_cancelled = result['status'] == 'cancelled'
            already_used = result['used_at'] is not None

            # Capture request metadata for security tracking
            ip_address = request.client.host if request.client else None
            user_agent = request.headers.get('user-agent', '')

            if not already_cancelled:
                # Mark user as unsubscribed with timestamp
                cur.execute("""
                    UPDATE users
                    SET status = 'cancelled', cancelled_at = NOW(), updated_at = NOW()
                    WHERE id = %s
                """, (user_id,))
                LOG.info(f"Unsubscribed user {user_id} ({email})")

            if not already_used:
                # Mark token as used
                cur.execute("""
                    UPDATE unsubscribe_tokens
                    SET used_at = NOW(), ip_address = %s, user_agent = %s
                    WHERE token = %s
                """, (ip_address, user_agent, token))
                LOG.info(f"Marked token as used for {email}")

            conn.commit()

            # Return success page
            return templates.TemplateResponse(
                "unsubscribe.html",
                {"request": request, "status": "success", "name": name, "email": email}
            )

    except Exception as e:
        LOG.error(f"Error processing unsubscribe: {e}")
        LOG.error(traceback.format_exc())

        return templates.TemplateResponse(
            "unsubscribe.html",
            {"request": request, "status": "error"},
            status_code=500
        )


# ------------------------------------------------------------------------------
# Beta Signup API Endpoints
# ------------------------------------------------------------------------------

def _fuzzy_search_ticker(search_term: str) -> dict | None:
    """
    Fuzzy search for ticker or company name using pg_trgm.
    Returns best US match or None if no good match found.

    Uses similarity threshold of 0.3 (pg_trgm default).
    Searches both ticker and company_name columns.

    Args:
        search_term: User input (could be ticker like "GOOG" or company name like "Apple")

    Returns:
        {"ticker": "GOOGL", "company_name": "Alphabet Inc.", "score": 0.8} or None
    """
    try:
        with db() as conn:
            with conn.cursor() as cur:
                # Search both ticker and company_name, return best match
                # The % operator uses similarity threshold (default 0.3)
                cur.execute("""
                    SELECT ticker, company_name,
                           GREATEST(
                               similarity(ticker, %s),
                               similarity(company_name, %s)
                           ) as score
                    FROM ticker_reference
                    WHERE country = 'US'
                      AND (ticker %% %s OR company_name %% %s)
                    ORDER BY score DESC
                    LIMIT 1
                """, (search_term, search_term, search_term, search_term))

                result = cur.fetchone()
                if result:
                    return {
                        "ticker": result["ticker"],
                        "company_name": result["company_name"],
                        "score": float(result["score"])
                    }
                return None
    except Exception as e:
        LOG.warning(f"Fuzzy search failed for '{search_term}': {e}")
        return None


@APP.get("/api/validate-ticker")
async def validate_ticker_endpoint(ticker: str = Query(..., min_length=1, max_length=10)):
    """
    Validate ticker for US-listed companies.
    Public endpoint (no auth required) for live form validation.

    Returns:
    - valid=True: Ticker found in database
    - valid=False: Ticker not found or invalid format
    """
    try:
        # Normalize ticker format (handles case, removes quotes, etc)
        normalized = normalize_ticker_format(ticker)

        # TIER 1: Format validation (reject obvious garbage immediately)
        # Use US-only validation for landing page signup
        if not validate_ticker_format_us_only(normalized):
            return {
                "valid": False,
                "message": "Invalid ticker format. US-listed companies only."
            }

        # TIER 2: Database whitelist (only allow approved tickers)
        config = get_ticker_config(normalized)

        # Check if ticker exists in database (has_full_config=True means it's real, False means fallback)
        if not config or not config.get('has_full_config', True):
            # TIER 2.5: Fuzzy search for suggestions (Nov 2025)
            # Search both ticker and company_name using pg_trgm
            suggestion = _fuzzy_search_ticker(ticker)
            if suggestion:
                return {
                    "valid": False,
                    "message": "Ticker not recognized",
                    "suggestion": {
                        "ticker": suggestion["ticker"],
                        "message": f"Did you mean {suggestion['ticker']} ({suggestion['company_name']})?"
                    }
                }
            return {
                "valid": False,
                "message": "Ticker not recognized"
            }

        # TIER 3: Country check - must be US-listed
        country = config.get("country", "")
        if country != "US":
            # Check if there's a US alternative to suggest
            suggestion = _fuzzy_search_ticker(ticker)
            if suggestion:
                return {
                    "valid": False,
                    "message": "US-listed companies only. Canadian and international tickers not supported.",
                    "suggestion": {
                        "ticker": suggestion["ticker"],
                        "message": f"Did you mean {suggestion['ticker']} ({suggestion['company_name']})?"
                    }
                }
            return {
                "valid": False,
                "message": "US-listed companies only. Canadian and international tickers not supported."
            }

        # All checks passed - valid US ticker from our whitelist âœ“
        return {
            "valid": True,
            "ticker": normalized,
            "company_name": config.get("company_name", "Unknown"),
            "exchange": config.get("exchange", "Unknown"),
            "country": country
        }

    except Exception as e:
        LOG.error(f"Ticker validation error for '{ticker}': {e}")
        return {
            "valid": False,
            "message": "Validation error. Please try again."
        }


@APP.get("/api/fmp-validate-ticker")
async def validate_ticker_for_research(ticker: str, type: str = 'transcript'):
    """
    Validate ticker for research summaries and return available transcripts, press releases, or company profiles.

    Query params:
        ticker: Stock ticker (AAPL, RY.TO, etc.)
        type: 'transcript', 'press_release', or 'profile'

    Returns:
        For transcripts: {"valid": true, "company_name": "...", "available_quarters": [...]}
        For press_releases: {"valid": true, "company_name": "...", "available_releases": [...]}
        For profiles: {"valid": true, "company_name": "...", "industry": "..."}
    """
    try:
        # Validate ticker exists in database
        config = get_ticker_config(ticker)
        if not config or not config.get('has_full_config', True):
            return {"valid": False, "error": "Ticker not found in database"}

        company_name = config.get("company_name", ticker)

        if type == 'profile':
            # Fetch 10-K filings from FMP
            try:
                fmp_url = f"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}?type=10-K&page=0&apikey={FMP_API_KEY}"
                response = requests.get(fmp_url, timeout=10)
                filings = response.json()

                # Extract years and dates (last 10 years)
                available_years = []
                for filing in filings[:10]:
                    try:
                        filing_date_str = filing.get("fillingDate", "")
                        sec_html_url = filing.get("finalLink", "")

                        # FMP's /sec_filings doesn't include "period" field
                        # Extract period end date from finalLink filename instead
                        # Example: https://www.sec.gov/.../jpm-20241231.htm â†’ 20241231
                        import re
                        period_date = None
                        year = None

                        if sec_html_url:
                            # Match 8-digit date pattern before .htm (e.g., jpm-20241231.htm, atge-20250630x10k.htm)
                            # Pattern handles: date.htm, date-10k.htm, datex10k.htm
                            match = re.search(r'(\d{8})[x\-]?(?:10[kq])?\.htm$', sec_html_url)
                            if match:
                                date_str = match.group(1)  # "20241231"
                                year = int(date_str[:4])  # 2024
                                period_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"  # "2024-12-31"

                        # Fallback to filing date if we couldn't extract from URL
                        if not year and filing_date_str:
                            year = int(filing_date_str[:4])
                            LOG.warning(f"[{ticker}] Could not extract period from finalLink, using filing year: {year}")

                        if not year:
                            continue

                        filing_date = filing_date_str[:10] if filing_date_str else None

                        available_years.append({
                            "year": year,
                            "filing_date": filing_date,
                            "period_end_date": period_date,
                            "sec_html_url": sec_html_url
                        })
                    except (ValueError, KeyError) as e:
                        LOG.warning(f"Skipping invalid filing entry for {ticker}: {e}")
                        continue

                return {
                    "valid": True,
                    "company_name": company_name,
                    "industry": config.get("industry", "N/A"),
                    "ticker": ticker,
                    "available_years": available_years,
                    "message": f"Found {len(available_years)} 10-K filings" if available_years else "No 10-K filings found. Upload file instead."
                }

            except Exception as e:
                LOG.error(f"Failed to fetch FMP 10-K list for {ticker}: {e}")
                # Fallback: Allow file upload if FMP fails
                return {
                    "valid": True,
                    "company_name": company_name,
                    "industry": config.get("industry", "N/A"),
                    "ticker": ticker,
                    "available_years": [],
                    "message": "FMP API error. Please upload 10-K file manually."
                }

        elif type == '10q':
            # Fetch 10-Q filings from FMP
            try:
                fmp_url = f"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}?type=10-Q&page=0&apikey={FMP_API_KEY}"
                response = requests.get(fmp_url, timeout=10)
                filings = response.json()

                # Extract quarters and dates (last 12 quarters = 3 years)
                available_quarters = []
                for filing in filings[:12]:
                    try:
                        filing_date_str = filing.get("fillingDate", "")
                        sec_html_url = filing.get("finalLink", "")

                        # FMP's /sec_filings doesn't include "period" field
                        # Extract period end date from finalLink filename instead
                        # Example: https://www.sec.gov/.../jpm-20250630.htm â†’ 20250630
                        import re
                        period_date = None
                        year = None
                        month = None

                        if sec_html_url:
                            # Match 8-digit date pattern before .htm (e.g., jpm-20250630.htm, atge-20250630x10q.htm)
                            # Pattern handles: date.htm, date-10q.htm, datex10q.htm
                            match = re.search(r'(\d{8})[x\-]?(?:10[kq])?\.htm$', sec_html_url)
                            if match:
                                date_str = match.group(1)  # "20250630"
                                year = int(date_str[:4])  # 2025
                                month = int(date_str[4:6])  # 06
                                period_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"  # "2025-06-30"

                        # Use period end date to calculate quarter (handles non-standard fiscal years)
                        if year and month:
                            # Map quarter end month to quarter number
                            # Standard: Q1=March(3), Q2=June(6), Q3=Sept(9), Q4=Dec(12)
                            if month == 3:
                                quarter = "Q1"
                            elif month == 6:
                                quarter = "Q2"
                            elif month == 9:
                                quarter = "Q3"
                            elif month == 12:
                                quarter = "Q4"
                            else:
                                # Non-standard fiscal quarter (e.g., month 1, 4, 7, 10)
                                quarter_num = (month - 1) // 3 + 1
                                quarter = f"Q{quarter_num}"

                            fiscal_year = year
                        elif filing_date_str:
                            # Fallback to old filing date logic if period extraction fails
                            LOG.warning(f"[{ticker}] Could not extract period from finalLink, using filing date fallback")
                            filing_date = filing_date_str[:10]
                            year = int(filing_date[:4])
                            month = int(filing_date[5:7])
                            day = int(filing_date[8:10])

                            # Check which quarter end was most recent before this filing date
                            if month >= 10 or (month == 9 and day == 30):
                                quarter = "Q3"
                                fiscal_year = year
                            elif month >= 7 or (month == 6 and day == 30):
                                quarter = "Q2"
                                fiscal_year = year
                            elif month >= 4 or (month == 3 and day == 31):
                                quarter = "Q1"
                                fiscal_year = year
                            else:
                                LOG.debug(f"Skipping {ticker} 10-Q filing on {filing_date} - would map to Q4 which doesn't exist for 10-Q")
                                continue
                            period_date = None
                        else:
                            continue

                        filing_date = filing_date_str[:10] if filing_date_str else None

                        # Skip Q4 10-Qs (shouldn't exist, but filter just in case)
                        if quarter == "Q4":
                            LOG.debug(f"Skipping {ticker} Q4 10-Q (period: {period_date}) - Q4 is covered by 10-K")
                            continue

                        available_quarters.append({
                            "quarter": quarter,
                            "fiscal_year": fiscal_year,
                            "filing_date": filing_date,
                            "period_end_date": period_date,
                            "sec_html_url": sec_html_url,
                            "label": f"{quarter} {fiscal_year}"
                        })
                    except (ValueError, KeyError) as e:
                        LOG.warning(f"Skipping invalid 10-Q filing entry for {ticker}: {e}")
                        continue

                return {
                    "valid": True,
                    "company_name": company_name,
                    "industry": config.get("industry", "N/A"),
                    "ticker": ticker,
                    "available_quarters": available_quarters,
                    "message": f"Found {len(available_quarters)} 10-Q filings" if available_quarters else "No 10-Q filings found. Upload file instead."
                }

            except Exception as e:
                LOG.error(f"Failed to fetch FMP 10-Q list for {ticker}: {e}")
                return {
                    "valid": True,
                    "company_name": company_name,
                    "industry": config.get("industry", "N/A"),
                    "ticker": ticker,
                    "available_quarters": [],
                    "message": "FMP API error. Please upload 10-Q file manually."
                }

        elif type == 'transcript':
            # Fetch transcript list from FMP
            transcripts = fetch_fmp_transcript_list(ticker, FMP_API_KEY)
            if not transcripts:
                return {
                    "valid": True,
                    "company_name": company_name,
                    "latest_quarter": None,
                    "available_quarters": [],
                    "warning": "No transcripts available from FMP",
                    "ticker": ticker
                }

            # Format quarters using FMP's fiscal year data directly (most recent first, limit to 8)
            # FMP returns dict format after parsing: {quarter: X, year: Y, date: "..."}
            # Example: {quarter: 2, year: 2026, date: "2025-08-27"} = Q2 FY2026
            quarters = []
            for t in transcripts[:8]:
                try:
                    # Validate required fields
                    if not isinstance(t, dict):
                        LOG.warning(f"Transcript item is not a dict for {ticker}: {type(t)}")
                        continue

                    if 'quarter' not in t or 'year' not in t or 'date' not in t:
                        LOG.warning(f"Transcript missing required fields for {ticker}: {list(t.keys())}")
                        continue

                    # Return objects with quarter, year, and call_date
                    quarters.append({
                        "quarter": t['quarter'],
                        "year": t['year'],
                        "call_date": t['date'][:10],  # Strip time, keep date only (YYYY-MM-DD)
                        "label": f"Q{t['quarter']} FY{t['year']}"
                    })
                except (ValueError, KeyError, IndexError, TypeError) as e:
                    LOG.warning(f"Failed to parse transcript data for {ticker}: {e} - item: {t}")
                    continue

            if not quarters:
                LOG.warning(f"No valid quarters parsed for {ticker} from {len(transcripts)} transcripts")
                return {
                    "valid": True,
                    "company_name": company_name,
                    "latest_quarter": None,
                    "available_quarters": [],
                    "warning": f"Found {len(transcripts)} transcripts but failed to parse them. Check logs.",
                    "ticker": ticker
                }

            latest = quarters[0]['label'] if quarters else None

            return {
                "valid": True,
                "company_name": company_name,
                "latest_quarter": latest,
                "available_quarters": quarters,
                "ticker": ticker
            }

        else:  # press_release
            # Fetch press releases from FMP
            releases = fetch_fmp_press_releases(ticker, FMP_API_KEY, limit=20)
            if not releases:
                return {
                    "valid": True,
                    "company_name": company_name,
                    "available_releases": [],
                    "warning": "No press releases available from FMP",
                    "ticker": ticker
                }

            return {
                "valid": True,
                "company_name": company_name,
                "available_releases": [
                    {"date": r["date"], "title": r["title"]}  # No truncation - store full title for exact match
                    for r in releases
                ],
                "ticker": ticker
            }

    except Exception as e:
        LOG.error(f"FMP validation failed for {ticker}: {e}", exc_info=True)
        return {"valid": False, "error": f"Failed to validate ticker: {str(e)}"}


@APP.get("/api/sec-validate-ticker")
async def validate_ticker_for_8k(ticker: str):
    """
    Validate ticker and fetch last 10 8-K filings from SEC Edgar.

    Query params:
        ticker: Stock ticker (AAPL, TSLA, etc.)

    Returns:
        {
            "valid": true,
            "company_name": "...",
            "cik": "0000320193",
            "available_8ks": [
                {
                    "filing_date": "Jan 30, 2025",
                    "accession_number": "0001193125-25-012345",
                    "documents_url": "https://www.sec.gov/cgi-bin/viewer?action=view&cik=...",
                    "sec_html_url": "https://www.sec.gov/...",
                    "title": "Results of Operations | Apple announces Q1 2024 results",
                    "item_codes": "2.02, 9.01",
                    "has_summary": false
                },
                ...
            ]
        }
    """
    try:
        # Validate ticker exists in database
        config = get_ticker_config(ticker)
        if not config or not config.get('has_full_config', True):
            return {"valid": False, "error": "Ticker not found in database"}

        company_name = config.get("company_name", ticker)

        LOG.info(f"[{ticker}] Fetching 8-K filings from SEC Edgar...")

        # Get CIK (cached or lookup from SEC)
        try:
            cik = get_cik_for_ticker(ticker)
        except ValueError as e:
            return {
                "valid": False,
                "error": str(e)
            }

        # Fetch last 3 8-K filings from SEC Edgar (conservative during early-break testing)
        # TODO: Increase back to 10 once time-based early break confirmed working in production
        filings = parse_sec_8k_filing_list(cik, count=3)

        if not filings:
            return {
                "valid": True,
                "company_name": company_name,
                "cik": cik,
                "available_8ks": [],
                "warning": "No 8-K filings found for this ticker"
            }

        # Process each filing: Get HTML URL, quick parse title + item codes
        enriched_filings = []

        for i, filing in enumerate(filings):
            try:
                # Get main 8-K HTML URL and Exhibit 99.1 from documents index page
                urls = get_8k_html_url(filing['documents_url'])
                sec_html_url = urls['main_8k_url']

                # Extract item codes with rate limiting
                item_codes = extract_8k_item_codes(sec_html_url, rate_limit_delay=0.15)

                # Check if summary already exists in database
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        SELECT id FROM sec_8k_filings
                        WHERE ticker = %s AND accession_number = %s
                    """, (ticker, filing['accession_number']))
                    has_summary = cur.fetchone() is not None

                exhibit_url = urls.get('exhibit_99_1_url')
                enriched_filings.append({
                    'filing_date': filing['filing_date'],
                    'accession_number': filing['accession_number'],
                    'documents_url': filing['documents_url'],  # For exhibit parsing
                    'sec_html_url': sec_html_url,
                    'exhibit_99_1_url': exhibit_url,
                    'item_codes': item_codes,
                    'has_summary': has_summary
                })

                LOG.info(f"[{ticker}] [{i+1}/{len(filings)}] Item codes: {item_codes}")
                LOG.info(f"[8K_VALIDATION_DEBUG] Returning exhibit_99_1_url in response: {exhibit_url}")

            except Exception as e:
                LOG.error(f"[{ticker}] Failed to parse filing {filing['accession_number']}: {e}")
                # Add entry with error
                enriched_filings.append({
                    'filing_date': filing['filing_date'],
                    'accession_number': filing['accession_number'],
                    'sec_html_url': None,
                    'title': "Error parsing filing",
                    'item_codes': "Unknown",
                    'has_summary': False,
                    'error': str(e)
                })

        LOG.info(f"[{ticker}] âœ… Successfully parsed {len(enriched_filings)} 8-K filings")

        return {
            "valid": True,
            "company_name": company_name,
            "cik": cik,
            "available_8ks": enriched_filings
        }

    except Exception as e:
        LOG.error(f"SEC 8-K validation failed for {ticker}: {e}", exc_info=True)
        return {"valid": False, "error": f"Failed to fetch 8-Ks: {str(e)}"}


class BetaSignupRequest(BaseModel):
    """Pydantic model for beta signup form"""
    name: str
    email: str
    ticker1: str
    ticker2: str = ""  # Optional
    ticker3: str = ""  # Optional


def generate_unsubscribe_token_for_user(user_id: int, email: str) -> str:
    """
    Generate cryptographically secure unsubscribe token for a user.
    Returns: 43-character URL-safe token

    Uses: users table (Dec 2025 schema)
    """
    token = secrets.token_urlsafe(32)  # 32 bytes = 43 chars base64

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                INSERT INTO unsubscribe_tokens (user_id, user_email, token)
                VALUES (%s, %s, %s)
                ON CONFLICT (token) DO NOTHING
                RETURNING token
            """, (user_id, email, token))
            result = cur.fetchone()

            if not result:
                # Token collision (astronomically rare), retry
                LOG.warning(f"Token collision for user {user_id}, regenerating")
                return generate_unsubscribe_token_for_user(user_id, email)

            conn.commit()
            LOG.info(f"Generated unsubscribe token for user {user_id} ({email})")
            return token
    except Exception as e:
        LOG.error(f"Error generating unsubscribe token for user {user_id}: {e}")
        raise


def get_user_id_from_email(email: str) -> Optional[int]:
    """
    Get user_id from email.
    Returns None if no user found.

    Uses: users table (Dec 2025 schema)
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT id FROM users WHERE email = %s", (email,))
            result = cur.fetchone()
            return result['id'] if result else None
    except Exception as e:
        LOG.error(f"Error getting user_id for {email}: {e}")
        return None


def get_or_create_unsubscribe_token(user_id_or_email, ticker1: str = None, ticker2: str = None, ticker3: str = None) -> str:
    """
    Get existing unsubscribe token or create new one for a user.
    Reuses token if user hasn't unsubscribed yet.
    Returns empty string if user not found.

    Parameters:
        user_id_or_email: Can be either int (user_id) or str (email for backward compatibility)
        ticker1, ticker2, ticker3: DEPRECATED - ignored (kept for backward compat)

    Uses: users table (Dec 2025 schema)
    """
    try:
        # Determine if we received user_id or email
        if isinstance(user_id_or_email, int):
            user_id = user_id_or_email
        else:
            # Legacy: email provided, lookup user_id
            email = user_id_or_email
            user_id = get_user_id_from_email(email)
            if not user_id:
                LOG.warning(f"No user found for email {email}")
                return ""

        with db() as conn, conn.cursor() as cur:
            # Get user info
            cur.execute("SELECT id, email FROM users WHERE id = %s", (user_id,))
            user = cur.fetchone()

            if not user:
                LOG.warning(f"User ID {user_id} not found, skipping unsubscribe token generation")
                return ""

            email = user['email']

            # Check if unused token exists for this user
            cur.execute("""
                SELECT token FROM unsubscribe_tokens
                WHERE user_id = %s AND used_at IS NULL
                ORDER BY created_at DESC LIMIT 1
            """, (user_id,))
            result = cur.fetchone()

            if result:
                return result['token']

            # No unused token found, generate new one
            return generate_unsubscribe_token_for_user(user_id, email)
    except Exception as e:
        LOG.error(f"Error getting unsubscribe token: {e}")
        # Fallback: return empty string (email will have generic unsubscribe link)
        return ""


@APP.post("/api/beta-signup")
async def beta_signup_endpoint(signup: BetaSignupRequest):
    """
    Handle beta sign-up form submissions with strict ticker validation.
    Public endpoint (no auth required).

    Uses: users + user_tickers tables (Dec 2025 schema)
    """
    try:
        # Validate and clean input
        name = signup.name.strip()
        email = signup.email.strip().lower()

        # Validate name
        if not name or len(name) > 255:
            return JSONResponse(
                status_code=400,
                content={"status": "error", "message": "Name must be 1-255 characters"}
            )

        # Validate email format
        import re
        email_regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(email_regex, email):
            return JSONResponse(
                status_code=400,
                content={"status": "error", "message": "Invalid email format"}
            )

        # Normalize and validate tickers (ticker1 required, ticker2/3 optional)
        tickers = []
        ticker_data = []
        ticker_inputs = [signup.ticker1, signup.ticker2, signup.ticker3]

        for i, ticker_input in enumerate(ticker_inputs):
            ticker_input = ticker_input.strip() if ticker_input else ""

            # Skip empty optional tickers
            if not ticker_input:
                if i == 0:  # ticker1 is required
                    return JSONResponse(
                        status_code=400,
                        content={"status": "error", "message": "At least one ticker is required"}
                    )
                continue

            normalized = normalize_ticker_format(ticker_input)
            config = get_ticker_config(normalized)

            if not config:
                return JSONResponse(
                    status_code=400,
                    content={"status": "error", "message": f"Ticker '{ticker_input}' not recognized"}
                )

            tickers.append(normalized)
            ticker_data.append({
                "ticker": normalized,
                "company": config.get("company_name", "Unknown")
            })

        # Check if email already exists
        existing_user = None
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT id, status, cancelled_at FROM users WHERE email = %s", (email,))
            existing_user = cur.fetchone()

        if existing_user:
            if existing_user['status'] == 'blocked':
                # Admin blocked this email - cannot re-subscribe
                return JSONResponse(
                    status_code=400,
                    content={"status": "error", "message": "This email cannot be used for signup."}
                )
            elif existing_user['status'] == 'cancelled':
                # User previously unsubscribed - allow re-subscribe
                # Update existing record instead of creating new one
                user_id = existing_user['id']
                previous_cancelled_at = existing_user['cancelled_at'].strftime('%B %d, %Y') if existing_user['cancelled_at'] else None

                with db() as conn, conn.cursor() as cur:
                    # Update user: reset to pending, clear cancelled_at, update terms
                    cur.execute("""
                        UPDATE users
                        SET name = %s, status = 'pending', cancelled_at = NULL,
                            terms_version = %s, terms_accepted_at = NOW(),
                            privacy_version = %s, privacy_accepted_at = NOW(),
                            updated_at = NOW()
                        WHERE id = %s
                    """, (name, TERMS_VERSION, PRIVACY_VERSION, user_id))

                    # Clear old tickers
                    cur.execute("DELETE FROM user_tickers WHERE user_id = %s", (user_id,))

                    # Insert new tickers
                    for ticker in tickers:
                        cur.execute("""
                            INSERT INTO user_tickers (user_id, ticker)
                            VALUES (%s, %s)
                        """, (user_id, ticker))

                    conn.commit()

                # Send admin notification (returning user)
                send_beta_signup_notification(name, email, tickers, returning_user=True, previous_cancelled_at=previous_cancelled_at)

                LOG.info(f"ðŸ”„ Returning user {user_id}: {email} re-subscribed with {', '.join(tickers)} (pending approval)")

                return {
                    "status": "success",
                    "message": "Welcome back! Your account is pending approval. You'll receive an email when activated.",
                    "tickers": ticker_data
                }
            else:
                # Active, pending, or paused - already has an account
                return JSONResponse(
                    status_code=400,
                    content={"status": "error", "message": "An account with this email already exists."}
                )

        # New user - create account
        with db() as conn, conn.cursor() as cur:
            # Insert user with 'pending' status (requires admin approval)
            cur.execute("""
                INSERT INTO users (
                    name, email, user_type, ticker_limit, status,
                    terms_version, terms_accepted_at, privacy_version, privacy_accepted_at
                )
                VALUES (%s, %s, 'beta', 3, 'pending', %s, NOW(), %s, NOW())
                RETURNING id
            """, (name, email, TERMS_VERSION, PRIVACY_VERSION))
            user_id = cur.fetchone()['id']

            # Insert tickers
            for ticker in tickers:
                cur.execute("""
                    INSERT INTO user_tickers (user_id, ticker)
                    VALUES (%s, %s)
                """, (user_id, ticker))

            conn.commit()

        # Generate unsubscribe token
        try:
            unsubscribe_token = generate_unsubscribe_token_for_user(user_id, email)
            LOG.info(f"Created unsubscribe token for user {user_id} ({email})")
        except Exception as e:
            LOG.error(f"Failed to create unsubscribe token for {email}: {e}")
            # Don't block signup if token generation fails

        # Send admin notification email (new user)
        send_beta_signup_notification(name, email, tickers)

        LOG.info(f"âœ… New beta signup {user_id}: {email} tracking {', '.join(tickers)} (pending approval)")

        return {
            "status": "success",
            "message": "Thank you for signing up! Your account is pending approval. You'll receive an email when activated.",
            "tickers": ticker_data
        }

    except Exception as e:
        LOG.error(f"Beta signup error: {e}")
        LOG.error(traceback.format_exc())
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": "Server error. Please try again."}
        )


@APP.get("/health")
def health_check():
    """
    Health check endpoint for Render and monitoring services.

    Returns worker status and last activity to prevent idle timeout.
    Render pings this endpoint to verify the service is alive.
    """
    global _job_worker_running

    # Check worker thread is alive
    worker_alive = _job_worker_running and (_job_worker_thread is not None and _job_worker_thread.is_alive())

    # Check database connectivity
    db_healthy = False
    db_error = None
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT 1")
            db_healthy = True
    except Exception as e:
        db_error = str(e)

    # Check for active jobs
    active_jobs = 0
    recent_completions = 0
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) as count FROM ticker_processing_jobs WHERE status = 'processing'")
            active_jobs = cur.fetchone()['count']

            cur.execute("""
                SELECT COUNT(*) as count FROM ticker_processing_jobs
                WHERE status = 'completed' AND completed_at > NOW() - INTERVAL '10 minutes'
            """)
            recent_completions = cur.fetchone()['count']
    except:
        pass

    # Overall health status
    is_healthy = worker_alive and db_healthy
    status_code = 200 if is_healthy else 503

    # Get memory usage
    memory_mb = memory_monitor.get_current_mb() if hasattr(memory_monitor, 'get_current_mb') else None

    response = {
        "status": "healthy" if is_healthy else "degraded",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "worker": {
            "running": _job_worker_running,
            "thread_alive": worker_alive,
            "worker_id": get_worker_id()
        },
        "database": {
            "connected": db_healthy,
            "error": db_error
        },
        "jobs": {
            "active": active_jobs,
            "recent_completions_10min": recent_completions
        },
        "circuit_breaker": {
            "state": job_circuit_breaker.state,
            "failure_count": job_circuit_breaker.failure_count
        },
        "system": {
            "memory_mb": memory_mb,
            "platform": sys.platform,
            "python_version": sys.version.split()[0],
            "render_instance": os.getenv('RENDER_INSTANCE_ID', 'not_render')
        },
        "heartbeat_monitor": {
            "restart_count": _worker_restart_count,
            "last_activity": _last_worker_activity.isoformat() if _last_worker_activity else None
        }
    }

    return JSONResponse(content=response, status_code=status_code)

@APP.get("/debug/auth")
def debug_auth(request: Request):
    """Debug endpoint to check authentication headers"""
    return {
        "x-admin-token": request.headers.get("x-admin-token"),
        "authorization": request.headers.get("authorization"),
        "expected_token_prefix": ADMIN_TOKEN[:10] + "..." if ADMIN_TOKEN else "None",
        "token_length": len(ADMIN_TOKEN) if ADMIN_TOKEN else 0
    }

# ------------------------------------------------------------------------------
# JOB QUEUE API ENDPOINTS
# ------------------------------------------------------------------------------

@APP.post("/jobs/submit")
async def submit_job_batch(request: Request, body: JobSubmitRequest):
    """Submit a batch of tickers for server-side processing"""
    require_admin(request)

    if not body.tickers:
        raise HTTPException(status_code=400, detail="No tickers specified")

    # AUTO-INITIALIZE: Ensure CSV sync + feeds exist for all tickers
    # This matches the PowerShell test workflow (calls /admin/init before /jobs/submit)
    LOG.info(f"ðŸ”§ Auto-initializing {len(body.tickers)} tickers before queueing...")

    try:
        init_result = await admin_init(request, InitRequest(
            tickers=body.tickers,
            force_refresh=False  # Use cache if already initialized this session
        ))

        if init_result.get('status') == 'success':
            total_feeds = init_result.get('total_feeds_created', 0)
            LOG.info(f"âœ… Initialization complete: {total_feeds} feeds created/verified")
        else:
            LOG.warning(f"âš ï¸ Initialization had issues: {init_result}")
    except Exception as e:
        LOG.error(f"âŒ Initialization failed: {e}")
        # Don't fail the entire batch - safety check in process_ticker_job will catch this
        LOG.warning(f"âš ï¸ Continuing despite init failure - individual jobs will retry")

    # Check queue capacity
    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT COUNT(*) as queued_count
            FROM ticker_processing_jobs
            WHERE status IN ('queued', 'processing')
        """)

        queued_count = cur.fetchone()['queued_count']

        if queued_count > 100:
            raise HTTPException(
                status_code=429,
                detail=f"Job queue is full ({queued_count} jobs pending). Try again later."
            )

    # Create batch
    batch_id = None
    job_ids = []

    # NEW: Determine report type (explicit param or day-of-week detection)
    if body.report_type:
        report_type = body.report_type
        LOG.info(f"Using explicit report_type: {report_type}")
    else:
        report_type, _ = get_report_type_and_lookback()
        LOG.info(f"Using day-of-week detection: {report_type}")

    with db() as conn, conn.cursor() as cur:
        # Create batch record
        cur.execute("""
            INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
            VALUES (%s, %s, %s)
            RETURNING batch_id
        """, (len(body.tickers), 'powershell', json.dumps({
            "minutes": body.minutes,
            "report_type": report_type,  # NEW: Pass report_type
            "batch_size": body.batch_size,
            "triage_batch_size": body.triage_batch_size,
            "mode": body.mode
        })))

        batch_id = cur.fetchone()['batch_id']

        # Create individual jobs with queue timeout (4 hours to wait in queue)
        # Processing timeout (45 min) is set when job is claimed in get_next_queued_job()
        queue_timeout_hours = 4
        timeout_at = datetime.now(timezone.utc) + timedelta(hours=queue_timeout_hours)

        for ticker in body.tickers:
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, ticker, config, timeout_at
                )
                VALUES (%s, %s, %s, %s)
                RETURNING job_id
            """, (batch_id, ticker, json.dumps({
                "minutes": body.minutes,
                "report_type": report_type,  # NEW: Pass report_type
                "batch_size": body.batch_size,
                "triage_batch_size": body.triage_batch_size,
                "mode": body.mode
            }), timeout_at))

            job_ids.append(str(cur.fetchone()['job_id']))

    LOG.info(f"ðŸ“¦ Batch {batch_id} created: {len(body.tickers)} tickers submitted")

    return {
        "status": "submitted",
        "batch_id": str(batch_id),
        "job_ids": job_ids,
        "tickers": body.tickers,
        "total_jobs": len(body.tickers),
        "message": f"Processing started server-side for {len(body.tickers)} tickers"
    }

@APP.get("/jobs/batch/{batch_id}")
async def get_batch_status(request: Request, batch_id: str):
    """Get status of all jobs in a batch"""
    require_admin(request)

    with db() as conn, conn.cursor() as cur:
        # Get batch info
        cur.execute("""
            SELECT * FROM ticker_processing_batches WHERE batch_id = %s
        """, (batch_id,))

        batch = cur.fetchone()
        if not batch:
            raise HTTPException(status_code=404, detail="Batch not found")

        # Get all jobs in batch
        cur.execute("""
            SELECT job_id, ticker, status, phase, progress,
                   error_message, started_at, completed_at,
                   duration_seconds, memory_mb
            FROM ticker_processing_jobs
            WHERE batch_id = %s
            ORDER BY created_at
        """, (batch_id,))

        jobs = [dict(row) for row in cur.fetchall()]

    # Calculate overall progress
    total_progress = sum(j['progress'] for j in jobs)
    avg_progress = total_progress // len(jobs) if jobs else 0

    completed = len([j for j in jobs if j['status'] == 'completed'])
    failed = len([j for j in jobs if j['status'] in ('failed', 'timeout')])
    processing = len([j for j in jobs if j['status'] == 'processing'])
    queued = len([j for j in jobs if j['status'] == 'queued'])

    # Determine batch status
    batch_status = batch['status']
    if completed + failed == len(jobs):
        batch_status = 'completed'
    elif processing > 0 or completed > 0:
        batch_status = 'processing'
    else:
        batch_status = 'queued'

    # Update batch status if changed
    if batch_status != batch['status']:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE ticker_processing_batches
                SET status = %s,
                    completed_jobs = %s,
                    failed_jobs = %s
                WHERE batch_id = %s
            """, (batch_status, completed, failed, batch_id))

    return {
        "batch_id": batch_id,
        "status": batch_status,
        "total_tickers": len(jobs),
        "completed": completed,
        "failed": failed,
        "processing": processing,
        "queued": queued,
        "overall_progress": avg_progress,
        "created_at": batch['created_at'].isoformat() if batch['created_at'] else None,
        "jobs": [{
            "job_id": str(j['job_id']),
            "ticker": j['ticker'],
            "status": j['status'],
            "phase": j['phase'],
            "progress": j['progress'],
            "error_message": j['error_message'],
            "duration_seconds": j['duration_seconds'],
            "memory_mb": j['memory_mb']
        } for j in jobs]
    }

@APP.get("/jobs/active-batches")
async def get_active_batches(request: Request):
    """Get all active batches with their job details"""
    require_admin(request)

    try:
        with db() as conn, conn.cursor() as cur:
            # First get batches with active jobs
            cur.execute("""
                SELECT
                    b.batch_id,
                    b.status as batch_status,
                    b.created_at,
                    b.total_jobs,
                    b.completed_jobs,
                    b.failed_jobs
                FROM ticker_processing_batches b
                WHERE b.created_at > NOW() - INTERVAL '24 hours'
                  AND EXISTS (
                      SELECT 1 FROM ticker_processing_jobs j
                      WHERE j.batch_id = b.batch_id
                        AND j.status IN ('queued', 'processing')
                  )
                ORDER BY b.created_at DESC
            """)
            batches = cur.fetchall()

            result = []
            for batch in batches:
                # Get jobs for this batch
                cur.execute("""
                    SELECT job_id, ticker, status, phase, progress
                    FROM ticker_processing_jobs
                    WHERE batch_id = %s
                    ORDER BY created_at
                """, (batch['batch_id'],))
                jobs = cur.fetchall()

                result.append({
                    "batch_id": str(batch['batch_id']),
                    "batch_status": batch['batch_status'],
                    "created_at": batch['created_at'].isoformat() if batch['created_at'] else None,
                    "total_jobs": batch['total_jobs'],
                    "completed_jobs": batch['completed_jobs'],
                    "failed_jobs": batch['failed_jobs'],
                    "jobs": [{"job_id": str(j['job_id']), "ticker": j['ticker'], "status": j['status'], "phase": j['phase'], "progress": j['progress']} for j in jobs]
                })

            return {
                "active_batches": len(result),
                "batches": result
            }
    except Exception as e:
        LOG.error(f"Error in /jobs/active-batches: {e}")
        LOG.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@APP.get("/jobs/stats")
async def get_job_stats(request: Request):
    """Get overall job queue statistics"""
    require_admin(request)

    with db() as conn, conn.cursor() as cur:
        # Job counts by status
        cur.execute("""
            SELECT status, COUNT(*) as count
            FROM ticker_processing_jobs
            GROUP BY status
        """)
        status_counts = {row['status']: row['count'] for row in cur.fetchall()}

        # Recent completions (last hour)
        cur.execute("""
            SELECT COUNT(*) as count,
                   AVG(duration_seconds) as avg_duration,
                   AVG(memory_mb) as avg_memory
            FROM ticker_processing_jobs
            WHERE status = 'completed'
            AND completed_at > NOW() - INTERVAL '1 hour'
        """)
        recent = cur.fetchone()

        # Active batches
        cur.execute("""
            SELECT COUNT(*) as count
            FROM ticker_processing_batches
            WHERE status IN ('queued', 'processing')
        """)
        active_batches = cur.fetchone()['count']

    return {
        "status_counts": status_counts,
        "recent_completions_1h": recent['count'] or 0,
        "avg_duration_seconds": float(recent['avg_duration']) if recent['avg_duration'] else None,
        "avg_memory_mb": float(recent['avg_memory']) if recent['avg_memory'] else None,
        "active_batches": active_batches,
        "circuit_breaker_state": job_circuit_breaker.state,
        "circuit_breaker_failures": job_circuit_breaker.failure_count,
        "worker_id": get_worker_id()
    }

@APP.get("/jobs/{job_id}")
async def get_job_detail(request: Request, job_id: str):
    """Get detailed status of a single job"""
    require_admin(request)

    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            SELECT j.*, b.batch_id
            FROM ticker_processing_jobs j
            JOIN ticker_processing_batches b ON j.batch_id = b.batch_id
            WHERE j.job_id = %s
        """, (job_id,))

        job = cur.fetchone()
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")

        return {
            "job_id": str(job['job_id']),
            "batch_id": str(job['batch_id']),
            "ticker": job['ticker'],
            "status": job['status'],
            "phase": job['phase'],
            "progress": job['progress'],
            "result": job['result'],
            "error_message": job['error_message'],
            "error_stacktrace": job['error_stacktrace'],
            "retry_count": job['retry_count'],
            "worker_id": job['worker_id'],
            "memory_mb": job['memory_mb'],
            "duration_seconds": job['duration_seconds'],
            "created_at": job['created_at'].isoformat() if job['created_at'] else None,
            "started_at": job['started_at'].isoformat() if job['started_at'] else None,
            "completed_at": job['completed_at'].isoformat() if job['completed_at'] else None,
            "config": job['config']
        }

@APP.post("/jobs/{job_id}/cancel")
async def cancel_job(request: Request, job_id: str):
    """Cancel a specific job"""
    require_admin(request)

    with db() as conn, conn.cursor() as cur:
        cur.execute("""
            UPDATE ticker_processing_jobs
            SET status = 'cancelled',
                error_message = 'Cancelled by user',
                completed_at = NOW(),
                last_updated = NOW()
            WHERE job_id = %s
            AND status IN ('queued', 'processing')
            RETURNING ticker, status, phase
        """, (job_id,))

        result = cur.fetchone()
        if result:
            LOG.warning(f"ðŸš« Job {job_id} cancelled by user (ticker: {result['ticker']}, was in: {result['phase']})")

            # Update batch counters
            cur.execute("""
                UPDATE ticker_processing_batches
                SET failed_jobs = failed_jobs + 1
                WHERE batch_id = (
                    SELECT batch_id FROM ticker_processing_jobs WHERE job_id = %s
                )
            """, (job_id,))

            return {
                "status": "cancelled",
                "job_id": job_id,
                "ticker": result['ticker'],
                "was_in_phase": result['phase']
            }
        else:
            # Check if job exists but already completed/failed
            cur.execute("""
                SELECT ticker, status FROM ticker_processing_jobs WHERE job_id = %s
            """, (job_id,))

            existing = cur.fetchone()
            if existing:
                raise HTTPException(
                    status_code=400,
                    detail=f"Job already {existing['status']}, cannot cancel"
                )
            else:
                raise HTTPException(status_code=404, detail="Job not found")

@APP.post("/jobs/batch/{batch_id}/cancel")
async def cancel_batch(request: Request, batch_id: str):
    """Cancel all jobs in a batch"""
    require_admin(request)

    try:
        with db() as conn, conn.cursor() as cur:
            # Cancel all queued/processing jobs in the batch
            cur.execute("""
                UPDATE ticker_processing_jobs
                SET status = 'cancelled',
                    error_message = 'Batch cancelled by user',
                    completed_at = NOW(),
                    last_updated = NOW()
                WHERE batch_id = %s
                AND status IN ('queued', 'processing')
                RETURNING job_id, ticker
            """, (batch_id,))

            cancelled_jobs = cur.fetchall()

            if cancelled_jobs:
                LOG.warning(f"ðŸš« Batch {batch_id} cancelled by user ({len(cancelled_jobs)} jobs)")
                for job in cancelled_jobs:
                    LOG.info(f"   Cancelled: {job['ticker']} (job_id: {job['job_id']})")

                # Update batch status
                cur.execute("""
                    UPDATE ticker_processing_batches
                    SET status = 'cancelled',
                        failed_jobs = failed_jobs + %s
                    WHERE batch_id = %s
                """, (len(cancelled_jobs), batch_id))

                return {
                    "status": "cancelled",
                    "batch_id": batch_id,
                    "jobs_cancelled": len(cancelled_jobs),
                    "tickers": [j['ticker'] for j in cancelled_jobs]
                }
            else:
                # Check if batch exists
                cur.execute("""
                    SELECT status, total_jobs FROM ticker_processing_batches WHERE batch_id = %s
                """, (batch_id,))

                batch = cur.fetchone()
                if batch:
                    return {
                        "status": "no_jobs_to_cancel",
                        "message": f"Batch is already {batch['status']}, no jobs to cancel",
                        "batch_id": batch_id
                    }
                else:
                    raise HTTPException(status_code=404, detail="Batch not found")
    except Exception as e:
        LOG.error(f"Error cancelling batch {batch_id}: {e}")
        LOG.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@APP.post("/jobs/circuit-breaker/reset")
async def reset_circuit_breaker(request: Request):
    """Manually reset the circuit breaker"""
    require_admin(request)

    job_circuit_breaker.reset()

    return {
        "status": "reset",
        "message": "Circuit breaker has been manually reset"
    }

# ------------------------------------------------------------------------------
# ADMIN ENDPOINTS (Existing)
# ------------------------------------------------------------------------------

@APP.post("/admin/migrate-feeds")
async def admin_migrate_feeds(request: Request):
    """NEW ARCHITECTURE V2: Verify feeds + ticker_feeds architecture is ready"""
    require_admin(request)

    try:
        # Step 1: Ensure schema is up to date (includes feeds + ticker_feeds tables)
        # NOTE: Schema initialization removed from here (was causing lock timeouts during concurrent processing)
        # Schema is now initialized once at application startup in startup_event() - see line ~16805

        # Step 2: Verify new architecture exists
        with db() as conn, conn.cursor() as cur:
            # Verify new architecture exists
            cur.execute("SELECT COUNT(*) as feed_count FROM feeds")
            new_feed_count = cur.fetchone()['feed_count']

            cur.execute("SELECT COUNT(*) as association_count FROM ticker_feeds")
            association_count = cur.fetchone()['association_count']

            LOG.info(f"âœ… NEW ARCHITECTURE V2 verified: {new_feed_count} feeds, {association_count} associations")

            return {
                "status": "success",
                "message": "NEW ARCHITECTURE V2 (category-per-relationship) is active",
                "feeds": new_feed_count,
                "associations": association_count
            }

    except Exception as e:
        LOG.error(f"âŒ Migration failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/admin/fix-foreign-key")
async def admin_fix_foreign_key(request: Request):
    """Fix the found_url foreign key constraint to point to feeds table (DEPRECATED)"""
    require_admin(request)

    try:
        # This endpoint is deprecated - foreign keys are handled in ensure_schema()

        return {
            "status": "success",
            "message": "Foreign key constraint fixed successfully"
        }

    except Exception as e:
        LOG.error(f"âŒ Foreign key fix failed: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/admin/init")
async def admin_init(request: Request, body: InitRequest):
    async with TICKER_PROCESSING_LOCK:
        """Initialize feeds for specified tickers using NEW ARCHITECTURE"""
        require_admin(request)

        # Global state to track if initialization already happened in this session
        global _schema_initialized, _github_synced
        if '_schema_initialized' not in globals():
            _schema_initialized = False
        if '_github_synced' not in globals():
            _github_synced = False

        # NEW ARCHITECTURE V2: Use functions directly from app.py (no import needed)

        LOG.info("=== INITIALIZATION STARTING ===")

        # CRITICAL: Clear any global state that might contaminate between tickers
        LOG.info("=== CLEARING GLOBAL STATE FOR FRESH TICKER PROCESSING ===")
        import gc
        gc.collect()  # Force garbage collection to clear any lingering objects

        # STEP 1: Schema initialization (now handled at app startup)
        # NOTE: ensure_schema() is called in @APP.on_event("startup") with advisory lock
        # This prevents lock contention from concurrent requests (test runs, ad-hoc user requests)
        LOG.info("=== SCHEMA INITIALIZATION SKIPPED (handled at startup) ===")

        # STEP 2: Import CSV from GitHub (ONCE per session)
        if not _github_synced:
            LOG.info("=== INITIALIZATION: Syncing ticker reference from GitHub ===")
            github_sync_result = sync_ticker_references_from_github()
            _github_synced = True
        else:
            LOG.info("=== GITHUB SYNC ALREADY COMPLETED - SKIPPING ===")
            github_sync_result = {"status": "skipped", "message": "Already synced in this session"}

        if github_sync_result["status"] != "success":
            LOG.warning(f"GitHub sync failed: {github_sync_result.get('message', 'Unknown error')}")
        else:
            LOG.info(f"GitHub sync successful: {github_sync_result.get('message', 'Completed')}")

        results = []

        # CRITICAL: Process each ticker in complete isolation using NEW ARCHITECTURE
        for ticker in body.tickers:
            # STEP 1: Create isolated ticker variable to prevent corruption
            isolated_ticker = str(ticker).strip()  # Force new string object

            # CRITICAL: Clear any residual state between ticker processing
            LOG.info(f"=== PROCESSING {isolated_ticker} - NEW ARCHITECTURE ===")
            import gc
            gc.collect()  # Force garbage collection between each ticker

            LOG.info(f"=== INITIALIZING TICKER: {isolated_ticker} ===")

            try:
                # STEP 2: Get metadata from database (no AI enhancement - CSV is source of truth)
                metadata = get_or_create_enhanced_ticker_metadata(isolated_ticker)

                LOG.info(f"[INIT] {isolated_ticker} metadata: company='{metadata.get('company_name', 'MISSING')}'")

                # STEP 3: Create feeds using NEW MANY-TO-MANY ARCHITECTURE
                feeds_created = create_feeds_for_ticker_new_architecture(isolated_ticker, metadata)

                if not feeds_created:
                    LOG.info(f"=== {isolated_ticker}: No new feeds created ===")
                    results.append({
                        "ticker": isolated_ticker,
                        "message": "No new feeds created",
                        "feeds_created": 0
                    })
                    continue

                # STEP 4: Process results from new architecture
                LOG.info(f"âœ… Successfully created {len(feeds_created)} feeds for {isolated_ticker} using NEW ARCHITECTURE")

                results.append({
                    "ticker": isolated_ticker,
                    "message": f"Successfully created feeds using new architecture",
                    "feeds_created": len(feeds_created),
                    "details": [{"feed_id": f["feed_id"], "category": f["config"].get("category", "unknown")} for f in feeds_created]
                })

            except Exception as e:
                LOG.error(f"âŒ Failed to create feeds for {isolated_ticker}: {e}")
                results.append({
                    "ticker": isolated_ticker,
                    "message": f"Failed to create feeds: {str(e)}",
                    "feeds_created": 0
                })

        # Return results
        LOG.info("=== INITIALIZATION COMPLETE ===")
        return {
            "status": "success",
            "message": "Feed initialization completed using NEW ARCHITECTURE",
            "results": results,
            "total_tickers": len(body.tickers),
            "successful": len([r for r in results if r["feeds_created"] > 0])
        }

def process_feeds_sequentially(feeds: List[Dict], mode: str = 'production', yahoo_quota: int = None, google_quota: int = None, ingestion_source: str = None) -> Dict[str, int]:
    """
    Process a list of feeds sequentially (used for Yahooâ†’Google pairs).
    Returns aggregated stats from all feeds.

    Args:
        feeds: List of feed configuration dicts (typically [Yahoo feed, Google feed])
        mode: 'production' or 'hourly' - controls Google title filtering
        yahoo_quota: Max articles for Yahoo feeds. None = unlimited.
        google_quota: Max articles for Google feeds. None = unlimited.
        ingestion_source: Source of ingestion ('daily_workflow', 'hourly_alert', etc.)

    Returns:
        Aggregated stats dict
    """
    aggregated_stats = {
        "processed": 0,
        "inserted": 0,
        "duplicates": 0,
        "blocked_spam": 0,
        "limit_reached": 0,
        "blocked_insider_trading": 0,
        "yahoo_rejected": 0
    }

    for feed in feeds:
        try:
            # Determine quota based on feed type
            is_yahoo = 'yahoo' in feed['url'].lower()
            is_google = 'google' in feed['url'].lower()

            quota = None
            if is_yahoo and yahoo_quota is not None:
                quota = yahoo_quota
            elif is_google and google_quota is not None:
                quota = google_quota

            stats = ingest_feed_basic_only(feed, mode=mode, quota=quota, ingestion_source=ingestion_source)
            aggregated_stats["processed"] += stats["processed"]
            aggregated_stats["inserted"] += stats["inserted"]
            aggregated_stats["duplicates"] += stats["duplicates"]
            aggregated_stats["blocked_spam"] += stats.get("blocked_spam", 0)
            aggregated_stats["limit_reached"] += stats.get("limit_reached", 0)
            aggregated_stats["blocked_insider_trading"] += stats.get("blocked_insider_trading", 0)
            aggregated_stats["yahoo_rejected"] += stats.get("yahoo_rejected", 0)
        except Exception as e:
            LOG.error(f"[{feed.get('ticker', 'UNKNOWN')}] Sequential feed processing failed for {feed['name']}: {e}")
            continue

    return aggregated_stats

@APP.post("/cron/ingest")
async def cron_ingest(
    request: Request,
    minutes: int = Query(default=15, description="Time window in minutes"),
    tickers: List[str] = Query(default=None, description="Specific tickers to ingest"),
    batch_size: int = Query(default=None, description="Batch size for concurrent processing"),
    triage_batch_size: int = Query(default=2, description="Batch size for triage processing"),
    mode: str = 'daily',
    report_type: str = 'daily'
):
    """Enhanced ingest with comprehensive memory monitoring and async batch processing"""
    # Set batch size from parameter or environment variable
    global SCRAPE_BATCH_SIZE
    if batch_size is not None:
        SCRAPE_BATCH_SIZE = max(1, min(batch_size, 10))  # Limit between 1-10
        LOG.info(f"BATCH SIZE OVERRIDE: Using batch_size={SCRAPE_BATCH_SIZE} from API parameter")
    
    start_time = time.time()
    require_admin(request)
    # NOTE: Schema initialization removed from here (was causing lock timeouts during concurrent processing)
    # Schema is now initialized once at application startup in startup_event() - see line ~16805

    # Initialize memory monitoring with error handling
    try:
        memory_monitor.start_monitoring()
        memory_monitor.take_snapshot("CRON_INGEST_START")
        LOG.info("=== CRON INGEST STARTING (WITH MEMORY MONITORING & ASYNC BATCHES) ===")
    except Exception as e:
        LOG.error(f"Memory monitoring failed to start: {e}")
        LOG.info("=== CRON INGEST STARTING (WITHOUT MEMORY MONITORING) ===")
    
    LOG.info(f"Processing window: {minutes} minutes")
    LOG.info(f"Target tickers: {tickers or 'ALL'}")
    LOG.info(f"Batch size: {SCRAPE_BATCH_SIZE}")
    
    try:
        # Reset statistics
        reset_ingestion_stats()
        reset_scraping_stats()
        reset_enhanced_scraping_stats()
        memory_monitor.take_snapshot("STATS_RESET")

        # Calculate dynamic scraping limits for each ticker using enhanced database
        dynamic_limits = {}
        if tickers:
            for ticker in tickers:
                dynamic_limits[ticker] = calculate_dynamic_scraping_limits(ticker)
                LOG.info(f"DYNAMIC SCRAPING LIMITS for {ticker}: {dynamic_limits[ticker]}")
        
        memory_monitor.take_snapshot("LIMITS_CALCULATED")
        
        # PHASE 1: Process feeds for new articles
        LOG.info("=== PHASE 1: PROCESSING FEEDS (NEW + EXISTING ARTICLES) ===")
        memory_monitor.take_snapshot("PHASE1_START")
        
        # Get feeds using NEW ARCHITECTURE (feeds + ticker_feeds)
        with resource_cleanup_context("database_connection"):
            with db() as conn, conn.cursor() as cur:
                if tickers:
                    cur.execute("""
                        SELECT f.id, f.url, f.name, tf.ticker, tf.category, f.retain_days, f.search_keyword, f.feed_ticker, tf.value_chain_type
                        FROM feeds f
                        JOIN ticker_feeds tf ON f.id = tf.feed_id
                        WHERE f.active = TRUE AND tf.active = TRUE AND tf.ticker = ANY(%s)
                        ORDER BY tf.ticker, tf.category, f.id
                    """, (tickers,))
                else:
                    cur.execute("""
                        SELECT f.id, f.url, f.name, tf.ticker, tf.category, f.retain_days, f.search_keyword, f.feed_ticker, tf.value_chain_type
                        FROM feeds f
                        JOIN ticker_feeds tf ON f.id = tf.feed_id
                        WHERE f.active = TRUE AND tf.active = TRUE
                        ORDER BY tf.ticker, tf.category, f.id
                    """)
                feeds = list(cur.fetchall())
        
        if not feeds:
            memory_monitor.take_snapshot("NO_FEEDS_FOUND")
            return {"status": "no_feeds", "message": "No active feeds found"}
        
        memory_monitor.take_snapshot("FEEDS_LOADED")
        
        # DEBUG: Check what feeds exist before processing (NEW ARCHITECTURE)
        with resource_cleanup_context("debug_feed_check"):
            with db() as conn, conn.cursor() as cur:
                if tickers:
                    cur.execute("""
                        SELECT tf.ticker, tf.category, COUNT(*) as count,
                               STRING_AGG(f.name, ' | ') as feed_names
                        FROM feeds f
                        JOIN ticker_feeds tf ON f.id = tf.feed_id
                        WHERE tf.ticker = ANY(%s) AND f.active = TRUE AND tf.active = TRUE
                        GROUP BY tf.ticker, tf.category
                        ORDER BY tf.ticker, tf.category
                    """, (tickers,))
                else:
                    cur.execute("""
                        SELECT tf.ticker, tf.category, COUNT(*) as count,
                               STRING_AGG(f.name, ' | ') as feed_names
                        FROM feeds f
                        JOIN ticker_feeds tf ON f.id = tf.feed_id
                        WHERE f.active = TRUE AND tf.active = TRUE
                        GROUP BY tf.ticker, tf.category
                        ORDER BY tf.ticker, tf.category
                    """)
                
                feed_debug = list(cur.fetchall())
                LOG.info("=== FEED DEBUG BEFORE PROCESSING ===")
                for feed_row in feed_debug:
                    LOG.info(f"  {feed_row['ticker']} | {feed_row['category']} | Count: {feed_row['count']} | Names: {feed_row['feed_names']}")
                LOG.info("=== END FEED DEBUG ===")
        
        ingest_stats = {
            "total_processed": 0,
            "total_inserted": 0,
            "total_duplicates": 0,
            "total_spam_blocked": 0,
            "total_limit_reached": 0,
            "total_insider_trading_blocked": 0,
            "total_yahoo_rejected": 0
        }

        # ASYNC FEED PROCESSING: Group feeds by strategy
        LOG.info("=== ASYNC FEED PROCESSING: Grouping feeds by strategy ===")

        # Group feeds by ticker and category
        company_feeds = []       # Will be processed: Googleâ†’Yahoo sequentially per ticker
        industry_feeds = []      # Will be processed: All in parallel (Google only)
        competitor_feeds = []    # Will be processed: Googleâ†’Yahoo sequentially per competitor
        value_chain_feeds = []   # Will be processed: Googleâ†’Yahoo sequentially per value chain company

        for feed in feeds:
            category = feed.get('category', 'company')
            if category == 'company':
                company_feeds.append(feed)
            elif category == 'industry':
                industry_feeds.append(feed)
            elif category == 'competitor':
                competitor_feeds.append(feed)
            elif category == 'value_chain':
                value_chain_feeds.append(feed)

        LOG.info(f"Feed groups - Company: {len(company_feeds)}, Industry: {len(industry_feeds)}, Competitor: {len(competitor_feeds)}, Value Chain: {len(value_chain_feeds)}")

        # Further group company and competitor feeds by ticker for sequential Googleâ†’Yahoo processing
        company_by_ticker = {}  # {ticker: [google_feed, yahoo_feed]}
        for feed in company_feeds:
            ticker = feed.get('ticker')
            if ticker not in company_by_ticker:
                company_by_ticker[ticker] = []
            company_by_ticker[ticker].append(feed)

        # Sort each ticker's feeds: Yahoo first, then Google (Yahoo is cleaner, gets priority)
        for ticker in company_by_ticker:
            company_by_ticker[ticker].sort(key=lambda f: 0 if 'yahoo' in f['url'].lower() else 1)

        # Group competitor feeds by (ticker, feed_ticker) for sequential processing
        competitor_by_key = {}  # {(ticker, feed_ticker): [google_feed, yahoo_feed]}
        for feed in competitor_feeds:
            ticker = feed.get('ticker')
            comp_ticker = feed.get('feed_ticker', 'unknown')
            key = (ticker, comp_ticker)
            if key not in competitor_by_key:
                competitor_by_key[key] = []
            competitor_by_key[key].append(feed)

        # Sort each competitor's feeds: Yahoo first, then Google (Yahoo is cleaner, gets priority)
        for key in competitor_by_key:
            competitor_by_key[key].sort(key=lambda f: 0 if 'yahoo' in f['url'].lower() else 1)

        # Group value chain feeds by (ticker, feed_ticker, value_chain_type) for sequential processing
        value_chain_by_key = {}  # {(ticker, vc_ticker, vc_type): [google_feed, yahoo_feed]}
        for feed in value_chain_feeds:
            ticker = feed.get('ticker')
            vc_ticker = feed.get('feed_ticker', 'unknown')
            vc_type = feed.get('value_chain_type', 'unknown')
            key = (ticker, vc_ticker, vc_type)
            if key not in value_chain_by_key:
                value_chain_by_key[key] = []
            value_chain_by_key[key].append(feed)

        # Sort each value chain company's feeds: Yahoo first, then Google (Yahoo is cleaner, gets priority)
        for key in value_chain_by_key:
            value_chain_by_key[key].sort(key=lambda f: 0 if 'yahoo' in f['url'].lower() else 1)

        # Process all groups in parallel using ThreadPoolExecutor
        # REDUCED: max_workers=8 (was 15) to reduce DB connection contention
        # With 3 concurrent tickers: 3 Ã— 8 = 24 peak DB connections (30% of 80-conn pool)
        LOG.info("=== Starting parallel feed processing with grouped strategy ===")
        processing_start_time = time.time()

        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = []

            # Submit company feed groups (sequential Yahooâ†’Google within each ticker)
            for ticker, ticker_feeds in company_by_ticker.items():
                future = executor.submit(process_feeds_sequentially, ticker_feeds, mode='production', yahoo_quota=None, google_quota=20, ingestion_source='daily_workflow')
                futures.append(("company", ticker, future))
                LOG.info(f"Submitted company feeds for {ticker}: {len(ticker_feeds)} feeds (Yahooâ†’Google sequential, Google quota=20)")

            # Submit industry feeds (all parallel, Google only, no quotas)
            for feed in industry_feeds:
                future = executor.submit(ingest_feed_basic_only, feed, mode='production', quota=None, ingestion_source='daily_workflow')
                futures.append(("industry", feed.get('search_keyword', 'unknown'), future))
            LOG.info(f"Submitted {len(industry_feeds)} industry feeds (all parallel, production mode)")

            # Submit competitor feed groups (sequential Yahooâ†’Google within each competitor)
            for (ticker, comp_ticker), comp_feeds in competitor_by_key.items():
                future = executor.submit(process_feeds_sequentially, comp_feeds, mode='production', yahoo_quota=None, google_quota=10, ingestion_source='daily_workflow')
                futures.append(("competitor", f"{ticker}/{comp_ticker}", future))
                LOG.info(f"Submitted competitor feeds for {ticker}/{comp_ticker}: {len(comp_feeds)} feeds (Yahooâ†’Google sequential, Google quota=10)")

            # Submit value chain feed groups (sequential Yahooâ†’Google within each value chain company)
            for (ticker, vc_ticker, vc_type), vc_feeds in value_chain_by_key.items():
                future = executor.submit(process_feeds_sequentially, vc_feeds, mode='production', yahoo_quota=None, google_quota=10, ingestion_source='daily_workflow')
                futures.append(("value_chain", f"{ticker}/{vc_ticker}/{vc_type}", future))
                LOG.info(f"Submitted value chain feeds for {ticker}/{vc_ticker} ({vc_type}): {len(vc_feeds)} feeds (Yahooâ†’Google sequential, Google quota=10)")

            # Collect results as they complete
            completed_count = 0
            for future_type, identifier, future in futures:
                try:
                    stats = future.result()
                    ingest_stats["total_processed"] += stats["processed"]
                    ingest_stats["total_inserted"] += stats["inserted"]
                    ingest_stats["total_duplicates"] += stats["duplicates"]
                    ingest_stats["total_spam_blocked"] += stats.get("blocked_spam", 0)
                    ingest_stats["total_limit_reached"] += stats.get("limit_reached", 0)
                    ingest_stats["total_insider_trading_blocked"] += stats.get("blocked_insider_trading", 0)
                    ingest_stats["total_yahoo_rejected"] += stats.get("yahoo_rejected", 0)

                    completed_count += 1
                    LOG.info(f"âœ… Completed {future_type} feed group: {identifier} ({completed_count}/{len(futures)})")

                    # Memory monitoring every 5 completions
                    if completed_count % 5 == 0:
                        memory_monitor.take_snapshot(f"FEED_PROCESSING_{completed_count}")
                        current_memory = memory_monitor.get_memory_info()
                        if current_memory["memory_mb"] > 500:  # 500MB threshold
                            LOG.warning(f"High memory during feed processing: {current_memory['memory_mb']:.1f}MB")
                            memory_monitor.force_garbage_collection()

                except Exception as e:
                    LOG.error(f"âŒ Failed {future_type} feed group: {identifier} - {e}")
                    continue

        processing_duration = time.time() - processing_start_time
        LOG.info(f"=== ASYNC FEED PROCESSING COMPLETE: {processing_duration:.2f} seconds ({len(futures)} groups processed) ===")
        
        memory_monitor.take_snapshot("PHASE1_FEEDS_COMPLETE")
        
        # Now get ALL articles from the timeframe for ticker-specific analysis
        cutoff = datetime.now(timezone.utc) - timedelta(minutes=minutes)
        articles_by_ticker = {}

        with resource_cleanup_context("database_connection"):
            with db() as conn, conn.cursor() as cur:
                if tickers:
                    cur.execute("""
                        WITH ranked_articles AS (
                            SELECT a.id, a.url, a.resolved_url, a.title, a.domain, a.published_at,
                                   tf.category, f.search_keyword, f.feed_ticker, tf.value_chain_type, ta.ticker,
                                   a.scraped_content, ta.ai_summary, a.url_hash, f.company_name,
                                   ROW_NUMBER() OVER (
                                       PARTITION BY ta.ticker,
                                           CASE
                                               WHEN tf.category = 'company' THEN tf.category
                                               WHEN tf.category = 'industry' THEN f.search_keyword
                                               WHEN tf.category = 'competitor' THEN f.feed_ticker
                                               WHEN tf.category = 'value_chain' THEN f.feed_ticker
                                           END
                                       ORDER BY a.published_at DESC NULLS LAST, ta.found_at DESC
                                   ) as rn
                            FROM articles a
                            JOIN ticker_articles ta ON a.id = ta.article_id
                            JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                            LEFT JOIN feeds f ON ta.feed_id = f.id
                            WHERE ta.ticker = ANY(%s)
                            AND (a.published_at >= %s OR a.published_at IS NULL)
                        )
                        SELECT id, url, resolved_url, title, domain, published_at,
                               category, search_keyword, feed_ticker, value_chain_type, ticker,
                               scraped_content, ai_summary, url_hash, company_name
                        FROM ranked_articles
                        WHERE (category = 'company' AND rn <= 50)
                           OR (category = 'industry' AND rn <= 25)
                           OR (category = 'competitor' AND rn <= 25)
                           OR (category = 'value_chain' AND rn <= 25)
                        ORDER BY ticker, category, rn
                    """, (tickers, cutoff))
                else:
                    cur.execute("""
                        WITH ranked_articles AS (
                            SELECT a.id, a.url, a.resolved_url, a.title, a.domain, a.published_at,
                                   tf.category, f.search_keyword, f.feed_ticker, tf.value_chain_type, ta.ticker,
                                   a.scraped_content, ta.ai_summary, a.url_hash, f.company_name,
                                   ROW_NUMBER() OVER (
                                       PARTITION BY ta.ticker,
                                           CASE
                                               WHEN tf.category = 'company' THEN tf.category
                                               WHEN tf.category = 'industry' THEN f.search_keyword
                                               WHEN tf.category = 'competitor' THEN f.feed_ticker
                                               WHEN tf.category = 'value_chain' THEN f.feed_ticker
                                           END
                                       ORDER BY a.published_at DESC NULLS LAST, ta.found_at DESC
                                   ) as rn
                            FROM articles a
                            JOIN ticker_articles ta ON a.id = ta.article_id
                            JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                            LEFT JOIN feeds f ON ta.feed_id = f.id
                            WHERE (a.published_at >= %s OR a.published_at IS NULL)
                        )
                        SELECT id, url, resolved_url, title, domain, published_at,
                               category, search_keyword, feed_ticker, value_chain_type, ticker,
                               scraped_content, ai_summary, url_hash, company_name
                        FROM ranked_articles
                        WHERE (category = 'company' AND rn <= 50)
                           OR (category = 'industry' AND rn <= 25)
                           OR (category = 'competitor' AND rn <= 25)
                           OR (category = 'value_chain' AND rn <= 25)
                        ORDER BY ticker, category, rn
                    """, (cutoff,))

                all_articles = list(cur.fetchall())

        memory_monitor.take_snapshot("ARTICLES_LOADED")

        # Log filtering statistics
        with db() as conn, conn.cursor() as cur:
            if tickers:
                cur.execute("""
                    SELECT
                        ta.ticker,
                        tf.category,
                        COUNT(*) FILTER (WHERE a.published_at >= %s OR a.published_at IS NULL) as within_period,
                        COUNT(*) FILTER (WHERE a.published_at < %s) as outside_period
                    FROM ticker_articles ta
                    JOIN articles a ON ta.article_id = a.id
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    WHERE ta.ticker = ANY(%s)
                    GROUP BY ta.ticker, tf.category
                    ORDER BY ta.ticker, tf.category
                """, (cutoff, cutoff, tickers))
            else:
                cur.execute("""
                    SELECT
                        ta.ticker,
                        tf.category,
                        COUNT(*) FILTER (WHERE a.published_at >= %s OR a.published_at IS NULL) as within_period,
                        COUNT(*) FILTER (WHERE a.published_at < %s) as outside_period
                    FROM ticker_articles ta
                    JOIN articles a ON ta.article_id = a.id
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    GROUP BY ta.ticker, tf.category
                    ORDER BY ta.ticker, tf.category
                """, (cutoff, cutoff))

            filter_stats = cur.fetchall()
            for stat in filter_stats:
                if stat['outside_period'] > 0:
                    LOG.info(f"ðŸ“… [{stat['ticker']}] {stat['category']}: {stat['within_period']} within report period, {stat['outside_period']} excluded (published outside {minutes}min window)")

        # Organize articles by ticker and category
        for article in all_articles:
            ticker = article["ticker"]
            category = article["category"] or "company"

            if ticker not in articles_by_ticker:
                articles_by_ticker[ticker] = {"company": [], "industry": [], "competitor": [], "value_chain": []}

            articles_by_ticker[ticker][category].append(article)

        total_articles = len(all_articles)
        LOG.info(f"=== PHASE 1 COMPLETE: {ingest_stats['total_inserted']} new + {total_articles} total in timeframe ===")
        memory_monitor.take_snapshot("PHASE1_COMPLETE")
        
        # PHASE 2: Pure AI triage
        LOG.info("=== PHASE 2: PURE AI TRIAGE ===")
        memory_monitor.take_snapshot("PHASE2_START")
        triage_results = {}
        flagged_articles = []  # Initialize before loop to prevent undefined variable if articles_by_ticker is empty

        for ticker in articles_by_ticker.keys():
            LOG.info(f"Running pure AI triage for {ticker}")
            memory_monitor.take_snapshot(f"TRIAGE_START_{ticker}")
            
            with resource_cleanup_context("ai_triage"):
                # Use fallback triage (Claude primary, OpenAI fallback)
                selected_results = await perform_ai_triage_with_fallback_async(articles_by_ticker[ticker], ticker, triage_batch_size)
                triage_results[ticker] = selected_results

            memory_monitor.take_snapshot(f"TRIAGE_COMPLETE_{ticker}")

            # Build flagged articles list (will be stored by process_ingest_phase)
            flagged_articles = []
            with resource_cleanup_context("database_connection"):
                for category, selected_items in selected_results.items():
                    articles = articles_by_ticker[ticker][category]
                    LOG.info(f"  Building flagged list for {category}: {len(selected_items)} selected from {len(articles)} articles")
                    for item in selected_items:
                        article_idx = item["id"]
                        if article_idx < len(articles):
                            article = articles[article_idx]
                            article_id = article.get("id")
                            if article_id:
                                flagged_articles.append(article_id)
                            else:
                                LOG.warning(f"  Article at index {article_idx} in {category} has no ID!")

            LOG.info(f"âœ… Built flagged articles list: {len(flagged_articles)} article IDs for {ticker}")
            if flagged_articles:
                LOG.info(f"   Sample IDs: {flagged_articles[:5]}...")
            else:
                # Diagnostic info when 0 articles flagged
                total_articles = sum(len(articles_by_ticker[ticker].get(cat, [])) for cat in ['company', 'industry', 'competitor', 'value_chain'])
                LOG.info(f"â„¹ï¸ [{ticker}] AI triage selected 0 articles from {total_articles} total")
                LOG.info(f"   This can happen when:")
                LOG.info(f"   1. No articles matched company name in title/description")
                LOG.info(f"   2. All articles were listicles, sector roundups, or generic watchlists")
                LOG.info(f"   3. Quiet news day with no material developments")
                LOG.info(f"   ðŸ’¡ Email #2 and #3 will generate 'no material developments' reports")

        memory_monitor.take_snapshot("PHASE2_COMPLETE")
        
        # PHASE 3: Send enhanced quick email
        LOG.info("=== PHASE 3: SENDING ENHANCED QUICK TRIAGE EMAIL ===")
        memory_monitor.take_snapshot("PHASE3_START")

        # NEW (Nov 2025): Use report_type from parameter (already set by caller)
        # report_type controls Email #1 subject label (Daily vs Weekly)
        LOG.info(f"Using report_type '{report_type}' for Email #1 subject label")

        with resource_cleanup_context("email_sending"):
            quick_email_sent = send_enhanced_quick_intelligence_email(articles_by_ticker, triage_results, minutes, mode=mode, report_type=report_type)
        
        LOG.info(f"Enhanced quick triage email sent: {quick_email_sent}")
        memory_monitor.take_snapshot("PHASE3_COMPLETE")

        # ============================================================================
        # PHASE 4 DISABLED - MOVED TO DIGEST PHASE (Oct 2025)
        # ============================================================================
        # REASON: Phase 4 (scraping) was running BEFORE Phase 1.5 (Google URL resolution),
        # causing all Google News URLs to fail scraping. Scraping now happens ONLY in
        # process_digest_phase() which runs AFTER Phase 1.5, ensuring URLs are resolved first.
        #
        # TIMELINE FIX:
        #   OLD (BROKEN): Phase 1 â†’ Phase 4 (scrape unresolved URLs) â†’ Phase 1.5 (resolve too late)
        #   NEW (FIXED):  Phase 1 â†’ Phase 1.5 (resolve) â†’ Digest Phase (scrape resolved URLs)
        #
        # See: fetch_digest_articles_with_enhanced_content() for scraping implementation
        # ============================================================================

        # PHASE 4: Ticker-specific content scraping and analysis (WITH ASYNC BATCH PROCESSING)
        # COMMENTED OUT - NOW HAPPENS IN DIGEST PHASE
        # LOG.info("=== PHASE 4: TICKER-SPECIFIC CONTENT SCRAPING AND ANALYSIS (ASYNC BATCHES) ===")
        # memory_monitor.take_snapshot("PHASE4_START")
        scraping_final_stats = {"scraped": 0, "failed": 0, "ai_analyzed": 0, "reused_existing": 0}

        # PHASE 4 CODE BLOCK COMMENTED OUT (150+ lines)
        # Scraping now happens in fetch_digest_articles_with_enhanced_content()
        # which is called during process_digest_phase() AFTER Phase 1.5 URL resolution

        # Initialize variables needed for response structure
        total_articles_to_process = 0
        # for target_ticker in articles_by_ticker.keys():
        #     selected = triage_results.get(target_ticker, {})
        #     for category in ["company", "industry", "competitor"]:
        #         total_articles_to_process += len(selected.get(category, []))

        # processed_count = 0
        # LOG.info(f"Starting Phase 4: {total_articles_to_process} total articles to process in batches of {SCRAPE_BATCH_SIZE}")

        # Track which tickers were successfully processed
        successfully_processed_tickers = set()

        # [PHASE 4 SCRAPING LOOP DISABLED - 150+ lines commented out]
        # The entire scraping loop has been removed because it ran BEFORE Phase 1.5 URL resolution.
        # Scraping now happens in fetch_digest_articles_with_enhanced_content() during digest phase.
        # See process_digest_phase() which calls fetch_digest_articles_with_enhanced_content()

        # Skip Phase 4 - scraping happens in digest phase after URL resolution
        LOG.info("=== PHASE 4 SKIPPED - Scraping moved to Digest Phase (after URL resolution) ===")
        memory_monitor.take_snapshot("PHASE4_SKIPPED")

        # [END OF PHASE 4 BLOCK - 150+ lines of scraping code removed]
        
        # CRITICAL: Final cleanup before returning response
        LOG.info("=== PERFORMING FINAL CLEANUP ===")
        memory_monitor.take_snapshot("BEFORE_FINAL_CLEANUP")
        
        try:
            await full_resource_cleanup()  # Single call with await
            memory_monitor.take_snapshot("AFTER_FINAL_CLEANUP")
            LOG.info("=== FINAL CLEANUP COMPLETE ===")
        except Exception as cleanup_error:
            LOG.error(f"Error during final cleanup: {cleanup_error}")
            memory_monitor.take_snapshot("CLEANUP_ERROR")
        
        processing_time = time.time() - start_time
        LOG.info(f"=== CRON INGEST COMPLETE - Total time: {processing_time:.1f}s ===")
        
        # Stop memory monitoring and get summary
        memory_summary = memory_monitor.stop_monitoring()

        # Prepare response with monitoring data
        response = {
            "status": "completed",
            "processing_time_seconds": round(processing_time, 1),
            "workflow": "ingest_triage_email1_only",

            "phase_1_ingest": {
                "total_processed": ingest_stats["total_processed"],
                "total_inserted": ingest_stats["total_inserted"],
                "total_duplicates": ingest_stats["total_duplicates"],
                "total_spam_blocked": ingest_stats["total_spam_blocked"],
                "total_limit_reached": ingest_stats["total_limit_reached"],
                "total_insider_trading_blocked": ingest_stats["total_insider_trading_blocked"],
                "total_yahoo_rejected": ingest_stats["total_yahoo_rejected"],
                "total_articles_in_timeframe": total_articles
            },
            "phase_2_triage": {
                "type": "pure_ai_triage_only",
                "tickers_processed": len(triage_results),
                "selections_by_ticker": {k: {cat: len(items) for cat, items in v.items()} for k, v in triage_results.items()},
                "flagged_articles": flagged_articles  # Article IDs flagged during triage
            },
            "phase_3_quick_email": {"sent": quick_email_sent},
            "phase_4_scraping": {
                "status": "disabled_moved_to_digest_phase",
                "reason": "Phase 4 scraping removed from ingest - now happens in digest phase after URL resolution",
                "note": "Scraping happens in fetch_digest_articles_with_enhanced_content() during process_digest_phase()"
            },
            "successfully_processed_tickers": list(successfully_processed_tickers),
            "message": "Ingest phase completed - scraping deferred to digest phase (after URL resolution)",
            "github_sync_required": False,  # No scraping happened, so no new content to sync

            # Memory monitoring data
            "memory_monitoring": {
                "enabled": True,
                "total_snapshots": len(memory_monitor.snapshots),
                "memory_summary": memory_summary,
                "peak_memory_mb": max([s.get("memory_mb", 0) for s in memory_monitor.snapshots]) if memory_monitor.snapshots else 0,
                "final_memory_mb": memory_summary.get("final_memory_mb") if memory_summary else None,
                "total_memory_change_mb": memory_summary.get("total_change_mb") if memory_summary else None
            }
        }
        
        return response
        
    except Exception as e:
        # CRITICAL ERROR HANDLING WITH MEMORY SNAPSHOT
        # Add ticker context for debugging concurrent jobs
        ticker_context = tickers[0] if tickers and len(tickers) == 1 else str(tickers)
        LOG.error(f"[{ticker_context}] CRITICAL ERROR in cron_ingest: {str(e)}")
        LOG.error(f"[{ticker_context}] Traceback: {traceback.format_exc()}")
        memory_monitor.take_snapshot("CRITICAL_ERROR")

        # Get detailed memory info at crash
        current_memory = memory_monitor.get_memory_info()
        tracemalloc_info = memory_monitor.get_tracemalloc_top(20)

        LOG.error(f"[{ticker_context}] MEMORY AT CRASH: {current_memory}")
        LOG.error(f"[{ticker_context}] TOP MEMORY ALLOCATIONS AT CRASH: {tracemalloc_info}")
        
        # Emergency cleanup
        try:
            cleanup_result = await full_resource_cleanup()
            LOG.info(f"Emergency cleanup completed: {cleanup_result}")
        except Exception as cleanup_error:
            LOG.error(f"Error during emergency cleanup: {cleanup_error}")
        
        return {
            "status": "critical_error",
            "message": str(e),
            "batch_size_used": SCRAPE_BATCH_SIZE,
            "memory_monitoring": {
                "snapshots_taken": len(memory_monitor.snapshots),
                "memory_at_crash": current_memory,
                "top_allocations": tracemalloc_info[:5],  # Top 5 memory allocations
                "error_occurred": True
            }
        }

def _update_ticker_stats(ticker_stats: Dict, total_stats: Dict, stats: Dict, category: str):
    """Helper to update statistics"""
    ticker_stats["inserted"] += stats["inserted"]
    ticker_stats["duplicates"] += stats["duplicates"]
    ticker_stats["blocked_spam"] += stats.get("blocked_spam", 0)
    ticker_stats["content_scraped"] += stats.get("content_scraped", 0)
    ticker_stats["content_failed"] += stats.get("content_failed", 0)
    ticker_stats["scraping_skipped"] += stats.get("scraping_skipped", 0)
    ticker_stats["ai_scored"] += stats.get("ai_scored", 0)
    ticker_stats["basic_scored"] += stats.get("basic_scored", 0)
    
    total_stats["feeds_processed"] += 1
    total_stats["total_inserted"] += stats["inserted"]
    total_stats["total_duplicates"] += stats["duplicates"]
    total_stats["total_blocked_spam"] += stats.get("blocked_spam", 0)
    total_stats["total_content_scraped"] += stats.get("content_scraped", 0)
    total_stats["total_content_failed"] += stats.get("content_failed", 0)
    total_stats["total_scraping_skipped"] += stats.get("scraping_skipped", 0)
    total_stats["total_ai_scored"] += stats.get("ai_scored", 0)
    total_stats["total_basic_scored"] += stats.get("basic_scored", 0)
    total_stats["by_category"][category] += stats["inserted"]

# DEPRECATED (Nov 2025): Use job queue via /admin/test or python app.py process
# This endpoint used old Phase 1+2 workflow without Phase 3 deduplication
# Commented out to prevent accidental use producing inconsistent Email #2 format
#
# @APP.post("/cron/digest")
# async def cron_digest(
#     request: Request,
#     minutes: int = Query(default=1440, description="Time window in minutes"),
#     tickers: List[str] = Query(default=None, description="Specific tickers for digest")
# ):
#     """Generate and send email digest with content scraping data and AI summaries"""
#     require_admin(request)
#     # NOTE: Schema initialization removed from here (was causing lock timeouts during concurrent processing)
#     # Schema is now initialized once at application startup in startup_event() - see line ~16805
#
#     try:
#         LOG.info(f"=== DIGEST GENERATION STARTING ===")
#         LOG.info(f"Time window: {minutes} minutes, Tickers: {tickers}")
#
#         # Use the existing enhanced digest function that sends emails
#         LOG.info("Calling enhanced digest function...")
#         result = await fetch_digest_articles_with_enhanced_content(minutes / 60, tickers)
#
#         # The function returns a detailed result dict, let's pass it through with additional metadata
#         if isinstance(result, dict):
#             result["minutes"] = minutes
#             result["requested_tickers"] = tickers
#             LOG.info(f"Digest result: {result.get('status', 'unknown')} - {result.get('articles', 0)} articles")
#             return result
#         else:
#             LOG.error(f"Unexpected result type from digest function: {type(result)}")
#             return {
#                 "status": "error",
#                 "message": "Unexpected result from digest generation",
#                 "minutes": minutes,
#                 "tickers": tickers
#             }
#
#     except Exception as e:
#         LOG.error(f"Digest generation failed: {e}")
#         LOG.error(f"Error details: {traceback.format_exc()}")
#         return {
#             "status": "error",
#             "message": str(e),
#             "tickers": tickers,
#             "minutes": minutes
#             }
        
# FIXED: Updated endpoint with proper request body handling
@APP.post("/admin/clean-feeds")
async def clean_old_feeds(request: Request, body: CleanFeedsRequest):
    async with TICKER_PROCESSING_LOCK:
        """Clean old Reddit/Twitter feeds from database"""
        require_admin(request)
        
        with db() as conn, conn.cursor() as cur:
            # Check if tables exist first (safe for fresh database)
            try:
                cur.execute("SELECT 1 FROM ticker_feeds LIMIT 1")
                cur.execute("SELECT 1 FROM feeds LIMIT 1")
            except Exception as e:
                LOG.info(f"ðŸ“‹ Tables don't exist yet (fresh database) - clean-feeds skipped: {e}")
                return {"status": "skipped", "message": "Tables don't exist yet - nothing to clean", "deleted": 0}

            # Delete feeds that contain Reddit, Twitter, SEC, StockTwits
            cleanup_patterns = [
                "Reddit", "Twitter", "SEC EDGAR", "StockTwits",
                "r/investing", "r/stocks", "r/SecurityAnalysis",
                "r/ValueInvesting", "r/energy", "@TalenEnergy"
            ]

            total_deleted = 0
            if body.tickers:
                # NEW ARCHITECTURE: Remove ticker-feed associations for specific tickers
                for pattern in cleanup_patterns:
                    # First, remove ticker-feed associations for feeds matching the pattern
                    cur.execute("""
                        DELETE FROM ticker_feeds
                        WHERE ticker = ANY(%s) AND feed_id IN (
                            SELECT id FROM feeds WHERE name LIKE %s
                        )
                    """, (body.tickers, f"%{pattern}%"))
                    total_deleted += cur.rowcount

                    # Then, delete feeds that have no more associations
                    cur.execute("""
                        DELETE FROM feeds
                        WHERE name LIKE %s AND id NOT IN (
                            SELECT DISTINCT feed_id FROM ticker_feeds WHERE active = TRUE
                        )
                    """, (f"%{pattern}%",))
                    total_deleted += cur.rowcount
            else:
                # NEW ARCHITECTURE: Delete all feeds matching patterns (and their associations)
                for pattern in cleanup_patterns:
                    # First remove all ticker-feed associations for these feeds
                    cur.execute("""
                        DELETE FROM ticker_feeds
                        WHERE feed_id IN (SELECT id FROM feeds WHERE name LIKE %s)
                    """, (f"%{pattern}%",))

                    # Then delete the feeds themselves
                    cur.execute("""
                        DELETE FROM feeds
                        WHERE name LIKE %s
                    """, (f"%{pattern}%",))
                    total_deleted += cur.rowcount
        
        return {"status": "cleaned", "feeds_deleted": total_deleted, "tickers": body.tickers or "all"}

@APP.get("/admin/ticker-metadata/{ticker}")
def get_ticker_metadata(request: Request, ticker: str):
    """Get AI-generated metadata for a ticker"""
    require_admin(request)
    
    config = get_ticker_config(ticker)
    if config:
        return {
            "ticker": ticker,
            "ai_generated": config.get("ai_generated", False),
            "industry_keywords": config.get("industry_keywords", []),
            "competitors": config.get("competitors", [])
        }
    
    return {"ticker": ticker, "message": "No metadata found. Use /admin/init to generate."}

@APP.get("/admin/domain-names")
def get_domain_names(request: Request, limit: int = 1000, offset: int = 0):
    """Export domain names database (domain â†’ formal name mappings)"""
    require_admin(request)

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT domain, formal_name, ai_generated, created_at, updated_at
                FROM domain_names
                ORDER BY domain
                LIMIT %s OFFSET %s
            """, (limit, offset))

            domains = cur.fetchall()

            # Get total count
            cur.execute("SELECT COUNT(*) as total FROM domain_names")
            total = cur.fetchone()['total']

            return {
                "status": "success",
                "total": total,
                "returned": len(domains),
                "limit": limit,
                "offset": offset,
                "domains": [dict(d) for d in domains]
            }
    except Exception as e:
        LOG.error(f"Failed to fetch domain names: {e}")
        return {
            "status": "error",
            "message": str(e)
        }

@APP.post("/admin/regenerate-metadata")
async def regenerate_metadata(request: Request, body: RegenerateMetadataRequest):
    """Force regeneration of AI metadata for a ticker"""
    async with TICKER_PROCESSING_LOCK:
        require_admin(request)

        if not OPENAI_API_KEY:
            return {"status": "error", "message": "OpenAI API key not configured"}

        LOG.info(f"Regenerating metadata for {body.ticker}")
        metadata = ticker_manager.get_or_create_metadata(body.ticker)

        # Rebuild feeds using NEW ARCHITECTURE V2
        feeds_created = create_feeds_for_ticker_new_architecture(body.ticker, metadata)

        return {
            "status": "regenerated",
            "ticker": body.ticker,
            "metadata": metadata,
            "feeds_created": len(feeds_created)
        }


@APP.post("/admin/export-user-csv")
async def admin_export_user_csv(request: Request):
    """
    Manually trigger CSV export of beta users.
    Requires X-Admin-Token authentication.
    """
    # Validate admin token
    admin_token = request.headers.get("X-Admin-Token")
    if not admin_token or admin_token != ADMIN_TOKEN:
        raise HTTPException(status_code=401, detail="Unauthorized")

    try:
        count = export_users_csv()
        return {
            "status": "success",
            "message": f"Exported {count} active users to CSV",
            "file_path": "data/user_tickers.csv",
            "exported_at": datetime.now().isoformat()
        }
    except Exception as e:
        LOG.error(f"CSV export failed: {e}")
        raise HTTPException(status_code=500, detail=f"Export failed: {str(e)}")


# DEPRECATED (Nov 2025): Use job queue via /admin/test or python app.py process
# This endpoint used old Phase 1+2 workflow without Phase 3 deduplication
# Commented out to prevent accidental use producing inconsistent Email #2 format
#
# @APP.post("/admin/force-digest")
# async def force_digest(request: Request, body: ForceDigestRequest):
#     """Force digest with existing articles (for testing) - Enhanced with AI analysis"""
#     require_admin(request)
#
#     with db() as conn, conn.cursor() as cur:
#         if body.tickers:
#             cur.execute("""
#                 SELECT
#                     a.url, a.resolved_url, a.title, a.description,
#                     ta.ticker, a.domain, a.published_at,
#                     ta.found_at, tf.category,
#                     f.search_keyword
#                 FROM articles a
#                 JOIN ticker_articles ta ON a.id = ta.article_id
#                 JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
#                 JOIN feeds f ON ta.feed_id = f.id
#                 WHERE ta.found_at >= %s
#                     AND ta.ticker = ANY(%s)
#                 ORDER BY ta.ticker, tf.category, COALESCE(a.published_at, ta.found_at) DESC, ta.found_at DESC
#             """, (datetime.now(timezone.utc) - timedelta(days=7), body.tickers))
#         else:
#             cur.execute("""
#                 SELECT
#                     a.url, a.resolved_url, a.title, a.description,
#                     ta.ticker, a.domain, a.published_at,
#                     ta.found_at, tf.category,
#                     f.search_keyword
#                 FROM articles a
#                 JOIN ticker_articles ta ON a.id = ta.article_id
#                 JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
#                 JOIN feeds f ON ta.feed_id = f.id
#                 WHERE ta.found_at >= %s
#                 ORDER BY ta.ticker, tf.category, COALESCE(a.published_at, ta.found_at) DESC, ta.found_at DESC
#             """, (datetime.now(timezone.utc) - timedelta(days=7),))
#
#         articles_by_ticker = {}
#         for row in cur.fetchall():
#             ticker = row["ticker"] or "UNKNOWN"
#             category = row["category"] or "company"
#
#             if ticker not in articles_by_ticker:
#                 articles_by_ticker[ticker] = {}
#             if category not in articles_by_ticker[ticker]:
#                 articles_by_ticker[ticker][category] = []
#
#             articles_by_ticker[ticker][category].append(dict(row))
#
#     total_articles = sum(
#         sum(len(arts) for arts in categories.values())
#         for categories in articles_by_ticker.values()
#     )
#
#     if total_articles == 0:
#         return {"status": "no_articles", "message": "No articles found in database"}
#
#     # FIXED: Extract HTML from tuple and add empty text attachment
#     html = await build_enhanced_digest_html(articles_by_ticker, 7)
#     tickers_str = ', '.join(articles_by_ticker.keys())
#     subject = f"FULL Stock Intelligence: {tickers_str} - {total_articles} articles"
#     success = send_email(subject, html)
#
#     return {
#         "status": "sent" if success else "failed",
#         "articles": total_articles,
#         "tickers": list(articles_by_ticker.keys()),
#         "recipient": ADMIN_EMAIL
#     }

@APP.post("/admin/wipe-database")
def wipe_database(request: Request):
    """DANGER: Completely wipe all feeds, ticker configs, and articles from database"""
    require_admin(request)
    
    # Extra safety check - require a confirmation header
    confirm = request.headers.get("x-confirm-wipe", "")
    if confirm != "YES-WIPE-EVERYTHING":
        raise HTTPException(
            status_code=400, 
            detail="Safety check failed. Add header 'X-Confirm-Wipe: YES-WIPE-EVERYTHING' to confirm"
        )
    
    with db() as conn, conn.cursor() as cur:
        # Delete everything in order of dependencies
        deleted_stats = {}
        
        # Delete all articles
        cur.execute("DELETE FROM ticker_articles")
        cur.execute("DELETE FROM articles")
        deleted_stats["articles"] = cur.rowcount
        
        # Delete all feeds (NEW ARCHITECTURE)
        cur.execute("DELETE FROM ticker_feeds")
        deleted_stats["ticker_feeds"] = cur.rowcount
        cur.execute("DELETE FROM feeds")
        deleted_stats["feeds"] = cur.rowcount
        
        # Delete all ticker configurations (keywords, competitors, etc.)
        cur.execute("DELETE FROM ticker_reference")  
        deleted_stats["ticker_references"] = cur.rowcount
        
        LOG.warning(f"DATABASE WIPED: {deleted_stats}")
    
    return {
        "status": "database_wiped",
        "deleted": deleted_stats,
        "warning": "All feeds, ticker configurations, and articles have been deleted"
    }

@APP.post("/admin/wipe-ticker")
def wipe_ticker(request: Request, ticker: str = Body(..., embed=True)):
    """Wipe all data for a specific ticker"""
    require_admin(request)
    
    with db() as conn, conn.cursor() as cur:
        # Check if tables exist first (safe for fresh database)
        try:
            cur.execute("SELECT 1 FROM ticker_articles LIMIT 1")
            cur.execute("SELECT 1 FROM ticker_feeds LIMIT 1")
            cur.execute("SELECT 1 FROM feeds LIMIT 1")
            cur.execute("SELECT 1 FROM ticker_reference LIMIT 1")
        except Exception as e:
            LOG.info(f"ðŸ“‹ Tables don't exist yet (fresh database) - wipe-ticker skipped: {e}")
            return {"status": "skipped", "message": "Tables don't exist yet - nothing to wipe", "ticker": ticker, "deleted": {}}

        deleted_stats = {}

        # Delete articles for this ticker
        cur.execute("DELETE FROM ticker_articles WHERE ticker = %s", (ticker,))
        deleted_stats["articles"] = cur.rowcount
        
        # Delete feeds for this ticker (NEW ARCHITECTURE)
        cur.execute("DELETE FROM ticker_feeds WHERE ticker = %s", (ticker,))
        deleted_stats["ticker_feeds"] = cur.rowcount

        # Delete orphaned feeds (feeds with no ticker associations)
        cur.execute("""
            DELETE FROM feeds
            WHERE id NOT IN (SELECT DISTINCT feed_id FROM ticker_feeds WHERE active = TRUE)
        """)
        deleted_stats["orphaned_feeds"] = cur.rowcount
        
        # Delete ticker configuration
        cur.execute("DELETE FROM ticker_reference WHERE ticker = %s", (ticker,))
        deleted_stats["ticker_reference"] = cur.rowcount
        
        LOG.info(f"Wiped all data for ticker {ticker}: {deleted_stats}")
    
    return {
        "status": "ticker_wiped",
        "ticker": ticker,
        "deleted": deleted_stats
    }

@APP.post("/admin/test-email")
def test_email(request: Request):
    """Send test email"""
    require_admin(request)
    
    test_html = """
    <html><body>
        <h2>Quantbrief Test Email</h2>
        <p>Your email configuration is working correctly!</p>
        <p>Time: {}</p>
        <p>AI Integration: {}</p>
        <p>Yahoo Source Extraction: Enabled</p>
    </body></html>
    """.format(
        datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "Enabled" if OPENAI_API_KEY else "Not configured"
    )
    
    success = send_email("Quantbrief Test Email", test_html)
    return {"status": "sent" if success else "failed", "recipient": ADMIN_EMAIL}

@APP.get("/admin/stats")
async def get_stats(
    request: Request,
    tickers: List[str] = Query(default=None, description="Filter stats by tickers")
):
    async with TICKER_PROCESSING_LOCK:
        """Get system statistics"""
        require_admin(request)
        # NOTE: Schema initialization removed from here (was causing lock timeouts during concurrent processing)
        # Schema is now initialized once at application startup in startup_event() - see line ~16805

        with db() as conn, conn.cursor() as cur:
            # Build queries based on tickers
            if tickers:
                # Article stats
                cur.execute("""
                    SELECT
                        COUNT(*) as total_articles,
                        COUNT(DISTINCT ta.ticker) as tickers,
                        COUNT(DISTINCT a.domain) as domains,
                        MAX(a.published_at) as latest_article
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                        AND ta.ticker = ANY(%s)
                """, (tickers,))
                stats = dict(cur.fetchone())
                
                # Stats by category
                cur.execute("""
                    SELECT tf.category, COUNT(*) as count
                    FROM ticker_articles ta
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                        AND ta.ticker = ANY(%s)
                    GROUP BY tf.category
                    ORDER BY tf.category
                """, (tickers,))
                stats["by_category"] = list(cur.fetchall())

                # Top domains
                cur.execute("""
                    SELECT a.domain, COUNT(*) as count
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                        AND ta.ticker = ANY(%s)
                    GROUP BY a.domain
                    ORDER BY count DESC
                    LIMIT 10
                """, (tickers,))
                stats["top_domains"] = list(cur.fetchall())

                # Articles by ticker and category
                cur.execute("""
                    SELECT ta.ticker, tf.category, COUNT(*) as count
                    FROM ticker_articles ta
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                        AND ta.ticker = ANY(%s)
                    GROUP BY ta.ticker, tf.category
                    ORDER BY ta.ticker, tf.category
                """, (tickers,))
                stats["by_ticker_category"] = list(cur.fetchall())
            else:
                # Article stats
                cur.execute("""
                    SELECT
                        COUNT(*) as total_articles,
                        COUNT(DISTINCT ta.ticker) as tickers,
                        COUNT(DISTINCT a.domain) as domains,
                        MAX(a.published_at) as latest_article
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                """)
                stats = dict(cur.fetchone())
                
                # Stats by category
                cur.execute("""
                    SELECT tf.category, COUNT(*) as count
                    FROM ticker_articles ta
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                    GROUP BY tf.category
                    ORDER BY tf.category
                """)
                stats["by_category"] = list(cur.fetchall())
                
                # Top domains
                cur.execute("""
                    SELECT a.domain, COUNT(*) as count
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                    GROUP BY a.domain
                    ORDER BY count DESC
                    LIMIT 10
                """)
                stats["top_domains"] = list(cur.fetchall())
                
                # Articles by ticker and category
                cur.execute("""
                    SELECT ta.ticker, tf.category, COUNT(*) as count
                    FROM ticker_articles ta
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    WHERE ta.found_at > NOW() - INTERVAL '7 days'
                    GROUP BY ta.ticker, tf.category
                    ORDER BY ta.ticker, tf.category
                """)
                stats["by_ticker_category"] = list(cur.fetchall())
            
            # Check AI metadata status
            cur.execute("""
                SELECT COUNT(*) as total, 
                       COUNT(CASE WHEN ai_generated THEN 1 END) as ai_generated
                FROM ticker_reference
                WHERE active = TRUE
            """)
            ai_stats = cur.fetchone()
            stats["ai_metadata"] = ai_stats
        
        return stats

@APP.post("/admin/reset-digest-flags")
async def reset_digest_flags(request: Request, body: ResetDigestRequest):
    async with TICKER_PROCESSING_LOCK:
        """Reset sent_in_digest flags for testing"""
        require_admin(request)
        # Note: ensure_schema() not needed for simple UPDATE operation

        with db() as conn, conn.cursor() as cur:
            # Check if ticker_articles table exists first (safe for fresh database)
            try:
                cur.execute("SELECT 1 FROM ticker_articles LIMIT 1")
            except Exception as e:
                LOG.info(f"ðŸ“‹ ticker_articles table doesn't exist yet (fresh database) - reset-digest-flags skipped: {e}")
                return {"status": "skipped", "message": "ticker_articles table doesn't exist yet - nothing to reset", "articles_reset": 0, "tickers": body.tickers or "all"}

            if body.tickers:
                cur.execute("UPDATE ticker_articles SET sent_in_digest = FALSE WHERE ticker = ANY(%s)", (body.tickers,))
            else:
                cur.execute("UPDATE ticker_articles SET sent_in_digest = FALSE")
            count = cur.rowcount
        
        return {"status": "reset", "articles_reset": count, "tickers": body.tickers or "all"}

@APP.post("/admin/cleanup-domains")
def admin_cleanup_domains(request: Request):
    """One-time cleanup of domain data"""
    require_admin(request)
    
    updated_count = cleanup_domain_data()
    
    return {
        "status": "completed",
        "records_updated": updated_count,
        "message": "Domain data has been cleaned up. Publication names converted to actual domains."
    }

@APP.post("/admin/reset-ai-analysis")
def reset_ai_analysis(request: Request, tickers: List[str] = Query(default=None, description="Specific tickers to reset")):
    """Reset AI analysis data and force re-scoring of existing articles"""
    require_admin(request)
    
    with db() as conn, conn.cursor() as cur:
        # AI analysis is no longer stored in database with new schema
        # This operation is no longer needed
        reset_count = 0
        
        LOG.info(f"Reset AI analysis for {reset_count} articles")
    
    return {
        "status": "ai_analysis_reset",
        "articles_reset": reset_count,
        "tickers": tickers or "all",
        "message": f"Cleared AI analysis data for {reset_count} articles. Run re-analysis to generate fresh scores."
    }

@APP.post("/admin/rerun-ai-analysis")
def rerun_ai_analysis(
    request: Request, 
    tickers: List[str] = Query(default=None, description="Specific tickers to re-analyze"),
    limit: int = Query(default=500, description="Max articles to process per run (no upper limit)")
):
    """Re-run AI analysis on existing articles that have NULL ai_impact - UNLIMITED PROCESSING"""
    require_admin(request)
    
    if not OPENAI_API_KEY:
        return {"status": "error", "message": "OpenAI API key not configured"}
    
    # Get ticker metadata for keywords
    ticker_metadata_cache = {}
    if tickers:
        for ticker in tickers:
            config = get_ticker_config(ticker)
            if config:
                ticker_metadata_cache[ticker] = {
                    "industry_keywords": config.get("industry_keywords", []),
                    "competitors": config.get("competitors", [])
                }
    
    with db() as conn, conn.cursor() as cur:
        # Get articles that need AI analysis - NO LIMIT restriction
        if tickers:
            cur.execute("""
                SELECT a.id, a.title, a.description, a.domain, ta.ticker, tf.category, f.search_keyword
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                JOIN feeds f ON ta.feed_id = f.id
                WHERE ta.ticker = ANY(%s)
                ORDER BY ta.found_at DESC
                LIMIT %s
            """, (tickers, limit))
        else:
            cur.execute("""
                SELECT a.id, a.title, a.description, a.domain, ta.ticker, tf.category, f.search_keyword
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                JOIN feeds f ON ta.feed_id = f.id
                ORDER BY ta.found_at DESC
                LIMIT %s
            """, (limit,))
        
        articles = list(cur.fetchall())
    
    if not articles:
        return {
            "status": "no_articles",
            "message": "No articles found that need AI re-analysis"
        }
    
    processed = 0
    updated = 0
    errors = 0
    
    LOG.info(f"Starting AI re-analysis of {len(articles)} articles (limit={limit})")
    
    for article in articles:
        try:
            processed += 1
            
            # Get appropriate keywords based on category
            ticker = article["ticker"]
            category = article["category"] or "company"
            
            if ticker in ticker_metadata_cache:
                metadata = ticker_metadata_cache[ticker]
                if category == "industry":
                    keywords = metadata.get("industry_keywords", [])
                elif category == "competitor":
                    keywords = metadata.get("competitors", [])
                else:
                    keywords = []
            else:
                keywords = []
            
            # Run AI analysis - UPDATED to handle 4-parameter return
            quality_score, ai_impact, ai_reasoning, components = calculate_quality_score(
                title=article["title"],
                domain=article["domain"],
                ticker=ticker,
                description=article["description"] or "",
                category=category,
                keywords=keywords
            )
            
            # Extract components for database storage
            source_tier = components.get('source_tier') if components else None
            event_multiplier = components.get('event_multiplier') if components else None
            event_multiplier_reason = components.get('event_multiplier_reason') if components else None
            relevance_boost = components.get('relevance_boost') if components else None
            relevance_boost_reason = components.get('relevance_boost_reason') if components else None
            numeric_bonus = components.get('numeric_bonus') if components else None
            penalty_multiplier = components.get('penalty_multiplier') if components else None
            penalty_reason = components.get('penalty_reason') if components else None
            
            # AI analysis is no longer stored in database with new schema
            # Analysis results are calculated on-demand
            updated += 1
            
            # Progress logging every 25 articles
            if processed % 25 == 0:
                LOG.info(f"Progress: {processed}/{len(articles)} articles processed, {updated} updated")
            
            # Small delay to avoid overwhelming OpenAI API
            time.sleep(0.1)
            
        except Exception as e:
            errors += 1
            LOG.error(f"Failed to re-analyze article {article['id']}: {e}")
            continue
    
    return {
        "status": "completed",
        "processed": processed,
        "updated": updated,
        "errors": errors,
        "limit_used": limit,
        "tickers": tickers or "all",
        "message": f"Re-analyzed {updated} articles with fresh AI scoring"
    }

@APP.get("/admin/test-yahoo-resolution")
def test_yahoo_resolution(request: Request, url: str = Query(...)):
    """Test Yahoo Finance URL resolution"""
    require_admin(request)
    
    result = domain_resolver.resolve_url_and_domain(url, "Test Title")
    return {
        "original_url": url,
        "resolved_url": result[0],
        "domain": result[1],
        "source_url": result[2]
    }

@APP.post("/admin/import-ticker-reference")
def import_ticker_reference(request: Request, file_path: str = Body(..., embed=True)):
    """Import ticker reference data from CSV file"""
    require_admin(request)

    file_path = r"C:\Users\14166\Desktop\QuantBrief\data\ticker_reference.csv"
    import csv
    import os
    from datetime import datetime
    
    if not os.path.exists(file_path):
        return {"status": "error", "message": f"File not found: {file_path}"}
    
    # Ensure table exists
    create_ticker_reference_table()
    
    imported = 0
    updated = 0
    errors = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            
            with db() as conn, conn.cursor() as cur:
                for row_num, row in enumerate(reader, start=2):
                    try:
                        # Parse arrays from string
                        industry_keywords = []
                        competitors = []
                        
                        if row.get('industry_keywords'):
                            industry_keywords = [kw.strip() for kw in row['industry_keywords'].split(',')]
                        
                        if row.get('competitors'):
                            competitors = [comp.strip() for comp in row['competitors'].split(',')]
                        
                        # Convert boolean
                        active = row.get('active', 'TRUE').upper() in ('TRUE', '1', 'YES')
                        ai_generated = row.get('ai_generated', 'FALSE').upper() in ('TRUE', '1', 'YES')
                        
                        cur.execute("""
                            INSERT INTO ticker_reference (
                                ticker, country, company_name, industry, sector, 
                                yahoo_ticker, exchange, active, industry_keywords, 
                                competitors, ai_generated
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (ticker) DO UPDATE SET
                                country = EXCLUDED.country,
                                company_name = EXCLUDED.company_name,
                                industry = EXCLUDED.industry,
                                sector = EXCLUDED.sector,
                                yahoo_ticker = EXCLUDED.yahoo_ticker,
                                exchange = EXCLUDED.exchange,
                                active = EXCLUDED.active,
                                industry_keywords = EXCLUDED.industry_keywords,
                                competitors = EXCLUDED.competitors,
                                updated_at = NOW()
                        """, (
                            row['ticker'], row['country'], row['company_name'],
                            row.get('industry'), row.get('sector'),
                            row.get('yahoo_ticker', row['ticker']), row.get('exchange'),
                            active, industry_keywords, competitors, ai_generated
                        ))
                        
                        if cur.rowcount == 1:
                            imported += 1
                        else:
                            updated += 1
                            
                    except Exception as e:
                        errors.append(f"Row {row_num}: {str(e)}")
                        
        return {
            "status": "completed",
            "imported": imported,
            "updated": updated,
            "errors": errors[:10],  # Limit error display
            "total_errors": len(errors)
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

# Pydantic models for request bodies
class TickerReferenceRequest(BaseModel):
    ticker: str
    country: str
    company_name: str
    industry: Optional[str] = None
    sector: Optional[str] = None
    sub_industry: Optional[str] = None
    exchange: Optional[str] = None
    currency: Optional[str] = None
    market_cap_category: Optional[str] = None
    yahoo_ticker: Optional[str] = None
    active: bool = True
    is_etf: bool = False
    industry_keyword_1: Optional[str] = None
    industry_keyword_2: Optional[str] = None
    industry_keyword_3: Optional[str] = None
    competitor_1_name: Optional[str] = None
    competitor_1_ticker: Optional[str] = None
    competitor_2_name: Optional[str] = None
    competitor_2_ticker: Optional[str] = None
    competitor_3_name: Optional[str] = None
    competitor_3_ticker: Optional[str] = None

class GitHubSyncRequest(BaseModel):
    commit_message: Optional[str] = None

class UpdateTickersRequest(BaseModel):
    tickers: List[str]
    commit_message: Optional[str] = None
    job_id: Optional[str] = None  # For idempotency tracking
    skip_render: Optional[bool] = True  # Default: skip render to prevent auto-deployment

# 1. GITHUB SYNC ENDPOINTS
@APP.post("/admin/sync-ticker-reference-from-github")
def sync_from_github(request: Request):
    """Sync ticker reference data FROM GitHub TO database"""
    require_admin(request)
    
    result = sync_ticker_references_from_github()
    return result

@APP.post("/admin/sync-ticker-reference-to-github")
def sync_to_github(request: Request, body: GitHubSyncRequest):
    """Sync ticker reference data FROM database TO GitHub"""
    require_admin(request)
    
    result = sync_ticker_references_to_github(body.commit_message)
    return result

@APP.post("/admin/update-specific-tickers-on-github")
def update_tickers_on_github(request: Request, body: UpdateTickersRequest):
    """Update only specific tickers on GitHub (for post-processing sync)"""
    require_admin(request)
    
    if not body.tickers:
        return {"status": "error", "message": "No tickers specified"}
    
    result = update_specific_tickers_on_github(body.tickers, body.commit_message)
    return result

# 2. CSV UPLOAD ENDPOINT
@APP.post("/admin/upload-ticker-csv")
async def upload_ticker_csv(request: Request, file: UploadFile = File(...)):
    """Upload CSV file and import ticker reference data"""
    require_admin(request)
    
    if not file.filename or not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="File must be a CSV file")
    
    try:
        content = await file.read()
        csv_content = content.decode('utf-8')
        
        result = import_ticker_reference_from_csv_content(csv_content)
        return result
        
    except UnicodeDecodeError:
        raise HTTPException(status_code=400, detail="CSV file must be UTF-8 encoded")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to process CSV: {str(e)}")

# 3. INDIVIDUAL TICKER MANAGEMENT
@APP.get("/admin/ticker-reference/{ticker}")
async def get_ticker_reference_endpoint(request: Request, ticker: str):
    async with TICKER_PROCESSING_LOCK:
        """Get specific ticker reference data"""
        require_admin(request)
        
        ticker_data = get_ticker_reference(ticker)
        if ticker_data:
            return {
                "status": "found",
                "ticker": ticker,
                "data": ticker_data
            }
        else:
            return {
                "status": "not_found",
                "ticker": ticker,
                "message": "Ticker reference not found"
            }

@APP.post("/admin/ticker-reference")
def add_ticker_reference_endpoint(request: Request, body: TickerReferenceRequest):
    """Add or update a single ticker reference manually"""
    require_admin(request)
    
    ticker_data = {
        'ticker': body.ticker,
        'country': body.country,
        'company_name': body.company_name,
        'industry': body.industry,
        'sector': body.sector,
        'sub_industry': body.sub_industry,
        'exchange': body.exchange,
        'currency': body.currency,
        'market_cap_category': body.market_cap_category,
        'yahoo_ticker': body.yahoo_ticker,
        'active': body.active,
        'is_etf': body.is_etf,
        'industry_keyword_1': body.industry_keyword_1,
        'industry_keyword_2': body.industry_keyword_2,
        'industry_keyword_3': body.industry_keyword_3,
        'competitor_1_name': body.competitor_1_name,
        'competitor_1_ticker': body.competitor_1_ticker,
        'competitor_2_name': body.competitor_2_name,
        'competitor_2_ticker': body.competitor_2_ticker,
        'competitor_3_name': body.competitor_3_name,
        'competitor_3_ticker': body.competitor_3_ticker,
        'data_source': 'manual_api'
    }
    
    success = store_ticker_reference(ticker_data)
    
    if success:
        return {
            "status": "success",
            "ticker": body.ticker,
            "message": f"Successfully stored ticker reference for {body.ticker}"
        }
    else:
        return {
            "status": "error",
            "ticker": body.ticker,
            "message": "Failed to store ticker reference"
        }

@APP.delete("/admin/ticker-reference/{ticker}")
def delete_ticker_reference_endpoint(request: Request, ticker: str):
    """Delete (deactivate) a ticker reference"""
    require_admin(request)
    
    success = delete_ticker_reference(ticker)
    
    if success:
        return {
            "status": "success",
            "ticker": ticker,
            "message": f"Successfully deactivated ticker reference for {ticker}"
        }
    else:
        return {
            "status": "error",
            "ticker": ticker,
            "message": "Ticker reference not found or already inactive"
        }

# 4. BULK TICKER OPERATIONS
@APP.get("/admin/ticker-references")
def list_ticker_references(
    request: Request,
    limit: int = Query(default=50, description="Number of records to return"),
    offset: int = Query(default=0, description="Number of records to skip"),
    country: Optional[str] = Query(default=None, description="Filter by country (US, CA, etc.)")
):
    """List ticker references with pagination and filtering"""
    require_admin(request)
    
    if limit > 1000:
        raise HTTPException(status_code=400, detail="Limit cannot exceed 1000")
    
    try:
        ticker_references = get_all_ticker_references(limit, offset, country)
        total_count = count_ticker_references(country)
        
        return {
            "status": "success",
            "data": ticker_references,
            "pagination": {
                "limit": limit,
                "offset": offset,
                "total": total_count,
                "returned": len(ticker_references)
            },
            "filter": {
                "country": country
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve ticker references: {str(e)}")

@APP.get("/admin/ticker-references/stats")
def get_ticker_reference_stats(request: Request):
    """Get statistics about ticker reference data"""
    require_admin(request)
    
    try:
        with db() as conn, conn.cursor() as cur:
            # Total counts by country
            cur.execute("""
                SELECT country, COUNT(*) as count
                FROM ticker_reference
                WHERE active = TRUE
                GROUP BY country
                ORDER BY count DESC
            """)
            by_country = list(cur.fetchall())
            
            # AI enhancement status
            cur.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN ai_generated THEN 1 END) as ai_enhanced,
                    COUNT(CASE WHEN industry_keyword_1 IS NOT NULL THEN 1 END) as has_keywords,
                    COUNT(CASE WHEN competitor_1_name IS NOT NULL THEN 1 END) as has_competitors
                FROM ticker_reference
                WHERE active = TRUE
            """)
            ai_stats = dict(cur.fetchone())
            
            # Recent updates
            cur.execute("""
                SELECT COUNT(*) as count
                FROM ticker_reference
                WHERE active = TRUE AND updated_at > NOW() - INTERVAL '7 days'
            """)
            recent_updates = cur.fetchone()["count"]
            
            return {
                "status": "success",
                "stats": {
                    "total_active_tickers": sum(row["count"] for row in by_country),
                    "by_country": by_country,
                    "ai_enhancement": {
                        "total_tickers": ai_stats["total"],
                        "ai_enhanced": ai_stats["ai_enhanced"],
                        "has_industry_keywords": ai_stats["has_keywords"],
                        "has_competitors": ai_stats["has_competitors"]
                    },
                    "recent_updates_7_days": recent_updates
                }
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get stats: {str(e)}")

# 5. TICKER VALIDATION AND TESTING
@APP.post("/admin/validate-ticker-format")
def validate_ticker_endpoint(request: Request, ticker: str = Body(..., embed=True)):
    """Test ticker format validation"""
    require_admin(request)
    
    normalized = normalize_ticker_format(ticker)
    is_valid = validate_ticker_format(normalized)
    exchange_info = get_ticker_exchange_info(normalized)
    
    return {
        "status": "success",
        "original_ticker": ticker,
        "normalized_ticker": normalized,
        "is_valid": is_valid,
        "exchange_info": exchange_info
    }

@APP.post("/admin/test-ticker-validation")
def test_ticker_validation_endpoint(request: Request):
    """Run comprehensive ticker validation tests"""
    require_admin(request)
    
    test_results = test_ticker_validation()
    return {
        "status": "success",
        "test_results": test_results
    }

# 6. ENHANCED METADATA INTEGRATION
@APP.post("/admin/enhance-ticker-with-ai")
def enhance_ticker_with_ai_endpoint(request: Request, ticker: str = Body(..., embed=True)):
    """Enhance a specific ticker with AI-generated metadata"""
    require_admin(request)
    
    if not OPENAI_API_KEY:
        return {"status": "error", "message": "OpenAI API key not configured"}
    
    try:
        # Get current ticker data
        ticker_data = get_ticker_reference(ticker)
        if not ticker_data:
            return {
                "status": "error",
                "message": f"Ticker {ticker} not found in reference database"
            }
        
        # Generate enhanced metadata
        enhanced_metadata = get_or_create_enhanced_ticker_metadata(ticker)
        
        return {
            "status": "success",
            "ticker": ticker,
            "original_data": ticker_data,
            "enhanced_metadata": enhanced_metadata,
            "message": f"Successfully enhanced {ticker} with AI-generated metadata"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "ticker": ticker,
            "message": f"Failed to enhance ticker: {str(e)}"
        }

# 7. WORKFLOW INTEGRATION ENDPOINTS
@APP.post("/admin/prepare-ticker-for-processing")
def prepare_ticker_for_processing(request: Request, ticker: str = Body(..., embed=True)):
    """Complete workflow: Sync from GitHub -> Get ticker metadata -> Ready for processing"""
    require_admin(request)
    
    try:
        # Step 1: Sync latest data from GitHub
        LOG.info(f"Preparing {ticker} for processing - syncing from GitHub")
        sync_result = sync_ticker_references_from_github()
        
        if sync_result["status"] != "success":
            return {
                "status": "error",
                "message": f"GitHub sync failed: {sync_result.get('message', 'Unknown error')}"
            }
        
        # Step 2: Get ticker data with enhancement
        ticker_metadata = get_or_create_enhanced_ticker_metadata(ticker)
        
        # Step 3: Check if ticker has required data
        ticker_reference = get_ticker_reference(ticker)
        
        return {
            "status": "ready",
            "ticker": ticker,
            "github_sync": {
                "imported": sync_result.get("database_import", {}).get("imported", 0),
                "updated": sync_result.get("database_import", {}).get("updated", 0)
            },
            "ticker_reference": ticker_reference,
            "enhanced_metadata": ticker_metadata,
            "message": f"Ticker {ticker} is ready for processing"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "ticker": ticker,
            "message": f"Preparation failed: {str(e)}"
        }

@APP.post("/admin/finalize-ticker-processing")
def finalize_ticker_processing(request: Request, body: UpdateTickersRequest):
    """Complete workflow: Update specific processed tickers back to GitHub"""
    require_admin(request)
    
    if not body.tickers:
        return {"status": "error", "message": "No tickers specified"}
    
    try:
        # Update the processed tickers back to GitHub
        result = update_specific_tickers_on_github(body.tickers, body.commit_message)
        
        return {
            "status": result["status"],
            "processed_tickers": body.tickers,
            "github_update": result,
            "message": f"Finalized processing for {len(body.tickers)} tickers"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "tickers": body.tickers,
            "message": f"Finalization failed: {str(e)}"
        }

@APP.post("/admin/sync-processed-tickers-to-github")
def sync_processed_tickers_to_github(request: Request, body: UpdateTickersRequest):
    """Prepare processed ticker data for sync (no GitHub commit)"""
    require_admin(request)
    
    if not body.tickers:
        return {"status": "error", "message": "No tickers specified"}
    
    LOG.info(f"=== PREPARING {len(body.tickers)} PROCESSED TICKERS FOR SYNC ===")
    
    try:
        # Get current successfully processed tickers that have AI enhancements
        successfully_enhanced_tickers = []
        
        with db() as conn, conn.cursor() as cur:
            # Check which tickers actually have AI enhancements
            cur.execute("""
                SELECT DISTINCT ticker 
                FROM ticker_reference 
                WHERE ticker = ANY(%s) 
                AND ai_generated = TRUE 
                AND (industry_keyword_1 IS NOT NULL OR competitor_1_name IS NOT NULL)
            """, (body.tickers,))
            
            successfully_enhanced_tickers = [row["ticker"] for row in cur.fetchall()]
        
        if not successfully_enhanced_tickers:
            return {
                "status": "no_changes", 
                "message": "No tickers found with AI enhancements",
                "requested_tickers": body.tickers
            }
        
        # Export the enhanced data to CSV format
        export_result = export_ticker_references_to_csv()
        
        return {
            "status": "ready_for_sync",
            "requested_tickers": body.tickers,
            "enhanced_tickers": successfully_enhanced_tickers,
            "csv_content": export_result.get("csv_content", ""),
            "message": f"Prepared {len(successfully_enhanced_tickers)} enhanced tickers for sync"
        }
        
    except Exception as e:
        LOG.error(f"Failed to prepare processed tickers: {e}")
        return {
            "status": "error",
            "message": f"Preparation failed: {str(e)}",
            "tickers": body.tickers
        }

@APP.get("/admin/memory")
async def get_memory_info():
    """Get current memory usage"""
    return memory_monitor.get_memory_info()

@APP.get("/admin/debug/digest-check/{ticker}")
async def debug_digest_check(request: Request, ticker: str):
    """Debug endpoint to check digest data for a specific ticker"""
    require_admin(request)

    try:
        with db() as conn, conn.cursor() as cur:
            # Check if tables exist first (safe for fresh database)
            try:
                cur.execute("SELECT 1 FROM articles LIMIT 1")
                cur.execute("SELECT 1 FROM ticker_articles LIMIT 1")
            except Exception as e:
                LOG.info(f"ðŸ“‹ Tables don't exist yet (fresh database) - digest-check skipped: {e}")
                return {"status": "skipped", "message": "Tables don't exist yet - cannot check digest", "ticker": ticker}

            # Check articles for this ticker in the last 24 hours
            cutoff = datetime.now(timezone.utc) - timedelta(hours=24)

            cur.execute("""
                SELECT COUNT(*) as total_articles,
                       COUNT(CASE WHEN ta.ai_summary IS NOT NULL THEN 1 END) as with_ai_summary,
                       COUNT(CASE WHEN a.scraped_content IS NOT NULL THEN 1 END) as with_content,
                       COUNT(CASE WHEN ta.sent_in_digest = TRUE THEN 1 END) as already_sent
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                WHERE ta.ticker = %s
                AND ta.found_at >= %s
            """, (ticker, cutoff))

            stats = dict(cur.fetchone())

            # Get sample articles
            cur.execute("""
                SELECT a.id, a.title, tf.category,
                       ta.ai_summary IS NOT NULL as has_ai_summary,
                       a.scraped_content IS NOT NULL as has_content,
                       ta.sent_in_digest
                FROM articles a
                JOIN ticker_articles ta ON a.id = ta.article_id
                JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                WHERE ta.ticker = %s
                AND ta.found_at >= %s
                ORDER BY ta.found_at DESC
                LIMIT 5
            """, (ticker, cutoff))

            sample_articles = [dict(row) for row in cur.fetchall()]

            return {
                "ticker": ticker,
                "time_window": "24 hours",
                "stats": stats,
                "sample_articles": sample_articles,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }

    except Exception as e:
        return {
            "error": str(e),
            "ticker": ticker
        }

@APP.get("/admin/memory-snapshots")
async def get_memory_snapshots():
    """Get all memory snapshots"""
    return {
        "snapshots": memory_monitor.snapshots,
        "tracemalloc_top": memory_monitor.get_tracemalloc_top() if memory_monitor.tracemalloc_started else []
    }

@APP.post("/admin/force-cleanup")
async def force_cleanup():
    """Force garbage collection"""
    cleanup_result = memory_monitor.force_garbage_collection()
    return {
        "status": "success", 
        "cleanup_result": cleanup_result
    }

# DISABLED (Nov 30, 2025): CSV is source of truth - never write DB back to ticker_reference.csv
# @APP.post("/admin/commit-csv-to-github")
# async def commit_csv_to_github_endpoint():
#     """HTTP endpoint to export DB to CSV and commit to GitHub"""
#     try:
#         # Step 1: Export database to CSV
#         export_result = export_ticker_references_to_csv()
#         if export_result["status"] != "success":
#             return export_result
#
#         # Step 2: Commit CSV to GitHub
#         commit_result = commit_csv_to_github(export_result["csv_content"])
#
#         return {
#             "status": commit_result["status"],
#             "export_info": {
#                 "ticker_count": export_result["ticker_count"],
#                 "csv_size": len(export_result["csv_content"])
#             },
#             "github_commit": commit_result,
#             "message": f"Exported {export_result['ticker_count']} tickers and committed to GitHub"
#         }
#
#     except Exception as e:
#         return {"status": "error", "message": str(e)}

@APP.post("/admin/update-domain-names")
async def update_domain_formal_names(request: Request):
    """Batch update domain formal names using Gemini 2.5 Flash in batches of 500"""
    require_admin(request)

    try:
        LOG.info("=== BATCH UPDATING DOMAIN FORMAL NAMES (Gemini 2.5 Flash) ===")

        # Step 1: Fetch all domains from database
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT domain FROM domain_names ORDER BY domain;")
            domains = [row["domain"] for row in cur.fetchall()]

        if not domains:
            return {"status": "error", "message": "No domains found in database"}

        total_domains = len(domains)
        LOG.info(f"Found {total_domains} domains to process")

        if not GEMINI_API_KEY:
            return {"status": "error", "message": "GEMINI_API_KEY not configured"}

        # Configure Gemini
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.5-flash')

        # Process in batches of 50 (smaller batches reduce impact of SAFETY blocks)
        BATCH_SIZE = 50
        total_batches = (total_domains + BATCH_SIZE - 1) // BATCH_SIZE

        total_updated = 0
        total_errors = 0
        batch_results = []

        import json
        import re
        from datetime import datetime

        for batch_num in range(total_batches):
            batch_start = batch_num * BATCH_SIZE
            batch_end = min(batch_start + BATCH_SIZE, total_domains)
            batch_domains = domains[batch_start:batch_end]

            LOG.info(f"=== Processing Batch {batch_num + 1}/{total_batches} ({len(batch_domains)} domains) ===")

            # Build prompt for this batch
            domain_list = "\n".join([f"- {domain}" for domain in batch_domains])

            prompt = f"""You are a domain name expert. Below is a list of domain names. For EACH domain, provide ONLY the formal brand/publication name as it should appear in professional contexts.

Rules:
1. Return EXACTLY one name per domain
2. Use proper capitalization (e.g., "The Wall Street Journal" not "wall street journal")
3. For news outlets, include "The" if official (e.g., "The Guardian")
4. For companies, use official brand name (e.g., "Bloomberg" not "Bloomberg LP")
5. Do NOT include domain extensions (.com, .net, etc.) in the formal name
6. If a domain is unknown or generic, return the domain name as-is with proper capitalization
7. IMPORTANT: Use only basic ASCII characters (A-Z, a-z, 0-9, spaces, hyphens). Convert accented characters to their base form (Ã©â†’e, Ã¼â†’u, Ã±â†’n, etc.)

Format your response as a JSON object where keys are domains and values are formal names:
{{
  "domain1.com": "Formal Name 1",
  "domain2.com": "Formal Name 2"
}}

Domains:
{domain_list}
"""

            # Call Gemini API
            try:
                start_time = datetime.now()

                generation_config = {
                    "temperature": 0.0,
                    "max_output_tokens": 8192
                }

                # Safety settings - disable all safety filters for domain name processing
                # Some domains (adult, gambling, etc.) might trigger safety filters
                # Using Python SDK enum format (not REST API dict format)
                safety_settings = {
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }

                response = model.generate_content(
                    prompt,
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )

                end_time = datetime.now()
                generation_time = (end_time - start_time).total_seconds()

                # Check finish_reason before accessing response.text
                if not response.candidates:
                    LOG.error(f"Batch {batch_num + 1}: No candidates in response")
                    raise ValueError("No candidates returned by Gemini")

                finish_reason = response.candidates[0].finish_reason
                if finish_reason != 1:  # 1 = STOP (normal completion), 2 = SAFETY, 3 = RECITATION, etc.
                    LOG.error(f"Batch {batch_num + 1}: Response blocked with finish_reason={finish_reason}")
                    LOG.error(f"Finish reasons: 1=STOP, 2=SAFETY, 3=RECITATION, 4=OTHER")
                    if hasattr(response.candidates[0], 'safety_ratings'):
                        LOG.error(f"Safety ratings: {response.candidates[0].safety_ratings}")
                    raise ValueError(f"Response blocked (finish_reason={finish_reason})")

                response_text = response.text
                LOG.info(f"Batch {batch_num + 1}: Gemini response received ({generation_time:.1f}s)")
                LOG.info(f"Response preview (first 200 chars): {response_text[:200]}")

                # Extract JSON from response
                json_content = None

                # Try to extract JSON from code block first
                if '```json' in response_text:
                    json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
                    if json_match:
                        json_content = json_match.group(1).strip()
                        LOG.info(f"Batch {batch_num + 1}: Extracted JSON from ```json code block ({len(json_content)} chars)")
                    else:
                        LOG.warning(f"Batch {batch_num + 1}: Found ```json marker but regex failed to extract")
                        LOG.warning(f"Response length: {len(response_text)}, First 500 chars: {response_text[:500]}")

                # Fallback: try to find raw JSON object if code block extraction failed
                if not json_content and '{' in response_text:
                    json_match = re.search(r'\{[\s\S]*\}', response_text)
                    if json_match:
                        json_content = json_match.group(0).strip()
                        LOG.info(f"Batch {batch_num + 1}: Extracted raw JSON object ({len(json_content)} chars)")
                    else:
                        LOG.warning(f"Batch {batch_num + 1}: Found '{{' but regex failed to extract")

                if not json_content or not json_content.strip():
                    LOG.error(f"Batch {batch_num + 1}: No valid JSON content extracted")
                    batch_results.append({
                        "batch": batch_num + 1,
                        "status": "error",
                        "message": "Could not extract JSON from Gemini response",
                        "domains_processed": 0
                    })
                    total_errors += len(batch_domains)
                    continue

                # Parse JSON
                try:
                    domain_mappings = json.loads(json_content)
                except json.JSONDecodeError as e:
                    LOG.error(f"Batch {batch_num + 1}: JSON parsing failed: {e}")
                    batch_results.append({
                        "batch": batch_num + 1,
                        "status": "error",
                        "message": f"Invalid JSON: {str(e)}",
                        "domains_processed": 0
                    })
                    total_errors += len(batch_domains)
                    continue

                # Update database for this batch
                batch_updated = 0
                batch_errors = 0

                with db() as conn, conn.cursor() as cur:
                    for domain in batch_domains:
                        formal_name = domain_mappings.get(domain)

                        if not formal_name:
                            LOG.warning(f"Missing formal name for: {domain}")
                            batch_errors += 1
                            continue

                        cur.execute("""
                            UPDATE domain_names
                            SET formal_name = %s
                            WHERE domain = %s
                        """, (formal_name, domain))

                        batch_updated += 1
                        # Only log first 5 and last 5 per batch to avoid spam
                        if batch_updated <= 5 or batch_updated > len(batch_domains) - 5:
                            LOG.info(f"âœ… {domain} â†’ {formal_name}")
                        elif batch_updated == 6:
                            LOG.info(f"... ({len(batch_domains) - 10} more domains) ...")

                total_updated += batch_updated
                total_errors += batch_errors

                batch_results.append({
                    "batch": batch_num + 1,
                    "status": "success",
                    "updated": batch_updated,
                    "errors": batch_errors,
                    "generation_time": f"{generation_time:.1f}s"
                })

                LOG.info(f"âœ… Batch {batch_num + 1}/{total_batches} complete: {batch_updated} updated, {batch_errors} errors")

            except Exception as batch_error:
                LOG.error(f"Batch {batch_num + 1} failed: {batch_error}")
                batch_results.append({
                    "batch": batch_num + 1,
                    "status": "error",
                    "message": str(batch_error),
                    "domains_processed": 0
                })
                total_errors += len(batch_domains)

        LOG.info(f"=== UPDATE COMPLETE ===")
        LOG.info(f"Total domains: {total_domains}")
        LOG.info(f"Updated: {total_updated}")
        LOG.info(f"Errors: {total_errors}")

        return {
            "status": "success",
            "total_domains": total_domains,
            "total_batches": total_batches,
            "batch_size": BATCH_SIZE,
            "updated": total_updated,
            "errors": total_errors,
            "batches": batch_results
        }

    except Exception as e:
        LOG.error(f"Domain name update failed: {e}")
        LOG.error(f"Error details: {traceback.format_exc()}")
        return {"status": "error", "message": str(e)}
def _commit_ticker_csv_to_github(csv_file: str, batch_num: int) -> bool:
    """Helper function to commit ticker CSV to GitHub during batch processing.

    Args:
        csv_file: Name of the CSV file (e.g., "ticker_reference_1.csv")
        batch_num: Current batch number for logging

    Returns:
        True if commit succeeded, False otherwise
    """
    try:
        csv_path = os.path.join("data", csv_file)

        # Read CSV content
        with open(csv_path, 'r', encoding='utf-8') as f:
            csv_content = f.read()

        # Prepare commit message
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
        commit_message = f"[skip render] Ticker metadata batch update - {timestamp}"

        # Check GitHub config
        if not GITHUB_TOKEN or not GITHUB_REPO:
            LOG.error("GitHub integration not configured")
            return False

        # GitHub API setup
        github_path = f"data/{csv_file}"
        api_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{github_path}"

        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28"
        }

        # Get current file SHA
        response = requests.get(api_url, headers=headers, timeout=30)

        file_sha = None
        if response.status_code == 200:
            current_file = response.json()
            file_sha = current_file["sha"]
        elif response.status_code != 404:
            LOG.error(f"Failed to get file SHA: {response.status_code}")
            return False

        # Encode and commit
        encoded_content = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')

        commit_data = {
            "message": commit_message,
            "content": encoded_content,
        }

        if file_sha:
            commit_data["sha"] = file_sha

        commit_response = requests.put(api_url, headers=headers, json=commit_data, timeout=120)

        if commit_response.status_code in [200, 201]:
            LOG.info(f"âœ… Successfully committed batch {batch_num} to GitHub")
            return True
        else:
            LOG.error(f"GitHub commit failed: {commit_response.status_code}")
            return False

    except Exception as e:
        LOG.error(f"Exception during GitHub commit: {e}")
        return False

@APP.post("/admin/update-ticker-metadata-csv")
async def update_ticker_metadata_csv(request: Request):
    """Batch update ticker metadata using Gemini 2.5 Pro"""
    require_admin(request)

    try:
        body = await request.json()
        csv_file = body.get("csv_file", "ticker_reference_1.csv")
        batch_size = body.get("batch_size", 5)
        max_batches = body.get("max_batches")  # Optional - for testing
        commit_frequency = body.get("commit_frequency", 20)  # Commit every N batches

        LOG.info(f"=== BATCH UPDATING TICKER METADATA (Gemini 2.5 Pro) ===")
        LOG.info(f"CSV File: {csv_file}")
        LOG.info(f"Batch Size: {batch_size}")
        LOG.info(f"Auto-commit frequency: Every {commit_frequency} batches")
        if max_batches:
            LOG.info(f"Test Mode: Processing first {max_batches} batches only")

        # Read CSV file
        csv_path = os.path.join("data", csv_file)
        if not os.path.exists(csv_path):
            return {"status": "error", "message": f"CSV file not found: {csv_path}"}

        # Read all rows
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            all_rows = list(reader)
            fieldnames = reader.fieldnames

        LOG.info(f"Loaded {len(all_rows)} rows from CSV")

        # Filter rows that need processing (industry_keyword_1 is empty)
        rows_to_process = [
            row for row in all_rows
            if not row.get('industry_keyword_1') or row.get('industry_keyword_1').strip() == ''
        ]

        total_to_process = len(rows_to_process)
        LOG.info(f"Found {total_to_process} rows needing metadata")

        if total_to_process == 0:
            return {"status": "success", "message": "All rows already have metadata"}

        if not GEMINI_API_KEY:
            return {"status": "error", "message": "GEMINI_API_KEY not configured"}

        # Configure Gemini
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.5-pro')

        # Calculate batches
        total_batches = (total_to_process + batch_size - 1) // batch_size
        if max_batches:
            total_batches = min(total_batches, max_batches)

        total_updated = 0
        total_errors = 0
        batch_results = []

        import json
        import re
        from datetime import datetime

        # Full comprehensive Claude metadata prompt (proven and comprehensive)
        system_prompt = """You are a financial analyst creating metadata for a hedge fund's stock monitoring system. Generate precise, actionable metadata that will be used for news article filtering and triage.

CRITICAL REQUIREMENTS:
- All competitors and value chain companies must be currently publicly traded with valid ticker symbols
- Fundamental driver keywords must capture QUANTIFIABLE market forces that move the stock (NOT industry labels)
- Industry keywords MUST be in Title Case (e.g., "Loan Growth", "EV Adoption", "Copper Prices")
- Benchmarks must be sector-specific, not generic market indices
- All information must be factually accurate
- The company name MUST be the official legal name (e.g., "Prologis Inc" not "PLD")
- If any field is unknown, output an empty array for lists and omit optional fields

TICKER FORMAT REQUIREMENTS:
- US companies: Use simple ticker (AAPL, MSFT)
- Canadian companies: Use .TO suffix (RY.TO, TD.TO, BMO.TO)
- UK companies: Use .L suffix (BP.L, VOD.L)
- Australian companies: Use .AX suffix (BHP.AX, CBA.AX)
- Other international: Use appropriate Yahoo Finance suffix
- Special classes: Use dash format (BRK-A, BRK-B, TECK-A.TO)

FUNDAMENTAL DRIVER KEYWORDS (exactly 3):
â­ **CRITICAL SHIFT:** Generate keywords that track EXTERNAL market forces that drive stock performance, NOT industry category labels.

**What Are Fundamental Drivers?**
Quantifiable metrics that:
1. Directly impact revenue, costs, or margins (10%+ move affects earnings)
2. Are reported with numbers in news (prices, volumes, percentages)
3. Change frequently (weekly/monthly volatility)
4. Are EXTERNAL to company (market-level, not company-specific)
5. Traders actively monitor (Bloomberg KPIs, analyst models)

**Scope Rules:**
âŒ NO company-specific: "Apple iPhone sales", "Tesla deliveries", "Exxon production"
âœ… YES category-specific: "smartphone sales", "electric vehicle sales", "oil production"
âœ… YES niche-specific if dominates: "CPAP device sales" (ResMed), "GLP-1 drug demand" (Novo)
âŒ NO industry labels: "Technology Sector", "Copper Mining", "Banking Industry"
âœ… YES market drivers: "cloud infrastructure spending", "copper price", "interest rates"

**Format:**
- Use 2-4 words maximum
- Lowercase for commodities/rates: "copper price", "jet fuel prices", "bitcoin price"
- Title Case for categories: "Enterprise Software Spending", "Auto Sales"
- Phrase as journalist writes headlines: "bitcoin price" not "BTC/USD"

**Framework by Business Type:**

COMMODITY PRODUCERS (mining, oil, agriculture):
â€¢ Copper miners (FCX): ["copper price", "China construction", "copper supply"]
â€¢ Oil producers (XOM): ["oil price", "refining margins", "natural gas price"]
â€¢ Gold miners (NEM): ["gold price", "mining costs", "central bank policy"]

CONSUMER (retail, restaurants, apparel):
â€¢ Apparel (NKE): ["athletic footwear sales", "China consumer spending", "cotton prices"]
â€¢ Restaurants (MCD): ["restaurant traffic", "food commodity prices", "labor costs"]
â€¢ E-commerce (AMZN): ["e-commerce sales", "consumer spending", "shipping costs"]

TRANSPORTATION (airlines, rails, trucking):
â€¢ Airlines (DAL): ["airline passenger demand", "jet fuel prices", "airline capacity"]
â€¢ Rail (UNP): ["rail freight volumes", "diesel fuel prices", "intermodal shipping"]

TECHNOLOGY (hardware, software, semiconductors):
â€¢ Cloud (MSFT): ["cloud infrastructure spending", "enterprise IT budgets", "AI infrastructure demand"]
â€¢ Semiconductors (NVDA): ["GPU demand", "AI chip demand", "data center spending"]
â€¢ Internet (META): ["digital advertising spending", "social media ad rates", "e-commerce advertising"]

FINANCIALS (banks, asset managers, insurance):
â€¢ Banks (JPM): ["interest rates", "loan demand", "credit quality"]
â€¢ Asset managers (BLK): ["asset management flows", "equity market performance", "ETF flows"]
â€¢ Insurance (PGR): ["insurance premiums", "catastrophic weather events", "investment yields"]

UTILITIES (electric, gas, renewables):
â€¢ Regulated (SO): ["electricity demand", "natural gas prices", "utility rate cases"]
â€¢ Independent power (VST): ["power prices", "natural gas prices", "electricity demand"]

HEALTHCARE (pharma, biotech, devices):
â€¢ Pharma (PFE): ["drug pricing policy", "prescription volumes", "patent expiration"]
â€¢ Devices (MDT): ["medical device sales", "cardiology procedures", "hospital capital spending"]

REAL ESTATE (REITs):
â€¢ Industrial (PLD): ["industrial real estate demand", "warehouse vacancy rates", "e-commerce logistics"]
â€¢ Data centers (EQIX): ["data center demand", "cloud infrastructure spending", "AI data center requirements"]

INDUSTRIALS (aerospace, defense, automation):
â€¢ Aerospace (BA): ["aircraft orders", "defense spending", "commercial aviation demand"]
â€¢ Construction (CAT): ["construction activity", "equipment rental rates", "infrastructure spending"]

**Special Cases:**

DIVERSIFIED COMPANIES - Allocate by profit contribution:
â€¢ Amazon (AWS 60% profit): ["cloud infrastructure spending", "e-commerce sales", "digital advertising spending"]
â€¢ Alphabet (Ads 80%): ["digital advertising spending", "search advertising rates", "cloud infrastructure growth"]

COMMODITY PROCESSORS - Focus on SPREADS not absolute prices:
â€¢ Oil refiners (VLO): ["refining margins", "gasoline demand", "crude oil prices"]

CYCLICALS - Lead with demand, then financing/inputs:
â€¢ Homebuilders (DHI): ["housing starts", "mortgage rates", "lumber prices"]
â€¢ Autos (GM): ["auto sales", "EV adoption", "semiconductor supply"]

REGULATORY-DRIVEN - If >30% value from policy:
â€¢ Pharma pricing risk: ["drug pricing policy", "prescription volumes", "biosimilar competition"]

NICHE LEADERS - If >70% market share, allow category-specific:
â€¢ ResMed: ["sleep apnea treatment", "CPAP reimbursement", "respiratory device sales"]

**Validation (Score 0-10):**
1. Headline Frequency: Daily (10), weekly (7-9), monthly (4-6), rare (0-3)
2. Quantifiable: Always has figures (10), usually (7-9), sometimes (4-6), rarely (0-3)
3. Stock Impact: 10% move = >10% EPS (10), 5-10% EPS (7-9), 2-5% EPS (4-6), <2% EPS (0-3)
4. Trader Monitoring: Core KPI (10), important (7-9), secondary (4-6), ignored (0-3)

Examples:
âœ… "copper price" (FCX): 40/40 excellent
âŒ "mining technology": 12/40 reject (use "copper supply")
âœ… "cloud infrastructure spending" (MSFT): 37/40 excellent

**Common Mistakes:**
âŒ "Copper Mining" â†’ âœ… "copper price"
âŒ "Apple iPhone sales" â†’ âœ… "smartphone sales"
âŒ "Supply Chain" â†’ âœ… "semiconductor supply"
âŒ "Battery Technology" â†’ âœ… "electric vehicle sales"

BUSINESS STRUCTURE GUIDANCE:

For MOST companies (single core business):
- Standard approach: 3 keywords for primary business
- Example: Netflix â†’ ["streaming subscriber growth", "content spending", "streaming competition"]

For DIVERSIFIED companies (2-3 major business lines):
- Keywords should cover top 2-3 revenue segments (prioritize profit contributors)
- Horizontal competitors can be a MIX across segments (no single competitor may compete in all areas)
- Example: Amazon â†’ Keywords: ["cloud infrastructure spending", "e-commerce sales", "digital advertising spending"]
  Horizontal Competitors: Walmart (retail), Microsoft (cloud), Shopify (platform)

For CONGLOMERATES (Berkshire, 3M, Honeywell):
- Focus on 3 largest operating segments OR conglomerate-level themes
- Competitors: Other diversified industrials/conglomerates
- Example: Berkshire â†’ ["Property Casualty Insurance", "Railroad Operations", "Diversified Holdings"]
  Competitors: 3M, Honeywell, Danaher

For REGIONAL/GEOGRAPHIC companies (utilities, Canadian banks):
- Competitors MUST be in same primary market/regulatory environment
- Example: RY.TO â†’ Competitors: TD.TO, BMO.TO, BNS.TO (NOT JPM, BAC)
- Example: Southern Company â†’ Competitors: Duke Energy, Dominion, Entergy (all Southeast regulated utilities)

HORIZONTAL COMPETITORS (0-3):
Select direct business competitors competing for the same customers in the same markets.

**Selection Criteria:**
- Must compete in SAME CUSTOMER SEGMENT and price point (not just same industry)
- GEOGRAPHIC PRIORITY: For regional companies, prioritize competitors in same primary market
- SCALE MATCHING: Select competitors of comparable size to ensure news about competitor materially affects target company
  - For mega-caps ($50B+): Prefer competitors >$5B market cap
  - For large-caps ($10-50B): Prefer competitors >$2B market cap
  - For mid-caps ($2-10B): Prefer competitors >$500M market cap
  - For small-caps (<$2B): More flexibility on scale (limited peer set)
- INFORMATION VALUE: Prioritize competitors whose news provides actionable signals
  - Prefer competitors that institutional analysts actively cover alongside target company (cross-read for sector insights)
  - Prefer sector bellwethers whose results/guidance signal broader market trends
  - Prefer competitors with higher disclosure frequency or newsflow (quarterly reports, production updates, operational KPIs)
  - If choosing between similar competitors, select the one with more predictive value for target company's performance
- Prioritize in order: (1) Direct peer (similar scale/model), (2) Adjacent competitor, (3) Bellwether (larger sector leader)
- Return 0-3 based on availability - quality over quantity
- Strongly prefer publicly traded companies with valid tickers
- Only include private if industry-dominant with significant newsflow (â‰¥5 major articles/month)
- Exclude: Subsidiaries, companies that do not currently trade independently on major stock exchanges, companies in active bankruptcy proceedings

**Bad match examples:**
âŒ AvalonBay (luxury apartments) for ELME (value-add apartments) - different customer segment
âŒ IBM (enterprise conglomerate) for D-Wave (quantum startup) - different scale/focus
âŒ JPMorgan (universal bank) for Robinhood (retail trading) - different customer demographic

**Format:**
[
  {"name": "Company Name", "ticker": "TICKER"},
  {"name": "Company Name", "ticker": "TICKER"},
  {"name": "Company Name", "ticker": "TICKER"}
]

VALUE CHAIN (0-2 upstream, 0-2 downstream):
Select suppliers (upstream) and/or customers (downstream) that provide material intelligence signals.

**UPSTREAM (0-2 suppliers/input providers):**
Include if:
- Represents >10% of COGS, OR
- CRITICAL ENABLER: Sole-source/oligopoly (â‰¤3 global suppliers), would halt production if lost, specialized technology/patents (e.g., TSMC for Intel, ASML for semiconductor manufacturers, rare earth suppliers for EVs)

Rank by materiality (highest % of COGS first):
- Slot 1: Most material supplier
- Slot 2: Second most material supplier

**DOWNSTREAM (0-2 buyers/customers):**
Include if:
- Represents >5% of revenue, OR
- Provides demand signal proxy for market trends

Rank by materiality (highest % of revenue first):
- Slot 1: Most material customer
- Slot 2: Second most material customer

**General Rules:**
- Strongly prefer publicly traded companies (only private if â‰¥5 major articles/month or industry-dominant with pricing power)
- Geographic flexibility (can be national/international, unlike horizontal competitors)
- Exclude: Subsidiaries, companies acquired in last 2 years, companies already listed in horizontal competitors
- For conglomerates: Prioritize by absolute materiality across all segments (don't force coverage of every segment)
- Return empty arrays if no material value chain exists (direct-to-consumer brands, fragmented supply/customer base)

**Format:**
{
  "upstream": [
    {"name": "Supplier Name", "ticker": "TICKER"},
    {"name": "Supplier Name", "ticker": "TICKER"}
  ],
  "downstream": [
    {"name": "Customer Name", "ticker": "TICKER"},
    {"name": "Customer Name", "ticker": "TICKER"}
  ]
}

**Examples:**

Tesla (manufacturer with supplier dependency):
{
  "horizontal_competitors": [
    {"name": "BYD Company", "ticker": "BYDDY"},
    {"name": "Ford Motor", "ticker": "F"},
    {"name": "General Motors", "ticker": "GM"}
  ],
  "value_chain": {
    "upstream": [
      {"name": "Panasonic", "ticker": "PCRFY"},
      {"name": "CATL", "ticker": "300750.SZ"}
    ],
    "downstream": []
  },
  "industry_keywords": ["electric vehicle sales", "battery costs", "EV regulations"]
}

Intel (B2B component with major customers):
{
  "horizontal_competitors": [
    {"name": "AMD", "ticker": "AMD"},
    {"name": "NVIDIA", "ticker": "NVDA"},
    {"name": "Qualcomm", "ticker": "QCOM"}
  ],
  "value_chain": {
    "upstream": [
      {"name": "TSMC", "ticker": "TSM"}
    ],
    "downstream": [
      {"name": "Microsoft", "ticker": "MSFT"},
      {"name": "Apple", "ticker": "AAPL"}
    ]
  },
  "industry_keywords": ["x86 CPU demand", "semiconductor manufacturing capacity", "data center chip sales"]
}

Denison Mines (commodity producer):
{
  "horizontal_competitors": [
    {"name": "Cameco", "ticker": "CCJ"},
    {"name": "NexGen Energy", "ticker": "NXE"},
    {"name": "Energy Fuels", "ticker": "UUUU"}
  ],
  "value_chain": {
    "upstream": [],
    "downstream": [
      {"name": "Centrus Energy", "ticker": "LEU"},
      {"name": "Constellation Energy", "ticker": "CEG"}
    ]
  },
  "industry_keywords": ["uranium price", "nuclear power demand", "uranium supply"]
}

Netflix (direct-to-consumer, no value chain):
{
  "horizontal_competitors": [
    {"name": "Disney", "ticker": "DIS"},
    {"name": "Warner Bros Discovery", "ticker": "WBD"},
    {"name": "Paramount Global", "ticker": "PARA"}
  ],
  "value_chain": {
    "upstream": [],
    "downstream": []
  },
  "industry_keywords": ["streaming subscriber growth", "content spending", "streaming competition"]
}

GEOGRAPHIC MARKETS (string format):
- List the primary countries/regions where the company has significant operations, revenue, or customers
- Format: "Region1 (major), Region2 (major), Region3 (minor)"
- Use parenthetical notes to indicate scale: (major), (major), (minor)
- Be specific for major markets, broader for minor markets
- Examples:
  â†’ EchoStar: "United States (major), Europe (minor), Latin America (minor)"
  â†’ Royal Bank: "Canada (major), United States (major), Caribbean (minor)"
  â†’ Apple: "United States (major), China (major), Europe (major)"
  â†’ Regional utility: "United States - Southeast (major)"
- If truly global with balanced presence: "Global operations"
- If unknown or insufficient information: ""

SUBSIDIARIES (string format):
- List up to 3 major operating subsidiaries or business units
- Format: "Subsidiary Name 1, Subsidiary Name 2, Subsidiary Name 3"
- Use COMMON/BRAND names, not legal entities (e.g., "Hughes Network Systems" not "Hughes Network Systems LLC")
- Include only material subsidiaries (significant revenue/operations)
- Exclude: Recently acquired companies, minor divisions, brands (brands go in aliases_brands_assets)
- Examples:
  â†’ EchoStar: "Hughes Network Systems, Viasat Inc., EchoStar Mobile"
  â†’ Berkshire Hathaway: "GEICO, BNSF Railway, Berkshire Hathaway Energy"
  â†’ Alphabet: "Google, YouTube, Waymo"
  â†’ JPMorgan: "Chase Bank, J.P. Morgan Wealth Management, J.P. Morgan Securities"
- If no significant subsidiaries or holding company structure unclear: ""

For EACH ticker in the batch, return a JSON object following this exact format:
{
    "ticker": "TICKER",
    "company_name": "Official Company Name",
    "sector": "GICS Sector",
    "industry": "GICS Industry",
    "sub_industry": "GICS Sub-Industry",
    "industry_keywords": ["keyword1", "keyword2", "keyword3"],
    "horizontal_competitors": [
        {"name": "Company Name", "ticker": "TICKER"},
        {"name": "Company Name", "ticker": "TICKER"},
        {"name": "Company Name", "ticker": "TICKER"}
    ],
    "value_chain": {
        "upstream": [
            {"name": "Supplier Name", "ticker": "TICKER"},
            {"name": "Supplier Name", "ticker": "TICKER"}
        ],
        "downstream": [
            {"name": "Customer Name", "ticker": "TICKER"},
            {"name": "Customer Name", "ticker": "TICKER"}
        ]
    },
    "sector_profile": {
        "core_inputs": ["input1", "input2", "input3"],
        "core_channels": ["channel1", "channel2", "channel3"],
        "core_geos": ["geo1", "geo2", "geo3"],
        "benchmarks": ["benchmark1", "benchmark2", "benchmark3"]
    },
    "aliases_brands_assets": {
        "aliases": ["alias1", "alias2", "alias3"],
        "brands": ["brand1", "brand2", "brand3"],
        "assets": ["asset1", "asset2", "asset3"]
    },
    "geographic_markets": "Region1 (major), Region2 (major), Region3 (minor)",
    "subsidiaries": "Subsidiary Name 1, Subsidiary Name 2, Subsidiary Name 3"
}

IMPORTANT: Return response as a JSON ARRAY containing one object per ticker. Do NOT wrap in markdown code blocks. Return ONLY the JSON array."""


        for batch_num in range(total_batches):
            batch_start = batch_num * batch_size
            batch_end = min(batch_start + batch_size, total_to_process)
            batch_rows = rows_to_process[batch_start:batch_end]

            LOG.info(f"=== Processing Batch {batch_num + 1}/{total_batches} ({len(batch_rows)} tickers) ===")

            # Build prompt with ticker list
            ticker_info = []
            for row in batch_rows:
                ticker = row['ticker']
                company_name = row.get('company_name', ticker)
                industry = row.get('industry', '')
                sector = row.get('sector', '')
                ticker_info.append(f"- {ticker}: {company_name} (Industry: {industry}, Sector: {sector})")

            ticker_list = "\n".join(ticker_info)

            user_prompt = f"""Generate metadata for these {len(batch_rows)} tickers:

{ticker_list}

Return ONLY a valid JSON array with metadata for each ticker."""

            # Call Gemini API
            try:
                start_time = datetime.now()

                generation_config = {
                    "temperature": 0.0,
                    "max_output_tokens": 32768
                }

                # Safety settings
                safety_settings = {
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }

                response = model.generate_content(
                    f"{system_prompt}\n\n{user_prompt}",
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )

                end_time = datetime.now()
                generation_time = (end_time - start_time).total_seconds()

                # Check finish_reason
                if not response.candidates:
                    LOG.error(f"Batch {batch_num + 1}: No candidates in response")
                    raise ValueError("No candidates returned by Gemini")

                finish_reason = response.candidates[0].finish_reason
                if finish_reason != 1:
                    LOG.error(f"Batch {batch_num + 1}: Response blocked with finish_reason={finish_reason}")
                    raise ValueError(f"Response blocked (finish_reason={finish_reason})")

                response_text = response.text
                LOG.info(f"Batch {batch_num + 1}: Gemini response received ({generation_time:.1f}s)")

                # Track API cost (metadata generation uses Gemini Pro)
                if hasattr(response, 'usage_metadata'):
                    usage = {
                        "input_tokens": response.usage_metadata.prompt_token_count,
                        "output_tokens": response.usage_metadata.candidates_token_count
                    }
                    calculate_gemini_api_cost(usage, "ticker_metadata_generation", model="pro")

                # Extract JSON from response
                json_content = None

                # Try to extract JSON array from code block first
                if '```json' in response_text:
                    json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
                    if json_match:
                        json_content = json_match.group(1).strip()
                        LOG.info(f"Batch {batch_num + 1}: Extracted JSON from code block")

                # Fallback: try to find raw JSON array
                if not json_content and '[' in response_text:
                    json_match = re.search(r'\[[\s\S]*\]', response_text)
                    if json_match:
                        json_content = json_match.group(0).strip()
                        LOG.info(f"Batch {batch_num + 1}: Extracted raw JSON array")

                if not json_content:
                    LOG.error(f"Batch {batch_num + 1}: No valid JSON content extracted")
                    batch_results.append({
                        "batch": batch_num + 1,
                        "status": "error",
                        "message": "Could not extract JSON from Gemini response",
                        "tickers_processed": 0
                    })
                    total_errors += len(batch_rows)
                    continue

                # Parse JSON array
                try:
                    metadata_array = json.loads(json_content)
                except json.JSONDecodeError as e:
                    LOG.error(f"Batch {batch_num + 1}: JSON parsing failed: {e}")
                    batch_results.append({
                        "batch": batch_num + 1,
                        "status": "error",
                        "message": f"Invalid JSON: {str(e)}",
                        "tickers_processed": 0
                    })
                    total_errors += len(batch_rows)
                    continue

                # Update CSV rows with metadata
                batch_updated = 0
                batch_errors = 0

                for i, row in enumerate(batch_rows):
                    ticker = row['ticker']

                    # Find matching metadata in response
                    metadata = None
                    for meta in metadata_array:
                        if meta.get('ticker') == ticker:
                            metadata = meta
                            break

                    if not metadata:
                        LOG.warning(f"Missing metadata for: {ticker}")
                        batch_errors += 1
                        continue

                    # Update row with metadata (comprehensive fields from Claude prompt)

                    # Basic company info
                    if metadata.get('company_name'):
                        row['company_name'] = metadata['company_name']
                    if metadata.get('sector'):
                        row['sector'] = metadata['sector']
                    if metadata.get('industry'):
                        row['industry'] = metadata['industry']
                    if metadata.get('sub_industry'):
                        row['sub_industry'] = metadata['sub_industry']

                    # Industry keywords (fundamental drivers)
                    keywords = metadata.get('industry_keywords', [])
                    if keywords and len(keywords) > 0:
                        row['industry_keyword_1'] = keywords[0] if len(keywords) > 0 else ''
                        row['industry_keyword_2'] = keywords[1] if len(keywords) > 1 else ''
                        row['industry_keyword_3'] = keywords[2] if len(keywords) > 2 else ''

                    # Horizontal competitors
                    competitors = metadata.get('horizontal_competitors', [])
                    if competitors and len(competitors) > 0:
                        if len(competitors) > 0:
                            row['competitor_1_name'] = competitors[0].get('name', '')
                            row['competitor_1_ticker'] = competitors[0].get('ticker', '')
                        if len(competitors) > 1:
                            row['competitor_2_name'] = competitors[1].get('name', '')
                            row['competitor_2_ticker'] = competitors[1].get('ticker', '')
                        if len(competitors) > 2:
                            row['competitor_3_name'] = competitors[2].get('name', '')
                            row['competitor_3_ticker'] = competitors[2].get('ticker', '')

                    # Value chain (upstream/downstream)
                    value_chain = metadata.get('value_chain', {})
                    upstream = value_chain.get('upstream', [])
                    downstream = value_chain.get('downstream', [])

                    if upstream and len(upstream) > 0:
                        if len(upstream) > 0:
                            row['upstream_1_name'] = upstream[0].get('name', '')
                            row['upstream_1_ticker'] = upstream[0].get('ticker', '')
                        if len(upstream) > 1:
                            row['upstream_2_name'] = upstream[1].get('name', '')
                            row['upstream_2_ticker'] = upstream[1].get('ticker', '')

                    if downstream and len(downstream) > 0:
                        if len(downstream) > 0:
                            row['downstream_1_name'] = downstream[0].get('name', '')
                            row['downstream_1_ticker'] = downstream[0].get('ticker', '')
                        if len(downstream) > 1:
                            row['downstream_2_name'] = downstream[1].get('name', '')
                            row['downstream_2_ticker'] = downstream[1].get('ticker', '')

                    # Geographic markets and subsidiaries
                    row['geographic_markets'] = metadata.get('geographic_markets', '')
                    row['subsidiaries'] = metadata.get('subsidiaries', '')

                    # NOTE: sector_profile and aliases_brands_assets fields are generated by
                    # the comprehensive prompt but not stored in CSV (no columns for them)

                    # Tracking fields
                    row['ai_generated'] = 'TRUE'
                    row['ai_enhanced_at'] = datetime.now().isoformat()

                    batch_updated += 1

                # Write entire CSV to disk after each batch (enables resume!)
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_rows)

                total_updated += batch_updated
                total_errors += batch_errors

                batch_results.append({
                    "batch": batch_num + 1,
                    "status": "success",
                    "updated": batch_updated,
                    "errors": batch_errors,
                    "generation_time": f"{generation_time:.1f}s"
                })

                LOG.info(f"âœ… Batch {batch_num + 1}/{total_batches} complete: {batch_updated} updated, {batch_errors} errors")
                LOG.info(f"   CSV saved to disk (resume-safe)")

                # Auto-commit to GitHub every N batches
                if (batch_num + 1) % commit_frequency == 0:
                    LOG.info(f"ðŸ”„ Auto-committing batch {batch_num + 1} to GitHub...")
                    commit_start = datetime.now()
                    commit_success = _commit_ticker_csv_to_github(csv_file, batch_num + 1)
                    commit_duration = (datetime.now() - commit_start).total_seconds()

                    if commit_success:
                        LOG.info(f"   âœ… Commit successful (took {commit_duration:.1f}s)")
                    else:
                        LOG.warning(f"   âš ï¸ Commit failed (continuing anyway, took {commit_duration:.1f}s)")

            except Exception as batch_error:
                LOG.error(f"Batch {batch_num + 1} failed: {batch_error}")
                LOG.error(f"   Stacktrace: {traceback.format_exc()}")
                batch_results.append({
                    "batch": batch_num + 1,
                    "status": "error",
                    "message": str(batch_error),
                    "tickers_processed": 0
                })
                total_errors += len(batch_rows)

        # Final commit to ensure all changes are saved to GitHub
        LOG.info(f"ðŸ”„ Final commit to GitHub...")
        final_commit_start = datetime.now()
        final_commit_success = _commit_ticker_csv_to_github(csv_file, total_batches)
        final_commit_duration = (datetime.now() - final_commit_start).total_seconds()

        if final_commit_success:
            LOG.info(f"âœ… Final commit successful (took {final_commit_duration:.1f}s)")
        else:
            LOG.warning(f"âš ï¸ Final commit failed (took {final_commit_duration:.1f}s)")

        LOG.info(f"=== UPDATE COMPLETE ===")
        LOG.info(f"Total processed: {total_to_process}")
        LOG.info(f"Updated: {total_updated}")
        LOG.info(f"Errors: {total_errors}")

        return {
            "status": "success",
            "csv_file": csv_file,
            "total_to_process": total_to_process,
            "total_batches": total_batches,
            "batch_size": batch_size,
            "updated": total_updated,
            "errors": total_errors,
            "batches": batch_results
        }

    except Exception as e:
        LOG.error(f"Ticker metadata update failed: {e}")
        LOG.error(f"Error details: {traceback.format_exc()}")
        return {"status": "error", "message": str(e)}

@APP.post("/admin/commit-ticker-csv")
async def commit_ticker_csv(request: Request):
    """Commit ticker_reference_1.csv to GitHub (preserves work-in-progress)"""
    require_admin(request)

    try:
        body = await request.json()
        csv_file = body.get("csv_file", "ticker_reference_1.csv")
        skip_render = body.get("skip_render", True)  # Default to skip render

        LOG.info(f"=== COMMITTING {csv_file} TO GITHUB ===")

        # Read CSV file from disk
        csv_path = os.path.join("data", csv_file)
        if not os.path.exists(csv_path):
            return {"status": "error", "message": f"CSV file not found: {csv_path}"}

        with open(csv_path, 'r', encoding='utf-8') as f:
            csv_content = f.read()

        LOG.info(f"Read {len(csv_content)} characters from {csv_path}")

        # Prepare commit message
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
        skip_prefix = "[skip render] " if skip_render else ""
        commit_message = f"{skip_prefix}Ticker metadata batch update - {timestamp}"

        # Use GitHub API to commit (similar to commit_csv_to_github but for ticker_reference_1.csv)
        if not GITHUB_TOKEN or not GITHUB_REPO:
            return {
                "status": "error",
                "message": "GitHub integration not configured"
            }

        # GitHub API URL for this specific file
        github_path = f"data/{csv_file}"
        api_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{github_path}"

        headers = {
            "Authorization": f"Bearer {GITHUB_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28"
        }

        # Get current file SHA
        LOG.info(f"Getting current file SHA from GitHub: {github_path}")
        response = requests.get(api_url, headers=headers, timeout=30)

        file_sha = None
        if response.status_code == 200:
            current_file = response.json()
            file_sha = current_file["sha"]
            LOG.info(f"Found existing file, SHA: {file_sha[:8]}")
        elif response.status_code == 404:
            LOG.info("File doesn't exist on GitHub, will create new file")
        else:
            return {
                "status": "error",
                "message": f"Failed to get current file info: {response.status_code}"
            }

        # Encode content to base64
        encoded_content = base64.b64encode(csv_content.encode('utf-8')).decode('utf-8')

        commit_data = {
            "message": commit_message,
            "content": encoded_content,
        }

        if file_sha:
            commit_data["sha"] = file_sha

        # Commit to GitHub
        LOG.info(f"Committing to GitHub: {commit_message}")
        commit_response = requests.put(api_url, headers=headers, json=commit_data, timeout=120)

        if commit_response.status_code in [200, 201]:
            LOG.info(f"âœ… Successfully committed {csv_file} to GitHub")
            return {
                "status": "success",
                "message": f"Committed {csv_file} to GitHub",
                "commit_message": commit_message,
                "github_path": github_path,
                "skip_render": skip_render
            }
        else:
            LOG.error(f"GitHub commit failed: {commit_response.status_code} - {commit_response.text}")
            return {
                "status": "error",
                "message": f"GitHub commit failed: {commit_response.status_code}"
            }

    except Exception as e:
        LOG.error(f"Commit ticker CSV failed: {e}")
        LOG.error(f"Error details: {traceback.format_exc()}")
        return {"status": "error", "message": str(e)}

# DISABLED (Nov 30, 2025): CSV is source of truth - never write DB back to ticker_reference.csv
# @APP.post("/admin/safe-incremental-commit")
# async def safe_incremental_commit(request: Request, body: UpdateTickersRequest):
#     """Safely commit individual tickers as they complete processing"""
#     require_admin(request)
#
#     if not body.tickers:
#         return {"status": "error", "message": "No tickers specified"}
#
#     LOG.info(f"=== SAFE INCREMENTAL COMMIT: {len(body.tickers)} TICKERS ===")
#
#     try:
#         # Step 1: Verify all tickers have AI-generated metadata
#         with db() as conn, conn.cursor() as cur:
#             cur.execute("""
#                 SELECT ticker, ai_generated, industry_keyword_1, competitor_1_name,
#                        ai_enhanced_at, updated_at
#                 FROM ticker_reference
#                 WHERE ticker = ANY(%s)
#                 ORDER BY ticker
#             """, (body.tickers,))
#
#             ticker_status = {}
#             for row in cur.fetchall():
#                 ticker = row["ticker"]
#                 has_ai_data = (row["ai_generated"] and
#                              (row["industry_keyword_1"] or row["competitor_1_name"]))
#                 ticker_status[ticker] = {
#                     "has_ai_metadata": has_ai_data,
#                     "ai_enhanced_at": row["ai_enhanced_at"],
#                     "updated_at": row["updated_at"]
#                 }
#
#         # Step 2: Filter to only commit tickers with AI metadata
#         valid_tickers = [t for t, status in ticker_status.items() if status["has_ai_metadata"]]
#         invalid_tickers = [t for t, status in ticker_status.items() if not status["has_ai_metadata"]]
#
#         if not valid_tickers:
#             return {
#                 "status": "no_changes",
#                 "message": "No tickers have AI-generated metadata to commit",
#                 "invalid_tickers": invalid_tickers,
#                 "ticker_status": ticker_status
#             }
#
#         # Step 3: Create backup before commit (include job_id for idempotency)
#         # [skip render] controlled by skip_render flag (default: True)
#         backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         job_id_suffix = f" [job:{body.job_id[:8]}]" if body.job_id else ""
#
#         # Add [skip render] prefix only if skip_render=True
#         skip_prefix = "[skip render] " if body.skip_render else ""
#         commit_message = f"{skip_prefix}Incremental update: {', '.join(valid_tickers)} - {backup_timestamp}{job_id_suffix}"
#
#         if not body.skip_render:
#             LOG.warning(f"âš ï¸ RENDER DEPLOYMENT WILL BE TRIGGERED by this commit")
#             LOG.warning(f"   Commit message: {commit_message}")
#         else:
#             LOG.info(f"âœ… [skip render] enabled - no deployment will be triggered")
#
#         # Step 4: Export and commit with retry logic
#         LOG.info(f"Exporting {len(valid_tickers)} enhanced tickers: {valid_tickers}")
#         export_result = export_ticker_references_to_csv()
#
#         if export_result["status"] != "success":
#             return {
#                 "status": "export_failed",
#                 "message": export_result["message"],
#                 "valid_tickers": valid_tickers,
#                 "invalid_tickers": invalid_tickers
#             }
#
#         LOG.info(f"Committing to GitHub with message: {commit_message}")
#
#         # Wrap GitHub commit in try/except to make it non-fatal
#         try:
#             commit_result = commit_csv_to_github(export_result["csv_content"], commit_message)
#
#             if commit_result["status"] == "success":
#                 # Step 5: Update commit tracking in database (with column existence check)
#                 try:
#                     with db() as conn, conn.cursor() as cur:
#                         # Verify column exists before attempting update (bulletproofing for schema mismatches)
#                         cur.execute("""
#                             SELECT column_name
#                             FROM information_schema.columns
#                             WHERE table_schema = 'public'
#                             AND table_name = 'ticker_reference'
#                             AND column_name = 'last_github_sync'
#                         """)
#
#                         if cur.fetchone():
#                             cur.execute("""
#                                 UPDATE ticker_reference
#                                 SET last_github_sync = %s
#                                 WHERE ticker = ANY(%s)
#                             """, (datetime.now(timezone.utc), valid_tickers))
#
#                             LOG.info(f"âœ… Updated GitHub sync timestamp for {len(valid_tickers)} tickers")
#                         else:
#                             LOG.warning("âš ï¸ Column 'last_github_sync' does not exist in ticker_reference table")
#                             LOG.warning("   CSV committed successfully, but sync timestamp not recorded")
#                             LOG.warning("   Run: ALTER TABLE ticker_reference ADD COLUMN last_github_sync TIMESTAMP;")
#
#                 except Exception as db_error:
#                     LOG.error(f"âš ï¸ Failed to update last_github_sync timestamp: {db_error}")
#                     LOG.warning("   CSV was committed to GitHub successfully, but DB timestamp update failed")
#                     # Don't fail the entire operation - GitHub commit succeeded
#
#         except Exception as commit_error:
#             LOG.error(f"âš ï¸ GitHub commit failed (non-fatal): {commit_error}")
#             commit_result = {
#                 "status": "error",
#                 "message": f"GitHub commit failed: {str(commit_error)}"
#             }
#
#         return {
#             "status": commit_result["status"],
#             "message": f"Successfully committed {len(valid_tickers)} tickers",
#             "committed_tickers": valid_tickers,
#             "skipped_tickers": invalid_tickers,
#             "ticker_status": ticker_status,
#             "export_info": {
#                 "total_tickers_in_csv": export_result["ticker_count"],
#                 "csv_size": len(export_result["csv_content"])
#             },
#             "commit_info": commit_result
#         }
#
#     except Exception as e:
#         LOG.error(f"Safe incremental commit failed: {e}")
#         LOG.error(f"Error details: {traceback.format_exc()}")
#         return {
#             "status": "error",
#             "message": f"Incremental commit failed: {str(e)}",
#             "requested_tickers": body.tickers
#         }

# ------------------------------------------------------------------------------
# ADMIN DASHBOARD ENDPOINTS
# ------------------------------------------------------------------------------

def check_admin_token(token: str) -> bool:
    """Validate admin token from query param or header"""
    admin_token = os.getenv('ADMIN_TOKEN')
    return token == admin_token if admin_token else False

def get_lookback_minutes() -> int:
    """
    DEPRECATED: Use get_daily_lookback_minutes() or get_report_type_and_lookback() instead.

    Get configured lookback window from system_config table.
    Used by production workflows (cron jobs and admin bulk actions).
    Test portal (/admin/test) has separate hardcoded settings.

    Returns:
        int: Lookback window in minutes (default: 1440 = 1 day)
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT value FROM system_config WHERE key = 'lookback_minutes'")
            row = cur.fetchone()
            if row:
                minutes = int(row['value'])  # Fixed: use dict access, not tuple index
                # Validate: must be at least 60 minutes (1 hour)
                if minutes < 60:
                    LOG.warning(f"âš ï¸ Invalid lookback_minutes in system_config: {minutes} < 60, using default 1440")
                    return 1440
                return minutes
            else:
                LOG.warning("âš ï¸ No lookback_minutes in system_config, using default 1440 (1 day)")
                return 1440  # 1 day default
    except Exception as e:
        LOG.error(f"Failed to fetch lookback_minutes: {e}, using default 1440")
        return 1440  # 1 day default


def get_daily_lookback_minutes() -> int:
    """
    Get daily lookback window from system_config (for Tuesday-Sunday reports).

    Returns:
        int: Lookback window in minutes (default: 1440 = 1 day)
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT value FROM system_config WHERE key = 'daily_lookback_minutes'")
            row = cur.fetchone()
            if row:
                minutes = int(row['value'])
                if minutes < 60:
                    LOG.warning(f"âš ï¸ Invalid daily_lookback_minutes: {minutes} < 60, using default 1440")
                    return 1440
                return minutes
            else:
                LOG.warning("âš ï¸ No daily_lookback_minutes in system_config, using default 1440")
                return 1440
    except Exception as e:
        LOG.error(f"Failed to fetch daily_lookback_minutes: {e}, using default 1440")
        return 1440


def get_weekly_lookback_minutes() -> int:
    """
    Get weekly lookback window from system_config (for Monday reports).

    Returns:
        int: Lookback window in minutes (default: 10080 = 7 days)
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT value FROM system_config WHERE key = 'weekly_lookback_minutes'")
            row = cur.fetchone()
            if row:
                minutes = int(row['value'])
                if minutes < 60:
                    LOG.warning(f"âš ï¸ Invalid weekly_lookback_minutes: {minutes} < 60, using default 10080")
                    return 10080
                return minutes
            else:
                LOG.warning("âš ï¸ No weekly_lookback_minutes in system_config, using default 10080")
                return 10080
    except Exception as e:
        LOG.error(f"Failed to fetch weekly_lookback_minutes: {e}, using default 10080")
        return 10080


def get_report_type_and_lookback(force_type: str = None) -> tuple:
    """
    Determine report type based on day of week (or force_type for testing).

    Monday = weekly report (7 days lookback)
    Tuesday-Sunday = daily report (1 day lookback)

    Args:
        force_type: Optional override for testing ('daily' or 'weekly')

    Returns:
        tuple: (report_type, lookback_minutes)
            - report_type: 'daily' or 'weekly'
            - lookback_minutes: int (1440 for daily, 10080 for weekly by default)
    """
    try:
        # Allow test override
        if force_type:
            if force_type == 'weekly':
                return ('weekly', get_weekly_lookback_minutes())
            else:
                return ('daily', get_daily_lookback_minutes())

        # Determine based on day of week
        import pytz
        eastern = pytz.timezone('America/Toronto')
        now = datetime.now(eastern)
        day_of_week = now.weekday()  # 0=Monday, 6=Sunday

        if day_of_week == 0:  # Monday
            return ('weekly', get_weekly_lookback_minutes())
        else:  # Tuesday-Sunday
            return ('daily', get_daily_lookback_minutes())

    except Exception as e:
        LOG.error(f"Failed to determine report type: {e}, defaulting to daily/1440")
        return ('daily', 1440)

def get_phase3_primary_model() -> str:
    """
    Get Phase 3 primary model from system_config.

    Returns:
        'claude' or 'gemini' (defaults to 'claude' if not found)
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT value FROM system_config WHERE key = 'phase3_primary_model'")
            row = cur.fetchone()
            if row:
                value = row['value']
                if value in ['claude', 'gemini']:
                    return value
                else:
                    LOG.warning(f"Invalid phase3_primary_model value: {value}, defaulting to 'claude'")
                    return 'claude'
            else:
                LOG.info("No phase3_primary_model in system_config, using default 'claude'")
                return 'claude'  # Default
    except Exception as e:
        LOG.error(f"Failed to fetch phase3_primary_model: {e}, using default 'claude'")
        return 'claude'  # Default


def is_phase_1_5_enabled() -> bool:
    """
    Check if Phase 1.5 (Known Information Filter) is enabled in system_config.

    When enabled, Phase 1.5 filters out claims that are already known from SEC filings
    before passing to Phase 2 for enrichment. This reduces redundant information
    in the final output.

    Returns:
        True if enabled, False if disabled or on error (fail closed for safety)
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("SELECT value FROM system_config WHERE key = 'phase_1_5_enabled'")
            row = cur.fetchone()
            if row:
                return row['value'].lower() == 'true'
            else:
                LOG.info("No phase_1_5_enabled in system_config, defaulting to False")
                return False  # Default to disabled
    except Exception as e:
        LOG.error(f"Failed to fetch phase_1_5_enabled: {e}, defaulting to False")
        return False  # Fail closed (disabled)


@APP.get("/admin")
def admin_dashboard(request: Request, token: str = Query(...)):
    """Admin dashboard landing page"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/users")
def admin_users_page(request: Request, token: str = Query(...)):
    """Beta user management page"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_users.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/queue")
def admin_queue_page(request: Request, token: str = Query(...)):
    """Email queue management page"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_queue.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/test")
def admin_test_page(request: Request, token: str = Query(...)):
    """Test runner page - web-based version of setup_job_queue.ps1"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_test.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/settings")
def admin_settings_page(request: Request, token: str = Query(...)):
    """Admin settings page - Lookback window and GitHub commit"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_settings.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/schedule")
def admin_schedule_page(request: Request, token: str = Query(...)):
    """Schedule Configuration - Timezone-aware cron scheduling"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_schedule.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/domains")
def admin_domains_page(request: Request, token: str = Query(...)):
    """Domain Analytics - Quality rankings and scrape health"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_domains.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/research")
def admin_research_page(request: Request, token: str = Query(...)):
    """Research Tools - Generate AI summaries of transcripts and press releases"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_research.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/cron")
def admin_cron_page(request: Request, token: str = Query(...)):
    """Cron Jobs - Manually trigger scheduled tasks (for staging/testing)"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_cron.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

@APP.get("/admin/terminal")
def admin_terminal_page(request: Request, token: str = Query(...)):
    """Research Terminal - Ask questions about saved research documents using AI"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    return templates.TemplateResponse("admin_terminal.html", {
        "request": request,
        "token": token,
        "staging_mode": STAGING_MODE
    })

# ------------------------------------------------------------------------------
# CRON JOB API ENDPOINTS (for /admin/cron page)
# ------------------------------------------------------------------------------

@APP.post("/api/cron/cleanup")
async def api_cron_cleanup(request: Request):
    """Manually trigger cleanup_old_queue_entries()"""
    body = await request.json()
    token = body.get('token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.info("ðŸ• [CRON API] Running cleanup via /api/cron/cleanup")
        cleanup_old_queue_entries()
        return {"status": "success", "message": "Cleanup completed"}
    except Exception as e:
        LOG.error(f"âŒ [CRON API] Cleanup failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cron/process")
async def api_cron_process(request: Request):
    """Manually trigger process_daily_workflow()"""
    body = await request.json()
    token = body.get('token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.info("ðŸ• [CRON API] Running process via /api/cron/process")
        process_daily_workflow()
        return {"status": "success", "message": "Process workflow started (check logs for progress)"}
    except Exception as e:
        LOG.error(f"âŒ [CRON API] Process failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cron/send")
async def api_cron_send(request: Request):
    """Manually trigger auto_send_cron_job()"""
    body = await request.json()
    token = body.get('token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.info("ðŸ• [CRON API] Running send via /api/cron/send")
        auto_send_cron_job()
        return {"status": "success", "message": "Send completed"}
    except Exception as e:
        LOG.error(f"âŒ [CRON API] Send failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cron/check-filings")
async def api_cron_check_filings(request: Request):
    """Manually trigger check_all_filings_cron()"""
    body = await request.json()
    token = body.get('token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.info("ðŸ• [CRON API] Running check_filings via /api/cron/check-filings")
        # Run in thread to allow asyncio.run() inside the function
        await asyncio.to_thread(check_all_filings_cron)
        return {"status": "success", "message": "Filings check completed"}
    except Exception as e:
        LOG.error(f"âŒ [CRON API] Check filings failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cron/alerts")
async def api_cron_alerts(request: Request):
    """Manually trigger process_hourly_alerts()"""
    body = await request.json()
    token = body.get('token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.info("ðŸ• [CRON API] Running alerts via /api/cron/alerts")
        # Run in thread to allow asyncio.run() inside the function
        await asyncio.to_thread(process_hourly_alerts)
        return {"status": "success", "message": "Hourly alerts completed"}
    except Exception as e:
        LOG.error(f"âŒ [CRON API] Alerts failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cron/export")
async def api_cron_export(request: Request):
    """Manually trigger export_users_csv()"""
    body = await request.json()
    token = body.get('token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.info("ðŸ• [CRON API] Running export via /api/cron/export")
        export_users_csv()
        return {"status": "success", "message": "Export completed"}
    except Exception as e:
        LOG.error(f"âŒ [CRON API] Export failed: {e}")
        return {"status": "error", "message": str(e)}

# Admin API endpoints
@APP.get("/api/admin/stats")
def get_admin_stats(token: str = Query(...)):
    """Get dashboard stats - uses unified queue logic to match /admin/queue page"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Count pending users (uses new users table)
            cur.execute("SELECT COUNT(*) as count FROM users WHERE status = 'pending'")
            pending_users = cur.fetchone()['count']

            # Count active users (uses new users table)
            cur.execute("SELECT COUNT(*) as count FROM users WHERE status = 'active'")
            active_users = cur.fetchone()['count']

            # Get unified queue status (same logic as /api/queue-status)
            # Query 1: Get active processing jobs (mode='daily')
            cur.execute("""
                SELECT ticker, status
                FROM ticker_processing_jobs
                WHERE config->>'mode' = 'daily'
                AND status IN ('queued', 'processing')
            """)
            active_jobs = cur.fetchall()

            # Query 2: Get email queue
            cur.execute("""
                SELECT ticker, status, sent_at
                FROM email_queue
            """)
            email_queue_rows = cur.fetchall()

            # Build unified status dict (same merging logic as /api/queue-status)
            tickers_dict = {}

            # Add email queue entries
            for row in email_queue_rows:
                tickers_dict[row['ticker']] = row['status']

            # Override with active jobs (processing takes precedence)
            for job in active_jobs:
                tickers_dict[job['ticker']] = 'processing'

            # Count by status
            ready_emails = sum(1 for status in tickers_dict.values() if status == 'ready')

            # Count sent today (from email_queue only)
            cur.execute("SELECT COUNT(*) as count FROM email_queue WHERE status = 'sent' AND sent_at >= CURRENT_DATE")
            sent_today = cur.fetchone()['count']

            return {
                "status": "success",
                "pending_users": pending_users,
                "active_users": active_users,
                "ready_emails": ready_emails,
                "sent_today": sent_today
            }
    except Exception as e:
        LOG.error(f"Failed to get admin stats: {e}")
        return {"status": "error", "message": str(e)}

# ------------------------------------------------------------------------------
# RESEARCH TERMINAL API ENDPOINTS
# ------------------------------------------------------------------------------

@APP.get("/api/admin/research-terminal-tickers")
def get_research_terminal_tickers(token: str = Query(...)):
    """Get tickers that have at least one research document (10-K, 10-Q, Transcript, or 8-K)"""
    if not check_admin_token(token):
        return {"error": "Unauthorized"}

    from modules.research_terminal import get_available_tickers
    return get_available_tickers(db)


@APP.get("/api/admin/research-terminal-documents")
def get_research_terminal_documents(ticker: str = Query(...), token: str = Query(...)):
    """Get available documents for a specific ticker (lazy loading on selection)"""
    if not check_admin_token(token):
        return {"error": "Unauthorized"}

    from modules.research_terminal import build_documents_list
    try:
        documents = build_documents_list(ticker, db)
        return {"ticker": ticker, "documents": documents}
    except Exception as e:
        LOG.error(f"[{ticker}] Failed to get research terminal documents: {e}")
        return {"error": str(e)}


@APP.post("/api/admin/research-terminal-qa")
async def research_terminal_qa(request: Request):
    """Ask a question about research documents for a ticker using Gemini 2.5 Flash"""
    require_admin(request)

    try:
        body = await request.json()
        ticker = body.get("ticker", "").strip().upper()
        question = body.get("question", "").strip()

        if not ticker:
            return {"error": "Ticker is required"}
        if not question:
            return {"error": "Question is required"}

        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not gemini_api_key:
            return {"error": "Gemini API key not configured"}

        from modules.research_terminal import query_research_terminal
        return query_research_terminal(ticker, question, db, gemini_api_key)

    except Exception as e:
        LOG.error(f"Research terminal Q&A failed: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


@APP.get("/api/admin/users")
def get_all_users(token: str = Query(...)):
    """Get all users with their tickers from normalized schema"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get all users
            cur.execute("""
                SELECT id, name, email, user_type, ticker_limit, status,
                       created_at, terms_accepted_at, privacy_accepted_at, cancelled_at
                FROM users
                ORDER BY
                    CASE status
                        WHEN 'pending' THEN 1
                        WHEN 'active' THEN 2
                        WHEN 'paused' THEN 3
                        WHEN 'cancelled' THEN 4
                    END,
                    created_at DESC
            """)
            users = cur.fetchall()

            # Get all tickers grouped by user
            cur.execute("""
                SELECT user_id, ARRAY_AGG(ticker ORDER BY ticker) as tickers
                FROM user_tickers
                GROUP BY user_id
            """)
            ticker_rows = cur.fetchall()
            user_tickers_map = {row['user_id']: row['tickers'] for row in ticker_rows}

            # Combine users with their tickers
            result = []
            for user in users:
                user_dict = dict(user)
                user_dict['tickers'] = user_tickers_map.get(user['id'], [])
                result.append(user_dict)

            return {
                "status": "success",
                "users": result
            }
    except Exception as e:
        LOG.error(f"Failed to get users: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/approve-user")
async def approve_user(request: Request):
    """Approve a pending user by user_id"""
    body = await request.json()
    token = body.get('token')
    user_id = body.get('user_id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_id:
        return {"status": "error", "message": "user_id is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE users
                SET status = 'active', updated_at = NOW()
                WHERE id = %s
                RETURNING id, email, name
            """, (user_id,))
            result = cur.fetchone()
            if result:
                conn.commit()
                LOG.info(f"âœ… Approved user {user_id}: {result['email']}")

                # Send welcome email to newly approved user
                send_welcome_email(
                    user_id=result['id'],
                    name=result['name'],
                    email=result['email']
                )

                return {"status": "success", "message": f"Approved {result['email']}"}
            else:
                return {"status": "error", "message": f"User {user_id} not found"}
    except Exception as e:
        LOG.error(f"Failed to approve user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/block-user")
async def block_user(request: Request):
    """Block a user by user_id - prevents future signups with this email"""
    body = await request.json()
    token = body.get('token')
    user_id = body.get('user_id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_id:
        return {"status": "error", "message": "user_id is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE users
                SET status = 'blocked', updated_at = NOW()
                WHERE id = %s
                RETURNING email
            """, (user_id,))
            result = cur.fetchone()
            if result:
                conn.commit()
                LOG.info(f"ðŸš« Blocked user {user_id}: {result['email']}")
                return {"status": "success", "message": f"Blocked {result['email']}"}
            else:
                return {"status": "error", "message": f"User {user_id} not found"}
    except Exception as e:
        LOG.error(f"Failed to block user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/pause-user")
async def pause_user(request: Request):
    """Pause an active user by user_id"""
    body = await request.json()
    token = body.get('token')
    user_id = body.get('user_id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_id:
        return {"status": "error", "message": "user_id is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE users
                SET status = 'paused', updated_at = NOW()
                WHERE id = %s
                RETURNING email, name
            """, (user_id,))
            result = cur.fetchone()
            if result:
                conn.commit()
                LOG.info(f"â¸ï¸ Paused user {user_id}: {result['email']}")
                return {"status": "success", "message": f"Paused {result['email']}"}
            else:
                return {"status": "error", "message": f"User {user_id} not found"}
    except Exception as e:
        LOG.error(f"Failed to pause user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/cancel-user")
async def cancel_user(request: Request):
    """Cancel a user by user_id"""
    body = await request.json()
    token = body.get('token')
    user_id = body.get('user_id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_id:
        return {"status": "error", "message": "user_id is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE users
                SET status = 'cancelled', updated_at = NOW()
                WHERE id = %s
                RETURNING email, name
            """, (user_id,))
            result = cur.fetchone()
            if result:
                conn.commit()
                LOG.info(f"ðŸ—‘ï¸ Cancelled user {user_id}: {result['email']}")
                return {"status": "success", "message": f"Cancelled {result['email']}"}
            else:
                return {"status": "error", "message": f"User {user_id} not found"}
    except Exception as e:
        LOG.error(f"Failed to cancel user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/reactivate-user")
async def reactivate_user(request: Request):
    """Reactivate a paused or cancelled user by user_id"""
    body = await request.json()
    token = body.get('token')
    user_id = body.get('user_id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_id:
        return {"status": "error", "message": "user_id is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE users
                SET status = 'active', updated_at = NOW()
                WHERE id = %s
                RETURNING email, name
            """, (user_id,))
            result = cur.fetchone()
            if result:
                conn.commit()
                LOG.info(f"â–¶ï¸ Reactivated user {user_id}: {result['email']}")
                return {"status": "success", "message": f"Reactivated {result['email']}"}
            else:
                return {"status": "error", "message": f"User {user_id} not found"}
    except Exception as e:
        LOG.error(f"Failed to reactivate user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/delete-user")
async def delete_user(request: Request):
    """Permanently delete a user by user_id (CASCADE deletes tickers and tokens)"""
    body = await request.json()
    token = body.get('token')
    user_id = body.get('user_id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_id:
        return {"status": "error", "message": "user_id is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get user info before deletion
            cur.execute("SELECT email, name FROM users WHERE id = %s", (user_id,))
            user = cur.fetchone()

            if not user:
                return {"status": "error", "message": f"User {user_id} not found"}

            # Delete user (CASCADE deletes user_tickers and unsubscribe_tokens)
            cur.execute("DELETE FROM users WHERE id = %s", (user_id,))
            conn.commit()

            LOG.info(f"ðŸ—‘ï¸ Permanently deleted user {user_id}: {user['email']}")
            return {"status": "success", "message": f"Deleted {user['email']}"}
    except Exception as e:
        LOG.error(f"Failed to delete user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/create-user")
async def create_user_admin(request: Request):
    """Create a new user from admin panel (supports admin/beta/paid types)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    email = body.get('email', '').strip().lower()
    name = body.get('name', '').strip()
    user_type = body.get('user_type', 'beta')
    tickers = body.get('tickers', [])
    status = body.get('status', 'active')

    # Validation
    if not email or '@' not in email:
        return {"status": "error", "message": "Valid email is required"}
    if not name:
        return {"status": "error", "message": "Name is required"}
    if user_type not in ['admin', 'beta', 'paid']:
        return {"status": "error", "message": "Invalid user_type. Must be: admin, beta, or paid"}
    if not isinstance(tickers, list):
        return {"status": "error", "message": "tickers must be an array"}

    # Set ticker limit based on user type
    ticker_limits = {'admin': None, 'beta': 3, 'paid': 10}
    ticker_limit = ticker_limits.get(user_type, 3)

    # Validate ticker count (unless admin)
    if ticker_limit and len(tickers) > ticker_limit:
        return {"status": "error", "message": f"{user_type} users can have at most {ticker_limit} tickers"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Check if email already exists
            cur.execute("SELECT id FROM users WHERE email = %s", (email,))
            if cur.fetchone():
                return {"status": "error", "message": f"User with email {email} already exists"}

            # Create user
            cur.execute("""
                INSERT INTO users (email, name, user_type, ticker_limit, status,
                                   terms_version, terms_accepted_at, privacy_version, privacy_accepted_at)
                VALUES (%s, %s, %s, %s, %s, '1.0', NOW(), '1.0', NOW())
                RETURNING id
            """, (email, name, user_type, ticker_limit, status))
            user_id = cur.fetchone()['id']

            # Add tickers
            for ticker in tickers:
                ticker = ticker.upper().strip()
                if ticker:
                    cur.execute("""
                        INSERT INTO user_tickers (user_id, ticker)
                        VALUES (%s, %s)
                        ON CONFLICT (user_id, ticker) DO NOTHING
                    """, (user_id, ticker))

            conn.commit()

            LOG.info(f"âœ… Created {user_type} user {user_id}: {email} with {len(tickers)} tickers")
            return {
                "status": "success",
                "message": f"Created {user_type} user {email}",
                "user_id": user_id
            }
    except Exception as e:
        LOG.error(f"Failed to create user: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/add-ticker")
async def add_user_ticker(request: Request):
    """Add a ticker to a user"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    user_id = body.get('user_id')
    ticker = body.get('ticker', '').upper().strip()

    if not user_id:
        return {"status": "error", "message": "user_id is required"}
    if not ticker:
        return {"status": "error", "message": "ticker is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get user info and current ticker count
            cur.execute("""
                SELECT u.email, u.user_type, u.ticker_limit, COUNT(ut.id) as ticker_count
                FROM users u
                LEFT JOIN user_tickers ut ON u.id = ut.user_id
                WHERE u.id = %s
                GROUP BY u.id, u.email, u.user_type, u.ticker_limit
            """, (user_id,))
            user = cur.fetchone()

            if not user:
                return {"status": "error", "message": f"User {user_id} not found"}

            # Check ticker limit (unless admin/unlimited)
            if user['ticker_limit'] and user['ticker_count'] >= user['ticker_limit']:
                return {
                    "status": "error",
                    "message": f"User has reached ticker limit ({user['ticker_limit']})"
                }

            # Add ticker
            cur.execute("""
                INSERT INTO user_tickers (user_id, ticker)
                VALUES (%s, %s)
                ON CONFLICT (user_id, ticker) DO NOTHING
                RETURNING id
            """, (user_id, ticker))
            result = cur.fetchone()

            if result:
                conn.commit()
                LOG.info(f"âž• Added ticker {ticker} to user {user_id} ({user['email']})")
                return {"status": "success", "message": f"Added {ticker}"}
            else:
                return {"status": "error", "message": f"Ticker {ticker} already exists for this user"}
    except Exception as e:
        LOG.error(f"Failed to add ticker: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/remove-ticker")
async def remove_user_ticker(request: Request):
    """Remove a ticker from a user"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    user_id = body.get('user_id')
    ticker = body.get('ticker', '').upper().strip()

    if not user_id:
        return {"status": "error", "message": "user_id is required"}
    if not ticker:
        return {"status": "error", "message": "ticker is required"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get user email for logging
            cur.execute("SELECT email FROM users WHERE id = %s", (user_id,))
            user = cur.fetchone()

            if not user:
                return {"status": "error", "message": f"User {user_id} not found"}

            # Remove ticker
            cur.execute("""
                DELETE FROM user_tickers
                WHERE user_id = %s AND ticker = %s
                RETURNING id
            """, (user_id, ticker))
            result = cur.fetchone()

            if result:
                conn.commit()
                LOG.info(f"âž– Removed ticker {ticker} from user {user_id} ({user['email']})")
                return {"status": "success", "message": f"Removed {ticker}"}
            else:
                return {"status": "error", "message": f"Ticker {ticker} not found for this user"}
    except Exception as e:
        LOG.error(f"Failed to remove ticker: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/generate-transcript-summary")
async def generate_transcript_summary_api(request: Request):
    """
    Generate AI summary of earnings transcript or press release using job queue.

    Returns job_id immediately for async processing.
    Client should poll /jobs/{job_id} for status.
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker_raw = body.get('ticker')
    report_type = body.get('report_type')  # 'transcript' or 'press_release'
    quarter = body.get('quarter')  # Integer (3) for transcripts
    year = body.get('year')  # Integer (2024) for transcripts
    pr_date = body.get('pr_date')  # String date for press releases
    pr_title = body.get('pr_title')  # String title for press releases (optional, for exact matching)

    try:
        # Validate and normalize ticker
        try:
            ticker, config = validate_and_normalize_ticker(ticker_raw)
        except ValueError as e:
            return {"status": "error", "message": str(e)}

        # Determine phase based on report type
        if report_type == 'transcript':
            phase = 'transcript_generation'
            LOG.info(f"ðŸ“‹ Creating transcript generation job for {ticker} Q{quarter} {year}")
        else:
            phase = 'press_release_generation'
            LOG.info(f"ðŸ“‹ Creating press release generation job for {ticker} ({pr_date})")

        # Build job config
        job_config = {
            "ticker": ticker,
            "report_type": report_type,
            "quarter": quarter,
            "year": year,
            "pr_date": pr_date,
            "pr_title": pr_title,
            "send_email": True
        }

        # Create job in database
        with db() as conn, conn.cursor() as cur:
            batch_id = str(uuid.uuid4())

            # Create batch
            cur.execute("""
                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                VALUES (%s, 'processing', 1, %s)
            """, (batch_id, json.dumps({"source": "admin_api", "report_type": report_type})))

            # Create job
            job_id = str(uuid.uuid4())
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, job_id, ticker, status, phase, progress, config
                )
                VALUES (%s, %s, %s, 'queued', %s, 0, %s)
            """, (batch_id, job_id, ticker, phase, json.dumps(job_config)))

            conn.commit()

        LOG.info(f"âœ… Created job {job_id} for {ticker} ({report_type})")

        return {
            "status": "success",
            "message": f"Job queued for {ticker} {report_type}",
            "job_id": job_id,
            "batch_id": batch_id
        }

    except Exception as e:
        LOG.error(f"Failed to create {report_type} job for {ticker}: {e}")
        return {"status": "error", "message": str(e)}

@APP.get("/api/admin/debug-transcript-summary")
async def debug_transcript_summary(ticker: str, token: str):
    """Debug endpoint to view raw transcript summary from database"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker, company_name, report_type, fiscal_quarter, fiscal_year,
                       report_date, summary_text, ai_provider, generated_at
                FROM transcript_summaries
                WHERE ticker = %s
                ORDER BY generated_at DESC
                LIMIT 1
            """, (ticker,))
            result = cur.fetchone()

            if result:
                # Return first 1000 chars of summary for preview
                return {
                    "status": "success",
                    "ticker": result['ticker'],
                    "company_name": result['company_name'],
                    "report_type": result['report_type'],
                    "quarter": result['fiscal_quarter'],
                    "year": result['fiscal_year'],
                    "report_date": str(result['report_date']),
                    "summary_preview": result['summary_text'][:2000] + "..." if len(result['summary_text']) > 2000 else result['summary_text'],
                    "summary_length": len(result['summary_text']),
                    "ai_provider": result['ai_provider'],
                    "generated_at": str(result['generated_at'])
                }
            else:
                return {"status": "error", "message": f"No summary found for {ticker}"}

    except Exception as e:
        LOG.error(f"Failed to fetch research summary: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


# ==============================================================================
# 8-K SEC FILING ENDPOINTS
# ==============================================================================

@APP.post("/api/admin/generate-8k-summary")
async def generate_8k_summary_api(request: Request):
    """
    Generate 8-K exhibit processing (extracts all EX-99.* files, sends separate emails).

    Body: {
        "ticker": "AAPL",
        "cik": "0000320193",
        "accession_number": "0001193125-25-012345",
        "filing_date": "Jan 30, 2025",
        "filing_title": "Results of Operations | Apple announces...",
        "documents_url": "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=...",
        "item_codes": "2.02, 9.01",
        "token": "..."
    }

    Returns: {"status": "success", "job_id": "..."}
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker_raw = body.get('ticker')
    cik = body.get('cik')
    accession_number = body.get('accession_number')
    filing_date = body.get('filing_date')
    documents_url = body.get('documents_url')  # SEC documents index page
    item_codes = body.get('item_codes')

    try:
        # Validate and normalize ticker
        try:
            ticker, config = validate_and_normalize_ticker(ticker_raw)
        except ValueError as e:
            return {"status": "error", "message": str(e)}

        LOG.info(f"ðŸ“‹ Creating 8-K processing job for {ticker} ({filing_date}) [normalized from '{ticker_raw}']")
        LOG.info(f"[8K_REQUEST_DEBUG] Documents URL: {documents_url}")

        # Build job config
        job_config = {
            "ticker": ticker,
            "cik": cik,
            "accession_number": accession_number,
            "filing_date": filing_date,
            "documents_url": documents_url,  # NEW: Parse all exhibits from documents page
            "item_codes": item_codes,
            "send_email": True
        }

        # Create job in ticker_processing_jobs table
        with db() as conn, conn.cursor() as cur:
            batch_id = str(uuid.uuid4())

            # Create batch
            cur.execute("""
                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                VALUES (%s, %s, 1, %s)
                RETURNING batch_id
            """, (batch_id, 'processing', json.dumps(job_config)))

            # Create job
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, ticker, status, phase, progress, config
                )
                VALUES (%s, %s, %s, %s, 0, %s)
                RETURNING job_id
            """, (batch_id, ticker, 'queued', '8k_summary_generation', json.dumps(job_config)))

            job_id = cur.fetchone()['job_id']
            conn.commit()

        LOG.info(f"âœ… Created 8-K summary job {job_id} for {ticker}")

        return {
            "status": "success",
            "message": f"8-K summary job created for {ticker}",
            "job_id": job_id,
            "ticker": ticker,
            "filing_date": filing_date
        }

    except Exception as e:
        LOG.error(f"Failed to create 8-K summary job: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


@APP.get("/api/admin/8k-filings")
def get_all_8k_filings(token: str = Query(...)):
    """
    Get all generated 8-K exhibit filings for Research Library.

    Returns exhibit-level records (multiple rows per filing if multiple exhibits).

    Returns: [
        {
            "ticker": "AAPL",
            "filing_date": "2025-01-30",
            "filing_title": "Results of Operations | Apple announces...",
            "item_codes": "2.02, 9.01",
            "accession_number": "0001193125-25-012345",
            "exhibit_number": "99.1",
            "exhibit_description": "Supplemental Information",
            "exhibit_type": "investor_presentation",
            "char_count": 131002,
            "raw_content": "...",
            "generated_at": "..."
        },
        ...
    ]
    """
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT
                    ticker,
                    company_name,
                    filing_date,
                    filing_title,
                    item_codes,
                    accession_number,
                    exhibit_number,
                    exhibit_description,
                    exhibit_type,
                    char_count,
                    raw_content,
                    summary_text,
                    generated_at
                FROM sec_8k_filings
                ORDER BY filing_date DESC, exhibit_number ASC
            """)

            filings = cur.fetchall()

            return {
                "status": "success",
                "filings": filings
            }

    except Exception as e:
        LOG.error(f"Failed to get 8-K filings: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/api/admin/delete-8k-filing")
async def delete_8k_filing_api(request: Request):
    """
    Delete 8-K exhibit(s) by ticker and accession_number.

    Two modes:
    1. With exhibit_number: Delete just that exhibit (View Research)
    2. Without exhibit_number: Delete ALL exhibits for that accession + cascade to parsed PRs (Generate Research)

    Body: {
        "ticker": "AAPL",
        "accession_number": "0001193125-25-012345",
        "exhibit_number": "99.1",  // Optional - if missing, deletes all exhibits
        "token": "...",
        "cascade": true  // Optional: also delete associated parsed PRs
    }
    """
    body = await request.json()
    token = body.get('token')
    ticker = body.get('ticker')
    accession_number = body.get('accession_number')
    exhibit_number = body.get('exhibit_number')  # Optional
    cascade = body.get('cascade', False)

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not all([ticker, accession_number]):
        return {"status": "error", "message": "Missing required fields: ticker, accession_number"}

    try:
        with db() as conn, conn.cursor() as cur:
            if exhibit_number:
                # Mode 1: Delete specific exhibit only (View Research)
                cur.execute("""
                    SELECT id, exhibit_description
                    FROM sec_8k_filings
                    WHERE ticker = %s
                      AND accession_number = %s
                      AND exhibit_number = %s
                """, (ticker, accession_number, exhibit_number))

                row = cur.fetchone()
                if not row:
                    return {
                        "status": "error",
                        "message": f"Exhibit {exhibit_number} not found for {ticker}"
                    }

                filing_id = row['id'] if isinstance(row, dict) else row[0]
                description = row['exhibit_description'] if isinstance(row, dict) else row[1]

                deleted_items = [f"Exhibit {exhibit_number}"]

                # Cascade delete company release if requested
                if cascade:
                    cur.execute("""
                        DELETE FROM company_releases
                        WHERE source_type = '8k_exhibit' AND source_id = %s
                        RETURNING id
                    """, (filing_id,))
                    if cur.fetchone():
                        deleted_items.append("company release")

                # Delete the exhibit
                cur.execute("DELETE FROM sec_8k_filings WHERE id = %s", (filing_id,))

                conn.commit()
                LOG.info(f"ðŸ—‘ï¸ Deleted 8-K exhibit {exhibit_number} for {ticker}")
                return {
                    "status": "success",
                    "message": f"Deleted {', '.join(deleted_items)}: {description}"
                }

            else:
                # Mode 2: Delete ALL exhibits for this accession (Generate Research)
                # First get all exhibit IDs for cascade delete
                cur.execute("""
                    SELECT id, exhibit_number
                    FROM sec_8k_filings
                    WHERE ticker = %s AND accession_number = %s
                """, (ticker, accession_number))

                rows = cur.fetchall()
                if not rows:
                    return {
                        "status": "error",
                        "message": f"No exhibits found for {ticker} accession {accession_number}"
                    }

                filing_ids = [r['id'] if isinstance(r, dict) else r[0] for r in rows]
                exhibit_nums = [r['exhibit_number'] if isinstance(r, dict) else r[1] for r in rows]

                deleted_items = [f"{len(filing_ids)} exhibit(s)"]

                # Cascade delete ALL company releases for these exhibits
                if cascade and filing_ids:
                    cur.execute("""
                        DELETE FROM company_releases
                        WHERE source_type = '8k_exhibit' AND source_id = ANY(%s)
                        RETURNING id
                    """, (filing_ids,))
                    release_count = len(cur.fetchall())
                    if release_count:
                        deleted_items.append(f"{release_count} company release(s)")

                # Delete all exhibits
                cur.execute("""
                    DELETE FROM sec_8k_filings
                    WHERE ticker = %s AND accession_number = %s
                """, (ticker, accession_number))

                conn.commit()
                LOG.info(f"ðŸ—‘ï¸ Deleted all 8-K exhibits for {ticker} accession {accession_number}: {exhibit_nums}")
                return {
                    "status": "success",
                    "message": f"Deleted {', '.join(deleted_items)} for {ticker}"
                }

    except Exception as e:
        LOG.error(f"Failed to delete 8-K exhibit: {e}")
        return {"status": "error", "message": str(e)}


# ==============================================================================
# PARSED PRESS RELEASES API ENDPOINTS
# ==============================================================================

@APP.get("/api/admin/parsed-press-releases")
def get_all_parsed_press_releases_api(token: str = Query(...)):
    """
    Get all parsed press releases for Research Library.

    Returns unified list of Gemini summaries from FMP PRs and 8-K exhibits.

    Returns: {
        "status": "success",
        "parsed_prs": [
            {
                "id": 1,
                "ticker": "AAPL",
                "company_name": "Apple Inc.",
                "source_type": "8k",
                "document_date": "2025-01-30T16:00:00",
                "document_title": "Apple Reports First Quarter Results",
                "exhibit_number": "99.1",
                "item_codes": "2.02",
                "char_count": 3500,
                "fed_to_phase1": false,
                "generated_at": "..."
            },
            ...
        ]
    }
    """
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn:
            parsed_prs = get_all_parsed_press_releases(conn, limit=200)

            # Format dates for JSON serialization
            for pr in parsed_prs:
                if pr.get('document_date'):
                    pr['document_date'] = pr['document_date'].isoformat() if hasattr(pr['document_date'], 'isoformat') else str(pr['document_date'])
                if pr.get('generated_at'):
                    pr['generated_at'] = pr['generated_at'].isoformat() if hasattr(pr['generated_at'], 'isoformat') else str(pr['generated_at'])

            return {
                "status": "success",
                "parsed_prs": parsed_prs
            }

    except Exception as e:
        LOG.error(f"Failed to get parsed press releases: {e}")
        return {"status": "error", "message": str(e)}


@APP.get("/api/admin/parsed-press-release/{pr_id}")
def get_parsed_press_release_detail(pr_id: int, token: str = Query(...)):
    """
    Get full details of a specific company release including the summary text.

    Now queries company_releases table (migrated from parsed_press_releases).

    Returns: {
        "status": "success",
        "parsed_pr": {
            "id": 1,
            "ticker": "AAPL",
            "parsed_summary": "...",
            ...
        }
    }
    """
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT
                    id, ticker, company_name, source_type, source_id,
                    filing_date as document_date,
                    report_title as document_title,
                    NULL as source_url,
                    exhibit_number, item_codes,
                    summary_markdown as parsed_summary,
                    ai_provider, ai_model,
                    token_count_input, token_count_output,
                    processing_duration_seconds,
                    FALSE as fed_to_phase1,
                    NULL as fed_to_phase1_at,
                    generated_at,
                    fiscal_year, fiscal_quarter
                FROM company_releases
                WHERE id = %s
            """, (pr_id,))

            row = cur.fetchone()

            if not row:
                return {"status": "error", "message": f"Company release {pr_id} not found"}

            # Convert to dict - handle both dict and tuple cursor types
            if isinstance(row, dict):
                pr = dict(row)
            else:
                columns = [desc[0] for desc in cur.description]
                pr = dict(zip(columns, row))

            # Format dates
            for key in ['document_date', 'fed_to_phase1_at', 'generated_at']:
                if pr.get(key) and hasattr(pr[key], 'isoformat'):
                    pr[key] = pr[key].isoformat()

            return {
                "status": "success",
                "parsed_pr": pr
            }

    except Exception as e:
        LOG.error(f"Failed to get parsed PR {pr_id}: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/api/admin/generate-parsed-pr")
async def generate_parsed_pr_api(request: Request):
    """
    Generate parsed press release summary on-demand.

    Can regenerate existing or create new from source document.

    Body: {
        "token": "...",
        "source_type": "fmp" or "8k",
        "source_id": 123,  // ID in source table
        // For regeneration, that's all we need - we fetch content from source table
    }

    Returns: {
        "status": "success",
        "parsed_pr_id": 456,
        "message": "Generated parsed summary for AAPL"
    }
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    source_type = body.get('source_type')
    source_id = body.get('source_id')

    if not source_type or not source_id:
        return {"status": "error", "message": "Missing required fields: source_type, source_id"}

    if source_type not in ['fmp', '8k']:
        return {"status": "error", "message": "source_type must be 'fmp' or '8k'"}

    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        return {"status": "error", "message": "GEMINI_API_KEY not configured"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Fetch source document details
            if source_type == 'fmp':
                # NEW: FMP doesn't have source table, fetch directly from company_releases
                # source_id is the company_releases.id for re-generation
                cur.execute("""
                    SELECT ticker, company_name, filing_date, report_title
                    FROM company_releases
                    WHERE id = %s AND source_type = 'fmp_press_release'
                """, (source_id,))
                source_row = cur.fetchone()

                if not source_row:
                    return {"status": "error", "message": f"FMP company release {source_id} not found"}

                ticker = source_row[0] if not isinstance(source_row, dict) else source_row['ticker']
                company_name = source_row[1] if not isinstance(source_row, dict) else source_row['company_name']
                document_date = source_row[2] if not isinstance(source_row, dict) else source_row['filing_date']
                document_title = source_row[3] if not isinstance(source_row, dict) else source_row['report_title']
                exhibit_number = None
                item_codes = None
                source_url = None

                # Fetch RAW content from FMP API
                fmp_api_key = os.getenv("FMP_API_KEY")
                if not fmp_api_key:
                    return {"status": "error", "message": "FMP_API_KEY not configured"}

                LOG.info(f"ðŸ“¥ Fetching raw PR content from FMP for {ticker} - {document_title}")

                # Format date for FMP API (YYYY-MM-DD)
                pr_date_str = document_date.strftime('%Y-%m-%d') if hasattr(document_date, 'strftime') else str(document_date)[:10]

                raw_pr = fetch_fmp_press_release_by_date(ticker, pr_date_str, document_title, fmp_api_key)

                if not raw_pr or not raw_pr.get('text'):
                    return {"status": "error", "message": f"Could not fetch raw PR content from FMP for {ticker} on {pr_date_str}"}

                content = raw_pr.get('text', '')
                LOG.info(f"âœ… Fetched raw PR: {len(content):,} chars")

            else:  # '8k'
                # Get from sec_8k_filings table
                cur.execute("""
                    SELECT id, ticker, company_name, filing_date, filing_title,
                           item_codes, sec_html_url, raw_content, exhibit_number
                    FROM sec_8k_filings
                    WHERE id = %s
                """, (source_id,))
                source_row = cur.fetchone()

                if not source_row:
                    return {"status": "error", "message": f"8-K filing {source_id} not found"}

                ticker = source_row[1]
                company_name = source_row[2]
                document_date = source_row[3]
                document_title = source_row[4]
                item_codes = source_row[5]
                source_url = source_row[6]
                content = source_row[7]  # raw_content
                exhibit_number = source_row[8]

            if not content:
                return {"status": "error", "message": f"No content found for {source_type} source {source_id}"}

            # Get ticker config for company name if not available
            if not company_name:
                config = get_ticker_config(ticker)
                company_name = config.get('company_name', ticker) if config else ticker

            LOG.info(f"ðŸ“ Generating company release for {ticker} ({source_type}, source_id={source_id})")

            # Generate summary using 8k_filing_prompt (unified for FMP and 8-K)
            result = generate_earnings_release_with_gemini(
                ticker=ticker,
                company_name=company_name,
                content=content,
                document_title=document_title,
                item_codes=item_codes,
                gemini_api_key=gemini_api_key
            )

            if not result or not result.get('parsed_summary'):
                return {"status": "error", "message": "Failed to generate company release summary"}

            # Extract result data
            metadata = result.get('metadata', {})
            json_output = result.get('json_data', {})
            markdown_summary = result.get('parsed_summary', '')

            # Extract fiscal period if available (8-K only, FMP doesn't have reliable data)
            fiscal_year = json_output.get('fiscal_year') if source_type == '8k' else None
            fiscal_quarter = json_output.get('fiscal_quarter') if source_type == '8k' else None

            # Generate email HTML
            from modules.company_profiles import generate_company_release_email, get_filing_stock_data
            stock_data = get_filing_stock_data(ticker)

            # Format filing date
            try:
                from datetime import datetime
                date_obj = datetime.strptime(str(document_date)[:10], '%Y-%m-%d')
                formatted_date = date_obj.strftime('%b %d, %Y')
            except:
                formatted_date = str(document_date)[:10]

            email_result = generate_company_release_email(
                ticker=ticker,
                company_name=company_name,
                release_type='fmp_press_release' if source_type == 'fmp' else '8k_exhibit',
                filing_date=formatted_date,
                json_output=json_output,
                exhibit_number=exhibit_number if source_type == '8k' else None,
                stock_price=stock_data.get('stock_price'),
                price_change_pct=stock_data.get('price_change_pct'),
                price_change_color=stock_data.get('price_change_color', '#1e6b4a'),
                ytd_return_pct=stock_data.get('ytd_return_pct'),
                ytd_return_color=stock_data.get('ytd_return_color', '#1e6b4a'),
                market_status=stock_data.get('market_status', 'Last Close'),
                return_label=stock_data.get('return_label', '1D')
            )
            email_html = email_result.get('html', '')

            # Save to company_releases table
            cur.execute("""
                INSERT INTO company_releases (
                    ticker, company_name, release_type, filing_date, report_title,
                    source_id, source_type, summary_json, summary_html, summary_markdown,
                    ai_provider, ai_model, processing_duration_seconds,
                    token_count_input, token_count_output,
                    fiscal_year, fiscal_quarter, exhibit_number, item_codes,
                    generated_at
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (ticker, filing_date, report_title)
                DO UPDATE SET
                    summary_json = EXCLUDED.summary_json,
                    summary_html = EXCLUDED.summary_html,
                    summary_markdown = EXCLUDED.summary_markdown,
                    fiscal_year = EXCLUDED.fiscal_year,
                    fiscal_quarter = EXCLUDED.fiscal_quarter,
                    generated_at = NOW()
                RETURNING id
            """, (
                ticker,
                company_name,
                'fmp_press_release' if source_type == 'fmp' else '8k_exhibit',
                str(document_date)[:10],
                document_title,
                source_id if source_type == '8k' else None,  # Only 8-K has source_id (sec_8k_filings)
                'fmp_press_release' if source_type == 'fmp' else '8k_exhibit',
                json.dumps(json_output),
                email_html,
                markdown_summary,
                'gemini',
                metadata.get('model', 'gemini-2.5-flash'),
                metadata.get('generation_time_seconds', 0),
                metadata.get('token_count_input', 0),
                metadata.get('token_count_output', 0),
                fiscal_year,
                fiscal_quarter,
                exhibit_number,
                item_codes
            ))

            result_row = cur.fetchone()
            company_release_id = result_row[0] if result_row else None
            conn.commit()

            if not company_release_id:
                return {"status": "error", "message": "Failed to save company release"}

            return {
                "status": "success",
                "parsed_pr_id": company_release_id,
                "message": f"Generated company release for {ticker}",
                "word_count": len(markdown_summary.split()),
                "generation_time": metadata.get('generation_time_seconds')
            }

    except Exception as e:
        LOG.error(f"Failed to generate parsed PR: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


@APP.post("/api/admin/delete-parsed-pr")
async def delete_parsed_pr_api(request: Request):
    """
    Delete a company release by ID.

    Now deletes from company_releases table (migrated from parsed_press_releases).

    Body: {
        "token": "...",
        "id": 123
    }
    """
    body = await request.json()
    token = body.get('token')
    parsed_pr_id = body.get('id')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not parsed_pr_id:
        return {"status": "error", "message": "Missing required field: id"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get ticker for logging
            cur.execute("""
                SELECT ticker, report_title
                FROM company_releases
                WHERE id = %s
            """, (parsed_pr_id,))
            row = cur.fetchone()

            if not row:
                return {"status": "error", "message": f"Company release {parsed_pr_id} not found"}

            # Handle both dict and tuple cursor types
            if isinstance(row, dict):
                ticker = row['ticker']
                title = row['report_title']
            else:
                ticker, title = row

            # Delete company release
            cur.execute("DELETE FROM company_releases WHERE id = %s", (parsed_pr_id,))

            conn.commit()

            LOG.info(f"âœ… Deleted parsed PR #{parsed_pr_id} for {ticker}")

            return {
                "status": "success",
                "message": f"Deleted parsed PR: {title or f'#{parsed_pr_id}'}"
            }

    except Exception as e:
        LOG.error(f"Failed to delete parsed PR: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/api/admin/generate-company-profile")
async def generate_company_profile_api(request: Request):
    """Generate AI company profile from 10-K or 10-Q filing (uses job queue)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker_raw = body.get('ticker')
    filing_type = body.get('filing_type', '10-K')  # '10-K' or '10-Q'
    fiscal_year = body.get('fiscal_year')
    fiscal_quarter = body.get('fiscal_quarter')  # Required for 10-Q (e.g., 'Q3')
    filing_date = body.get('filing_date') or None  # Convert empty string to None
    period_end_date = body.get('period_end_date') or None  # Convert empty string to None (fixes ATGE issue)
    sec_html_url = body.get('sec_html_url')  # FMP mode (optional)
    file_content = body.get('file_content')  # Base64 encoded (optional)
    file_name = body.get('file_name')  # Optional

    try:
        # Validate and normalize ticker
        try:
            ticker, config = validate_and_normalize_ticker(ticker_raw)
        except ValueError as e:
            return {"status": "error", "message": str(e)}

        filing_desc = f"{fiscal_quarter} {fiscal_year}" if filing_type == '10-Q' else f"FY{fiscal_year}"
        LOG.info(f"ðŸ“Š Creating {filing_type} profile job for {ticker} {filing_desc} (normalized from '{ticker_raw}')")

        # Validate parameters
        if filing_type == '10-Q' and not fiscal_quarter:
            return {"status": "error", "message": "fiscal_quarter is required for 10-Q filings"}

        # Determine mode: FMP (SEC.gov HTML) or file upload
        job_config = {
            "ticker": ticker,
            "filing_type": filing_type,
            "fiscal_year": fiscal_year,
            "filing_date": filing_date,
            "period_end_date": period_end_date,
            "send_email": True
        }

        # Add quarter for 10-Q
        if filing_type == '10-Q':
            job_config["fiscal_quarter"] = fiscal_quarter

        if sec_html_url:
            # FMP MODE: Use SEC.gov HTML URL
            LOG.info(f"Using FMP mode: {sec_html_url}")
            job_config["sec_html_url"] = sec_html_url
        elif file_content and file_name:
            # FILE UPLOAD MODE: Save uploaded file temporarily
            file_ext = file_name.split('.')[-1].lower()
            temp_path = f"/tmp/{ticker}_10K_FY{fiscal_year}.{file_ext}"

            with open(temp_path, 'wb') as f:
                f.write(base64.b64decode(file_content))

            LOG.info(f"Saved uploaded file to {temp_path} ({os.path.getsize(temp_path)} bytes)")

            job_config["file_path"] = temp_path
            job_config["file_name"] = file_name
            job_config["file_ext"] = file_ext
        else:
            return {"status": "error", "message": "Either sec_html_url or file_content must be provided"}

        # Create job in ticker_processing_jobs table
        with db() as conn, conn.cursor() as cur:
            batch_id = str(uuid.uuid4())

            # Create batch
            cur.execute("""
                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                VALUES (%s, %s, 1, %s)
                RETURNING batch_id
            """, (batch_id, 'processing', json.dumps(job_config)))

            # Create job with appropriate phase
            phase = '10q_generation' if filing_type == '10-Q' else 'profile_generation'
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, ticker, status, phase, progress, config
                )
                VALUES (%s, %s, %s, %s, 0, %s)
                RETURNING job_id
            """, (batch_id, ticker, 'queued', phase, json.dumps(job_config)))

            job_id = cur.fetchone()['job_id']
            conn.commit()

        LOG.info(f"Created company profile job {job_id} for {ticker}")

        return {
            "status": "success",
            "message": f"Company profile job created for {ticker}",
            "job_id": job_id,
            "ticker": ticker,
            "fiscal_year": fiscal_year
        }

    except Exception as e:
        LOG.error(f"Failed to create company profile job: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/generate-10q-profile")
async def generate_10q_profile_api(request: Request):
    """Generate AI 10-Q quarterly profile (uses job queue)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    fiscal_year = body.get('fiscal_year')
    fiscal_quarter = body.get('fiscal_quarter')  # e.g., "Q3"
    filing_date = body.get('filing_date') or None  # Convert empty string to None
    period_end_date = body.get('period_end_date') or None  # Convert empty string to None (fixes edge cases)
    sec_html_url = body.get('sec_html_url')

    try:
        LOG.info(f"ðŸ“Š Creating 10-Q profile job for {ticker} {fiscal_quarter} {fiscal_year}")

        # Get ticker config
        config = get_ticker_config(ticker)
        if not config:
            return {"status": "error", "message": f"Ticker {ticker} not found in database"}

        # Build job config for 10-Q
        job_config = {
            "ticker": ticker,
            "filing_type": "10-Q",
            "fiscal_year": fiscal_year,
            "fiscal_quarter": fiscal_quarter,
            "filing_date": filing_date,
            "period_end_date": period_end_date,
            "sec_html_url": sec_html_url,
            "send_email": True
        }

        # Create job in ticker_processing_jobs table
        with db() as conn, conn.cursor() as cur:
            batch_id = str(uuid.uuid4())

            # Create batch
            cur.execute("""
                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                VALUES (%s, %s, 1, %s)
                RETURNING batch_id
            """, (batch_id, 'processing', json.dumps(job_config)))

            # Create job with phase '10q_generation'
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, ticker, status, phase, progress, config
                )
                VALUES (%s, %s, %s, %s, 0, %s)
                RETURNING job_id
            """, (batch_id, ticker, 'queued', '10q_generation', json.dumps(job_config)))

            job_id = cur.fetchone()['job_id']
            conn.commit()

        LOG.info(f"Created 10-Q profile job {job_id} for {ticker}")

        return {
            "status": "success",
            "message": f"10-Q profile job created for {ticker} {fiscal_quarter} {fiscal_year}",
            "job_id": job_id,
            "ticker": ticker,
            "fiscal_year": fiscal_year,
            "fiscal_quarter": fiscal_quarter
        }

    except Exception as e:
        LOG.error(f"Failed to create 10-Q profile job: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/generate-presentation")
async def generate_presentation_api(request: Request):
    """Generate AI analysis of investor presentation PDF (uses job queue + Gemini multimodal)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    presentation_date = body.get('presentation_date')  # YYYY-MM-DD
    presentation_type = body.get('presentation_type')  # earnings, investor_day, analyst_day, conference
    presentation_title = body.get('presentation_title')
    file_content = body.get('file_content')  # Base64 encoded PDF
    file_name = body.get('file_name')

    try:
        LOG.info(f"ðŸ“Š Creating investor presentation job for {ticker} - {presentation_title}")

        # Get ticker config
        config = get_ticker_config(ticker)
        if not config:
            return {"status": "error", "message": f"Ticker {ticker} not found in database"}

        # Save uploaded PDF temporarily
        file_ext = file_name.split('.')[-1].lower()
        if file_ext != 'pdf':
            return {"status": "error", "message": "Only PDF files are supported"}

        temp_path = f"/tmp/{ticker}_presentation_{presentation_date}.pdf"
        with open(temp_path, 'wb') as f:
            f.write(base64.b64decode(file_content))

        LOG.info(f"Saved presentation PDF to {temp_path} ({os.path.getsize(temp_path)} bytes)")

        # Build job config
        job_config = {
            "ticker": ticker,
            "presentation_date": presentation_date,
            "presentation_type": presentation_type,
            "presentation_title": presentation_title,
            "file_path": temp_path,
            "file_name": file_name,
            "send_email": True
        }

        # Create job in ticker_processing_jobs table
        with db() as conn, conn.cursor() as cur:
            batch_id = str(uuid.uuid4())

            # Create batch
            cur.execute("""
                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                VALUES (%s, %s, 1, %s)
                RETURNING batch_id
            """, (batch_id, 'processing', json.dumps(job_config)))

            # Create job with phase 'presentation_generation'
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, ticker, status, phase, progress, config
                )
                VALUES (%s, %s, %s, %s, 0, %s)
                RETURNING job_id
            """, (batch_id, ticker, 'queued', 'presentation_generation', json.dumps(job_config)))

            job_id = cur.fetchone()['job_id']
            conn.commit()

        LOG.info(f"Created presentation job {job_id} for {ticker}")

        return {
            "status": "success",
            "message": f"Investor presentation job created for {ticker}",
            "job_id": job_id,
            "ticker": ticker,
            "presentation_date": presentation_date
        }

    except Exception as e:
        LOG.error(f"Failed to create presentation job: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.get("/api/admin/company-profiles")
async def get_company_profiles_api(token: str = None, filing_type: str = "10-K"):
    """Get all company profiles (10-K or 10-Q filings) from database

    Args:
        token: Admin authentication token
        filing_type: Either '10-K' or '10-Q' (defaults to '10-K' for backward compatibility)
    """
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    # Validate filing_type parameter
    if filing_type not in ['10-K', '10-Q']:
        return {"status": "error", "message": f"Invalid filing_type: {filing_type}. Must be '10-K' or '10-Q'"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Fetch filings based on filing_type parameter
            cur.execute("""
                SELECT
                    ticker,
                    company_name,
                    industry,
                    fiscal_year,
                    fiscal_quarter,
                    filing_date,
                    profile_markdown,
                    source_file,
                    ai_provider,
                    ai_model,
                    generation_time_seconds,
                    token_count_input,
                    token_count_output,
                    status,
                    generated_at
                FROM sec_filings
                WHERE filing_type = %s
                ORDER BY generated_at DESC
            """, (filing_type,))

            profiles = cur.fetchall()

            # Convert to list of dicts with metadata
            result = []
            for profile in profiles:
                result.append({
                    "ticker": profile['ticker'],
                    "company_name": profile['company_name'],
                    "industry": profile['industry'],
                    "fiscal_year": profile['fiscal_year'],
                    "fiscal_quarter": profile['fiscal_quarter'],  # For 10-Q filings
                    "filing_date": str(profile['filing_date']) if profile['filing_date'] else None,
                    "profile_markdown": profile['profile_markdown'],
                    "source_file": profile['source_file'],
                    "ai_provider": profile['ai_provider'],
                    "gemini_model": profile['ai_model'],  # Frontend expects 'gemini_model' key
                    "generation_time_seconds": profile['generation_time_seconds'],
                    "token_count_input": profile['token_count_input'],
                    "token_count_output": profile['token_count_output'],
                    "status": profile['status'],
                    "generated_at": str(profile['generated_at']),
                    "char_count": len(profile['profile_markdown']) if profile['profile_markdown'] else 0
                })

            return {
                "status": "success",
                "profiles": result,
                "count": len(result)
            }

    except Exception as e:
        LOG.error(f"Failed to fetch company profiles: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/delete-company-profile")
async def delete_company_profile_api(request: Request):
    """Delete a company profile (10-K) from database"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    if not ticker:
        return {"status": "error", "message": "Ticker required"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Delete 10-K filings for this ticker (may be multiple years)
            cur.execute("""
                DELETE FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
            """, (ticker,))
            conn.commit()

            if cur.rowcount > 0:
                LOG.info(f"ðŸ—‘ï¸ Deleted {cur.rowcount} 10-K profile(s) for {ticker}")
                return {"status": "success", "message": f"{cur.rowcount} profile(s) for {ticker} deleted successfully"}
            else:
                return {"status": "error", "message": f"No 10-K profiles found for {ticker}"}

    except Exception as e:
        LOG.error(f"Failed to delete company profile for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/delete-sec-filing")
async def delete_sec_filing_api(request: Request):
    """Delete a SEC filing (10-K or 10-Q) from database

    Body parameters:
        token: Admin auth token
        ticker: Ticker symbol
        filing_type: '10-K' or '10-Q'
        fiscal_year: Fiscal year (required)
        fiscal_quarter: Fiscal quarter for 10-Q (e.g., 'Q1', 'Q2', optional for 10-K)
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    filing_type = body.get('filing_type', '10-K')
    fiscal_year = body.get('fiscal_year')
    fiscal_quarter = body.get('fiscal_quarter')  # For 10-Q

    if not ticker:
        return {"status": "error", "message": "Ticker required"}
    if not fiscal_year:
        return {"status": "error", "message": "Fiscal year required"}
    if filing_type not in ['10-K', '10-Q']:
        return {"status": "error", "message": "Invalid filing_type. Must be '10-K' or '10-Q'"}

    try:
        with db() as conn, conn.cursor() as cur:
            if filing_type == '10-Q':
                if not fiscal_quarter:
                    return {"status": "error", "message": "Fiscal quarter required for 10-Q"}

                # Delete specific 10-Q filing
                cur.execute("""
                    DELETE FROM sec_filings
                    WHERE ticker = %s AND filing_type = '10-Q'
                    AND fiscal_year = %s AND fiscal_quarter = %s
                """, (ticker, fiscal_year, fiscal_quarter))
                doc_label = f"{fiscal_quarter} {fiscal_year}"
            else:
                # Delete specific 10-K filing
                cur.execute("""
                    DELETE FROM sec_filings
                    WHERE ticker = %s AND filing_type = '10-K' AND fiscal_year = %s
                """, (ticker, fiscal_year))
                doc_label = f"FY {fiscal_year}"

            conn.commit()

            if cur.rowcount > 0:
                LOG.info(f"ðŸ—‘ï¸ Deleted {filing_type} {doc_label} for {ticker}")
                return {"status": "success", "message": f"{filing_type} {doc_label} for {ticker} deleted successfully"}
            else:
                return {"status": "error", "message": f"No {filing_type} {doc_label} found for {ticker}"}

    except Exception as e:
        LOG.error(f"Failed to delete {filing_type} for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/email-sec-filing")
async def email_sec_filing_api(request: Request):
    """Email a SEC filing (10-K or 10-Q) to admin

    Body parameters:
        token: Admin auth token
        ticker: Ticker symbol
        filing_type: '10-K' or '10-Q'
        fiscal_year: Fiscal year (required)
        fiscal_quarter: Fiscal quarter for 10-Q (e.g., 'Q1', 'Q2', optional for 10-K)
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    filing_type = body.get('filing_type', '10-K')
    fiscal_year = body.get('fiscal_year')
    fiscal_quarter = body.get('fiscal_quarter')  # For 10-Q

    if not ticker:
        return {"status": "error", "message": "Ticker required"}
    if not fiscal_year:
        return {"status": "error", "message": "Fiscal year required"}
    if filing_type not in ['10-K', '10-Q']:
        return {"status": "error", "message": "Invalid filing_type. Must be '10-K' or '10-Q'"}

    try:
        # Get filing from database
        with db() as conn, conn.cursor() as cur:
            if filing_type == '10-Q':
                if not fiscal_quarter:
                    return {"status": "error", "message": "Fiscal quarter required for 10-Q"}

                cur.execute("""
                    SELECT
                        ticker, company_name, industry, fiscal_year, fiscal_quarter, filing_date,
                        profile_markdown, source_file, ai_provider, ai_model,
                        generation_time_seconds, token_count_input, token_count_output
                    FROM sec_filings
                    WHERE ticker = %s AND filing_type = '10-Q'
                    AND fiscal_year = %s AND fiscal_quarter = %s
                """, (ticker, fiscal_year, fiscal_quarter))
                doc_label = f"{fiscal_quarter} {fiscal_year}"
            else:
                cur.execute("""
                    SELECT
                        ticker, company_name, industry, fiscal_year, filing_date,
                        profile_markdown, source_file, ai_provider, ai_model,
                        generation_time_seconds, token_count_input, token_count_output
                    FROM sec_filings
                    WHERE ticker = %s AND filing_type = '10-K' AND fiscal_year = %s
                """, (ticker, fiscal_year))
                doc_label = f"FY {fiscal_year}"

            profile = cur.fetchone()
            if not profile:
                return {"status": "error", "message": f"No {filing_type} {doc_label} found for {ticker}"}

        # Import the email generation function from company_profiles module
        from modules.company_profiles import generate_company_profile_email

        # Get real-time stock data using unified helper (yfinance â†’ Polygon â†’ database â†’ None)
        stock_data = get_filing_stock_data(profile['ticker'])

        # Generate email HTML
        email_data = generate_company_profile_email(
            ticker=profile['ticker'],
            company_name=profile['company_name'],
            industry=profile['industry'] or 'N/A',
            fiscal_year=profile['fiscal_year'],
            filing_date=str(profile['filing_date']) if profile['filing_date'] else None,
            profile_markdown=profile['profile_markdown'],
            stock_price=stock_data['stock_price'],
            price_change_pct=stock_data['price_change_pct'],
            price_change_color=stock_data['price_change_color'],
            ytd_return_pct=stock_data['ytd_return_pct'],
            ytd_return_color=stock_data['ytd_return_color'],
            market_status=stock_data['market_status'],
            return_label=stock_data['return_label'],
            filing_type=filing_type,
            fiscal_quarter=profile.get('fiscal_quarter')  # Include quarter for 10-Q
        )

        # Send email (use subject generated by function)
        send_email(
            subject=email_data["subject"],
            html_body=email_data["html"],
            to=os.getenv("ADMIN_EMAIL"),
            bcc=None
        )

        period_label = f"{profile.get('fiscal_quarter')} {profile['fiscal_year']}" if filing_type == '10-Q' else f"FY {profile['fiscal_year']}"
        LOG.info(f"ðŸ“§ Emailed {filing_type} {period_label} for {ticker} to admin")
        return {"status": "success", "message": f"{filing_type} {period_label} emailed successfully"}

    except Exception as e:
        LOG.error(f"Failed to email {filing_type} for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/regenerate-company-profile")
async def regenerate_company_profile_api(request: Request):
    """Regenerate an existing company profile using same source"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    if not ticker:
        return {"status": "error", "message": "Ticker required"}

    try:
        # Get existing 10-K profile to reuse fiscal_year and filing_date
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT fiscal_year, filing_date, source_file
                FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
                ORDER BY fiscal_year DESC
                LIMIT 1
            """, (ticker,))

            existing = cur.fetchone()
            if not existing:
                return {"status": "error", "message": f"No existing 10-K profile found for {ticker}"}

            fiscal_year = existing['fiscal_year']
            filing_date = existing['filing_date']
            source_file = existing['source_file']

        # Check if source was from SEC.gov (FMP mode) or file upload
        # For now, we'll only support regenerating FMP mode profiles
        # File upload mode would require the user to re-upload the file
        if not source_file or 'SEC.gov' not in source_file:
            return {
                "status": "error",
                "message": "Regeneration only supported for SEC.gov profiles. Please upload a new file for file-based profiles."
            }

        # For FMP mode, we need to fetch the SEC HTML URL again
        # This is a simplified version - in production you'd want to store the sec_html_url in the database
        return {
            "status": "info",
            "message": f"To regenerate {ticker}, please use the 'Company Profiles' tab and generate a new profile. Automatic regeneration coming soon!"
        }

    except Exception as e:
        LOG.error(f"Failed to regenerate company profile for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/email-company-profile")
async def email_company_profile_api(request: Request):
    """Email a company profile (10-K) to admin"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    if not ticker:
        return {"status": "error", "message": "Ticker required"}

    try:
        # Get latest 10-K profile from database
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT
                    ticker, company_name, industry, fiscal_year, filing_date,
                    profile_markdown, source_file, ai_provider, ai_model,
                    generation_time_seconds, token_count_input, token_count_output
                FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
                ORDER BY fiscal_year DESC
                LIMIT 1
            """, (ticker,))

            profile = cur.fetchone()
            if not profile:
                return {"status": "error", "message": f"No 10-K profile found for {ticker}"}

        # Import the email generation function from company_profiles module
        from modules.company_profiles import generate_company_profile_email

        # Get real-time stock data using unified helper (yfinance â†’ Polygon â†’ database â†’ None)
        stock_data = get_filing_stock_data(profile['ticker'])

        # Generate email HTML
        email_data = generate_company_profile_email(
            ticker=profile['ticker'],
            company_name=profile['company_name'],
            industry=profile['industry'] or 'N/A',
            fiscal_year=profile['fiscal_year'],
            filing_date=str(profile['filing_date']) if profile['filing_date'] else None,
            profile_markdown=profile['profile_markdown'],
            stock_price=stock_data['stock_price'],
            price_change_pct=stock_data['price_change_pct'],
            price_change_color=stock_data['price_change_color'],
            ytd_return_pct=stock_data['ytd_return_pct'],
            ytd_return_color=stock_data['ytd_return_color'],
            market_status=stock_data['market_status'],
            return_label=stock_data['return_label']
        )

        # Send email
        send_email(
            subject=email_data['subject'],
            html_body=email_data['html'],
            to=ADMIN_EMAIL  # Admin email
        )

        LOG.info(f"ðŸ“§ Emailed company profile for {ticker} to {ADMIN_EMAIL}")
        return {
            "status": "success",
            "message": f"Profile for {ticker} emailed successfully to {ADMIN_EMAIL}"
        }

    except Exception as e:
        LOG.error(f"Failed to email company profile for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

# =============================================================================
# RESEARCH LIBRARY API ENDPOINTS - Investor Presentations, Transcripts, Press Releases
# =============================================================================

@APP.get("/api/admin/investor-presentations")
async def get_investor_presentations_api(token: str = None):
    """Get all investor presentation analyses from database"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT
                    ticker,
                    company_name,
                    industry,
                    presentation_date,
                    presentation_type,
                    presentation_title,
                    profile_markdown,
                    page_count,
                    file_size_bytes,
                    ai_provider,
                    ai_model,
                    generation_time_seconds,
                    token_count_input,
                    token_count_output,
                    status,
                    generated_at
                FROM sec_filings
                WHERE filing_type = 'PRESENTATION'
                ORDER BY presentation_date DESC, generated_at DESC
            """)

            presentations = cur.fetchall()

            result = []
            for p in presentations:
                result.append({
                    "ticker": p['ticker'],
                    "company_name": p['company_name'],
                    "industry": p['industry'],
                    "presentation_date": str(p['presentation_date']) if p['presentation_date'] else None,
                    "presentation_type": p['presentation_type'],
                    "presentation_title": p['presentation_title'],
                    "profile_markdown": p['profile_markdown'],
                    "page_count": p['page_count'],
                    "file_size_bytes": p['file_size_bytes'],
                    "file_size_mb": round(p['file_size_bytes'] / 1048576, 2) if p['file_size_bytes'] else None,
                    "ai_provider": p['ai_provider'],
                    "ai_model": p['ai_model'],
                    "generation_time_seconds": p['generation_time_seconds'],
                    "token_count_input": p['token_count_input'],
                    "token_count_output": p['token_count_output'],
                    "status": p['status'],
                    "generated_at": str(p['generated_at']),
                    "char_count": len(p['profile_markdown']) if p['profile_markdown'] else 0
                })

            return {
                "status": "success",
                "presentations": result,
                "count": len(result)
            }

    except Exception as e:
        LOG.error(f"Failed to fetch investor presentations: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.get("/api/admin/transcripts")
async def get_transcripts_api(token: str = None):
    """Get all earnings transcript summaries from database"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT
                    ticker,
                    fiscal_quarter,
                    fiscal_year,
                    report_date,
                    summary_text,
                    ai_provider,
                    ai_model,
                    processing_duration_seconds,
                    generated_at
                FROM transcript_summaries
                WHERE report_type = 'transcript'
                ORDER BY fiscal_year DESC, fiscal_quarter DESC, generated_at DESC
            """)

            transcripts = cur.fetchall()

            result = []
            for t in transcripts:
                result.append({
                    "ticker": t['ticker'],
                    "quarter": t['fiscal_quarter'],
                    "year": t['fiscal_year'],
                    "quarter_label": f"{t['fiscal_quarter']} FY{t['fiscal_year']}",  # quarter already has 'Q' prefix (e.g., 'Q2')
                    "report_date": str(t['report_date']) if t['report_date'] else None,
                    "summary_text": t['summary_text'],
                    "ai_provider": t['ai_provider'],
                    "ai_model": t['ai_model'],
                    "processing_duration_seconds": t['processing_duration_seconds'],
                    "generated_at": str(t['generated_at']),
                    "char_count": len(t['summary_text']) if t['summary_text'] else 0,
                    "word_count": len(t['summary_text'].split()) if t['summary_text'] else 0
                })

            return {
                "status": "success",
                "transcripts": result,
                "count": len(result)
            }

    except Exception as e:
        LOG.error(f"Failed to fetch transcripts: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

# DEPRECATED: Old /api/admin/press-releases endpoint removed
# Use /api/admin/parsed-press-releases instead (queries company_releases table)

@APP.post("/api/admin/delete-investor-presentation")
async def delete_investor_presentation_api(request: Request):
    """Delete an investor presentation from database"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    presentation_date = body.get('presentation_date')

    if not ticker or not presentation_date:
        return {"status": "error", "message": "Ticker and presentation_date required"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                DELETE FROM sec_filings
                WHERE ticker = %s
                  AND filing_type = 'PRESENTATION'
                  AND presentation_date = %s
            """, (ticker, presentation_date))
            conn.commit()

            if cur.rowcount > 0:
                LOG.info(f"ðŸ—‘ï¸ Deleted investor presentation for {ticker} ({presentation_date})")
                return {"status": "success", "message": f"Presentation for {ticker} deleted successfully"}
            else:
                return {"status": "error", "message": f"No presentation found for {ticker} on {presentation_date}"}

    except Exception as e:
        LOG.error(f"Failed to delete investor presentation: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/delete-transcript")
async def delete_transcript_api(request: Request):
    """Delete a transcript summary from database"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    quarter = body.get('quarter')
    year = body.get('year')
    report_type = body.get('report_type', 'transcript')  # 'transcript' or 'press_release'
    ai_provider = body.get('ai_provider')  # 'claude' or 'gemini' for transcript disambiguation

    if not ticker:
        return {"status": "error", "message": "Ticker required"}

    try:
        with db() as conn, conn.cursor() as cur:
            if report_type == 'transcript':
                if not quarter or not year:
                    return {"status": "error", "message": "Quarter and year required for transcripts"}

                # Include ai_provider to only delete the specific version (Gemini or Sonnet)
                if ai_provider:
                    cur.execute("""
                        DELETE FROM transcript_summaries
                        WHERE ticker = %s
                          AND report_type = 'transcript'
                          AND fiscal_quarter = %s
                          AND fiscal_year = %s
                          AND ai_provider = %s
                    """, (ticker, quarter, year, ai_provider))
                else:
                    # Fallback: delete all versions if ai_provider not specified
                    cur.execute("""
                        DELETE FROM transcript_summaries
                        WHERE ticker = %s
                          AND report_type = 'transcript'
                          AND fiscal_quarter = %s
                          AND fiscal_year = %s
                    """, (ticker, quarter, year))

                identifier = f"Q{quarter} {year} ({ai_provider or 'all versions'})"
            else:  # press_release (DEPRECATED - this branch should never be reached)
                return {"status": "error", "message": "Press release deletion not supported. Use Company Releases instead."}

            conn.commit()

            if cur.rowcount > 0:
                LOG.info(f"ðŸ—‘ï¸ Deleted {report_type} for {ticker} ({identifier})")
                return {"status": "success", "message": f"{report_type.replace('_', ' ').title()} for {ticker} deleted successfully"}
            else:
                return {"status": "error", "message": f"No {report_type} found for {ticker}"}

    except Exception as e:
        LOG.error(f"Failed to delete {report_type}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/email-research")
async def email_research_api(request: Request):
    """Email any research document (presentation, transcript, press release) to admin"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = body.get('ticker')
    research_type = body.get('type')  # 'presentation', 'transcript', 'press_release'

    if not ticker or not research_type:
        return {"status": "error", "message": "Ticker and type required"}

    try:
        content = None
        subject = None

        with db() as conn, conn.cursor() as cur:
            if research_type == 'presentation':
                presentation_date = body.get('presentation_date')
                if not presentation_date:
                    return {"status": "error", "message": "Presentation date required"}

                cur.execute("""
                    SELECT ticker, company_name, presentation_date, presentation_type,
                           presentation_title, profile_markdown
                    FROM sec_filings
                    WHERE ticker = %s
                      AND filing_type = 'PRESENTATION'
                      AND presentation_date = %s
                    LIMIT 1
                """, (ticker, presentation_date))

                doc = cur.fetchone()
                if not doc:
                    return {"status": "error", "message": f"No presentation found for {ticker}"}

                content = doc['profile_markdown']
                # Use presentation_title if available, otherwise fallback to date
                pres_title = doc.get('presentation_title', f"Investor Presentation - {doc['presentation_date']}")
                subject = f"{ticker} {pres_title}"

            elif research_type == 'transcript':
                quarter = body.get('quarter')
                year = body.get('year')
                ai_provider = body.get('ai_provider')  # 'claude' or 'gemini' for disambiguation
                if not quarter or not year:
                    return {"status": "error", "message": "Quarter and year required"}

                # Include ai_provider to fetch the specific version (Gemini or Sonnet)
                if ai_provider:
                    cur.execute("""
                        SELECT ticker, fiscal_quarter, fiscal_year, report_date, summary_text, summary_json, ai_provider, ai_model
                        FROM transcript_summaries
                        WHERE ticker = %s
                          AND report_type = 'transcript'
                          AND fiscal_quarter = %s
                          AND fiscal_year = %s
                          AND ai_provider = %s
                        LIMIT 1
                    """, (ticker, quarter, year, ai_provider))
                else:
                    # Fallback: fetch any version if ai_provider not specified
                    cur.execute("""
                        SELECT ticker, fiscal_quarter, fiscal_year, report_date, summary_text, summary_json, ai_provider, ai_model
                        FROM transcript_summaries
                        WHERE ticker = %s
                          AND report_type = 'transcript'
                          AND fiscal_quarter = %s
                          AND fiscal_year = %s
                        LIMIT 1
                    """, (ticker, quarter, year))

                doc = cur.fetchone()
                if not doc:
                    return {"status": "error", "message": f"No transcript found for {ticker}"}

                # Check for JSON (v2 format) - required for research library
                if not doc.get('summary_json'):
                    return {"status": "error", "message": "Legacy transcript format detected. Please regenerate this transcript to view via email."}

                content = doc['summary_text']  # Still used for content check
                model_label = f" ({doc.get('ai_model', doc.get('ai_provider', 'AI'))})" if ai_provider else ""
                # doc['fiscal_quarter'] from database already has "Q" prefix (e.g., "Q2")
                subject = f"{ticker} {doc['fiscal_quarter']} {year} Earnings Call Transcript{model_label}"

            elif research_type == 'press_release':
                return {"status": "error", "message": "Press release emailing not supported. Use Company Releases instead."}

            elif research_type == '8k_filing':
                accession_number = body.get('accession_number')
                exhibit_number = body.get('exhibit_number')

                if not accession_number or not exhibit_number:
                    return {"status": "error", "message": "Accession number and exhibit number required"}

                cur.execute("""
                    SELECT ticker, company_name, filing_date, filing_title, item_codes,
                           accession_number, exhibit_number, exhibit_description,
                           raw_content, summary_text
                    FROM sec_8k_filings
                    WHERE ticker = %s
                      AND accession_number = %s
                      AND exhibit_number = %s
                    LIMIT 1
                """, (ticker, accession_number, exhibit_number))

                doc = cur.fetchone()
                if not doc:
                    return {"status": "error", "message": f"No 8-K exhibit found for {ticker}"}

                content = doc['raw_content'] or doc['summary_text']

                # Format filing date to match automated path (e.g., "Nov 19, 2024")
                try:
                    from datetime import datetime
                    filing_date = doc['filing_date']
                    if isinstance(filing_date, str):
                        date_obj = datetime.strptime(filing_date[:10], '%Y-%m-%d')
                    else:
                        date_obj = filing_date
                    formatted_date = date_obj.strftime('%b %d, %Y')
                except:
                    formatted_date = str(doc['filing_date'])

                # Subject matches automated format (line 17145)
                subject = f"{ticker} 8-K Raw - Exhibit {doc['exhibit_number']} - {formatted_date}"

            elif research_type == 'parsed_pr':
                # Fetch from company_releases table by ID (NEW: uses same email function as automated generation)
                doc_id = body.get('id')
                if not doc_id:
                    return {"status": "error", "message": "Document ID required for company release"}

                cur.execute("""
                    SELECT id, ticker, company_name, source_type, filing_date,
                           report_title, summary_json, fiscal_quarter, fiscal_year,
                           exhibit_number
                    FROM company_releases
                    WHERE id = %s AND ticker = %s
                    LIMIT 1
                """, (doc_id, ticker))

                doc = cur.fetchone()
                if not doc:
                    return {"status": "error", "message": f"No company release found for {ticker} with ID {doc_id}"}

                # Reconstruct json_output for generate_company_release_email()
                json_output = doc['summary_json'] if isinstance(doc['summary_json'], dict) else json.loads(doc['summary_json'])

                # Note: content variable not used for this research type (email function uses json_output directly)
                content = "N/A"  # Set placeholder to pass content check below

        if not content or content == "N/A":
            # Skip check for parsed_pr type (uses json_output instead)
            if research_type != 'parsed_pr':
                return {"status": "error", "message": "No content found"}

        # Get real-time stock data using unified helper (yfinance â†’ Polygon â†’ database â†’ None)
        config = get_ticker_config(ticker)
        company_name = config.get("company_name", ticker) if config else ticker
        stock_data = get_filing_stock_data(ticker)

        # Generate formatted email using proper template
        if research_type == 'presentation':
            # Use company profile email template for presentations
            email_data = generate_company_profile_email(
                ticker=ticker,
                company_name=doc['company_name'],
                industry=None,  # Presentations don't have industry field
                fiscal_year=None,  # Presentations use date not fiscal year
                filing_date=doc['presentation_date'],
                profile_markdown=content,
                stock_price=stock_data['stock_price'],
                price_change_pct=stock_data['price_change_pct'],
                price_change_color=stock_data['price_change_color'],
                ytd_return_pct=stock_data['ytd_return_pct'],
                ytd_return_color=stock_data['ytd_return_color'],
                market_status=stock_data['market_status'],
                return_label=stock_data['return_label'],
                filing_type="PRESENTATION"
            )
            html_body = email_data['html']
            subject = email_data['subject']

        elif research_type == 'transcript':
            # Use transcript email v2 template (JSON-based)
            from modules.transcript_summaries import generate_transcript_email_v2

            json_output = doc['summary_json'] if isinstance(doc['summary_json'], dict) else json.loads(doc['summary_json'])

            # Extract quarter number from "Q3" format (DB stores "Q3", function expects 3)
            quarter_num = int(doc['fiscal_quarter'].replace('Q', '')) if doc['fiscal_quarter'] else None

            email_data = generate_transcript_email_v2(
                ticker=ticker,
                json_output=json_output,
                config=config,
                content_type='transcript',
                quarter=quarter_num,
                year=doc['fiscal_year'],
                report_date=doc.get('report_date'),
                stock_data=stock_data
            )
            html_body = email_data['html']
            subject = email_data['subject']

        # press_release research type removed - returns error at line 26005-26006 directing users to Company Releases

        elif research_type == '8k_filing':
            # Use raw 8-K email template
            from jinja2 import Environment, FileSystemLoader
            template_env = Environment(loader=FileSystemLoader('templates'))
            raw_email_template = template_env.get_template('email_8k_raw_content.html')

            html_body = raw_email_template.render(
                ticker=ticker,
                company_name=doc.get('company_name', ticker),
                filing_date=doc['filing_date'],
                item_codes=doc.get('item_codes', 'N/A'),
                exhibit_number=doc['exhibit_number'],
                exhibit_description=doc['exhibit_description'],
                exhibit_url=f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={doc['accession_number']}",
                char_count=f"{len(content):,}",
                raw_content_html=content
            )
            # subject already set above

        elif research_type == 'parsed_pr':
            # Use company release email template (SAME as automated generation)
            # Format filing date nicely
            try:
                from datetime import datetime
                filing_date = doc['filing_date']
                if isinstance(filing_date, str):
                    date_obj = datetime.strptime(filing_date[:10], '%Y-%m-%d')
                else:
                    date_obj = filing_date
                formatted_date = date_obj.strftime('%b %d, %Y')
            except:
                formatted_date = str(doc['filing_date'])

            # Call SAME email generation function as automated path (line 16910)
            email_data = generate_company_release_email(
                ticker=ticker,
                company_name=doc.get('company_name', company_name),
                release_type=doc.get('source_type', 'fmp_press_release'),
                filing_date=formatted_date,
                json_output=json_output,
                exhibit_number=doc.get('exhibit_number'),
                stock_price=stock_data['stock_price'],
                price_change_pct=stock_data['price_change_pct'],
                price_change_color=stock_data['price_change_color'],
                ytd_return_pct=stock_data['ytd_return_pct'],
                ytd_return_color=stock_data['ytd_return_color'],
                market_status=stock_data['market_status'],
                return_label=stock_data['return_label']
            )
            html_body = email_data['html']
            subject = email_data['subject']  # Subject automatically distinguishes earnings vs material events

        send_email(subject=subject, html_body=html_body, to=ADMIN_EMAIL)

        LOG.info(f"ðŸ“§ Emailed {research_type} for {ticker} to {ADMIN_EMAIL}")
        return {
            "status": "success",
            "message": f"{research_type.replace('_', ' ').title()} for {ticker} emailed successfully to {ADMIN_EMAIL}"
        }

    except Exception as e:
        LOG.error(f"Failed to email {research_type} for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

# =============================================================================
# PARALLEL FMP API HELPERS (for faster research status loading)
# =============================================================================

async def fetch_fmp_10k_list_async(ticker: str, session: aiohttp.ClientSession) -> List[Dict]:
    """Fetch 10-K filings from FMP using aiohttp (parallel-safe)"""
    try:
        fmp_url = f"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}?type=10-K&page=0&apikey={FMP_API_KEY}"
        async with session.get(fmp_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
            filings = await response.json()

            # Extract years and dates (last 10 years)
            available_years = []
            for filing in filings[:10]:
                try:
                    filing_date_str = filing.get("fillingDate", "")
                    sec_html_url = filing.get("finalLink", "")

                    # Extract period end date from finalLink filename
                    # Example: https://www.sec.gov/.../jpm-20241231.htm â†’ 20241231
                    period_date = None
                    year = None

                    if sec_html_url:
                        match = re.search(r'(\d{8})\.htm$', sec_html_url)
                        if match:
                            date_str = match.group(1)  # "20241231"
                            year = int(date_str[:4])  # 2024
                            period_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"  # "2024-12-31"

                    # Fallback to filing date if we couldn't extract from URL
                    if not year and filing_date_str:
                        year = int(filing_date_str[:4])

                    if not year:
                        continue

                    filing_date = filing_date_str[:10] if filing_date_str else None

                    available_years.append({
                        "year": year,
                        "filing_date": filing_date,
                        "period_end_date": period_date,
                        "sec_html_url": sec_html_url
                    })
                except (ValueError, KeyError) as e:
                    LOG.warning(f"Skipping invalid 10-K filing entry for {ticker}: {e}")
                    continue

            return available_years
    except Exception as e:
        LOG.error(f"Failed to fetch FMP 10-K list for {ticker}: {e}")
        return []


async def fetch_fmp_10q_list_async(ticker: str, session: aiohttp.ClientSession) -> List[Dict]:
    """Fetch 10-Q filings from FMP using aiohttp (parallel-safe)"""
    try:
        fmp_url = f"https://financialmodelingprep.com/api/v3/sec_filings/{ticker}?type=10-Q&page=0&apikey={FMP_API_KEY}"
        async with session.get(fmp_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
            filings = await response.json()

            # Extract quarters and dates (last 12 quarters = 3 years)
            available_quarters = []
            for filing in filings[:12]:
                try:
                    filing_date_str = filing.get("fillingDate", "")
                    sec_html_url = filing.get("finalLink", "")

                    # Extract period end date from finalLink filename
                    period_date = None
                    year = None
                    month = None

                    if sec_html_url:
                        match = re.search(r'(\d{8})\.htm$', sec_html_url)
                        if match:
                            date_str = match.group(1)  # "20250630"
                            year = int(date_str[:4])  # 2025
                            month = int(date_str[4:6])  # 06
                            period_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"  # "2025-06-30"

                    # Use period end date to calculate quarter
                    if year and month:
                        if month == 3:
                            quarter = "Q1"
                        elif month == 6:
                            quarter = "Q2"
                        elif month == 9:
                            quarter = "Q3"
                        elif month == 12:
                            quarter = "Q4"
                        else:
                            quarter_num = (month - 1) // 3 + 1
                            quarter = f"Q{quarter_num}"
                        fiscal_year = year
                    elif filing_date_str:
                        # Fallback to filing date logic
                        filing_date = filing_date_str[:10]
                        year = int(filing_date[:4])
                        month = int(filing_date[5:7])
                        day = int(filing_date[8:10])

                        if month >= 10 or (month == 9 and day == 30):
                            quarter = "Q3"
                            fiscal_year = year
                        elif month >= 7 or (month == 6 and day == 30):
                            quarter = "Q2"
                            fiscal_year = year
                        elif month >= 4 or (month == 3 and day == 31):
                            quarter = "Q1"
                            fiscal_year = year
                        else:
                            continue
                        period_date = None
                    else:
                        continue

                    filing_date = filing_date_str[:10] if filing_date_str else None

                    # Skip Q4 10-Qs
                    if quarter == "Q4":
                        continue

                    available_quarters.append({
                        "quarter": quarter,
                        "fiscal_year": fiscal_year,
                        "filing_date": filing_date,
                        "period_end_date": period_date,
                        "sec_html_url": sec_html_url,
                        "label": f"{quarter} {fiscal_year}"
                    })
                except (ValueError, KeyError) as e:
                    LOG.warning(f"Skipping invalid 10-Q filing entry for {ticker}: {e}")
                    continue

            return available_quarters
    except Exception as e:
        LOG.error(f"Failed to fetch FMP 10-Q list for {ticker}: {e}")
        return []


async def fetch_fmp_transcripts_async(ticker: str, session: aiohttp.ClientSession) -> List[Dict]:
    """Fetch transcripts from FMP using aiohttp (parallel-safe)"""
    try:
        # Use existing fetch_fmp_transcript_list but make it async-safe
        # NOTE: This function still uses requests internally, but we'll refactor later
        # For now, run it in executor to avoid blocking
        loop = asyncio.get_event_loop()
        transcripts = await loop.run_in_executor(None, fetch_fmp_transcript_list, ticker, FMP_API_KEY)

        if not transcripts:
            return []

        # Format quarters using FMP's fiscal year data directly
        quarters = []
        for t in transcripts[:8]:
            try:
                quarters.append({
                    "quarter": t['quarter'],
                    "year": t['year'],
                    "call_date": t['date'][:10],
                    "label": f"Q{t['quarter']} FY{t['year']}"
                })
            except (ValueError, KeyError, IndexError) as e:
                LOG.warning(f"Failed to parse transcript data for {ticker}: {e}")
                continue

        return quarters
    except Exception as e:
        LOG.error(f"Failed to fetch FMP transcripts for {ticker}: {e}")
        return []


async def fetch_fmp_press_releases_async(ticker: str, session: aiohttp.ClientSession) -> List[Dict]:
    """Fetch press releases from FMP using aiohttp (parallel-safe)"""
    try:
        # Use existing fetch_fmp_press_releases but make it async-safe
        loop = asyncio.get_event_loop()
        releases = await loop.run_in_executor(None, fetch_fmp_press_releases, ticker, FMP_API_KEY, 20)

        if not releases:
            return []

        # Format releases (limit to 10 most recent)
        formatted_releases = []
        for release in releases[:10]:
            formatted_releases.append({
                "date": release.get("date"),
                "title": release.get("title")
            })

        return formatted_releases
    except Exception as e:
        LOG.error(f"Failed to fetch FMP press releases for {ticker}: {e}")
        return []

@APP.get("/api/admin/ticker-research-status")
async def get_ticker_research_status(ticker: str = Query(...), token: str = Query(...)):
    """Get comprehensive research status for a ticker - what's available from FMP + what's already generated"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    ticker = ticker.upper().strip()

    try:
        # Validate ticker exists in database
        config = get_ticker_config(ticker)
        if not config or not config.get('has_full_config', True):
            return {"status": "error", "message": "Ticker not found in database"}

        company_name = config.get("company_name", ticker)
        industry = config.get("industry", "N/A")

        # PARALLEL API CALLS - All 4 FMP endpoints fetched simultaneously
        async with aiohttp.ClientSession() as session:
            available_10k, available_10q, available_transcripts, available_press_releases = await asyncio.gather(
                fetch_fmp_10k_list_async(ticker, session),
                fetch_fmp_10q_list_async(ticker, session),
                fetch_fmp_transcripts_async(ticker, session),
                fetch_fmp_press_releases_async(ticker, session),
                return_exceptions=True  # Continue even if one fails
            )

        # Handle exceptions from parallel calls
        if isinstance(available_10k, Exception):
            LOG.error(f"10-K fetch failed for {ticker}: {available_10k}")
            available_10k = []
        if isinstance(available_10q, Exception):
            LOG.error(f"10-Q fetch failed for {ticker}: {available_10q}")
            available_10q = []
        if isinstance(available_transcripts, Exception):
            LOG.error(f"Transcript fetch failed for {ticker}: {available_transcripts}")
            available_transcripts = []
        if isinstance(available_press_releases, Exception):
            LOG.error(f"Press release fetch failed for {ticker}: {available_press_releases}")
            available_press_releases = []

        # Check what's already generated in database
        with db() as conn, conn.cursor() as cur:
            # Check generated 10-Ks
            cur.execute("""
                SELECT fiscal_year, generated_at
                FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
            """, (ticker,))
            generated_10k = {row['fiscal_year']: str(row['generated_at']) for row in cur.fetchall()}

            # Check generated 10-Qs
            cur.execute("""
                SELECT fiscal_year, fiscal_quarter, generated_at
                FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-Q'
            """, (ticker,))
            generated_10q = {f"{row['fiscal_year']}-Q{row['fiscal_quarter']}": str(row['generated_at']) for row in cur.fetchall()}

            # Check generated transcripts
            cur.execute("""
                SELECT fiscal_quarter, fiscal_year, generated_at
                FROM transcript_summaries
                WHERE ticker = %s AND report_type = 'transcript'
            """, (ticker,))
            generated_transcripts = {f"{row['fiscal_year']}-Q{row['fiscal_quarter']}": str(row['generated_at']) for row in cur.fetchall()}

            # Check generated press releases (NEW: company_releases table)
            cur.execute("""
                SELECT filing_date, report_title, generated_at
                FROM company_releases
                WHERE ticker = %s AND source_type = 'fmp_press_release'
            """, (ticker,))
            generated_press_releases = {
                f"{row['filing_date']}|{row['report_title']}": str(row['generated_at'])
                for row in cur.fetchall()
            }

        # Combine FMP data with generation status
        for item in available_10k:
            item['generated'] = generated_10k.get(item['year'])

        for item in available_10q:
            # Handle both 'year' and 'fiscal_year' keys, 'quarter' can be "Q1" or 1
            year = item.get('fiscal_year') or item.get('year')
            quarter = item.get('quarter', '')
            # Strip Q prefix if present
            if isinstance(quarter, str) and quarter.startswith('Q'):
                quarter = quarter[1:]
            key = f"{year}-Q{quarter}"
            item['generated'] = generated_10q.get(key)

            # Normalize fields to match transcript format (frontend expects 'year' and numeric 'quarter')
            item['year'] = year
            item['quarter'] = int(quarter)

        for item in available_transcripts:
            year = item.get('year')
            quarter = item.get('quarter')
            if year and quarter:
                key = f"{year}-Q{quarter}"
                item['generated'] = generated_transcripts.get(key)

        for item in available_press_releases:
            # Key by date|title to support multiple PRs per day
            pr_date = item.get('date', '').split()[0] if item.get('date') else ''
            pr_title = item.get('title', '')
            key = f"{pr_date}|{pr_title}"
            item['generated'] = generated_press_releases.get(key)

        return {
            "status": "success",
            "ticker": ticker,
            "company_name": company_name,
            "industry": industry,
            "available_10k": available_10k,
            "available_10q": available_10q,
            "available_transcripts": available_transcripts,
            "available_press_releases": available_press_releases
        }

    except Exception as e:
        LOG.error(f"Failed to get research status for {ticker}: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

async def check_ticker_missing_financials_status(ticker: str):
    """
    Check one ticker for missing financials - fetches 10-K, 10-Q, Transcript in PARALLEL.
    Returns: status dict with missing items info
    """
    try:
        # Get ticker config
        config = get_ticker_config(ticker)
        if not config:
            return None  # Skip tickers not in database

        company_name = config.get('company_name', ticker)

        # PARALLEL FMP API CALLS - All 3 fetched simultaneously
        profile_response, tenq_response, transcript_response = await asyncio.gather(
            validate_ticker_for_research(ticker=ticker, type="profile"),
            validate_ticker_for_research(ticker=ticker, type="10q"),
            validate_ticker_for_research(ticker=ticker, type="transcript"),
            return_exceptions=True
        )

        # Handle exceptions from parallel calls
        if isinstance(profile_response, Exception):
            LOG.error(f"10-K fetch failed for {ticker}: {profile_response}")
            profile_response = {"valid": False}
        if isinstance(tenq_response, Exception):
            LOG.error(f"10-Q fetch failed for {ticker}: {tenq_response}")
            tenq_response = {"valid": False}
        if isinstance(transcript_response, Exception):
            LOG.error(f"Transcript fetch failed for {ticker}: {transcript_response}")
            transcript_response = {"valid": False}

        # Extract latest from FMP
        latest_10k_year = None
        if profile_response.get("valid"):
            years = profile_response.get("available_years", [])
            if years:
                latest_10k_year = years[0]["year"]

        latest_10q = None
        if tenq_response.get("valid"):
            quarters = tenq_response.get("available_quarters", [])
            if quarters:
                q = quarters[0]
                if isinstance(q, dict):
                    quarter_str = q.get("quarter", "")
                    quarter_num = int(quarter_str[1]) if quarter_str and len(quarter_str) >= 2 else None
                    if quarter_num and q.get("fiscal_year"):
                        latest_10q = {"quarter": quarter_num, "year": q["fiscal_year"]}

        latest_transcript = None
        if transcript_response.get("valid"):
            quarters = transcript_response.get("available_quarters", [])
            if quarters:
                q = quarters[0]
                if isinstance(q, dict):
                    quarter_num = q.get("quarter") if isinstance(q.get("quarter"), int) else None
                    year = q.get("year")
                    if quarter_num and year:
                        latest_transcript = {"quarter": quarter_num, "year": year}

        # Check what we have in database
        with db() as conn, conn.cursor() as cur:
            # Check 10-K
            cur.execute("""
                SELECT fiscal_year FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
                ORDER BY fiscal_year DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()
            our_10k_year = result['fiscal_year'] if result else None

            # Check 10-Q
            cur.execute("""
                SELECT fiscal_year, fiscal_quarter FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-Q'
                ORDER BY fiscal_year DESC, fiscal_quarter DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()
            our_10q = None
            if result:
                quarter_num = int(result['fiscal_quarter'][1]) if result['fiscal_quarter'] and len(result['fiscal_quarter']) > 1 else None
                if quarter_num:
                    our_10q = {"year": result['fiscal_year'], "quarter": quarter_num}

            # Check transcript
            cur.execute("""
                SELECT fiscal_year, fiscal_quarter FROM transcript_summaries
                WHERE ticker = %s AND report_type = 'transcript'
                ORDER BY fiscal_year DESC, fiscal_quarter DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()
            our_transcript = None
            if result:
                quarter_str = result['fiscal_quarter']
                quarter_num = int(quarter_str[1]) if quarter_str and len(quarter_str) > 1 else None
                if quarter_num:
                    our_transcript = {"year": result['fiscal_year'], "quarter": quarter_num}

        # Determine missing items
        missing_10k = latest_10k_year and (not our_10k_year or latest_10k_year > our_10k_year)
        missing_10q = latest_10q and (not our_10q or
                                       latest_10q['year'] > our_10q['year'] or
                                       (latest_10q['year'] == our_10q['year'] and latest_10q['quarter'] > our_10q['quarter']))
        missing_transcript = latest_transcript and (not our_transcript or
                                                     latest_transcript['year'] > our_transcript['year'] or
                                                     (latest_transcript['year'] == our_transcript['year'] and latest_transcript['quarter'] > our_transcript['quarter']))

        return {
            "ticker": ticker,
            "company_name": company_name,
            "missing_10k": missing_10k,
            "missing_10q": missing_10q,
            "missing_transcript": missing_transcript,
            "latest_10k_year": latest_10k_year,
            "latest_10q": latest_10q,
            "latest_transcript": latest_transcript,
            "our_10k_year": our_10k_year,
            "our_10q": our_10q,
            "our_transcript": our_transcript
        }

    except Exception as e:
        LOG.error(f"Error checking status for {ticker}: {e}")
        return None


@APP.post("/api/admin/start-financials-check")
async def start_financials_check(request: Request):
    """
    NEW: Start background job to check missing financials (non-blocking).
    Returns immediately with job_id. Use GET /api/admin/financials-check-status to poll.
    """
    try:
        body = await request.json()
        token = body.get('token')

        if not check_admin_token(token):
            return {"status": "error", "message": "Unauthorized"}

        global _financials_check_job

        with _financials_check_lock:
            # Check if a job is already running
            if _financials_check_job["status"] == "running":
                return {
                    "status": "error",
                    "message": "Check already in progress",
                    "job_id": _financials_check_job["job_id"]
                }

            # Create new job
            import uuid
            job_id = str(uuid.uuid4())[:8]
            _financials_check_job = {
                "job_id": job_id,
                "status": "running",
                "progress": 0,
                "message": "Starting check...",
                "started_at": datetime.now(timezone.utc).isoformat(),
                "completed_at": None,
                "results": None,
                "error": None
            }

        # Start background task (don't await it)
        asyncio.create_task(_run_financials_check_job(job_id))

        return {
            "status": "success",
            "message": "Check started",
            "job_id": job_id
        }

    except Exception as e:
        LOG.error(f"Failed to start financials check: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


@APP.get("/api/admin/financials-check-status")
def get_financials_check_status(token: str, job_id: str = None):
    """
    NEW: Get status of background financials check job.
    Poll this endpoint every 2-3 seconds until status is 'complete' or 'error'.
    """
    try:
        if not check_admin_token(token):
            return {"status": "error", "message": "Unauthorized"}

        with _financials_check_lock:
            # Return current job status
            return {
                "status": "success",
                "job": {
                    "job_id": _financials_check_job["job_id"],
                    "status": _financials_check_job["status"],
                    "progress": _financials_check_job["progress"],
                    "message": _financials_check_job["message"],
                    "started_at": _financials_check_job["started_at"],
                    "completed_at": _financials_check_job["completed_at"],
                    "results": _financials_check_job["results"],
                    "error": _financials_check_job["error"]
                }
            }

    except Exception as e:
        LOG.error(f"Failed to get financials check status: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


async def _run_financials_check_job(job_id: str):
    """
    Background task to check missing financials for all active user tickers.
    Updates global _financials_check_job state as it progresses.
    """
    try:
        global _financials_check_job

        # Get all unique tickers from active users (using normalized schema)
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT DISTINCT ut.ticker
                FROM user_tickers ut
                JOIN users u ON ut.user_id = u.id
                WHERE u.status = 'active'
                ORDER BY ut.ticker
            """)
            user_tickers = [row['ticker'] for row in cur.fetchall() if row['ticker']]

        if not user_tickers:
            with _financials_check_lock:
                _financials_check_job["status"] = "complete"
                _financials_check_job["progress"] = 100
                _financials_check_job["message"] = "No active users found"
                _financials_check_job["completed_at"] = datetime.now(timezone.utc).isoformat()
                _financials_check_job["results"] = []
            return

        total_tickers = len(user_tickers)
        LOG.info(f"[FINANCIALS CHECK] Starting check for {total_tickers} tickers")

        # Update status: fetching data
        with _financials_check_lock:
            _financials_check_job["progress"] = 10
            _financials_check_job["message"] = f"Checking {total_tickers} tickers with FMP..."

        # PARALLEL PROCESSING - All tickers checked simultaneously
        ticker_results = await asyncio.gather(*[
            check_ticker_missing_financials_status(ticker)
            for ticker in user_tickers
        ], return_exceptions=True)

        # Collect results from parallel processing
        tickers_status = []
        for idx, result in enumerate(ticker_results):
            if isinstance(result, Exception):
                LOG.error(f"Ticker status check failed: {result}")
                continue
            if result is not None:  # Skip None returns (ticker not in database)
                tickers_status.append(result)

            # Update progress
            progress = 10 + int((idx + 1) / total_tickers * 85)  # 10-95%
            with _financials_check_lock:
                _financials_check_job["progress"] = progress
                _financials_check_job["message"] = f"Checked {idx + 1}/{total_tickers} tickers..."

        # Complete
        with _financials_check_lock:
            _financials_check_job["status"] = "complete"
            _financials_check_job["progress"] = 100
            _financials_check_job["message"] = f"Check complete - found {len(tickers_status)} tickers"
            _financials_check_job["completed_at"] = datetime.now(timezone.utc).isoformat()
            _financials_check_job["results"] = tickers_status

        LOG.info(f"[FINANCIALS CHECK] Complete - {len(tickers_status)} tickers processed")

    except Exception as e:
        LOG.error(f"[FINANCIALS CHECK] Failed: {e}", exc_info=True)
        with _financials_check_lock:
            _financials_check_job["status"] = "error"
            _financials_check_job["progress"] = 0
            _financials_check_job["message"] = "Check failed"
            _financials_check_job["completed_at"] = datetime.now(timezone.utc).isoformat()
            _financials_check_job["error"] = str(e)


@APP.get("/api/admin/missing-financials-status")
async def get_missing_financials_status(token: str):
    """
    DEPRECATED: Use POST /api/admin/start-financials-check + polling instead.
    This endpoint still works but blocks the HTTP request for 10-30 seconds.

    Get status of missing financials (10-K, 10-Q, Transcripts) for all active user tickers.
    Returns list of tickers with what's available from FMP vs what we have in database.

    OPTIMIZED: Uses parallel FMP API calls (all tickers processed concurrently).
    """
    try:
        if not check_admin_token(token):
            return {"status": "error", "message": "Unauthorized"}

        # Get all unique tickers from active users (using normalized schema)
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT DISTINCT ut.ticker
                FROM user_tickers ut
                JOIN users u ON ut.user_id = u.id
                WHERE u.status = 'active'
                ORDER BY ut.ticker
            """)
            user_tickers = [row['ticker'] for row in cur.fetchall() if row['ticker']]

        if not user_tickers:
            return {"status": "success", "tickers": [], "message": "No active users found"}

        # PARALLEL PROCESSING - All tickers checked simultaneously
        ticker_results = await asyncio.gather(*[
            check_ticker_missing_financials_status(ticker)
            for ticker in user_tickers
        ], return_exceptions=True)

        # Collect results from parallel processing
        tickers_status = []
        for result in ticker_results:
            if isinstance(result, Exception):
                LOG.error(f"Ticker status check failed: {result}")
                continue
            if result is not None:  # Skip None returns (ticker not in database)
                tickers_status.append(result)

        return {
            "status": "success",
            "tickers": tickers_status,
            "total": len(tickers_status)
        }

    except Exception as e:
        LOG.error(f"Failed to get missing financials status: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


# ==============================================================================
# CRON JOB - Check for New Filings (10-K, 10-Q, Transcript, Press Releases)
# ==============================================================================

def db_get_latest_10k(ticker: str) -> Optional[int]:
    """Get latest 10-K fiscal year from database. Returns None if not found."""
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT fiscal_year FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
                ORDER BY fiscal_year DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()
            return result['fiscal_year'] if result else None
    except Exception as e:
        LOG.error(f"Error getting latest 10-K for {ticker}: {e}")
        return None


def db_get_latest_10q(ticker: str) -> Optional[Dict]:
    """Get latest 10-Q from database. Returns dict with year and quarter, or None."""
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT fiscal_year, fiscal_quarter FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-Q'
                ORDER BY fiscal_year DESC, fiscal_quarter DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()
            if result and result['fiscal_quarter']:
                quarter_str = result['fiscal_quarter']
                quarter_num = int(quarter_str[1]) if len(quarter_str) >= 2 else None
                if quarter_num:
                    return {"year": result['fiscal_year'], "quarter": quarter_num}
            return None
    except Exception as e:
        LOG.error(f"Error getting latest 10-Q for {ticker}: {e}")
        return None


def db_get_latest_transcript(ticker: str) -> Optional[Dict]:
    """Get latest transcript from database. Returns dict with year and quarter, or None."""
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT fiscal_year, fiscal_quarter FROM transcript_summaries
                WHERE ticker = %s AND report_type = 'transcript'
                ORDER BY fiscal_year DESC, fiscal_quarter DESC LIMIT 1
            """, (ticker,))
            result = cur.fetchone()
            if result and result['fiscal_quarter']:
                quarter_str = result['fiscal_quarter']
                quarter_num = int(quarter_str[1]) if len(quarter_str) >= 2 else None
                if quarter_num:
                    return {"year": result['fiscal_year'], "quarter": quarter_num}
            return None
    except Exception as e:
        LOG.error(f"Error getting latest transcript for {ticker}: {e}")
        return None


def db_get_latest_earnings_release(ticker: str) -> Optional[Dict]:
    """
    Get latest earnings release from 8-K filings for future executive summary integration.

    Returns: {
        "filing_date": "2025-01-30",
        "exhibit_description": "Earnings Release",
        "raw_content": "...",
        "char_count": 91692
    }

    Returns None if no earnings release found for this ticker.
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT
                    filing_date,
                    exhibit_description,
                    raw_content,
                    char_count
                FROM sec_8k_filings
                WHERE ticker = %s
                  AND exhibit_type = 'earnings_release'
                ORDER BY filing_date DESC
                LIMIT 1
            """, (ticker,))

            result = cur.fetchone()
            if result:
                return {
                    'filing_date': result['filing_date'],
                    'description': result['exhibit_description'],
                    'raw_content': result['raw_content'],
                    'char_count': result['char_count']
                }
            return None

    except Exception as e:
        LOG.error(f"Error getting latest earnings release for {ticker}: {e}")
        return None


def db_has_any_10k_for_ticker(ticker: str) -> bool:
    """
    Check if ticker has ANY 10-K filings (for first-check detection).

    Used by cron for silent initialization logic.

    Returns:
        True if ticker has at least one 10-K, False otherwise
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) as count FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-K'
            """, (ticker,))
            result = cur.fetchone()
            return result['count'] > 0 if result else False
    except Exception as e:
        LOG.error(f"Error checking 10-K filings for {ticker}: {e}")
        return False


def db_has_any_10q_for_ticker(ticker: str) -> bool:
    """
    Check if ticker has ANY 10-Q filings (for first-check detection).

    Used by cron for silent initialization logic.

    Returns:
        True if ticker has at least one 10-Q, False otherwise
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) as count FROM sec_filings
                WHERE ticker = %s AND filing_type = '10-Q'
            """, (ticker,))
            result = cur.fetchone()
            return result['count'] > 0 if result else False
    except Exception as e:
        LOG.error(f"Error checking 10-Q filings for {ticker}: {e}")
        return False


def db_has_any_transcript_for_ticker(ticker: str) -> bool:
    """
    Check if ticker has ANY transcripts (for first-check detection).

    Used by cron for silent initialization logic.

    Returns:
        True if ticker has at least one transcript, False otherwise
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) as count FROM transcript_summaries
                WHERE ticker = %s AND report_type = 'transcript'
            """, (ticker,))
            result = cur.fetchone()
            return result['count'] > 0 if result else False
    except Exception as e:
        LOG.error(f"Error checking transcripts for {ticker}: {e}")
        return False


def db_has_any_press_releases_for_ticker(ticker: str) -> bool:
    """
    DEPRECATED: Use modules.company_releases.db_has_any_fmp_releases_for_ticker() instead.

    Check if ticker has ANY FMP press releases in company_releases table.

    Used for silent initialization logic:
    - If ticker has no press releases â†’ First check (backfill silently)
    - If ticker has press releases â†’ Subsequent check (send emails for new ones)

    Returns:
        bool: True if ticker has at least one press release in DB, False otherwise
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT EXISTS(
                    SELECT 1 FROM company_releases
                    WHERE ticker = %s AND source_type = 'fmp_press_release'
                )
            """, (ticker,))
            result = cur.fetchone()
            return result[0] if result else False
    except Exception as e:
        LOG.error(f"Error checking if {ticker} has press releases: {e}")
        return False


async def check_filings_for_ticker(ticker: str) -> Dict:
    """
    Check for NEW filings and press releases for one ticker.
    Returns dict with counts of new items found and processed.

    Logic:
    - 10-K/10-Q/Transcript: Compare LATEST from FMP vs LATEST from DB
    - Press Releases: Check last 4 from FMP, process if not in DB
    """
    from datetime import datetime

    new_items = {
        "ticker": ticker,
        "10k": 0,
        "10q": 0,
        "transcript": 0,
        "press_release": 0,
        "8k": 0
    }

    try:
        # Get ticker config
        config = get_ticker_config(ticker)
        if not config:
            LOG.warning(f"[{ticker}] Skipping: not in database")
            return new_items

        # === 10-K CHECK ===
        try:
            profile_response = await validate_ticker_for_research(ticker=ticker, type="profile")
            if profile_response.get("valid"):
                years = profile_response.get("available_years", [])
                if years:
                    # Detect first check: Does this ticker have ANY 10-K filings in DB?
                    is_first_check = not db_has_any_10k_for_ticker(ticker)

                    if is_first_check:
                        LOG.info(f"[{ticker}] ðŸ†• First 10-K check - will initialize DB silently (no email)")

                    latest_year_data = years[0]
                    latest_year_fmp = latest_year_data["year"]
                    latest_year_db = db_get_latest_10k(ticker)

                    if not latest_year_db or latest_year_fmp > latest_year_db:
                        # Determine email flag based on initialization status
                        if is_first_check:
                            LOG.info(f"[{ticker}] ðŸ’¾ [INTERNAL] Initializing DB with 10-K FY{latest_year_fmp}")
                            send_email_flag = True  # Send to admin with [INTERNAL] tag
                        else:
                            LOG.info(f"[{ticker}] ðŸ†• [INTERNAL] NEW 10-K: FY{latest_year_fmp}")
                            send_email_flag = True

                        # Queue job for generation
                        batch_id = str(uuid.uuid4())
                        job_config = {
                            "ticker": ticker,
                            "filing_type": "10-K",
                            "fiscal_year": latest_year_fmp,
                            "filing_date": latest_year_data.get("filing_date"),
                            "period_end_date": latest_year_data.get("period_end_date"),
                            "sec_html_url": latest_year_data.get("sec_html_url"),
                            "send_email": send_email_flag  # Dynamic based on first check
                        }

                        with db() as conn, conn.cursor() as cur:
                            cur.execute("""
                                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                VALUES (%s, %s, 1, %s)
                            """, (batch_id, 'processing', json.dumps(job_config)))

                            cur.execute("""
                                INSERT INTO ticker_processing_jobs (
                                    batch_id, ticker, status, phase, progress, config
                                )
                                VALUES (%s, %s, %s, %s, 0, %s)
                                RETURNING job_id
                            """, (batch_id, ticker, 'queued', 'profile_generation', json.dumps(job_config)))

                            conn.commit()

                        new_items["10k"] = 1
        except Exception as e:
            LOG.error(f"[{ticker}] 10-K check failed: {e}")

        # === 10-Q CHECK ===
        try:
            tenq_response = await validate_ticker_for_research(ticker=ticker, type="10q")
            if tenq_response.get("valid"):
                quarters = tenq_response.get("available_quarters", [])
                if quarters:
                    # Detect first check: Does this ticker have ANY 10-Q filings in DB?
                    is_first_check = not db_has_any_10q_for_ticker(ticker)

                    if is_first_check:
                        LOG.info(f"[{ticker}] ðŸ†• First 10-Q check - will initialize DB silently (no email)")

                    q = quarters[0]
                    if isinstance(q, dict):
                        quarter_str = q.get("quarter", "")
                        quarter_fmp = int(quarter_str[1]) if quarter_str and len(quarter_str) >= 2 else None
                        year_fmp = q.get("fiscal_year")

                        if quarter_fmp and year_fmp:
                            latest_10q_db = db_get_latest_10q(ticker)

                            needs_update = False
                            if not latest_10q_db:
                                needs_update = True
                            elif year_fmp > latest_10q_db['year']:
                                needs_update = True
                            elif year_fmp == latest_10q_db['year'] and quarter_fmp > latest_10q_db['quarter']:
                                needs_update = True

                            if needs_update:
                                # Determine email flag based on initialization status
                                if is_first_check:
                                    LOG.info(f"[{ticker}] ðŸ’¾ [INTERNAL] Initializing DB with 10-Q Q{quarter_fmp} {year_fmp}")
                                    send_email_flag = True  # Send to admin with [INTERNAL] tag
                                else:
                                    LOG.info(f"[{ticker}] ðŸ†• [INTERNAL] NEW 10-Q: Q{quarter_fmp} {year_fmp}")
                                    send_email_flag = True

                                # Queue job for generation
                                batch_id = str(uuid.uuid4())
                                job_config = {
                                    "ticker": ticker,
                                    "filing_type": "10-Q",
                                    "fiscal_year": year_fmp,
                                    "fiscal_quarter": f"Q{quarter_fmp}",
                                    "filing_date": q.get("filing_date"),
                                    "period_end_date": q.get("period_end_date"),
                                    "sec_html_url": q.get("sec_html_url"),
                                    "send_email": send_email_flag  # Dynamic based on first check
                                }

                                with db() as conn, conn.cursor() as cur:
                                    cur.execute("""
                                        INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                        VALUES (%s, %s, 1, %s)
                                    """, (batch_id, 'processing', json.dumps(job_config)))

                                    cur.execute("""
                                        INSERT INTO ticker_processing_jobs (
                                            batch_id, ticker, status, phase, progress, config
                                        )
                                        VALUES (%s, %s, %s, %s, 0, %s)
                                        RETURNING job_id
                                    """, (batch_id, ticker, 'queued', '10q_generation', json.dumps(job_config)))

                                    conn.commit()

                                new_items["10q"] = 1
        except Exception as e:
            LOG.error(f"[{ticker}] 10-Q check failed: {e}")

        # === TRANSCRIPT CHECK ===
        try:
            transcript_response = await validate_ticker_for_research(ticker=ticker, type="transcript")
            if transcript_response.get("valid"):
                quarters = transcript_response.get("available_quarters", [])
                if quarters:
                    # Detect first check: Does this ticker have ANY transcripts in DB?
                    is_first_check = not db_has_any_transcript_for_ticker(ticker)

                    if is_first_check:
                        LOG.info(f"[{ticker}] ðŸ†• First transcript check - will initialize DB silently (no email)")

                    if isinstance(quarters[0], dict):
                        quarter_fmp = quarters[0].get('quarter')
                        year_fmp = quarters[0].get('year')
                    else:
                        # Fallback: Parse from label string
                        import re
                        match = re.match(r"Q(\d+)\s+FY(\d{4})", quarters[0].get('label', ''))
                        if match:
                            quarter_fmp = int(match.group(1))
                            year_fmp = int(match.group(2))
                        else:
                            quarter_fmp = None
                            year_fmp = None

                    if quarter_fmp and year_fmp:
                        latest_transcript_db = db_get_latest_transcript(ticker)

                        needs_update = False
                        if not latest_transcript_db:
                            needs_update = True
                        elif year_fmp > latest_transcript_db['year']:
                            needs_update = True
                        elif year_fmp == latest_transcript_db['year'] and quarter_fmp > latest_transcript_db['quarter']:
                            needs_update = True

                        if needs_update:
                            # Determine email flag based on initialization status
                            if is_first_check:
                                LOG.info(f"[{ticker}] ðŸ’¾ [INTERNAL] Initializing DB with transcript Q{quarter_fmp} FY{year_fmp}")
                                send_email_flag = True  # Send to admin with [INTERNAL] tag
                            else:
                                LOG.info(f"[{ticker}] ðŸ†• [INTERNAL] NEW Transcript: Q{quarter_fmp} FY{year_fmp}")
                                send_email_flag = True

                            # Queue job for generation
                            batch_id = str(uuid.uuid4())
                            job_config = {
                                "ticker": ticker,
                                "quarter": quarter_fmp,
                                "year": year_fmp,
                                "send_email": send_email_flag  # Dynamic based on first check
                            }

                            with db() as conn, conn.cursor() as cur:
                                cur.execute("""
                                    INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                    VALUES (%s, %s, 1, %s)
                                """, (batch_id, 'processing', json.dumps(job_config)))

                                cur.execute("""
                                    INSERT INTO ticker_processing_jobs (
                                        batch_id, ticker, status, phase, progress, config
                                    )
                                    VALUES (%s, %s, %s, %s, 0, %s)
                                    RETURNING job_id
                                """, (batch_id, ticker, 'queued', 'transcript_generation', json.dumps(job_config)))

                                conn.commit()

                            new_items["transcript"] = 1
        except Exception as e:
            LOG.error(f"[{ticker}] Transcript check failed: {e}")

        # === PRESS RELEASES CHECK (DISABLED - Dec 2025) ===
        # FMP press release auto-processing disabled. Manual generation via /admin/research still works.
        # To re-enable: uncomment the section below.
        #
        # try:
        #     pr_response = await validate_ticker_for_research(ticker=ticker, type="press_release")
        #     if pr_response.get("valid"):
        #         releases = pr_response.get("available_releases", [])
        #
        #         # Detect first check: Does this ticker have ANY FMP press releases in DB?
        #         from modules.company_releases import (
        #             db_has_any_fmp_releases_for_ticker,
        #             db_check_fmp_release_exists,
        #             db_get_latest_fmp_release_datetime
        #         )
        #         is_first_check = not db_has_any_fmp_releases_for_ticker(ticker)
        #
        #         if is_first_check:
        #             # First check: Initialize DB with ONLY the latest (first) PR silently
        #             LOG.info(f"[{ticker}] ðŸ†• First press release check - will initialize DB with latest PR only (no email)")
        #
        #             if releases:
        #                 first_pr = releases[0]
        #                 pr_datetime = first_pr.get('date', '')  # Full datetime: "2025-11-13 10:00:00"
        #                 pr_title = first_pr.get('title', 'Press Release')
        #
        #                 if pr_datetime and pr_title:
        #                     # Check if already exists (safety check)
        #                     exists = db_check_fmp_release_exists(ticker, pr_datetime, pr_title)
        #                     if not exists:
        #                         LOG.info(f"[{ticker}] ðŸ’¾ [INTERNAL] Initializing DB with latest PR from {pr_datetime}: {pr_title[:60]}...")
        #
        #                         # Queue job for generation
        #                         batch_id = str(uuid.uuid4())
        #                         job_config = {
        #                             "ticker": ticker,
        #                             "report_type": "press_release",
        #                             "pr_date": pr_datetime,  # Store full datetime
        #                             "pr_title": pr_title,
        #                             "send_email": True  # Send to admin with [INTERNAL] tag
        #                         }
        #
        #                         with db() as conn, conn.cursor() as cur:
        #                             cur.execute("""
        #                                 INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
        #                                 VALUES (%s, %s, 1, %s)
        #                             """, (batch_id, 'processing', json.dumps(job_config)))
        #
        #                             cur.execute("""
        #                                 INSERT INTO ticker_processing_jobs (
        #                                     batch_id, ticker, status, phase, progress, config
        #                                 )
        #                                 VALUES (%s, %s, %s, %s, 0, %s)
        #                                 RETURNING job_id
        #                             """, (batch_id, ticker, 'queued', 'press_release_generation', json.dumps(job_config)))
        #
        #                             conn.commit()
        #
        #                         new_items["press_release"] += 1
        #                     else:
        #                         LOG.info(f"[{ticker}] âœ… Latest PR already in DB, skipping initialization")
        #         else:
        #             # Subsequent checks: Process ALL PRs newer than latest in DB
        #             latest_db_datetime_str = db_get_latest_fmp_release_datetime(ticker)
        #
        #             if latest_db_datetime_str:
        #                 LOG.info(f"[{ticker}] ðŸ” Checking for PRs newer than {latest_db_datetime_str}")
        #
        #                 # Parse DB datetime string to datetime object for accurate comparison
        #                 try:
        #                     latest_db_datetime = datetime.strptime(latest_db_datetime_str, '%Y-%m-%d %H:%M:%S')
        #                 except ValueError:
        #                     LOG.error(f"[{ticker}] Invalid datetime format from DB: {latest_db_datetime_str}")
        #                     latest_db_datetime = None
        #
        #                 if not latest_db_datetime:
        #                     LOG.warning(f"[{ticker}] Could not parse latest DB datetime, skipping check")
        #                 else:
        #                     # Check ALL releases from FMP (not just first 4)
        #                     for pr in releases:
        #                         pr_datetime_str = pr.get('date', '')  # Full datetime: "2025-11-13 14:00:00"
        #                         pr_title = pr.get('title', 'Press Release')
        #
        #                         if pr_datetime_str and pr_title:
        #                             # Parse FMP datetime string to datetime object
        #                             try:
        #                                 pr_datetime = datetime.strptime(pr_datetime_str, '%Y-%m-%d %H:%M:%S')
        #                             except ValueError:
        #                                 LOG.warning(f"[{ticker}] Invalid datetime format from FMP: {pr_datetime_str}, skipping")
        #                                 continue
        #
        #                             # Only process PRs NEWER than latest in DB (datetime object comparison)
        #                             if pr_datetime > latest_db_datetime:
        #                                 # Check if already exists (dedup protection)
        #                                 exists = db_check_fmp_release_exists(ticker, pr_datetime_str, pr_title)
        #                                 if not exists:
        #                                     LOG.info(f"[{ticker}] ðŸ†• NEW Press Release: {pr_title[:60]}... ({pr_datetime_str})")
        #
        #                                     # Queue job for generation (with email)
        #                                     batch_id = str(uuid.uuid4())
        #                                     job_config = {
        #                                         "ticker": ticker,
        #                                         "report_type": "press_release",
        #                                         "pr_date": pr_datetime_str,  # Store full datetime string
        #                                         "pr_title": pr_title,
        #                                         "send_email": True  # Send email for new PRs
        #                                     }
        #
        #                                     with db() as conn, conn.cursor() as cur:
        #                                         cur.execute("""
        #                                             INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
        #                                             VALUES (%s, %s, 1, %s)
        #                                         """, (batch_id, 'processing', json.dumps(job_config)))
        #
        #                                         cur.execute("""
        #                                             INSERT INTO ticker_processing_jobs (
        #                                                 batch_id, ticker, status, phase, progress, config
        #                                             )
        #                                             VALUES (%s, %s, %s, %s, 0, %s)
        #                                             RETURNING job_id
        #                                         """, (batch_id, ticker, 'queued', 'press_release_generation', json.dumps(job_config)))
        #
        #                                         conn.commit()
        #
        #                                     new_items["press_release"] += 1
        #                             else:
        #                                 # FMP sorted newest first, rest are ALL older - stop processing
        #                                 LOG.info(f"[{ticker}] â­ï¸ Reached PRs older than DB ({pr_datetime_str} <= {latest_db_datetime_str}), stopping")
        #                                 break
        #             else:
        #                 LOG.warning(f"[{ticker}] âš ï¸ DB has PRs but couldn't get latest datetime, skipping check")
        # except Exception as e:
        #     LOG.error(f"[{ticker}] Press release check failed: {e}")

        # === 8-K SEC FILINGS CHECK ===
        try:
            # Fetch 8-Ks directly from SEC Edgar (not through validate_ticker_for_research)
            from modules.company_profiles import (
                get_cik_for_ticker,
                parse_sec_8k_filing_list,
                get_8k_html_url,
                extract_8k_item_codes
            )

            try:
                cik = get_cik_for_ticker(ticker)
                filings = parse_sec_8k_filing_list(cik, count=3)

                # ENRICHMENT STEP: Add item_codes (matches manual workflow)
                # This is what /api/sec-validate-ticker does at lines 19097-19140
                enriched_filings = []
                for i, filing in enumerate(filings):
                    try:
                        # Get main 8-K HTML URL from documents index page
                        urls = get_8k_html_url(filing['documents_url'])
                        sec_html_url = urls['main_8k_url']

                        # Extract item codes with rate limiting
                        item_codes = extract_8k_item_codes(sec_html_url, rate_limit_delay=0.15)

                        # Build enriched filing dict
                        enriched_filings.append({
                            'filing_date': filing['filing_date'],
                            'accession_number': filing['accession_number'],
                            'documents_url': filing['documents_url'],
                            'item_codes': item_codes
                        })

                        LOG.info(f"[{ticker}] [{i+1}/{len(filings)}] Item codes: {item_codes}")

                    except Exception as e:
                        LOG.warning(f"[{ticker}] Failed to enrich filing {filing['accession_number']}: {e}")
                        # Add filing with safe defaults
                        enriched_filings.append({
                            'filing_date': filing['filing_date'],
                            'accession_number': filing['accession_number'],
                            'documents_url': filing['documents_url'],
                            'item_codes': 'Unknown'
                        })

                sec_8k_response = {
                    "valid": True if enriched_filings else False,
                    "cik": cik,
                    "available_8ks": enriched_filings
                }
            except Exception as e:
                LOG.warning(f"[{ticker}] Could not fetch 8-Ks from SEC Edgar: {e}")
                sec_8k_response = {"valid": False}

            if sec_8k_response.get("valid"):
                available_8ks = sec_8k_response.get("available_8ks", [])

                # Detect first check: Does this ticker have ANY 8-K filings in DB?
                from modules.company_releases import (
                    db_has_any_8k_for_ticker,
                    db_check_8k_filing_exists,
                    db_get_latest_8k_filing_date
                )
                is_first_check = not db_has_any_8k_for_ticker(ticker)

                if is_first_check:
                    # First check: Initialize DB with ONLY the latest (first) 8-K silently
                    LOG.info(f"[{ticker}] ðŸ†• First 8-K check - will initialize DB with latest 8-K only (no email)")

                    if available_8ks:
                        latest_8k = available_8ks[0]
                        accession_number = latest_8k.get('accession_number')
                        filing_date = latest_8k.get('filing_date')
                        item_codes = latest_8k.get('item_codes')
                        documents_url = latest_8k.get('documents_url')

                        if accession_number and filing_date and documents_url:
                            # Check if already exists (safety check)
                            exists = db_check_8k_filing_exists(ticker, filing_date, accession_number)
                            if not exists:
                                LOG.info(f"[{ticker}] ðŸ’¾ [INTERNAL] Initializing DB with latest 8-K from {filing_date} - Items: {item_codes}")

                                # Queue job for generation
                                batch_id = str(uuid.uuid4())
                                job_config = {
                                    "ticker": ticker,
                                    "cik": sec_8k_response.get("cik"),
                                    "accession_number": accession_number,
                                    "filing_date": filing_date,
                                    "item_codes": item_codes,
                                    "documents_url": documents_url,
                                    "send_email": True  # Send to admin with [INTERNAL] tag
                                }

                                with db() as conn, conn.cursor() as cur:
                                    cur.execute("""
                                        INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                        VALUES (%s, %s, 1, %s)
                                    """, (batch_id, 'processing', json.dumps(job_config)))

                                    cur.execute("""
                                        INSERT INTO ticker_processing_jobs (
                                            batch_id, ticker, status, phase, progress, config
                                        )
                                        VALUES (%s, %s, %s, %s, 0, %s)
                                        RETURNING job_id
                                    """, (batch_id, ticker, 'queued', '8k_summary_generation', json.dumps(job_config)))

                                    conn.commit()

                                new_items["8k"] = 1
                            else:
                                LOG.info(f"[{ticker}] âœ… Latest 8-K already in DB, skipping initialization")
                else:
                    # Subsequent checks: Process NEW 8-Ks not in DB (time-based with early break)
                    latest_db_date_str = db_get_latest_8k_filing_date(ticker)

                    if latest_db_date_str:
                        LOG.info(f"[{ticker}] ðŸ” Checking for 8-Ks newer than {latest_db_date_str}")

                        # Parse DB date string to date object for accurate comparison
                        try:
                            from datetime import datetime
                            latest_db_date = datetime.strptime(latest_db_date_str, '%Y-%m-%d').date()
                        except ValueError:
                            LOG.error(f"[{ticker}] Invalid date format from DB: {latest_db_date_str}")
                            latest_db_date = None

                        if not latest_db_date:
                            LOG.warning(f"[{ticker}] Could not parse latest DB date, skipping check")
                        else:
                            # Check 8-Ks from SEC (newest first, break on older)
                            for filing in available_8ks:
                                accession_number = filing.get('accession_number')
                                filing_date_str = filing.get('filing_date')
                                item_codes = filing.get('item_codes')
                                documents_url = filing.get('documents_url')

                                if accession_number and filing_date_str and documents_url:
                                    # Parse SEC filing date (format: "Jan 30, 2025" or "2025-01-30")
                                    try:
                                        # Try parsing common SEC formats
                                        for fmt in ['%b %d, %Y', '%Y-%m-%d']:
                                            try:
                                                filing_date = datetime.strptime(filing_date_str, fmt).date()
                                                break
                                            except ValueError:
                                                continue
                                        else:
                                            LOG.warning(f"[{ticker}] Invalid filing date format: {filing_date_str}, skipping")
                                            continue
                                    except Exception as e:
                                        LOG.warning(f"[{ticker}] Could not parse filing date: {filing_date_str}, skipping")
                                        continue

                                    # Only process 8-Ks NEWER than OR EQUAL TO latest in DB (date comparison)
                                    # Use < (strictly less than) to allow same-day processing
                                    if filing_date < latest_db_date:
                                        # SEC returns newest first, so rest are all older - stop processing
                                        LOG.info(f"[{ticker}] â­ï¸ Reached 8-Ks older than DB ({filing_date} < {latest_db_date}), stopping")
                                        break

                                    # Check if this specific 8-K already processed (handles same-day via accession)
                                    exists = db_check_8k_filing_exists(ticker, filing_date_str, accession_number)
                                    if not exists:
                                        LOG.info(f"[{ticker}] ðŸ†• NEW 8-K from {filing_date_str} - Items: {item_codes}")

                                        # Queue job for generation (with email)
                                        batch_id = str(uuid.uuid4())
                                        job_config = {
                                            "ticker": ticker,
                                            "cik": sec_8k_response.get("cik"),
                                            "accession_number": accession_number,
                                            "filing_date": filing_date_str,
                                            "item_codes": item_codes,
                                            "documents_url": documents_url,
                                            "send_email": True  # Send email for new 8-Ks
                                        }

                                        with db() as conn, conn.cursor() as cur:
                                            cur.execute("""
                                                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                                VALUES (%s, %s, 1, %s)
                                            """, (batch_id, 'processing', json.dumps(job_config)))

                                            cur.execute("""
                                                INSERT INTO ticker_processing_jobs (
                                                    batch_id, ticker, status, phase, progress, config
                                                )
                                                VALUES (%s, %s, %s, %s, 0, %s)
                                                RETURNING job_id
                                            """, (batch_id, ticker, 'queued', '8k_summary_generation', json.dumps(job_config)))

                                            conn.commit()

                                        new_items["8k"] += 1
                    else:
                        LOG.warning(f"[{ticker}] âš ï¸ DB has 8-Ks but couldn't get latest date, skipping check")
        except Exception as e:
            LOG.error(f"[{ticker}] 8-K check failed: {e}")

        return new_items

    except Exception as e:
        LOG.error(f"[{ticker}] Error checking filings: {e}")
        return new_items


def check_all_filings_cron():
    """
    Cron job: Check all active user tickers for new filings.
    Called by: python app.py check_filings

    Used for:
    - 6:30 AM daily check
    - Hourly checks (8:30 AM, 9:30 AM, 10:30 AM... 9:30 PM)
    """
    LOG.info("="*80)
    LOG.info("ðŸ” CHECKING FOR NEW FILINGS")
    LOG.info("="*80)

    try:
        # Get active user tickers from normalized schema
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT DISTINCT ut.ticker
                FROM users u
                JOIN user_tickers ut ON u.id = ut.user_id
                WHERE u.status = 'active'
            """)
            rows = cur.fetchall()

        # Collect unique tickers
        tickers = sorted([row['ticker'].upper() for row in rows if row['ticker']])
        LOG.info(f"ðŸ“Š Checking {len(tickers)} tickers: {', '.join(tickers)}")

        # Check each ticker
        total_new = {"10k": 0, "10q": 0, "transcript": 0, "press_release": 0}

        async def check_all():
            results = []
            for ticker in tickers:
                result = await check_filings_for_ticker(ticker)
                results.append(result)
            return results

        results = asyncio.run(check_all())

        # Aggregate results
        for result in results:
            total_new["10k"] += result["10k"]
            total_new["10q"] += result["10q"]
            total_new["transcript"] += result["transcript"]
            total_new["press_release"] += result["press_release"]

        LOG.info("="*80)
        LOG.info("âœ… FILINGS CHECK COMPLETE")
        LOG.info(f"   10-K: {total_new['10k']} new")
        LOG.info(f"   10-Q: {total_new['10q']} new")
        LOG.info(f"   Transcripts: {total_new['transcript']} new")
        LOG.info(f"   Press Releases: {total_new['press_release']} new")
        LOG.info("="*80)

    except Exception as e:
        LOG.error(f"âŒ Filings check failed: {e}")
        raise


async def process_ticker_missing_financials(ticker: str):
    """
    Process one ticker for missing financials - fetches 10-K, 10-Q, Transcript in PARALLEL.
    Creates jobs for all missing items (10-K, 10-Q, Transcripts).
    Returns: ticker_jobs (list of job dicts) or [] on error
    """
    try:
        # Get ticker config
        config = get_ticker_config(ticker)
        if not config:
            LOG.warning(f"Skipping {ticker}: not in database")
            return ([], [])

        # PARALLEL FMP API CALLS - All 3 fetched simultaneously
        profile_response, tenq_response, transcript_response = await asyncio.gather(
            validate_ticker_for_research(ticker=ticker, type="profile"),
            validate_ticker_for_research(ticker=ticker, type="10q"),
            validate_ticker_for_research(ticker=ticker, type="transcript"),
            return_exceptions=True
        )

        # Handle exceptions from parallel calls
        if isinstance(profile_response, Exception):
            LOG.error(f"10-K fetch failed for {ticker}: {profile_response}")
            profile_response = {"valid": False}
        if isinstance(tenq_response, Exception):
            LOG.error(f"10-Q fetch failed for {ticker}: {tenq_response}")
            tenq_response = {"valid": False}
        if isinstance(transcript_response, Exception):
            LOG.error(f"Transcript fetch failed for {ticker}: {transcript_response}")
            transcript_response = {"valid": False}

        ticker_jobs = []

        # Process 10-K
        if profile_response.get("valid"):
            years = profile_response.get("available_years", [])
            if years:
                latest_year_data = years[0]
                latest_year = latest_year_data["year"]

                # Check if we have this year
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        SELECT fiscal_year FROM sec_filings
                        WHERE ticker = %s AND filing_type = '10-K'
                        ORDER BY fiscal_year DESC LIMIT 1
                    """, (ticker,))
                    result = cur.fetchone()
                    our_year = result['fiscal_year'] if result else None

                if not our_year or latest_year > our_year:
                    # Queue 10-K generation job
                    batch_id = str(uuid.uuid4())
                    job_config = {
                        "ticker": ticker,
                        "filing_type": "10-K",
                        "fiscal_year": latest_year,
                        "filing_date": latest_year_data.get("filing_date"),
                        "period_end_date": latest_year_data.get("period_end_date"),
                        "sec_html_url": latest_year_data.get("sec_html_url"),
                        "send_email": True
                    }

                    with db() as conn, conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                            VALUES (%s, %s, 1, %s)
                        """, (batch_id, 'processing', json.dumps(job_config)))

                        cur.execute("""
                            INSERT INTO ticker_processing_jobs (
                                batch_id, ticker, status, phase, progress, config
                            )
                            VALUES (%s, %s, %s, %s, 0, %s)
                            RETURNING job_id
                        """, (batch_id, ticker, 'queued', 'profile_generation', json.dumps(job_config)))

                        job_id = cur.fetchone()['job_id']
                        conn.commit()

                    ticker_jobs.append({"ticker": ticker, "items": [f"10-K FY{latest_year}"]})
                    LOG.info(f"âœ… Queued 10-K job for {ticker} FY{latest_year}")

        # Process 10-Q
        if tenq_response.get("valid"):
            quarters = tenq_response.get("available_quarters", [])
            if quarters:
                q = quarters[0]
                if isinstance(q, dict):
                    quarter_str = q.get("quarter", "")
                    quarter = int(quarter_str[1]) if quarter_str and len(quarter_str) >= 2 else None
                    year = q.get("fiscal_year")

                    if quarter and year:
                        # Check if we have this quarter
                        with db() as conn, conn.cursor() as cur:
                            cur.execute("""
                                SELECT fiscal_year, fiscal_quarter FROM sec_filings
                                WHERE ticker = %s AND filing_type = '10-Q'
                                ORDER BY fiscal_year DESC, fiscal_quarter DESC LIMIT 1
                            """, (ticker,))
                            result = cur.fetchone()
                            our_10q = None
                            if result:
                                quarter_num = int(result['fiscal_quarter'][1]) if result['fiscal_quarter'] and len(result['fiscal_quarter']) > 1 else None
                                if quarter_num:
                                    our_10q = {"year": result['fiscal_year'], "quarter": quarter_num}

                        if not our_10q or year > our_10q['year'] or (year == our_10q['year'] and quarter > our_10q['quarter']):
                            filing_date = q.get("filing_date")
                            period_end_date = q.get("period_end_date")
                            sec_html_url = q.get("sec_html_url")

                            # Queue 10-Q generation job
                            batch_id = str(uuid.uuid4())
                            job_config = {
                                "ticker": ticker,
                                "filing_type": "10-Q",
                                "fiscal_year": year,
                                "fiscal_quarter": f"Q{quarter}",
                                "filing_date": filing_date,
                                "period_end_date": period_end_date,
                                "sec_html_url": sec_html_url,
                                "send_email": True
                            }

                            with db() as conn, conn.cursor() as cur:
                                cur.execute("""
                                    INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                    VALUES (%s, %s, 1, %s)
                                """, (batch_id, 'processing', json.dumps(job_config)))

                                cur.execute("""
                                    INSERT INTO ticker_processing_jobs (
                                        batch_id, ticker, status, phase, progress, config
                                    )
                                    VALUES (%s, %s, %s, %s, 0, %s)
                                    RETURNING job_id
                                """, (batch_id, ticker, 'queued', '10q_generation', json.dumps(job_config)))

                                job_id = cur.fetchone()['job_id']
                                conn.commit()

                            # Append to existing ticker or create new entry
                            existing = next((j for j in ticker_jobs if j["ticker"] == ticker), None)
                            if existing:
                                existing["items"].append(f"10-Q Q{quarter} {year}")
                            else:
                                ticker_jobs.append({"ticker": ticker, "items": [f"10-Q Q{quarter} {year}"]})
                            LOG.info(f"âœ… Queued 10-Q job for {ticker} Q{quarter} {year}")

        # Process Transcript
        if transcript_response.get("valid"):
            quarters = transcript_response.get("available_quarters", [])
            if quarters:
                if isinstance(quarters[0], dict):
                    quarter = quarters[0].get('quarter')
                    year = quarters[0].get('year')
                else:
                    # Fallback: Parse from label string
                    import re
                    match = re.match(r"Q(\d+)\s+FY(\d{4})", quarters[0]['label'])
                    if match:
                        quarter = int(match.group(1))
                        year = int(match.group(2))
                    else:
                        LOG.warning(f"Failed to parse transcript quarter for {ticker}: {quarters[0]}")
                        quarter = None
                        year = None

                if quarter and year:
                    # Check if we have this transcript
                    with db() as conn, conn.cursor() as cur:
                        cur.execute("""
                            SELECT fiscal_year, fiscal_quarter FROM transcript_summaries
                            WHERE ticker = %s AND report_type = 'transcript'
                            ORDER BY fiscal_year DESC, fiscal_quarter DESC LIMIT 1
                        """, (ticker,))
                        result = cur.fetchone()
                        our_transcript = None
                        if result:
                            quarter_str = result['fiscal_quarter']
                            quarter_num = int(quarter_str[1]) if quarter_str and len(quarter_str) > 1 else None
                            if quarter_num:
                                our_transcript = {"year": result['fiscal_year'], "quarter": quarter_num}

                    if not our_transcript or year > our_transcript['year'] or (year == our_transcript['year'] and quarter > our_transcript['quarter']):
                        # Queue transcript generation job (same as 10-K/10-Q)
                        batch_id = str(uuid.uuid4())
                        job_config = {
                            "ticker": ticker,
                            "quarter": quarter,
                            "year": year,
                            "send_email": True
                        }

                        with db() as conn, conn.cursor() as cur:
                            cur.execute("""
                                INSERT INTO ticker_processing_batches (batch_id, status, total_jobs, config)
                                VALUES (%s, %s, 1, %s)
                            """, (batch_id, 'processing', json.dumps(job_config)))

                            cur.execute("""
                                INSERT INTO ticker_processing_jobs (
                                    batch_id, ticker, status, phase, progress, config
                                )
                                VALUES (%s, %s, %s, %s, 0, %s)
                                RETURNING job_id
                            """, (batch_id, ticker, 'queued', 'transcript_generation', json.dumps(job_config)))

                            job_id = cur.fetchone()['job_id']
                            conn.commit()

                        # Append to existing ticker or create new entry
                        existing = next((j for j in ticker_jobs if j["ticker"] == ticker), None)
                        if existing:
                            existing["items"].append(f"Transcript Q{quarter} FY{year}")
                        else:
                            ticker_jobs.append({"ticker": ticker, "items": [f"Transcript Q{quarter} FY{year}"]})
                        LOG.info(f"âœ… Queued transcript job for {ticker} Q{quarter} FY{year}")

        return ticker_jobs

    except Exception as e:
        LOG.error(f"Error processing {ticker}: {e}")
        return []


@APP.post("/api/admin/generate-missing-financials")
async def generate_missing_financials(request: Request):
    """
    Generate missing financials (10-K, 10-Q, Transcripts) for selected tickers.
    Queues jobs for missing items only (skips if we have latest).

    OPTIMIZED: Uses parallel FMP API calls (all tickers processed concurrently).
    """
    try:
        body = await request.json()
        token = body.get('token')

        if not check_admin_token(token):
            return {"status": "error", "message": "Unauthorized"}

        selected_tickers = body.get('tickers', [])
        if not selected_tickers:
            return {"status": "error", "message": "No tickers selected"}

        LOG.info(f"ðŸ“‘ Generating missing financials for {len(selected_tickers)} tickers")

        # PARALLEL PROCESSING - All tickers processed simultaneously
        ticker_results = await asyncio.gather(*[
            process_ticker_missing_financials(ticker)
            for ticker in selected_tickers
        ], return_exceptions=True)

        # Collect results from parallel processing
        jobs_created = []

        for result in ticker_results:
            if isinstance(result, Exception):
                LOG.error(f"Ticker processing failed: {result}")
                continue

            ticker_jobs = result
            jobs_created.extend(ticker_jobs)

        # Transcripts now queued as jobs (no longer generated inline)
        # Worker will process them with proper concurrency control

        # Calculate totals
        total_jobs = sum(len(j["items"]) for j in jobs_created)
        ticker_count = len(jobs_created)

        return {
            "status": "success",
            "message": f"Queued {total_jobs} jobs for {ticker_count} tickers",
            "jobs": jobs_created,
            "total_jobs": total_jobs,
            "ticker_count": ticker_count
        }

    except Exception as e:
        LOG.error(f"Failed to generate missing financials: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/restart-worker")
async def restart_worker_api(request: Request):
    """Restart worker thread only (gentle, 0 downtime)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        LOG.warning("ðŸ”„ Admin requested worker thread restart")
        restart_worker_thread()
        return {"status": "success", "message": "Worker thread restarted successfully"}
    except Exception as e:
        LOG.error(f"Failed to restart worker: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/admin/restart-server")
async def restart_server_api(request: Request):
    """Force full server restart (exits process, Render auto-restarts, ~10-20s downtime)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    LOG.critical("ðŸ’€ Admin requested FULL SERVER RESTART - exiting process in 2 seconds")
    LOG.critical("Render will automatically detect the crash and restart the service")

    # Give time for response to be sent
    import asyncio
    await asyncio.sleep(2)

    # Force exit (Render will restart the entire service)
    os._exit(1)

# Email Queue API endpoints
@APP.get("/api/queue-status")
def get_queue_status(token: str = Query(...)):
    """
    Get unified queue status - combines active job processing AND email queue.

    Shows:
    - Active jobs from ticker_processing_jobs (mode='daily', status='processing')
    - Completed emails from email_queue (ready/sent/failed/cancelled)

    This provides end-to-end visibility from job start to email delivery.
    """
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Query 1: Get active processing jobs (mode='daily')
            cur.execute("""
                SELECT
                    j.ticker,
                    j.status as job_status,
                    j.phase,
                    j.progress,
                    j.worker_id,
                    j.started_at,
                    j.last_updated,
                    j.error_message,
                    j.config->>'recipients' as recipients_json,
                    EXTRACT(EPOCH FROM (NOW() - j.started_at)) / 60 AS minutes_running
                FROM ticker_processing_jobs j
                WHERE j.config->>'mode' = 'daily'
                AND j.status IN ('queued', 'processing')
                ORDER BY j.created_at
            """)
            active_jobs = cur.fetchall()

            # Query 1b: Get job stats from production batches (mode='daily')
            # Expected = total_jobs from today's production batch (captured at run start)
            # Failed = count of failed jobs in those batches
            cur.execute("""
                SELECT
                    (SELECT COALESCE(SUM(total_jobs), 0)
                     FROM ticker_processing_batches
                     WHERE created_at >= CURRENT_DATE
                     AND config->>'mode' = 'daily') as expected,
                    COUNT(*) FILTER (WHERE j.status = 'failed') as failed_run,
                    COUNT(*) FILTER (WHERE j.status = 'completed') as completed
                FROM ticker_processing_jobs j
                JOIN ticker_processing_batches b ON j.batch_id = b.batch_id
                WHERE b.created_at >= CURRENT_DATE
                AND b.config->>'mode' = 'daily'
            """)
            job_stats = cur.fetchone()
            expected_count = job_stats['expected'] or 0
            failed_run_count = job_stats['failed_run'] or 0
            completed_count = job_stats['completed'] or 0

            # Query 2: Get email queue (completed emails) - PRODUCTION ONLY
            cur.execute("""
                SELECT ticker, company_name, recipients, email_subject,
                       article_count, status, error_message, heartbeat,
                       created_at, updated_at, sent_at, report_type
                FROM email_queue
                WHERE is_production = TRUE
                ORDER BY
                    CASE status
                        WHEN 'processing' THEN 1
                        WHEN 'ready' THEN 2
                        WHEN 'failed' THEN 3
                        WHEN 'sent' THEN 4
                        WHEN 'cancelled' THEN 5
                    END,
                    ticker
            """)
            email_queue_rows = cur.fetchall()

            # Build unified ticker list
            tickers_dict = {}

            # Add email queue entries
            for row in email_queue_rows:
                tickers_dict[row['ticker']] = {
                    "ticker": row['ticker'],
                    "company_name": row.get('company_name'),
                    "recipients": row.get('recipients'),
                    "email_subject": row.get('email_subject'),
                    "article_count": row.get('article_count'),
                    "status": row['status'],  # ready, sent, failed, cancelled
                    "error_message": row.get('error_message'),
                    "heartbeat": row.get('heartbeat'),
                    "created_at": row.get('created_at'),
                    "updated_at": row.get('updated_at'),
                    "sent_at": row.get('sent_at'),
                    "report_type": row.get('report_type', 'daily'),  # NEW: Add report_type
                    "source": "email_queue",
                    "progress": None,  # Email queue doesn't have progress
                    "phase": None
                }

            # Add active jobs (override if ticker exists in email_queue)
            for job in active_jobs:
                ticker = job['ticker']

                # Parse recipients from JSON if available
                recipients_json = job.get('recipients_json')
                recipients = json.loads(recipients_json) if recipients_json else None

                tickers_dict[ticker] = {
                    "ticker": ticker,
                    "company_name": None,  # Will be filled when email generated
                    "recipients": recipients,
                    "email_subject": None,
                    "article_count": None,
                    "status": "processing",  # Active job
                    "error_message": job.get('error_message'),
                    "heartbeat": job.get('last_updated'),
                    "created_at": job.get('started_at'),
                    "updated_at": job.get('last_updated'),
                    "sent_at": None,
                    "source": "job_queue",
                    "progress": job.get('progress'),  # 0-100%
                    "phase": job.get('phase'),  # e.g. "ingest_complete", "digest_start"
                    "minutes_running": round(job.get('minutes_running', 0), 1) if job.get('minutes_running') else 0
                }

            # Convert dict to list and sort
            tickers_list = list(tickers_dict.values())
            tickers_list.sort(key=lambda x: (
                1 if x['status'] == 'processing' else
                2 if x['status'] == 'ready' else
                3 if x['status'] == 'failed' else
                4 if x['status'] == 'sent' else 5,
                x['ticker']
            ))

            # Calculate stats
            # Note: failed_sent is email send failures from email_queue
            # failed_run comes from ticker_processing_jobs (job failures after retries)
            stats = {
                "expected": expected_count,
                "processing": sum(1 for t in tickers_list if t['status'] == 'processing'),
                "failed_run": failed_run_count,
                "ready": sum(1 for t in tickers_list if t['status'] == 'ready'),
                "cancelled": sum(1 for t in tickers_list if t['status'] == 'cancelled'),
                "sent": sum(1 for t in tickers_list if t['status'] == 'sent'),
                "failed_sent": sum(1 for t in tickers_list if t['status'] == 'failed')
            }

            return {
                "status": "success",
                "tickers": tickers_list,
                # Flatten stats for frontend (expects data.ready, not data.stats.ready)
                "expected": stats["expected"],
                "processing": stats["processing"],
                "failed_run": stats["failed_run"],
                "completed": completed_count,
                "ready": stats["ready"],
                "cancelled": stats["cancelled"],
                "sent": stats["sent"],
                "failed_sent": stats["failed_sent"]
            }
    except Exception as e:
        LOG.error(f"Failed to get queue status: {e}")
        LOG.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}

@APP.post("/api/send-all-ready")
async def send_all_ready_api(request: Request):
    """Send all ready emails"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    result = send_all_ready_emails_impl()
    return result

@APP.post("/api/fix-inconsistent-emails")
async def fix_inconsistent_emails_api(request: Request):
    """Fix emails that have sent_at set but status is still 'ready' (data inconsistency)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Find inconsistent emails
            cur.execute("""
                SELECT ticker, status, sent_at
                FROM email_queue
                WHERE status = 'ready' AND sent_at IS NOT NULL
            """)
            inconsistent = cur.fetchall()

            # Fix them by setting status to 'sent'
            cur.execute("""
                UPDATE email_queue
                SET status = 'sent'
                WHERE status = 'ready' AND sent_at IS NOT NULL
            """)
            fixed_count = cur.rowcount
            conn.commit()

            LOG.warning(f"ðŸ”§ Fixed {fixed_count} emails with inconsistent status (ready but already sent)")

            return {
                "status": "success",
                "fixed_count": fixed_count,
                "fixed_tickers": [e['ticker'] for e in inconsistent],
                "message": f"Fixed {fixed_count} emails that were already sent but had status='ready'"
            }
    except Exception as e:
        LOG.error(f"Failed to fix inconsistent emails: {e}")
        return {"status": "error", "message": str(e)}
@APP.post("/api/rerun-ticker")
async def rerun_ticker_api(request: Request):
    """
    Rerun single ticker - uses unified job queue.

    FIX #3 (Dec 2025): Preserves original report_type from email_queue entry
    so a Monday weekly report retried on Tuesday still uses weekly lookback.
    """
    body = await request.json()
    token = body.get('token')
    ticker = body.get('ticker')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not ticker:
        return {"status": "error", "message": "Ticker required"}

    try:
        # Get recipients AND report_type from email_queue (Fix #3)
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT recipients, report_type FROM email_queue WHERE ticker = %s
            """, (ticker,))
            row = cur.fetchone()

            if not row or not row['recipients']:
                return {"status": "error", "message": f"Ticker {ticker} not found in queue"}

            recipients = row['recipients']
            # Preserve original report_type, or detect from day-of-week if not set
            original_report_type = row.get('report_type')
            if original_report_type:
                report_type = original_report_type
                # Use correct lookback for the report type
                if report_type == 'weekly':
                    lookback_minutes = get_weekly_lookback_minutes()
                else:
                    lookback_minutes = get_daily_lookback_minutes()
                LOG.info(f"[{ticker}] ðŸ“‹ Using preserved report_type: {report_type} ({lookback_minutes} min)")
            else:
                # Fallback: detect from day-of-week
                report_type, lookback_minutes = get_report_type_and_lookback()
                LOG.info(f"[{ticker}] ðŸ“‹ Using day-of-week detection: {report_type} ({lookback_minutes} min)")

        # Submit to job queue system
        with db() as conn, conn.cursor() as cur:
            # Create batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (1, 'admin_ui_rerun', json.dumps({
                "minutes": lookback_minutes,
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily",
                "report_type": report_type
            })))

            batch_id = cur.fetchone()['batch_id']

            # Create single job
            # Queue timeout (4 hours) - processing timeout (45 min) set when claimed
            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            cur.execute("""
                INSERT INTO ticker_processing_jobs (
                    batch_id, ticker, config, timeout_at
                )
                VALUES (%s, %s, %s, %s)
            """, (batch_id, ticker, json.dumps({
                "minutes": lookback_minutes,
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily",
                "report_type": report_type,
                "recipients": recipients
            }), timeout_at))

            conn.commit()

        LOG.info(f"[{ticker}] ðŸ”„ Re-run triggered by admin (batch {batch_id}, report_type={report_type})")
        return {
            "status": "success",
            "ticker": ticker,
            "batch_id": str(batch_id),
            "report_type": report_type,
            "message": f"Processing started ({report_type} report). Check dashboard in 2-3 minutes."
        }

    except Exception as e:
        LOG.error(f"Failed to rerun ticker {ticker}: {e}")
        return {"status": "error", "message": str(e)}


# =============================================================================
# DEPRECATED: Old synchronous regenerate endpoint - replaced by worker-based
# /api/regenerate-email-job endpoint (Dec 2025)
#
# Keeping commented out for reference. The new endpoint:
# - Uses job queue for background processing
# - Supports parallel execution (multiple tickers at once)
# - Provides real-time progress updates
# - No HTTP timeout risk
# =============================================================================
# @APP.post("/api/regenerate-email")
# async def regenerate_email_api(request: Request):
#     """
#     Regenerate user-facing Email #3 for a ticker using the SAME articles from the original run.
#
#     This endpoint:
#     1. Fetches original article IDs from executive_summaries table
#     2. Fetches those exact articles with AI summaries
#     3. Regenerates executive summary using AI (Phase 1 + Phase 2 + Phase 3)
#     4. Saves new summary to database
#     5. Generates new Email #3 HTML (user-facing)
#     6. Updates email_queue
#     7. Sends Email #2 (Content QA) + Email #3 preview to admin
#
#     Key: Uses article IDs as source of truth (not date filters).
#     """
#     body = await request.json()
#     token = body.get('token')
#     ticker = body.get('ticker')
#     date_str = body.get('date')  # Optional, defaults to today
#
#     if not check_admin_token(token):
#         return {"status": "error", "message": "Unauthorized"}
#
#     if not ticker:
#         return {"status": "error", "message": "Ticker required"}
#
#     try:
#         LOG.info(f"ðŸ”„ [{ticker}] Regenerating user-facing Email #3 for CURRENT_DATE (UTC)")
#
#         # Step 1: Fetch ticker config
#         config = get_ticker_config(ticker)
#         if not config:
#             return {"status": "error", "message": f"No config found for {ticker}"}
#
#         ... (550+ lines of deprecated synchronous code removed)
#
#         See process_regenerate_email_phase() for the worker-based implementation
#         that handles all the same logic but with progress tracking and parallel execution.
#
#     except Exception as e:
#         LOG.error(f"âŒ Failed to regenerate email for {ticker}: {e}")
#         LOG.error(traceback.format_exc())
#         return {"status": "error", "message": str(e)}
# =============================================================================


@APP.post("/api/quality-review-job")
async def submit_quality_review_job(request: Request):
    """
    Submit quality review job(s) to job queue for background processing.

    Supports both individual ticker review and bulk review.
    Jobs process in background with 3 concurrent workers.
    Returns batch_id for status polling via /jobs/batch/{batch_id}
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    tickers = body.get('tickers', [])
    if not tickers:
        return {"status": "error", "message": "No tickers provided"}

    # Normalize tickers to list
    if isinstance(tickers, str):
        tickers = [tickers]

    review_date = body.get('review_date')  # Optional, defaults to today in worker
    send_email = body.get('send_email', True)

    try:
        # Create batch
        batch_id = str(uuid.uuid4())

        with db() as conn, conn.cursor() as cur:
            # Insert batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (
                    batch_id, total_jobs, created_by, config
                )
                VALUES (%s, %s, %s, %s)
            """, (
                batch_id,
                len(tickers),
                'admin',
                json.dumps({
                    'job_type': 'quality_review',
                    'review_date': review_date,
                    'send_email': send_email
                })
            ))

            # Set timeout to 15 minutes (quality review is fast: 2-4 min typical)
            timeout_at = datetime.now(timezone.utc) + timedelta(minutes=15)

            # Create individual jobs
            for ticker in tickers:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id,
                        ticker,
                        phase,
                        config,
                        timeout_at
                    )
                    VALUES (%s, %s, %s, %s, %s)
                """, (
                    batch_id,
                    ticker.upper(),
                    'quality_review_generation',
                    json.dumps({
                        'review_date': review_date,
                        'send_email': send_email
                    }),
                    timeout_at
                ))

        LOG.info(f"âœ… Quality review batch {batch_id} created: {len(tickers)} jobs submitted")

        return {
            "status": "success",
            "batch_id": batch_id,
            "total_jobs": len(tickers),
            "message": f"Quality review jobs submitted for {len(tickers)} ticker{'s' if len(tickers) > 1 else ''}. Poll /jobs/batch/{batch_id} for status."
        }

    except Exception as e:
        LOG.error(f"âŒ Failed to submit quality review jobs: {e}")
        LOG.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}


@APP.post("/api/regenerate-email-job")
async def submit_regenerate_email_job(request: Request):
    """
    Submit email regeneration job(s) to job queue for background processing.

    Supports both individual ticker regeneration and bulk regeneration.
    Jobs process in background with concurrent workers.
    Returns batch_id for status polling via /jobs/batch/{batch_id}

    This is the worker-based version of /api/regenerate-email for parallel execution.
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    tickers = body.get('tickers', [])
    ticker = body.get('ticker')  # Support single ticker param too

    # Handle single ticker param
    if ticker and not tickers:
        tickers = [ticker]

    if not tickers:
        return {"status": "error", "message": "No tickers provided"}

    # Normalize tickers to list
    if isinstance(tickers, str):
        tickers = [tickers]

    try:
        # Create batch
        batch_id = str(uuid.uuid4())

        with db() as conn, conn.cursor() as cur:
            # Insert batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (
                    batch_id, total_jobs, created_by, config
                )
                VALUES (%s, %s, %s, %s)
            """, (
                batch_id,
                len(tickers),
                'admin',
                json.dumps({
                    'job_type': 'regenerate_email'
                })
            ))

            # Set timeout to 15 minutes (regeneration is ~2-4 min typical)
            timeout_at = datetime.now(timezone.utc) + timedelta(minutes=15)

            # Create individual jobs
            for t in tickers:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id,
                        ticker,
                        phase,
                        config,
                        timeout_at
                    )
                    VALUES (%s, %s, %s, %s, %s)
                """, (
                    batch_id,
                    t.upper(),
                    'regenerate_email_generation',
                    json.dumps({}),
                    timeout_at
                ))

            conn.commit()

        LOG.info(f"âœ… Email regeneration batch {batch_id} created: {len(tickers)} jobs submitted")

        return {
            "status": "success",
            "batch_id": batch_id,
            "total_jobs": len(tickers),
            "message": f"Email regeneration jobs submitted for {len(tickers)} ticker{'s' if len(tickers) > 1 else ''}. Poll /jobs/batch/{batch_id} for status."
        }

    except Exception as e:
        LOG.error(f"âŒ Failed to submit email regeneration jobs: {e}")
        LOG.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}


@APP.post("/api/retry-failed-and-cancelled")
async def retry_failed_and_cancelled_api(request: Request):
    """
    Retry all failed and cancelled tickers (non-ready only) - uses existing job queue.

    FIX #5 (Dec 2025): Preserves original report_type from email_queue entry
    so a Monday weekly report retried on Tuesday still uses weekly lookback.
    """
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        # CRITICAL: Load CSV from GitHub BEFORE processing
        try:
            sync_ticker_references_from_github()
        except Exception as e:
            LOG.error(f"âŒ CSV sync failed: {e} - continuing anyway")

        # Get all non-ready tickers (failed + cancelled) with their recipients AND report_type
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker, company_name, recipients, status, report_type
                FROM email_queue
                WHERE status IN ('failed', 'cancelled')
                ORDER BY ticker
            """)
            retry_rows = cur.fetchall()

        if not retry_rows:
            return {
                "status": "success",
                "ticker_count": 0,
                "message": "No failed or cancelled tickers to retry"
            }

        # Build ticker info dict with recipients and report_type
        ticker_info = {
            row['ticker']: {
                'recipients': row['recipients'],
                'report_type': row.get('report_type') or 'daily'  # Fallback if not set
            }
            for row in retry_rows
        }

        # Submit to existing job queue system
        tickers_list = sorted(list(ticker_info.keys()))

        # Get default report type for batch config (individual jobs override)
        default_report_type, default_lookback = get_report_type_and_lookback()
        LOG.info(f"ðŸ“… Default Report Type: {default_report_type.upper()}, Default Lookback: {default_lookback}min")

        with db() as conn, conn.cursor() as cur:
            # Create batch record (uses default config, individual jobs may differ)
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'admin_ui_retry', json.dumps({
                "minutes": default_lookback,
                "report_type": default_report_type,
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily"
            })))

            batch_id = cur.fetchone()['batch_id']

            # Create individual jobs - PRESERVE original report_type per ticker (Fix #5)
            # Queue timeout (4 hours) - processing timeout (45 min) set when claimed
            # retry_count defaults to 0, giving fresh 3 attempts
            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for ticker in tickers_list:
                info = ticker_info[ticker]
                report_type = info['report_type']
                # Use correct lookback for each ticker's report type
                if report_type == 'weekly':
                    lookback_minutes = get_weekly_lookback_minutes()
                else:
                    lookback_minutes = get_daily_lookback_minutes()

                LOG.info(f"[{ticker}] ðŸ“‹ Retry with preserved report_type: {report_type} ({lookback_minutes} min)")

                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, ticker, json.dumps({
                    "minutes": lookback_minutes,
                    "report_type": report_type,
                    "batch_size": 3,
                    "triage_batch_size": 3,
                    "mode": "daily",
                    "recipients": info['recipients']
                }), timeout_at))

            conn.commit()

        LOG.info(f"ðŸ”„ Retry failed & cancelled: {len(tickers_list)} tickers (batch {batch_id})")

        # Build response with ticker details
        affected_tickers = [
            {
                "ticker": row['ticker'],
                "company_name": row['company_name'],
                "status": row['status']
            }
            for row in retry_rows
        ]

        return {
            "status": "success",
            "batch_id": str(batch_id),
            "ticker_count": len(ticker_recipients),
            "affected_tickers": affected_tickers,
            "tickers": tickers_list,
            "message": f"Retrying {len(ticker_recipients)} failed/cancelled tickers. Processing time: ~15-30 minutes."
        }

    except Exception as e:
        LOG.error(f"Failed to retry failed & cancelled tickers: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/api/rerun-all-queue")
async def rerun_all_queue_api(request: Request):
    """Rerun ALL tickers in queue from scratch (regardless of status) - uses existing job queue"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        # CRITICAL: Load CSV from GitHub BEFORE processing
        try:
            sync_ticker_references_from_github()
        except Exception as e:
            LOG.error(f"âŒ CSV sync failed: {e} - continuing anyway")

        # Get ALL tickers with their recipients and status
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker, company_name, recipients, status
                FROM email_queue
                ORDER BY ticker
            """)
            all_rows = cur.fetchall()

        if not all_rows:
            return {
                "status": "success",
                "ticker_count": 0,
                "message": "No tickers in queue to rerun"
            }

        # Build ticker_recipients dict
        ticker_recipients = {
            row['ticker']: row['recipients']
            for row in all_rows
        }

        # Submit to existing job queue system
        tickers_list = sorted(list(ticker_recipients.keys()))

        # NEW: Determine report type based on day of week (same logic as other bulk endpoints)
        report_type, lookback_minutes = get_report_type_and_lookback()
        LOG.info(f"ðŸ“… Report Type: {report_type.upper()}, Lookback: {lookback_minutes}min")

        with db() as conn, conn.cursor() as cur:
            # Create batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'admin_ui_rerun_all', json.dumps({
                "minutes": lookback_minutes,
                "report_type": report_type,  # NEW: 'daily' or 'weekly'
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily"
            })))

            batch_id = cur.fetchone()['batch_id']

            # Create individual jobs with mode='daily' and recipients
            # Queue timeout (4 hours) - processing timeout (45 min) set when claimed
            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for ticker in tickers_list:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, ticker, json.dumps({
                    "minutes": lookback_minutes,
                    "report_type": report_type,  # NEW: 'daily' or 'weekly'
                    "batch_size": 3,
                    "triage_batch_size": 3,
                    "mode": "daily",
                    "recipients": ticker_recipients[ticker]
                }), timeout_at))

            conn.commit()

        LOG.info(f"ðŸ”„ Re-run ALL queue: {len(tickers_list)} tickers (batch {batch_id})")

        # Build response with status breakdown
        status_counts = {}
        for row in all_rows:
            status = row['status']
            status_counts[status] = status_counts.get(status, 0) + 1

        return {
            "status": "success",
            "batch_id": str(batch_id),
            "ticker_count": len(ticker_recipients),
            "status_breakdown": status_counts,
            "message": f"Reprocessing ALL {len(ticker_recipients)} tickers from scratch. Processing time: ~30-60 minutes."
        }

    except Exception as e:
        LOG.error(f"Failed to rerun all queue: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/undo-cancel-ready-emails")
async def undo_cancel_ready_emails_api(request: Request):
    """Undo cancellation - restore ALL cancelled emails to their previous status (smart restore)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get affected emails for response
            cur.execute("""
                SELECT ticker, company_name, status, previous_status
                FROM email_queue
                WHERE status = 'cancelled'
            """)
            affected_emails = cur.fetchall()

            # Smart restore: Use previous_status if available, otherwise default to 'ready'
            cur.execute("""
                UPDATE email_queue
                SET status = COALESCE(previous_status, 'ready'),
                    previous_status = NULL,
                    updated_at = NOW()
                WHERE status = 'cancelled'
            """)
            restored_count = cur.rowcount
            conn.commit()

            LOG.info(f"âœ… Restored {restored_count} cancelled emails to previous status")

            # Build response with restoration details
            restored_tickers = [
                {
                    "ticker": email['ticker'],
                    "company_name": email['company_name'],
                    "restored_to": email['previous_status'] or 'ready',
                    "was_cancelled": True
                }
                for email in affected_emails
            ]

            return {
                "status": "success",
                "restored_count": restored_count,
                "restored_tickers": restored_tickers,
                "message": f"Restored {restored_count} cancelled emails to previous status"
            }
    except Exception as e:
        LOG.error(f"Failed to restore cancelled emails: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/generate-user-reports")
async def generate_user_reports_api(request: Request):
    """Generate reports for selected users only (bulk processing) - uses existing job queue"""
    body = await request.json()
    token = body.get('token')
    user_ids = body.get('user_ids', [])

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not user_ids:
        return {"status": "error", "message": "No users selected"}

    try:
        # CRITICAL: Load CSV from GitHub BEFORE processing
        try:
            sync_ticker_references_from_github()
        except Exception as e:
            LOG.error(f"âŒ CSV sync failed: {e} - continuing anyway")

        # Load tickers for selected users only and build ticker â†’ recipients mapping
        ticker_recipients = {}

        with db() as conn, conn.cursor() as cur:
            # Use parameterized query to fetch selected user accounts by ID (normalized schema)
            placeholders = ','.join(['%s'] * len(user_ids))
            cur.execute(f"""
                SELECT u.id, u.name, u.email,
                       ARRAY_AGG(ut.ticker ORDER BY ut.added_at) AS tickers
                FROM users u
                LEFT JOIN user_tickers ut ON u.id = ut.user_id
                WHERE u.id IN ({placeholders})
                AND u.status = 'active'
                GROUP BY u.id, u.name, u.email
                ORDER BY u.created_at
            """, user_ids)
            users = cur.fetchall()

            if not users:
                return {
                    "status": "error",
                    "message": "No active users found with selected account IDs"
                }

            LOG.info(f"Found {len(users)} selected user accounts")

            # Deduplicate tickers and build recipient mapping
            for user in users:
                email = user['email']
                tickers = user['tickers'] or []  # Handle NULL if no tickers
                for ticker in tickers:
                    if ticker:
                        ticker = ticker.upper().strip()
                        if ticker not in ticker_recipients:
                            ticker_recipients[ticker] = []
                        if email not in ticker_recipients[ticker]:
                            ticker_recipients[ticker].append(email)

            LOG.info(f"Dedup complete: {len(ticker_recipients)} unique tickers from {len(users)} users")

        # Submit to existing job queue system
        tickers_list = sorted(list(ticker_recipients.keys()))

        # NEW: Determine report type based on day of week (same logic as cron)
        report_type, lookback_minutes = get_report_type_and_lookback()
        LOG.info(f"ðŸ“… Report Type: {report_type.upper()}, Lookback: {lookback_minutes}min")

        with db() as conn, conn.cursor() as cur:
            # Create batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'admin_ui', json.dumps({
                "minutes": lookback_minutes,
                "report_type": report_type,  # NEW: 'daily' or 'weekly'
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily"  # CRITICAL: Daily workflow mode
            })))

            batch_id = cur.fetchone()['batch_id']

            # Create individual jobs with mode='daily' and recipients
            # Queue timeout (4 hours) - processing timeout (45 min) set when claimed
            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for ticker in tickers_list:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, ticker, json.dumps({
                    "minutes": lookback_minutes,
                    "report_type": report_type,  # NEW: 'daily' or 'weekly'
                    "batch_size": 3,
                    "triage_batch_size": 3,
                    "mode": "daily",  # CRITICAL: Daily workflow mode
                    "recipients": ticker_recipients[ticker]  # CRITICAL: Recipients for Email #3
                }), timeout_at))

            conn.commit()

        LOG.info(f"ðŸ“Š Batch {batch_id} created for {len(users)} selected accounts: {len(tickers_list)} unique tickers")
        LOG.info(f"   Report Type: {report_type.upper()}, Lookback: {lookback_minutes}min")

        return {
            "status": "success",
            "batch_id": str(batch_id),
            "user_count": len(users),
            "ticker_count": len(tickers_list),
            "tickers": tickers_list,
            "message": f"Processing {len(tickers_list)} unique tickers from {len(users)} selected account(s). Check back in 10-20 minutes."
        }

    except Exception as e:
        LOG.error(f"Failed to generate user reports: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/generate-all-reports")
async def generate_all_reports_api(request: Request):
    """Generate reports for ALL active users - uses existing job queue"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        # CRITICAL: Load CSV from GitHub BEFORE processing
        try:
            sync_ticker_references_from_github()
        except Exception as e:
            LOG.error(f"âŒ CSV sync failed: {e} - continuing anyway")

        # Load all active users (same as process_daily_workflow)
        ticker_recipients = load_active_users()

        if not ticker_recipients:
            return {
                "status": "error",
                "message": "No active users found"
            }

        # Submit to existing job queue system
        tickers_list = sorted(list(ticker_recipients.keys()))

        # NEW: Determine report type based on day of week (same logic as cron)
        report_type, lookback_minutes = get_report_type_and_lookback()
        LOG.info(f"ðŸ“… Report Type: {report_type.upper()}, Lookback: {lookback_minutes}min")

        with db() as conn, conn.cursor() as cur:
            # Create batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'admin_ui', json.dumps({
                "minutes": lookback_minutes,
                "report_type": report_type,  # NEW: 'daily' or 'weekly'
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily"  # CRITICAL: Daily workflow mode
            })))

            batch_id = cur.fetchone()['batch_id']

            # Create individual jobs with mode='daily' and recipients
            # Queue timeout (4 hours) - processing timeout (45 min) set when claimed
            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for ticker in tickers_list:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, ticker, json.dumps({
                    "minutes": lookback_minutes,
                    "report_type": report_type,  # NEW: 'daily' or 'weekly'
                    "batch_size": 3,
                    "triage_batch_size": 3,
                    "mode": "daily",  # CRITICAL: Daily workflow mode
                    "recipients": ticker_recipients[ticker]  # CRITICAL: Recipients for Email #3
                }), timeout_at))

            conn.commit()

        LOG.info(f"ðŸ“Š Batch {batch_id} created for all active users: {len(tickers_list)} unique tickers")
        LOG.info(f"   Report Type: {report_type.upper()}, Lookback: {lookback_minutes}min")

        return {
            "status": "success",
            "batch_id": str(batch_id),
            "ticker_count": len(tickers_list),
            "tickers": tickers_list,
            "message": f"Processing {len(tickers_list)} unique tickers from all active users. This will take approximately 30-60 minutes."
        }

    except Exception as e:
        LOG.error(f"Failed to generate all reports: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/clear-all-reports")
async def clear_all_reports_api(request: Request):
    """Clear all queue entries (manual trigger equivalent to: python app.py cleanup)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Delete all email queue entries
            cur.execute("DELETE FROM email_queue")
            deleted_emails = cur.rowcount

            # Also delete today's job records (provides clean slate)
            cur.execute("""
                DELETE FROM ticker_processing_jobs
                WHERE created_at >= CURRENT_DATE
            """)
            deleted_jobs = cur.rowcount

            # Delete today's batch records
            cur.execute("""
                DELETE FROM ticker_processing_batches
                WHERE created_at >= CURRENT_DATE
            """)
            deleted_batches = cur.rowcount

            conn.commit()

        LOG.info(f"ðŸ—‘ï¸ Cleared all reports: {deleted_emails} emails, {deleted_jobs} jobs, {deleted_batches} batches deleted")

        return {
            "status": "success",
            "deleted_emails": deleted_emails,
            "deleted_jobs": deleted_jobs,
            "deleted_batches": deleted_batches,
            "message": f"Deleted {deleted_emails} queue entries, {deleted_jobs} jobs, {deleted_batches} batches"
        }

    except Exception as e:
        LOG.error(f"Failed to clear all reports: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/retry-failed-jobs")
async def retry_failed_jobs_api(request: Request):
    """Retry failed jobs from ticker_processing_jobs (not email_queue)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        # Get failed jobs from ticker_processing_jobs
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT j.ticker, j.config
                FROM ticker_processing_jobs j
                JOIN ticker_processing_batches b ON j.batch_id = b.batch_id
                WHERE j.status = 'failed'
                AND b.created_at >= CURRENT_DATE
                AND b.config->>'mode' = 'daily'
            """)
            failed_jobs = cur.fetchall()

        if not failed_jobs:
            return {
                "status": "success",
                "ticker_count": 0,
                "message": "No failed jobs to retry"
            }

        # Build ticker_recipients dict from job configs
        ticker_recipients = {}
        for job in failed_jobs:
            ticker = job['ticker']
            config = job['config'] if isinstance(job['config'], dict) else json.loads(job['config'])
            recipients = config.get('recipients', [])
            ticker_recipients[ticker] = recipients

        # Submit to job queue
        tickers_list = sorted(list(ticker_recipients.keys()))

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'admin_ui_retry_failed', json.dumps({
                "minutes": get_lookback_minutes(),
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily"
            })))

            batch_id = cur.fetchone()['batch_id']

            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for ticker in tickers_list:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, ticker, json.dumps({
                    "minutes": get_lookback_minutes(),
                    "batch_size": 3,
                    "triage_batch_size": 3,
                    "mode": "daily",
                    "recipients": ticker_recipients[ticker]
                }), timeout_at))

            conn.commit()

        LOG.info(f"ðŸ”„ Retry failed jobs: {len(tickers_list)} tickers (batch {batch_id})")

        return {
            "status": "success",
            "batch_id": str(batch_id),
            "ticker_count": len(tickers_list),
            "tickers": tickers_list,
            "message": f"Retrying {len(tickers_list)} failed jobs"
        }

    except Exception as e:
        LOG.error(f"Failed to retry failed jobs: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/clear-job-queue")
async def clear_job_queue_api(request: Request):
    """Clear only job queue (ticker_processing_jobs and batches) - not email queue"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                DELETE FROM ticker_processing_jobs
                WHERE created_at >= CURRENT_DATE
            """)
            deleted_jobs = cur.rowcount

            cur.execute("""
                DELETE FROM ticker_processing_batches
                WHERE created_at >= CURRENT_DATE
            """)
            deleted_batches = cur.rowcount

            conn.commit()

        LOG.info(f"ðŸ—‘ï¸ Cleared job queue: {deleted_jobs} jobs, {deleted_batches} batches")

        return {
            "status": "success",
            "deleted_jobs": deleted_jobs,
            "deleted_batches": deleted_batches,
            "message": f"Deleted {deleted_jobs} jobs and {deleted_batches} batches"
        }

    except Exception as e:
        LOG.error(f"Failed to clear job queue: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/clear-email-queue")
async def clear_email_queue_api(request: Request):
    """Clear only email queue - not job queue"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("DELETE FROM email_queue")
            deleted_count = cur.rowcount
            conn.commit()

        LOG.info(f"ðŸ—‘ï¸ Cleared email queue: {deleted_count} entries")

        return {
            "status": "success",
            "deleted_count": deleted_count,
            "message": f"Deleted {deleted_count} email queue entries"
        }

    except Exception as e:
        LOG.error(f"Failed to clear email queue: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/regen-all-emails")
async def regen_all_emails_api(request: Request):
    """Regenerate Email #3 for all emails in queue (uses existing articles)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        # Get all tickers from email_queue
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT ticker, recipients
                FROM email_queue
            """)
            email_rows = cur.fetchall()

        if not email_rows:
            return {
                "status": "success",
                "ticker_count": 0,
                "message": "No emails in queue to regenerate"
            }

        # Build ticker list
        tickers_list = [row['ticker'] for row in email_rows]

        # Submit regeneration jobs to queue
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'admin_ui_regen_all', json.dumps({
                "mode": "regen_email"
            })))

            batch_id = cur.fetchone()['batch_id']

            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for row in email_rows:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, row['ticker'], json.dumps({
                    "mode": "regen_email",
                    "recipients": row['recipients']
                }), timeout_at))

            conn.commit()

        LOG.info(f"â™»ï¸ Regen all emails: {len(tickers_list)} tickers (batch {batch_id})")

        return {
            "status": "success",
            "batch_id": str(batch_id),
            "ticker_count": len(tickers_list),
            "tickers": tickers_list,
            "message": f"Regenerating {len(tickers_list)} emails"
        }

    except Exception as e:
        LOG.error(f"Failed to regen all emails: {e}")
        return {"status": "error", "message": str(e)}

# DISABLED (Nov 30, 2025): CSV is source of truth - never write DB back to ticker_reference.csv
# @APP.post("/api/commit-ticker-csv")
# async def commit_ticker_csv_api(request: Request):
#     """
#     Manually commit ticker_reference.csv to GitHub.
#     Triggers Render deployment (~2-3 min downtime).
#     Admin-only endpoint.
#     """
#     body = await request.json()
#     token = body.get('token')
#
#     if not check_admin_token(token):
#         return {"status": "error", "message": "Unauthorized"}
#
#     try:
#         LOG.info("ðŸ”„ Manual GitHub commit requested via admin dashboard")
#         result = commit_ticker_reference_to_github()
#
#         if result['status'] == 'success':
#             LOG.info(f"âœ… Manual commit successful: {result.get('commit_sha', 'N/A')[:8]}")
#             return {
#                 "status": "success",
#                 "message": result['message'],
#                 "rows": result['rows'],
#                 "commit_sha": result.get('commit_sha'),
#                 "commit_url": result.get('commit_url'),
#                 "timestamp": result.get('timestamp')
#             }
#         else:
#             LOG.error(f"âŒ Manual commit failed: {result.get('message')}")
#             return {
#                 "status": "error",
#                 "message": result.get('message', 'Unknown error'),
#                 "rows": result.get('rows', 0)
#             }
#
#     except Exception as e:
#         LOG.error(f"Manual GitHub commit failed: {e}")
#         LOG.error(traceback.format_exc())
#         return {"status": "error", "message": str(e)}

@APP.get("/api/get-domain-stats")
async def get_domain_stats_api(token: str = Query(...)):
    """Get domain quality and scrape stats"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get domains with stats
            cur.execute("""
                SELECT
                    domain,
                    formal_name,
                    quality_score_avg,
                    quality_count,
                    scrape_attempts,
                    scrape_failures,
                    scrape_success_rate,
                    updated_at
                FROM domain_names
                WHERE quality_count >= 5 OR scrape_attempts >= 5
                ORDER BY quality_score_avg DESC NULLS LAST
                LIMIT 250
            """)

            domains = cur.fetchall()

            return {
                "status": "success",
                "domains": [dict(d) for d in domains],
                "count": len(domains)
            }

    except Exception as e:
        LOG.error(f"Failed to get domain stats: {e}")
        return {"status": "error", "message": str(e)}

@APP.get("/api/get-lookback-window")
async def get_lookback_window_api(token: str = Query(...)):
    """Get current production lookback window from system_config"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        minutes = get_lookback_minutes()
        hours = minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        label = f"{days} days" if days > 0 else f"{int(hours)} hours"

        return {
            "status": "success",
            "minutes": minutes,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to get lookback window: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/set-lookback-window")
async def set_lookback_window_api(
    token: str = Query(...),
    minutes: int = Query(...)
):
    """Update production lookback window in system_config"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    # Validation: 1 hour to 7 days
    if minutes < 60:
        return {
            "status": "error",
            "message": "Lookback must be at least 60 minutes (1 hour)"
        }

    if minutes > 10080:
        return {
            "status": "error",
            "message": "Lookback cannot exceed 10,080 minutes (7 days)"
        }

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE system_config
                SET value = %s,
                    updated_at = NOW(),
                    updated_by = 'admin_dashboard'
                WHERE key = 'lookback_minutes'
            """, (str(minutes),))
            conn.commit()

        hours = minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        label = f"{days} days" if days > 0 else f"{int(hours)} hours"

        LOG.info(f"âœ… Production lookback window updated to {minutes} minutes ({label}) via admin dashboard")

        return {
            "status": "success",
            "message": f"Lookback window updated to {label}",
            "minutes": minutes,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to update lookback window: {e}")
        return {"status": "error", "message": str(e)}

# NEW (Nov 2025): Daily/Weekly lookback endpoints
@APP.get("/api/get-daily-lookback-window")
async def get_daily_lookback_window_api(token: str = Query(...)):
    """Get daily lookback window (for Tuesday-Sunday reports)"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        minutes = get_daily_lookback_minutes()
        hours = minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        label = f"{days} days" if days > 0 else f"{int(hours)} hours"

        return {
            "status": "success",
            "minutes": minutes,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to get daily lookback: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/set-daily-lookback-window")
async def set_daily_lookback_window_api(
    token: str = Query(...),
    minutes: int = Query(...)
):
    """Update daily lookback window (for Tuesday-Sunday reports)"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    # Validation: 1 hour to 7 days
    if minutes < 60 or minutes > 10080:
        return {
            "status": "error",
            "message": "Lookback must be 60-10080 minutes (1 hour to 7 days)"
        }

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE system_config
                SET value = %s, updated_at = NOW()
                WHERE key = 'daily_lookback_minutes'
            """, (str(minutes),))
            conn.commit()

        hours = minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        label = f"{days} days" if days > 0 else f"{int(hours)} hours"

        LOG.info(f"âœ… Daily lookback window updated to {minutes} minutes ({label})")

        return {
            "status": "success",
            "message": f"Daily lookback updated to {label}",
            "minutes": minutes,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to update daily lookback: {e}")
        return {"status": "error", "message": str(e)}

@APP.get("/api/get-weekly-lookback-window")
async def get_weekly_lookback_window_api(token: str = Query(...)):
    """Get weekly lookback window (for Monday reports)"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        minutes = get_weekly_lookback_minutes()
        hours = minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        label = f"{days} days" if days > 0 else f"{int(hours)} hours"

        return {
            "status": "success",
            "minutes": minutes,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to get weekly lookback: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/set-weekly-lookback-window")
async def set_weekly_lookback_window_api(
    token: str = Query(...),
    minutes: int = Query(...)
):
    """Update weekly lookback window (for Monday reports)"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    # Validation: 1 hour to 7 days
    if minutes < 60 or minutes > 10080:
        return {
            "status": "error",
            "message": "Lookback must be 60-10080 minutes (1 hour to 7 days)"
        }

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE system_config
                SET value = %s, updated_at = NOW()
                WHERE key = 'weekly_lookback_minutes'
            """, (str(minutes),))
            conn.commit()

        hours = minutes / 60
        days = int(hours / 24) if hours >= 24 else 0
        label = f"{days} days" if days > 0 else f"{int(hours)} hours"

        LOG.info(f"âœ… Weekly lookback window updated to {minutes} minutes ({label})")

        return {
            "status": "success",
            "message": f"Weekly lookback updated to {label}",
            "minutes": minutes,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to update weekly lookback: {e}")
        return {"status": "error", "message": str(e)}

# NEW (Dec 2025): Phase 1.5 Known Information Filter toggle
@APP.get("/api/get-phase15-enabled")
async def get_phase15_enabled_api(token: str = Query(...)):
    """Get Phase 1.5 (Known Information Filter) enabled status"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        enabled = is_phase_1_5_enabled()
        return {
            "status": "success",
            "enabled": enabled,
            "label": "ON" if enabled else "OFF"
        }

    except Exception as e:
        LOG.error(f"Failed to get Phase 1.5 status: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/set-phase15-enabled")
async def set_phase15_enabled_api(request: Request):
    """Enable or disable Phase 1.5 (Known Information Filter)"""
    # Check admin token from header (matches Phase 3 model pattern)
    token = request.headers.get('X-Admin-Token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        body = await request.json()
        enabled_str = body.get('enabled')

        if enabled_str is None:
            return {"status": "error", "message": "Missing 'enabled' parameter"}

        # Handle both string ("true"/"false") and boolean values
        if isinstance(enabled_str, bool):
            enabled = enabled_str
        else:
            enabled = str(enabled_str).lower() == 'true'

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE system_config
                SET value = %s, updated_at = NOW()
                WHERE key = 'phase_1_5_enabled'
            """, ('true' if enabled else 'false',))
            conn.commit()

        label = "ON" if enabled else "OFF"
        LOG.info(f"âœ… Phase 1.5 Known Info Filter set to {label}")

        return {
            "status": "success",
            "message": f"Phase 1.5 set to {label}",
            "enabled": enabled,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to update Phase 1.5 status: {e}")
        return {"status": "error", "message": str(e)}

@APP.get("/api/get-phase3-model")
async def get_phase3_model_api(token: str = Query(...)):
    """Get current Phase 3 primary model setting"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        model = get_phase3_primary_model()

        if model == 'claude':
            label = "Claude Sonnet 4.5 (primary) with Gemini 2.5 Pro fallback"
        else:  # gemini
            label = "Gemini 2.5 Pro (primary) with Claude Sonnet 4.5 fallback"

        return {
            "status": "success",
            "primary_model": model,
            "label": label
        }

    except Exception as e:
        LOG.error(f"Failed to get Phase 3 model: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/set-phase3-model")
async def set_phase3_model_api(request: Request):
    """Update Phase 3 primary model setting"""
    # Check admin token from header
    token = request.headers.get('X-Admin-Token')
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        body = await request.json()
        model = body.get('model')

        if not model:
            return {"status": "error", "message": "Missing 'model' parameter"}

        if model not in ['claude', 'gemini']:
            return {"status": "error", "message": "Invalid model. Must be 'claude' or 'gemini'"}

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                INSERT INTO system_config (key, value, description, updated_by, updated_at)
                VALUES ('phase3_primary_model', %s, 'Phase 3 primary AI model: claude or gemini', 'admin_dashboard', NOW())
                ON CONFLICT (key) DO UPDATE SET value = %s, updated_at = NOW(), updated_by = 'admin_dashboard'
            """, (model, model))
            conn.commit()

        if model == 'claude':
            message = "Phase 3 model updated to Claude Sonnet 4.5 (primary) with Gemini 2.5 Pro fallback"
        else:
            message = "Phase 3 model updated to Gemini 2.5 Pro (primary) with Claude Sonnet 4.5 fallback"

        LOG.info(f"âœ… Phase 3 model setting updated to: {model} via admin dashboard")

        return {
            "status": "success",
            "message": message
        }

    except Exception as e:
        LOG.error(f"Failed to set Phase 3 model: {e}")
        return {"status": "error", "message": str(e)}

# ==============================================================================
# SCHEDULE CONFIGURATION API (Nov 26, 2025)
# ==============================================================================

@APP.get("/api/schedule/config")
async def get_schedule_config_api(token: str = Query(...)):
    """Get schedule configuration for daily/weekly reports"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get daily schedule config
            cur.execute("""
                SELECT day_of_week, report_type, process_time, send_time,
                       cleanup_offset_minutes
                FROM schedule_config
                ORDER BY day_of_week
            """)
            schedule_rows = cur.fetchall()

        # Convert to serializable format
        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        schedule = []
        for row in schedule_rows:
            schedule.append({
                'day_of_week': row['day_of_week'],
                'day_name': day_names[row['day_of_week']],
                'report_type': row['report_type'],
                'process_time': row['process_time'].strftime('%H:%M') if row['process_time'] else None,
                'send_time': row['send_time'].strftime('%H:%M') if row['send_time'] else None,
                'cleanup_offset_minutes': row['cleanup_offset_minutes']
            })

        return {
            "status": "success",
            "schedule": schedule
        }

    except Exception as e:
        LOG.error(f"Failed to get schedule config: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/api/schedule/update-day")
async def update_schedule_day_api(request: Request):
    """Update schedule for a specific day"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        day_of_week = body.get('day_of_week')
        report_type = body.get('report_type')
        process_time = body.get('process_time')
        send_time = body.get('send_time')

        if day_of_week is None or day_of_week < 0 or day_of_week > 6:
            return {"status": "error", "message": "Invalid day_of_week (must be 0-6)"}

        if report_type not in ['daily', 'weekly', 'none']:
            return {"status": "error", "message": "Invalid report_type (must be daily, weekly, or none)"}

        with db() as conn, conn.cursor() as cur:
            if report_type == 'none':
                cur.execute("""
                    UPDATE schedule_config
                    SET report_type = %s, process_time = NULL, send_time = NULL, updated_at = NOW()
                    WHERE day_of_week = %s
                """, (report_type, day_of_week))
            else:
                # Validate times
                if not process_time or not send_time:
                    return {"status": "error", "message": "process_time and send_time required for daily/weekly"}

                cur.execute("""
                    UPDATE schedule_config
                    SET report_type = %s, process_time = %s, send_time = %s, updated_at = NOW()
                    WHERE day_of_week = %s
                """, (report_type, process_time, send_time, day_of_week))

            conn.commit()

        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        LOG.info(f"âœ… Schedule updated for {day_names[day_of_week]}: {report_type}")

        return {
            "status": "success",
            "message": f"Schedule updated for {day_names[day_of_week]}"
        }

    except Exception as e:
        LOG.error(f"Failed to update schedule day: {e}")
        return {"status": "error", "message": str(e)}


@APP.post("/api/schedule/update-offsets")
async def update_schedule_offsets_api(request: Request):
    """Update cleanup offset minutes (applies to all days)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        cleanup_offset = body.get('cleanup_offset_minutes')

        if cleanup_offset is None:
            return {"status": "error", "message": "cleanup_offset_minutes required"}

        if cleanup_offset < 0 or cleanup_offset > 180:
            return {"status": "error", "message": "cleanup_offset must be 0-180 minutes"}

        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE schedule_config
                SET cleanup_offset_minutes = %s, updated_at = NOW()
            """, (cleanup_offset,))
            conn.commit()

        LOG.info(f"âœ… Schedule offsets updated: cleanup={cleanup_offset}min")

        return {
            "status": "success",
            "message": f"Cleanup offset updated: {cleanup_offset}min"
        }

    except Exception as e:
        LOG.error(f"Failed to update schedule offsets: {e}")
        return {"status": "error", "message": str(e)}


@APP.get("/api/schedule/current-time")
async def get_current_toronto_time_api(token: str = Query(...)):
    """Get current Toronto time for display in UI"""
    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        eastern = pytz.timezone('America/Toronto')
        toronto_now = datetime.now(eastern)
        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

        return {
            "status": "success",
            "time": toronto_now.strftime('%H:%M:%S'),
            "date": toronto_now.strftime('%Y-%m-%d'),
            "day_of_week": toronto_now.weekday(),
            "day_name": day_names[toronto_now.weekday()],
            "timezone": "America/Toronto",
            "is_dst": bool(toronto_now.dst())
        }

    except Exception as e:
        LOG.error(f"Failed to get Toronto time: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cancel-ready-emails")
async def cancel_ready_emails_api(request: Request):
    """Cancel ready emails - prevent 8:30am auto-send (tracks previous_status for smart restore)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get affected emails for response
            cur.execute("""
                SELECT ticker, company_name, recipients, status
                FROM email_queue
                WHERE status IN ('ready', 'processing')
            """)
            affected_emails = cur.fetchall()

            # Track previous_status before cancelling
            cur.execute("""
                UPDATE email_queue
                SET previous_status = status,
                    status = 'cancelled',
                    updated_at = NOW()
                WHERE status IN ('ready', 'processing')
            """)
            cancelled_count = cur.rowcount
            conn.commit()

            LOG.warning(f"â›” CANCEL READY EMAILS: Cancelled {cancelled_count} emails (prevents 8:30am send)")

            # Build response with affected tickers
            affected_tickers = [
                {
                    "ticker": email['ticker'],
                    "company_name": email['company_name'],
                    "recipient_count": len(email['recipients']) if email['recipients'] else 0,
                    "previous_status": email['status']
                }
                for email in affected_emails
            ]

            return {
                "status": "success",
                "cancelled_count": cancelled_count,
                "affected_tickers": affected_tickers,
                "message": f"Cancelled {cancelled_count} ready emails (prevents 8:30am auto-send)"
            }
    except Exception as e:
        LOG.error(f"Cancel ready emails failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cancel-in-progress-runs")
async def cancel_in_progress_runs_api(request: Request):
    """Cancel all in-progress ticker processing jobs (stops current runs)"""
    body = await request.json()
    token = body.get('token')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get all active batches with their jobs
            cur.execute("""
                SELECT
                    b.batch_id,
                    b.created_by,
                    b.created_at
                FROM ticker_processing_batches b
                WHERE EXISTS (
                    SELECT 1 FROM ticker_processing_jobs j
                    WHERE j.batch_id = b.batch_id
                      AND j.status IN ('queued', 'processing')
                )
                ORDER BY b.created_at DESC
            """)
            active_batches = cur.fetchall()

            if not active_batches:
                return {
                    "status": "success",
                    "cancelled_jobs": 0,
                    "cancelled_batches": 0,
                    "message": "No active jobs to cancel"
                }

            # Get detailed job information for response
            all_jobs = []
            for batch in active_batches:
                cur.execute("""
                    SELECT job_id, ticker, status, phase, progress
                    FROM ticker_processing_jobs
                    WHERE batch_id = %s
                      AND status IN ('queued', 'processing')
                """, (batch['batch_id'],))
                jobs = cur.fetchall()
                all_jobs.extend([{
                    "batch_id": str(batch['batch_id']),
                    "batch_created_by": batch['created_by'],
                    "ticker": j['ticker'],
                    "status": j['status'],
                    "phase": j['phase'],
                    "progress": j['progress']
                } for j in jobs])

            # Cancel all active jobs across all batches
            total_cancelled = 0
            affected_tickers = []
            for batch in active_batches:
                cur.execute("""
                    UPDATE ticker_processing_jobs
                    SET status = 'cancelled',
                        error_message = 'Cancelled by admin via UI',
                        last_updated = NOW()
                    WHERE batch_id = %s
                      AND status IN ('queued', 'processing')
                    RETURNING ticker
                """, (batch['batch_id'],))
                cancelled = cur.fetchall()
                total_cancelled += len(cancelled)
                affected_tickers.extend([row['ticker'] for row in cancelled])

            # Also delete orphaned email_queue entries for cancelled tickers
            if affected_tickers:
                cur.execute("""
                    DELETE FROM email_queue
                    WHERE ticker = ANY(%s)
                      AND status IN ('queued', 'processing')
                """, (affected_tickers,))
                deleted_queue = cur.rowcount
                LOG.info(f"ðŸ—‘ï¸ Deleted {deleted_queue} orphaned email_queue entries for cancelled tickers")

            conn.commit()

            LOG.warning(f"ðŸš« CANCEL IN PROGRESS RUNS: Cancelled {total_cancelled} jobs across {len(active_batches)} batches")

            return {
                "status": "success",
                "cancelled_jobs": total_cancelled,
                "cancelled_batches": len(active_batches),
                "affected_jobs": all_jobs,
                "message": f"Cancelled {total_cancelled} in-progress jobs across {len(active_batches)} batches"
            }
    except Exception as e:
        LOG.error(f"Cancel in-progress runs failed: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/cancel-ticker")
async def cancel_ticker_api(request: Request):
    """Cancel individual ticker"""
    body = await request.json()
    token = body.get('token')
    ticker = body.get('ticker')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                UPDATE email_queue
                SET status = 'cancelled', updated_at = NOW()
                WHERE ticker = %s AND status = 'ready'
            """, (ticker,))
            conn.commit()

            LOG.info(f"âŒ Cancelled ticker: {ticker}")
            return {"status": "success", "ticker": ticker, "message": f"Cancelled {ticker}"}
    except Exception as e:
        LOG.error(f"Failed to cancel ticker: {e}")
        return {"status": "error", "message": str(e)}

@APP.post("/api/send-ticker")
async def send_ticker_api(request: Request):
    """Send individual ticker email"""
    body = await request.json()
    token = body.get('token')
    ticker = body.get('ticker')

    if not check_admin_token(token):
        return {"status": "error", "message": "Unauthorized"}

    if not ticker:
        return {"status": "error", "message": "Ticker required"}

    try:
        with db() as conn, conn.cursor() as cur:
            # Get ready email for this ticker
            cur.execute("""
                SELECT ticker, company_name, recipients, email_html, email_subject, article_count
                FROM email_queue
                WHERE ticker = %s AND status = 'ready' AND sent_at IS NULL
            """, (ticker,))

            email = cur.fetchone()

            if not email:
                return {"status": "error", "message": f"No ready email found for {ticker}"}

            recipients = email['recipients']
            admin_email = os.getenv('ADMIN_EMAIL', 'support@weavara.io')

            # Send to each recipient with unique unsubscribe token
            for recipient in recipients:
                # Generate unique unsubscribe token for this recipient
                unsubscribe_token = get_or_create_unsubscribe_token(recipient)

                # Replace placeholder with full unsubscribe URL
                unsubscribe_url = f"https://weavara.io/unsubscribe?token={unsubscribe_token}" if unsubscribe_token else "https://weavara.io/unsubscribe"
                final_html = email['email_html'].replace(
                    '{{UNSUBSCRIBE_TOKEN}}',
                    unsubscribe_url
                )

                # Send email
                success = send_email_with_dry_run(
                    subject=email['email_subject'],
                    html=final_html,
                    to=recipient,
                    bcc=admin_email
                )

                if not success:
                    raise Exception(f"Failed to send to {recipient}")

                LOG.info(f"âœ… Sent {ticker} to {recipient}")

            # Mark as sent
            cur.execute("""
                UPDATE email_queue
                SET status = 'sent', sent_at = NOW()
                WHERE ticker = %s
            """, (ticker,))
            conn.commit()

            LOG.info(f"âœ… {ticker} sent to {len(recipients)} recipients")

            return {
                "status": "success",
                "ticker": ticker,
                "recipients_count": len(recipients),
                "message": f"Sent {ticker} to {len(recipients)} recipients"
            }

    except Exception as e:
        LOG.error(f"Failed to send ticker {ticker}: {e}")
        return {"status": "error", "message": str(e)}

@APP.get("/api/view-email/{ticker}")
def view_email_api(ticker: str, token: str = Query(...)):
    """View Email #3 (Final User Email) preview"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT email_html
                FROM email_queue
                WHERE ticker = %s
            """, (ticker,))
            row = cur.fetchone()

            if not row or not row['email_html']:
                return HTMLResponse("Email not found", status_code=404)

            return HTMLResponse(row['email_html'])
    except Exception as e:
        LOG.error(f"Failed to view email: {e}")
        return HTMLResponse(f"Error: {str(e)}", status_code=500)

@APP.get("/api/view-email-1/{ticker}")
async def view_email_1_api(ticker: str, token: str = Query(...)):
    """View Email #1 (Article Selection QA) snapshot"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT email_1_html
                FROM email_queue
                WHERE ticker = %s
            """, (ticker,))
            row = cur.fetchone()

            if not row or not row['email_1_html']:
                return HTMLResponse(
                    f"<h1>Email #1 not found for {ticker}</h1>"
                    f"<p>Email #1 is generated during processing (Phase 1 - Triage).</p>"
                    f"<p>Run processing for this ticker to generate Email #1.</p>",
                    status_code=404
                )

            return HTMLResponse(row['email_1_html'])
    except Exception as e:
        LOG.error(f"Failed to view Email #1: {e}")
        return HTMLResponse(f"Error: {str(e)}", status_code=500)

@APP.get("/api/view-email-2/{ticker}")
async def view_email_2_api(ticker: str, token: str = Query(...)):
    """View Email #2 (Content QA) snapshot"""
    if not check_admin_token(token):
        return HTMLResponse("Unauthorized", status_code=401)

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT email_2_html
                FROM email_queue
                WHERE ticker = %s
            """, (ticker,))
            row = cur.fetchone()

            if not row or not row['email_2_html']:
                return HTMLResponse(
                    f"<h1>Email #2 not found for {ticker}</h1>"
                    f"<p>Email #2 is generated during processing (Phase 2 - Digest).</p>"
                    f"<p>Run processing for this ticker to generate Email #2.</p>",
                    status_code=404
                )

            return HTMLResponse(row['email_2_html'])
    except Exception as e:
        LOG.error(f"Failed to view Email #2: {e}")
        return HTMLResponse(f"Error: {str(e)}", status_code=500)

# ------------------------------------------------------------------------------
# DAILY WORKFLOW PROCESSING FUNCTIONS
# ------------------------------------------------------------------------------

def load_active_users() -> Dict[str, List[str]]:
    """
    Load active users and their tickers from the normalized schema.
    Returns: {ticker: [list of recipient emails]}

    Uses: users + user_tickers tables (Dec 2025 schema)
    """
    LOG.info("Loading active users...")

    ticker_recipients = {}

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT u.email, ut.ticker
                FROM users u
                JOIN user_tickers ut ON u.id = ut.user_id
                WHERE u.status = 'active'
                ORDER BY u.created_at, ut.ticker
            """)
            rows = cur.fetchall()

            # Count unique users
            unique_emails = set(row['email'] for row in rows)
            LOG.info(f"Found {len(unique_emails)} active users with {len(rows)} ticker subscriptions")

            for row in rows:
                email = row['email']
                ticker = row['ticker'].upper().strip()

                if ticker not in ticker_recipients:
                    ticker_recipients[ticker] = []

                # Deduplicate emails (shouldn't happen with UNIQUE constraint, but safety first)
                if email not in ticker_recipients[ticker]:
                    ticker_recipients[ticker].append(email)

            LOG.info(f"Dedup complete: {len(ticker_recipients)} unique tickers")
            for ticker, emails in ticker_recipients.items():
                LOG.info(f"  {ticker}: {len(emails)} recipients")

            return ticker_recipients

    except Exception as e:
        LOG.error(f"Failed to load users: {e}")
        return {}


def send_admin_notification(results: Dict):
    """Send admin notification after processing completes"""
    LOG.info("Sending admin notification...")

    total = results['total']
    succeeded = results['succeeded']
    failed = results['failed']

    # Get failed tickers
    failed_tickers = []
    for r in results['results']:
        if isinstance(r, dict) and r.get('status') == 'failed':
            failed_tickers.append({
                'ticker': r.get('ticker'),
                'error': r.get('error', 'unknown')
            })

    failed_list_html = ""
    if failed_tickers:
        for ft in failed_tickers:
            failed_list_html += f"<li>{ft['ticker']} - {ft['error']}</li>"
        failed_list_html = f"<ul>{failed_list_html}</ul>"
    else:
        failed_list_html = "<p>None</p>"

    admin_email = os.getenv('ADMIN_EMAIL', 'support@weavara.io')
    admin_token = os.getenv('ADMIN_TOKEN', '')
    dashboard_url = f"https://weavara.io/admin/queue?token={admin_token}"

    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <style>
            body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; }}
            .container {{ max-width: 600px; margin: 0 auto; padding: 20px; }}
            .header {{ background: linear-gradient(135deg, #1e3a8a 0%, #1e40af 100%); color: white; padding: 20px; border-radius: 8px; }}
            .stat {{ display: inline-block; margin: 10px 20px 10px 0; }}
            .stat-value {{ font-size: 32px; font-weight: bold; }}
            .stat-label {{ font-size: 12px; opacity: 0.8; }}
            .success {{ color: #10b981; }}
            .danger {{ color: #ef4444; }}
            .btn {{ display: inline-block; background: #1e40af; color: white; padding: 12px 24px; text-decoration: none; border-radius: 6px; margin-top: 20px; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>Queue Ready - {datetime.now().strftime('%B %d, %Y')}</h1>
                <p>Processing completed at {datetime.now().strftime('%I:%M %p')} ET</p>
            </div>

            <div style="margin: 20px 0;">
                <div class="stat">
                    <div class="stat-value success">âœ… {succeeded}</div>
                    <div class="stat-label">Ready</div>
                </div>
                <div class="stat">
                    <div class="stat-value danger">âŒ {failed}</div>
                    <div class="stat-label">Failed</div>
                </div>
            </div>

            <a href="{dashboard_url}" class="btn">Review Dashboard â†’</a>

            <p style="margin-top: 20px; color: #6b7280; font-size: 14px;">
                Auto-send scheduled: 8:30 AM
            </p>

            <h3>Failed Tickers</h3>
            {failed_list_html}
        </div>
    </body>
    </html>
    """

    subject = f"[ADMIN] Queue Ready - {datetime.now().strftime('%B %d, %Y')}"
    send_email(subject, html, to=admin_email)
    LOG.info(f"Admin notification sent to {admin_email}")


def send_email_with_dry_run(subject: str, html: str, to, bcc=None) -> bool:
    """
    Email sending wrapper with DRY_RUN mode support.
    In DRY_RUN mode, all emails redirect to admin.
    """
    dry_run = os.getenv('DRY_RUN', 'false').lower() == 'true'
    admin_email = os.getenv('ADMIN_EMAIL', 'support@weavara.io')

    if dry_run:
        LOG.warning(f"ðŸ§ª DRY_RUN: Redirecting email to {admin_email}")
        LOG.warning(f"   Original TO: {to}")
        LOG.warning(f"   Original BCC: {bcc}")

        # Override recipients
        actual_to = admin_email
        actual_bcc = None
        subject = f"[DRY RUN] {subject}"
    else:
        actual_to = to
        actual_bcc = bcc

    return send_email(subject, html, to=actual_to, bcc=actual_bcc)


def send_all_ready_emails_impl() -> Dict:
    """
    Send all emails with status='ready' that haven't been sent yet.
    Replaces {{UNSUBSCRIBE_TOKEN}} placeholder with unique token per recipient.
    """
    LOG.info("=== SENDING ALL READY EMAILS ===")

    try:
        with db() as conn, conn.cursor() as cur:
            # Get all ready emails not yet sent
            cur.execute("""
                SELECT ticker, company_name, recipients, email_html, email_subject, article_count
                FROM email_queue
                WHERE status = 'ready'
                AND sent_at IS NULL
                AND is_production = TRUE
                ORDER BY ticker
            """)

            emails = cur.fetchall()

            if not emails:
                LOG.info("No emails to send")
                return {
                    'status': 'success',
                    'sent_count': 0,
                    'message': 'No emails ready to send'
                }

            sent_count = 0
            failed_tickers = []
            admin_email = os.getenv('ADMIN_EMAIL', 'support@weavara.io')

            for email in emails:
                ticker = email['ticker']
                recipients = email['recipients']

                try:
                    # Send to each recipient with unique unsubscribe token
                    for recipient in recipients:
                        # Generate unique unsubscribe token for this recipient
                        token = get_or_create_unsubscribe_token(recipient)

                        # Replace placeholder with full unsubscribe URL
                        unsubscribe_url = f"https://weavara.io/unsubscribe?token={token}" if token else "https://weavara.io/unsubscribe"
                        final_html = email['email_html'].replace(
                            '{{UNSUBSCRIBE_TOKEN}}',
                            unsubscribe_url
                        )

                        # Send email
                        success = send_email_with_dry_run(
                            subject=email['email_subject'],
                            html=final_html,
                            to=recipient,
                            bcc=admin_email
                        )

                        if not success:
                            raise Exception(f"Failed to send to {recipient}")

                        LOG.info(f"âœ… Sent {ticker} to {recipient}")

                    # Mark as sent
                    cur.execute("""
                        UPDATE email_queue
                        SET status = 'sent', sent_at = NOW()
                        WHERE ticker = %s
                    """, (ticker,))
                    conn.commit()

                    sent_count += 1
                    LOG.info(f"âœ… {ticker} sent to {len(recipients)} recipients")

                except Exception as e:
                    failed_tickers.append(ticker)
                    LOG.error(f"âŒ Failed to send {ticker}: {e}")

            LOG.info(f"Send complete: {sent_count} sent, {len(failed_tickers)} failed")

            return {
                'status': 'success',
                'sent_count': sent_count,
                'failed_count': len(failed_tickers),
                'failed_tickers': failed_tickers
            }

    except Exception as e:
        LOG.error(f"Send all ready emails failed: {e}")
        return {
            'status': 'error',
            'message': str(e)
        }


# ------------------------------------------------------------------------------
# SCHEDULER SYSTEM (Nov 26, 2025)
# Timezone-aware scheduling - replaces 7 separate Render cron jobs with 1
# Runs every 30 minutes via: python app.py scheduler
# ------------------------------------------------------------------------------

def get_schedule_config_for_day(day_of_week: int) -> Optional[Dict]:
    """
    Get schedule configuration for a specific day of week.

    Args:
        day_of_week: 0=Monday, 6=Sunday (Python weekday())

    Returns:
        Dict with: report_type, process_time, send_time, cleanup_offset_minutes, filings_offset_minutes
        None if day not found
    """
    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT report_type, process_time, send_time,
                       cleanup_offset_minutes, filings_offset_minutes
                FROM schedule_config
                WHERE day_of_week = %s
            """, (day_of_week,))
            row = cur.fetchone()
            if row:
                return {
                    'report_type': row['report_type'],
                    'process_time': row['process_time'],
                    'send_time': row['send_time'],
                    'cleanup_offset_minutes': row['cleanup_offset_minutes'] or 60,
                    'filings_offset_minutes': row['filings_offset_minutes'] or 60
                }
            return None
    except Exception as e:
        LOG.error(f"Failed to get schedule config for day {day_of_week}: {e}")
        return None


def is_within_time_window(current_time, target_time, window_minutes: int = 15) -> bool:
    """
    Check if current time is within Â±window_minutes of target time.

    Args:
        current_time: datetime.time object (current Toronto time)
        target_time: datetime.time object (target scheduled time)
        window_minutes: Window size in minutes (default Â±15)

    Returns:
        True if current_time is within window of target_time
    """
    if target_time is None:
        return False

    # Convert times to minutes since midnight for easy comparison
    current_minutes = current_time.hour * 60 + current_time.minute
    target_minutes = target_time.hour * 60 + target_time.minute

    # Calculate difference (handle midnight wraparound)
    diff = abs(current_minutes - target_minutes)
    if diff > 720:  # More than 12 hours, must be wraparound
        diff = 1440 - diff  # 1440 = minutes in a day

    return diff <= window_minutes


def run_scheduler():
    """
    Master scheduler - runs every 30 minutes via Render cron.
    Dispatches to report-related functions based on Toronto time + database config.

    SIMPLIFIED (Nov 26, 2025): Only handles 4 report-related functions:
    1. Cleanup - Delete old queue entries
    2. Process - Run/queue reports (daily or weekly)
    3. Send - Send ready emails
    4. Export - Nightly CSV backup

    Memory-intensive jobs run as SEPARATE crons for isolation:
    - Morning filings check (6am daily)
    - Hourly filings check (8:30am-10:30pm)
    - Hourly alerts (9am-11pm)

    Render cron: */30 * * * * python app.py scheduler
    """
    # Get current Toronto time
    eastern = pytz.timezone('America/Toronto')
    toronto_now = datetime.now(eastern)
    current_time = toronto_now.time()
    day_of_week = toronto_now.weekday()  # 0=Monday, 6=Sunday

    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    day_name = day_names[day_of_week]

    LOG.info("="*80)
    LOG.info(f"ðŸ• SCHEDULER RUN: {toronto_now.strftime('%Y-%m-%d %H:%M:%S')} Toronto ({day_name})")
    LOG.info("="*80)

    tasks_run = []

    # =========================================================================
    # REPORT WORKFLOW TASKS (based on schedule_config)
    # =========================================================================

    config = get_schedule_config_for_day(day_of_week)

    if config is None:
        LOG.warning(f"âš ï¸ No schedule config found for {day_name} (day {day_of_week})")
    elif config['report_type'] == 'none':
        LOG.info(f"ðŸ“… {day_name}: No processing scheduled (report_type='none')")
    else:
        report_type = config['report_type']
        process_time = config['process_time']
        send_time = config['send_time']
        cleanup_offset = config['cleanup_offset_minutes']

        LOG.info(f"ðŸ“… {day_name}: {report_type.upper()} report")
        LOG.info(f"   Process: {process_time}, Send: {send_time}")
        LOG.info(f"   Cleanup offset: -{cleanup_offset}min")

        # Calculate cleanup time
        if process_time:
            process_minutes = process_time.hour * 60 + process_time.minute

            # Cleanup time
            cleanup_minutes = process_minutes - cleanup_offset
            if cleanup_minutes < 0:
                cleanup_minutes += 1440  # Wrap to previous day
            cleanup_hour = cleanup_minutes // 60
            cleanup_min = cleanup_minutes % 60
            cleanup_time = datetime.strptime(f"{cleanup_hour:02d}:{cleanup_min:02d}", "%H:%M").time()

            LOG.info(f"   Derived: Cleanup={cleanup_time}")

            # --- CLEANUP ---
            if is_within_time_window(current_time, cleanup_time, window_minutes=15):
                LOG.info(f"âœ… Running CLEANUP (scheduled: {cleanup_time}, current: {current_time})")
                try:
                    cleanup_old_queue_entries()
                    tasks_run.append('cleanup')
                except Exception as e:
                    LOG.error(f"âŒ Cleanup failed: {e}")

            # --- PROCESS (DAILY WORKFLOW) ---
            if is_within_time_window(current_time, process_time, window_minutes=15):
                LOG.info(f"âœ… Running PROCESS ({report_type.upper()}) (scheduled: {process_time}, current: {current_time})")
                try:
                    # Pass report_type from schedule_config to ensure UI settings are respected
                    process_daily_workflow(force_report_type=report_type)
                    tasks_run.append('process')
                except Exception as e:
                    LOG.error(f"âŒ Process failed: {e}")

        # --- SEND EMAILS ---
        if send_time and is_within_time_window(current_time, send_time, window_minutes=15):
            LOG.info(f"âœ… Running SEND EMAILS (scheduled: {send_time}, current: {current_time})")
            try:
                auto_send_cron_job()
                tasks_run.append('send')
            except Exception as e:
                LOG.error(f"âŒ Send emails failed: {e}")

    # =========================================================================
    # NIGHTLY BACKUP (runs at 11:59 PM every day)
    # =========================================================================

    # Check if it's around midnight (11:45 PM - 12:15 AM window)
    backup_time = datetime.strptime("23:59", "%H:%M").time()
    if is_within_time_window(current_time, backup_time, window_minutes=15):
        LOG.info(f"âœ… Running NIGHTLY BACKUP (scheduled: {backup_time}, current: {current_time})")
        try:
            export_users_csv()
            tasks_run.append('backup')
        except Exception as e:
            LOG.error(f"âŒ Nightly backup failed: {e}")

    # =========================================================================
    # SUMMARY
    # =========================================================================

    LOG.info("="*80)
    if tasks_run:
        LOG.info(f"ðŸ“Š SCHEDULER COMPLETE: Ran {len(tasks_run)} task(s): {', '.join(tasks_run)}")
    else:
        LOG.info(f"ðŸ“Š SCHEDULER COMPLETE: No tasks scheduled for {current_time.strftime('%H:%M')} Toronto")
    LOG.info("="*80)

    return {
        'status': 'success',
        'toronto_time': toronto_now.isoformat(),
        'day_of_week': day_name,
        'tasks_run': tasks_run
    }


# ------------------------------------------------------------------------------
# CRON JOB HELPER FUNCTIONS
# ------------------------------------------------------------------------------

def cleanup_old_queue_entries():
    """
    6:00 AM: Delete old email queue entries (safety measure).
    Prevents stale test emails from being sent.
    """
    LOG.info("ðŸ§¹ Cleaning up old queue entries...")

    try:
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                DELETE FROM email_queue
                WHERE created_at < CURRENT_DATE
                OR (is_production = FALSE AND created_at < NOW() - INTERVAL '1 day')
            """)
            deleted = cur.rowcount
            conn.commit()

        LOG.info(f"âœ… Cleanup complete: {deleted} old entries deleted")

    except Exception as e:
        LOG.error(f"âŒ Cleanup failed: {e}")
        raise


def process_daily_workflow(force_report_type: str = None):
    """
    7:00 AM: Process all active beta users via job queue system.
    Generates emails and queues for 8:30 AM send.

    NEW (Nov 20, 2025): Determines report type based on day of week.
    - Monday: Weekly report (7 days lookback)
    - Tuesday-Sunday: Daily report (1 day lookback)

    NEW (Nov 26, 2025): Can be overridden by scheduler via force_report_type.
    When called by scheduler, uses report_type from schedule_config table.

    Args:
        force_report_type: Optional override ('daily' or 'weekly'). If provided,
                          uses this instead of day-of-week detection. Used by
                          scheduler to ensure schedule_config settings are respected.
    """
    LOG.info("="*80)
    LOG.info("=== DAILY WORKFLOW START ===")
    LOG.info("="*80)

    try:
        # CRITICAL: Load CSV from GitHub BEFORE processing (same as test workflow)
        try:
            sync_ticker_references_from_github()
        except Exception as e:
            LOG.error(f"âŒ CSV sync failed: {e} - continuing anyway")

        # Load active users
        ticker_recipients = load_active_users()

        if not ticker_recipients:
            LOG.warning("No active users found")
            return

        # Determine report type - use override if provided, else day-of-week detection
        report_type, lookback_minutes = get_report_type_and_lookback(force_type=force_report_type)
        LOG.info(f"ðŸ“… Report Type: {report_type.upper()}")
        LOG.info(f"â±ï¸  Lookback: {lookback_minutes} minutes ({lookback_minutes/1440:.1f} days)")

        # Submit to existing job queue system (same as admin UI buttons)
        tickers_list = sorted(list(ticker_recipients.keys()))

        with db() as conn, conn.cursor() as cur:
            # Create batch record
            cur.execute("""
                INSERT INTO ticker_processing_batches (total_jobs, created_by, config)
                VALUES (%s, %s, %s)
                RETURNING batch_id
            """, (len(tickers_list), 'cron_job', json.dumps({
                "minutes": lookback_minutes,
                "report_type": report_type,  # NEW: 'daily' or 'weekly'
                "batch_size": 3,
                "triage_batch_size": 3,
                "mode": "daily"  # CRITICAL: Daily workflow mode (for email queue)
            })))

            batch_id = cur.fetchone()['batch_id']

            # Create individual jobs with mode='daily' and recipients
            # Queue timeout (4 hours) - processing timeout (45 min) set when claimed
            timeout_at = datetime.now(timezone.utc) + timedelta(hours=4)
            for ticker in tickers_list:
                cur.execute("""
                    INSERT INTO ticker_processing_jobs (
                        batch_id, ticker, config, timeout_at
                    )
                    VALUES (%s, %s, %s, %s)
                """, (batch_id, ticker, json.dumps({
                    "minutes": lookback_minutes,
                    "report_type": report_type,  # NEW: 'daily' or 'weekly'
                    "batch_size": 3,
                    "triage_batch_size": 3,
                    "mode": "daily",  # CRITICAL: Daily workflow mode
                    "recipients": ticker_recipients[ticker]  # CRITICAL: Recipients for Email #3
                }), timeout_at))

            conn.commit()

        LOG.info(f"âœ… Batch {batch_id} created for {len(tickers_list)} unique tickers")
        LOG.info(f"   Report Type: {report_type.upper()}, Lookback: {lookback_minutes}min")
        LOG.info(f"   Jobs will be processed by background worker")
        LOG.info("âœ… Daily workflow complete (jobs submitted to queue)")

    except Exception as e:
        LOG.error(f"âŒ Daily workflow failed: {e}")
        LOG.error(f"Traceback: {traceback.format_exc()}")
        raise


def auto_send_cron_job():
    """
    8:30 AM: Auto-send all ready emails to users.
    Only runs if admin hasn't manually sent already.
    """
    LOG.info("="*80)
    LOG.info("=== 8:30 AM AUTO-SEND ===")
    LOG.info("="*80)

    try:
        with db() as conn, conn.cursor() as cur:
            # Check if there are any ready emails not yet sent
            cur.execute("""
                SELECT COUNT(*) as count
                FROM email_queue
                WHERE status = 'ready'
                AND sent_at IS NULL
                AND is_production = TRUE
            """)

            count = cur.fetchone()['count']

            if count == 0:
                LOG.info("No emails to send (admin may have sent manually)")
                return

            LOG.info(f"Auto-sending {count} ready emails")

            # Use same function as manual send
            result = send_all_ready_emails_impl()

            # Generate stats report
            # generate_stats_report()  # TODO: Implement stats report

            LOG.info(f"âœ… Auto-send complete: {result['sent_count']} sent")

    except Exception as e:
        LOG.error(f"âŒ Auto-send failed: {e}")
        raise


def export_users_csv():
    """
    11:59 PM: Export users to CSV for backup.
    Creates two files:
    1. data/users/user_tickers.csv - ACTIVE users only with comma-separated tickers
    2. data/users/users_YYYYMMDD.csv - ALL users for legal audit trail

    Uses normalized schema: users + user_tickers tables (Dec 2025)
    Both files are committed to GitHub for legal compliance (CASL/PIPEDA).
    """
    LOG.info("ðŸ“„ Exporting users to CSV...")

    try:
        import csv
        from datetime import datetime

        timestamp = datetime.now().strftime('%Y%m%d')

        # Create users directory if needed
        users_dir = 'data/users'
        os.makedirs(users_dir, exist_ok=True)

        # File paths
        active_users_path = os.path.join(users_dir, 'user_tickers.csv')
        backup_path = os.path.join(users_dir, f'users_{timestamp}.csv')

        # Fetch ALL users with their tickers from normalized schema
        with db() as conn, conn.cursor() as cur:
            # Get all users
            cur.execute("""
                SELECT id, name, email, user_type, ticker_limit, status,
                       created_at, terms_accepted_at, privacy_accepted_at, cancelled_at
                FROM users
                ORDER BY created_at DESC
            """)
            all_users = cur.fetchall()

            # Get all tickers grouped by user
            cur.execute("""
                SELECT user_id, STRING_AGG(ticker, ',' ORDER BY ticker) as tickers
                FROM user_tickers
                GROUP BY user_id
            """)
            ticker_rows = cur.fetchall()
            user_tickers = {row['user_id']: row['tickers'] for row in ticker_rows}

        # Filter ACTIVE users for processing file
        active_users = [u for u in all_users if u['status'] == 'active']

        # File 1: user_tickers.csv (ACTIVE only)
        LOG.info(f"ðŸ“ Writing active users file: {active_users_path}")
        with open(active_users_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['name', 'email', 'tickers', 'user_type'])

            for user in active_users:
                tickers = user_tickers.get(user['id'], '')
                writer.writerow([
                    user['name'],
                    user['email'],
                    tickers,
                    user['user_type']
                ])

        LOG.info(f"âœ… Active users CSV: {len(active_users)} users â†’ {active_users_path}")

        # File 2: users_YYYYMMDD.csv (ALL users, full audit trail)
        LOG.info(f"ðŸ“ Writing backup file: {backup_path}")
        with open(backup_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'name', 'email', 'tickers', 'user_type', 'ticker_limit',
                'status', 'created_at', 'terms_accepted_at', 'privacy_accepted_at', 'cancelled_at'
            ])

            for user in all_users:
                tickers = user_tickers.get(user['id'], '')
                writer.writerow([
                    user['name'],
                    user['email'],
                    tickers,
                    user['user_type'],
                    user['ticker_limit'],
                    user['status'],
                    user['created_at'],
                    user['terms_accepted_at'],
                    user['privacy_accepted_at'],
                    user['cancelled_at']
                ])

        LOG.info(f"âœ… Backup CSV: {len(all_users)} users â†’ {backup_path}")

        # Commit both files to GitHub via API (legal audit trail)
        LOG.info("ðŸ“¤ Committing user CSVs to GitHub via API...")
        try:
            # Read CSV files back from disk (they're small, ~10KB each)
            with open(active_users_path, 'r', encoding='utf-8') as f:
                active_csv_content = f.read()

            with open(backup_path, 'r', encoding='utf-8') as f:
                backup_csv_content = f.read()

            # Commit file 1: user_tickers.csv (active users only)
            LOG.info("ðŸ“¤ Committing file 1/2: user_tickers.csv")
            result1 = commit_csv_to_github(
                csv_content=active_csv_content,
                commit_message=f"Daily user export - {timestamp}\n\nActive users: {len(active_users)}\nFile: user_tickers.csv",
                file_path="data/users/user_tickers.csv"
            )

            if result1['status'] == 'success':
                LOG.info(f"âœ… Committed user_tickers.csv to GitHub (SHA: {result1['commit_sha'][:8]})")
            else:
                LOG.warning(f"âš ï¸ Failed to commit user_tickers.csv: {result1['message']}")

            # Commit file 2: users_YYYYMMDD.csv (all users, legal audit trail)
            LOG.info(f"ðŸ“¤ Committing file 2/2: users_{timestamp}.csv")
            result2 = commit_csv_to_github(
                csv_content=backup_csv_content,
                commit_message=f"Daily user export - {timestamp}\n\nTotal users: {len(all_users)}\nBackup file: users_{timestamp}.csv\n\nLegal audit trail for CASL/PIPEDA compliance.",
                file_path=f"data/users/users_{timestamp}.csv"
            )

            if result2['status'] == 'success':
                LOG.info(f"âœ… Committed users_{timestamp}.csv to GitHub (SHA: {result2['commit_sha'][:8]})")
                LOG.info(f"ðŸ“¦ Legal audit trail complete: {backup_path}")
            else:
                LOG.warning(f"âš ï¸ Failed to commit backup file: {result2['message']}")

            # Summary
            if result1['status'] == 'success' and result2['status'] == 'success':
                LOG.info(f"âœ… Both files committed to GitHub successfully")
            elif result1['status'] == 'success' or result2['status'] == 'success':
                LOG.warning(f"âš ï¸ Partial success: One file committed, one failed")
            else:
                LOG.warning(f"âš ï¸ Both GitHub commits failed")

        except Exception as e:
            LOG.warning(f"âš ï¸ GitHub API operations failed: {e}")
            LOG.warning(f"   Files saved locally: {active_users_path}, {backup_path}")

        return {
            "active_users_path": active_users_path,
            "backup_path": backup_path,
            "active_count": len(active_users),
            "total_count": len(all_users)
        }

    except Exception as e:
        LOG.error(f"âŒ CSV export failed: {e}")
        raise


# ------------------------------------------------------------------------------
# CLI Support for PowerShell Commands
# ------------------------------------------------------------------------------
@APP.post("/cli/run")
def cli_run(request: Request, body: CLIRequest):
    """CLI endpoint for PowerShell commands"""
    require_admin(request)
    
    results = {}
    
    if body.action in ["ingest", "both"]:
        # Initialize feeds using NEW ARCHITECTURE V2
        # NOTE: Schema initialization removed from here (was causing lock timeouts during concurrent processing)
        # Schema is now initialized once at application startup in startup_event() - see line ~16805
        for ticker in body.tickers:
            metadata = ticker_manager.get_or_create_metadata(ticker)
            create_feeds_for_ticker_new_architecture(ticker, metadata)
        
        # Run ingestion
        ingest_result = cron_ingest(request, body.minutes, body.tickers)
        results["ingest"] = ingest_result
    
    if body.action in ["digest", "both"]:
        # Run digest
        digest_result = cron_digest(request, body.minutes, body.tickers)
        results["digest"] = digest_result
    
    return results


# ------------------------------------------------------------------------------
# Hourly Alerts System (Separate Implementation)
# ------------------------------------------------------------------------------

def process_ticker_feeds_hourly(ticker: str) -> Dict[str, int]:
    """
    Process all feeds for a single ticker concurrently (hourly alerts version).

    Uses production feed processing logic with ThreadPoolExecutor.
    Returns aggregated stats for this ticker.
    """
    stats = {
        "processed": 0,
        "inserted": 0,
        "duplicates": 0,
        "blocked_spam": 0
    }

    try:
        LOG.info(f"[{ticker}] ðŸ“° Starting feed processing for hourly alerts")

        # Get feeds for this ticker from database (feeds already created at 7 AM)
        with db() as conn, conn.cursor() as cur:
            cur.execute("""
                SELECT f.id, f.url, f.name, f.search_keyword, f.feed_ticker, tf.category, tf.ticker
                FROM feeds f
                JOIN ticker_feeds tf ON f.id = tf.feed_id
                WHERE tf.ticker = %s AND f.active = TRUE AND tf.active = TRUE
                ORDER BY tf.category, f.id
            """, (ticker,))
            all_feeds = list(cur.fetchall())

        if not all_feeds:
            LOG.warning(f"[{ticker}] âš ï¸ No feeds found in database")
            return stats

        LOG.info(f"[{ticker}] ðŸ“‹ Found {len(all_feeds)} feeds")

        # Group feeds by strategy (same as production)
        company_feeds = []
        industry_feeds = []
        competitor_feeds = []

        for feed in all_feeds:
            category = feed.get('category', 'company')
            if category == 'company':
                company_feeds.append(feed)
            elif category == 'industry':
                industry_feeds.append(feed)
            elif category == 'competitor':
                competitor_feeds.append(feed)

        LOG.info(f"[{ticker}] Feed groups - Company: {len(company_feeds)}, Industry: {len(industry_feeds)}, Competitor: {len(competitor_feeds)}")

        # Group company feeds: Yahoo first, then Google (Yahoo is cleaner, gets priority)
        company_feeds.sort(key=lambda f: 0 if 'yahoo' in f['url'].lower() else 1)

        # Group competitor feeds by feed_ticker: Yahoo first, then Google
        competitor_by_key = {}
        for feed in competitor_feeds:
            comp_ticker = feed.get('feed_ticker', 'unknown')
            if comp_ticker not in competitor_by_key:
                competitor_by_key[comp_ticker] = []
            competitor_by_key[comp_ticker].append(feed)

        # Sort each competitor's feeds: Yahoo first, then Google (Yahoo is cleaner, gets priority)
        for key in competitor_by_key:
            competitor_by_key[key].sort(key=lambda f: 0 if 'yahoo' in f['url'].lower() else 1)

        # Process all feed groups concurrently using ThreadPoolExecutor
        # max_workers=8 (same as production)
        LOG.info(f"[{ticker}] ðŸš€ Starting concurrent feed processing (max_workers=8)")
        processing_start = time.time()

        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = []

            # Submit company feeds (Yahooâ†’Google sequential, with hourly Google title filter)
            if company_feeds:
                future = executor.submit(process_feeds_sequentially, company_feeds, mode='hourly', yahoo_quota=None, google_quota=None, ingestion_source='hourly_alert')
                futures.append(("company", future))
                LOG.info(f"[{ticker}]   â†’ Submitted company feeds: {len(company_feeds)} feeds (Yahooâ†’Google sequential, hourly mode with Google filter)")

            # Submit industry feeds (all parallel, hourly mode, no filter for industry)
            for feed in industry_feeds:
                future = executor.submit(ingest_feed_basic_only, feed, mode='hourly', quota=None, ingestion_source='hourly_alert')
                futures.append(("industry", future))
            if industry_feeds:
                LOG.info(f"[{ticker}]   â†’ Submitted {len(industry_feeds)} industry feeds (all parallel, hourly mode)")

            # Submit competitor feeds (Yahooâ†’Google sequential per competitor, with hourly Google title filter)
            for comp_ticker, comp_feeds in competitor_by_key.items():
                future = executor.submit(process_feeds_sequentially, comp_feeds, mode='hourly', yahoo_quota=None, google_quota=None, ingestion_source='hourly_alert')
                futures.append(("competitor", future))
                LOG.info(f"[{ticker}]   â†’ Submitted competitor {comp_ticker}: {len(comp_feeds)} feeds (Yahooâ†’Google sequential, hourly mode with Google filter)")

            # Collect results as they complete
            completed_count = 0
            for future_type, future in futures:
                try:
                    feed_stats = future.result()
                    stats["processed"] += feed_stats.get("processed", 0)
                    stats["inserted"] += feed_stats.get("inserted", 0)
                    stats["duplicates"] += feed_stats.get("duplicates", 0)
                    stats["blocked_spam"] += feed_stats.get("blocked_spam", 0)

                    completed_count += 1
                except Exception as e:
                    LOG.error(f"[{ticker}] âŒ Feed group {future_type} failed: {e}")

        processing_time = time.time() - processing_start
        LOG.info(f"[{ticker}] âœ… Feed processing complete in {processing_time:.1f}s - Inserted: {stats['inserted']}, Duplicates: {stats['duplicates']}")

        return stats

    except Exception as e:
        LOG.error(f"[{ticker}] âŒ Ticker feed processing failed: {e}")
        LOG.error(traceback.format_exc())
        return stats


def process_hourly_alerts():
    """
    Process hourly stock alerts - ingests articles and sends consolidated email to admin.

    NEW (Oct 2025): Concurrent processing using production architecture.

    This function:
    1. Loads active users via load_active_users() (same as daily workflow)
    2. Deduplicates tickers across users
    3. Processes RSS feeds CONCURRENTLY using ThreadPoolExecutor
    4. Resolves Google News URLs
    5. Queries articles from time window:
       - 9 AM: All articles from midnight (overnight catch-up)
       - Other hours: Articles from previous hour boundary only (incremental)
    6. Sends ONE consolidated email to admin only

    Performance: ~20-40 seconds (was ~10 minutes with sequential processing)

    Schedule: Cron controls timing (0 14-3 * * * UTC = 9 AM - 10 PM EST)
    """
    eastern = pytz.timezone('America/Toronto')
    now_est = datetime.now(timezone.utc).astimezone(eastern)
    current_hour = now_est.hour  # Extract hour for next_alert_time calculation

    LOG.info(f"ðŸ“° Starting hourly alerts processing at {now_est.strftime('%I:%M %p')} EST")

    try:
        # Step 1: Load active users and their tickers using production helper
        # Reuses same function as daily workflow (process_daily_workflow)
        ticker_recipients = load_active_users()

        if not ticker_recipients:
            LOG.info("No active users found for hourly alerts")
            return

        # Extract unique tickers and count users
        unique_tickers = sorted(list(ticker_recipients.keys()))
        unique_emails = set(email for emails in ticker_recipients.values() for email in emails)
        LOG.info(f"Found {len(unique_emails)} active users")
        LOG.info(f"Processing {len(unique_tickers)} unique tickers: {', '.join(unique_tickers)}")

        # Step 2: Process RSS feeds for unique tickers CONCURRENTLY
        # Uses production ThreadPoolExecutor pattern with MAX_CONCURRENT_JOBS
        MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "3"))
        LOG.info(f"ðŸš€ Processing up to {MAX_CONCURRENT_JOBS} tickers concurrently")

        midnight_est = now_est.replace(hour=0, minute=0, second=0, microsecond=0)
        feed_processing_start = time.time()

        # Process tickers concurrently
        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_JOBS) as executor:
            futures = []

            for ticker in unique_tickers:
                future = executor.submit(process_ticker_feeds_hourly, ticker)
                futures.append((ticker, future))

            # Wait for all tickers to complete
            total_stats = {"inserted": 0, "duplicates": 0, "processed": 0}
            for ticker, future in futures:
                try:
                    ticker_stats = future.result()
                    total_stats["inserted"] += ticker_stats.get("inserted", 0)
                    total_stats["duplicates"] += ticker_stats.get("duplicates", 0)
                    total_stats["processed"] += ticker_stats.get("processed", 0)
                except Exception as e:
                    LOG.error(f"âŒ Ticker {ticker} processing failed: {e}")

        feed_processing_time = time.time() - feed_processing_start
        LOG.info(f"âœ… Feed processing complete in {feed_processing_time:.1f}s - Total inserted: {total_stats['inserted']}, duplicates: {total_stats['duplicates']}")

        # Step 3.5: Resolve Google News URLs (Phase 1.5)
        # Reuse production infrastructure to resolve Google News URLs that may have been ingested
        LOG.info("ðŸ”— Phase 1.5: Resolving Google News URLs for hourly alerts...")
        resolution_start = time.time()

        # Convert midnight EST to UTC for database query
        midnight_utc = midnight_est.astimezone(timezone.utc)

        for ticker in unique_tickers:
            try:
                # Get unresolved Google News articles for this ticker from today
                with db() as conn, conn.cursor() as cur:
                    cur.execute("""
                        SELECT DISTINCT a.id
                        FROM articles a
                        JOIN ticker_articles ta ON a.id = ta.article_id
                        WHERE ta.ticker = %s
                        AND a.published_at >= %s
                        AND a.url LIKE '%%news.google.com%%'
                        AND a.resolved_url IS NULL
                    """, (ticker, midnight_utc))

                    article_ids = [row['id'] for row in cur.fetchall()]

                if article_ids:
                    LOG.info(f"[{ticker}] Resolving {len(article_ids)} Google News URLs...")
                    asyncio.run(resolve_flagged_google_news_urls(ticker, article_ids))
                    LOG.info(f"[{ticker}] âœ… Resolution complete")
            except Exception as e:
                LOG.error(f"[{ticker}] âŒ Resolution failed: {e}")
                # Continue processing other tickers

        resolution_time = time.time() - resolution_start
        LOG.info(f"âœ… Phase 1.5 complete in {resolution_time:.1f}s")

        # Calculate time window for incremental hourly queries
        # Recalculate current time after feed processing completes (to catch just-ingested articles)
        end_time_est = datetime.now(timezone.utc).astimezone(eastern)
        end_time_utc = end_time_est.astimezone(timezone.utc)
        end_hour = end_time_est.hour

        # Calculate start time using hour boundaries
        if end_hour == 9:
            # First run of day: catch all overnight articles from midnight
            start_time_est = midnight_est
            operator = ">="  # Include midnight boundary
        else:
            # Subsequent runs: use previous hour boundary
            previous_hour = end_hour - 1
            start_time_est = end_time_est.replace(hour=previous_hour, minute=0, second=0, microsecond=0)
            operator = ">"  # Exclude start boundary (already in previous email)

        start_time_utc = start_time_est.astimezone(timezone.utc)

        LOG.info(f"ðŸ“… Time window: {start_time_est.strftime('%I:%M %p')} to {end_time_est.strftime('%I:%M %p')} EST (operator: {operator})")

        # Step 4: Query ALL articles for all unique tickers (one consolidated admin email)
        try:
            LOG.info(f"Generating consolidated hourly alert for {len(unique_tickers)} tickers: {', '.join(unique_tickers)}")

            # Query articles ingested in the current time window
            with db() as conn, conn.cursor() as cur:
                query = f"""
                    SELECT DISTINCT ON (a.id)
                        a.id, a.title, a.url, a.resolved_url, a.domain, a.published_at,
                        ta.ticker, tf.category, f.search_keyword, f.feed_ticker, tf.value_chain_type
                    FROM articles a
                    JOIN ticker_articles ta ON a.id = ta.article_id
                    JOIN ticker_feeds tf ON ta.feed_id = tf.feed_id AND ta.ticker = tf.ticker
                    JOIN feeds f ON ta.feed_id = f.id
                    WHERE ta.ticker = ANY(%s)
                    AND a.created_at {operator} %s
                    AND a.created_at <= %s
                    AND (ta.is_rejected = FALSE OR ta.is_rejected IS NULL)
                    ORDER BY a.id, a.published_at DESC
                """
                cur.execute(query, (unique_tickers, start_time_utc, end_time_utc))
                articles = cur.fetchall()

            # Sort articles newest to oldest (jumbled across tickers)
            articles = sorted(articles, key=lambda x: x['published_at'], reverse=True)

            if not articles:
                LOG.info("No articles found for any tickers, skipping email")
            else:
                LOG.info(f"Found {len(articles)} total articles across {len(unique_tickers)} tickers")

                # Build article data for template
                article_data = []
                for article in articles:
                    # Category display mapping
                    category_map = {
                        'company': 'Company',
                        'industry': 'Industry',
                        'competitor': 'Competitor'
                    }
                    category_display = category_map.get(article['category'], article['category'].title())

                    # Check if quality domain
                    domain = article.get('domain', '')
                    is_quality = domain.lower() in [
                        'wsj.com', 'bloomberg.com', 'reuters.com', 'ft.com', 'barrons.com',
                        'cnbc.com', 'forbes.com', 'marketwatch.com', 'seekingalpha.com'
                    ]

                    # Check if paywall
                    is_paywall = is_paywall_article(domain)

                    # Format date and time in EST
                    pub_at_est = article['published_at'].astimezone(eastern)
                    date_str = pub_at_est.strftime('%b %d')  # "Oct 14"
                    time_str = pub_at_est.strftime('%I:%M %p').lstrip('0')  # "3:42 PM"

                    # Get domain name
                    domain_name = get_or_create_formal_domain_name(domain) if domain else "Unknown Source"

                    article_data.append({
                        'ticker': article['ticker'],
                        'category': article['category'],  # Raw category for template conditionals
                        'category_display': category_display,
                        'search_keyword': article.get('search_keyword'),  # For industry badge
                        'competitor_ticker': article.get('feed_ticker'),  # For competitor badge
                        'value_chain_type': article.get('value_chain_type'),  # For upstream/downstream badge
                        'title': article['title'],
                        'resolved_url': article.get('resolved_url') or article.get('url', '#'),
                        'domain_name': domain_name,
                        'date_str': date_str,
                        'time_str': time_str,
                        'is_quality': is_quality,
                        'is_paywall': is_paywall
                    })

                # Generate email HTML using template
                hour_str = now_est.strftime('%I:%M %p').lstrip('0')  # "3:00 PM"
                current_time_str = f"{now_est.strftime('%B %d, %Y')} â€¢ {hour_str} EST"
                tickers_str = ', '.join(unique_tickers)

                # Next alert time (next hour, or "Tomorrow 9 AM" if after 10 PM)
                next_alert_time = "Tomorrow 9:00 AM EST" if current_hour >= 22 else f"{(current_hour + 1) % 12 or 12}:00 {'PM' if current_hour >= 11 else 'AM'} EST"

                html = templates.TemplateResponse("email_hourly_alert.html", {
                    "request": {},  # Dummy request for Jinja2
                    "current_time": current_time_str,
                    "tickers_str": tickers_str,
                    "total_articles": len(article_data),
                    "hour_str": hour_str,
                    "articles": article_data,
                    "next_alert_time": next_alert_time,
                    "unsubscribe_url": "#"  # Placeholder - admin-only email
                }).body.decode('utf-8')

                # Send to admin ONLY - same pattern as Email #1 and #2
                subject = f"Hourly Alerts: {tickers_str} ({len(article_data)} articles) - {hour_str} EST"
                send_email(subject=subject, html_body=html, to=ADMIN_EMAIL)
                LOG.info(f"âœ… Hourly alert sent to {ADMIN_EMAIL} ({len(article_data)} articles)")

        except Exception as e:
            LOG.error(f"âŒ Error generating hourly alert: {e}")
            LOG.error(traceback.format_exc())

        LOG.info(f"âœ… Hourly alerts processing complete at {now_est.strftime('%I:%M %p')} EST")

    except Exception as e:
        LOG.error(f"âŒ Hourly alerts processing failed: {e}")
        LOG.error(traceback.format_exc())


@with_deadlock_retry()
def insert_article_minimal(url: str, resolved_url: str, title: str, description: str, published_at, domain: str) -> Optional[int]:
    """
    Minimal article insertion for hourly alerts - no scraping, no AI.
    Reuses existing database structure.

    Returns article_id if inserted, None if duplicate.
    """
    url_hash = get_url_hash(url, resolved_url, domain, title)

    with db() as conn, conn.cursor() as cur:
        try:
            cur.execute("""
                INSERT INTO articles (url, resolved_url, title, description, published_at, domain, url_hash)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (url_hash) DO NOTHING
                RETURNING id
            """, (url, resolved_url, title, description, published_at, domain, url_hash))

            result = cur.fetchone()
            if result:
                return result['id']
            else:
                # Article already exists (duplicate)
                return None

        except Exception as e:
            LOG.debug(f"Article insertion failed: {e}")
            return None


@with_deadlock_retry()
def link_article_to_ticker_minimal(article_id: int, ticker: str, feed_id: int):
    """
    Minimal article-ticker linking for hourly alerts (NORMALIZED - Nov 24, 2025).
    All metadata derived via JOIN to ticker_feeds and feeds tables using feed_id.
    """
    with db() as conn, conn.cursor() as cur:
        try:
            cur.execute("""
                INSERT INTO ticker_articles (article_id, ticker, feed_id)
                VALUES (%s, %s, %s)
                ON CONFLICT (article_id, ticker) DO NOTHING
            """, (article_id, ticker, feed_id))

        except Exception as e:
            LOG.debug(f"Article-ticker linking failed: {e}")


# ------------------------------------------------------------------------------
# Main
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    import sys

    # Check if we're running a cron function
    if len(sys.argv) > 1:
        func_name = sys.argv[1]

        if func_name == "cleanup":
            cleanup_old_queue_entries()
        elif func_name == "process":
            process_daily_workflow()
        elif func_name == "send":
            auto_send_cron_job()
        elif func_name == "export":
            export_users_csv()
        # DISABLED (Nov 30, 2025): CSV is source of truth - never write DB back to ticker_reference.csv
        # elif func_name == "commit":
        #     # Daily GitHub commit (triggers deployment)
        #     result = commit_ticker_reference_to_github()
        #     if result['status'] == 'success':
        #         print(f"âœ… {result['message']}")
        #         print(f"   Rows: {result['rows']}")
        #         print(f"   Commit: {result.get('commit_sha', 'N/A')[:8]}")
        #         print(f"   URL: {result.get('commit_url', 'N/A')}")
        #         sys.exit(0)
        #     else:
        #         print(f"âŒ Error: {result['message']}")
        #         sys.exit(1)
        elif func_name == "alerts":
            # Hourly alerts (9 AM - 10 PM EST)
            process_hourly_alerts()
        elif func_name == "check_filings":
            # Check for new filings (6:30 AM + hourly 8:30 AM - 9:30 PM)
            check_all_filings_cron()
        elif func_name == "scheduler":
            # Master scheduler - runs every 30 min, dispatches to other functions
            # Replaces 7 separate cron jobs with 1 timezone-aware scheduler
            result = run_scheduler()
            print(f"âœ… Scheduler complete: {result['toronto_time']}")
            print(f"   Day: {result['day_of_week']}")
            print(f"   Tasks run: {result['tasks_run'] if result['tasks_run'] else 'None'}")
        else:
            print(f"Unknown function: {func_name}")
            print("Available functions: cleanup, process, send, export, commit, alerts, check_filings, scheduler")
            sys.exit(1)
    else:
        # Normal server startup
        import uvicorn
        port = int(os.getenv("PORT", "10000"))
        uvicorn.run(APP, host="0.0.0.0", port=port)
